<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>fred&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://f7ed.com/"/>
  <updated>2025-01-31T12:29:41.123Z</updated>
  <id>https://f7ed.com/</id>
  
  <author>
    <name>f7ed</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>「Algebraic ECCs」: Lec4 Singleton + Plotkin Bounds and RS Code</title>
    <link href="https://f7ed.com/2025/01/13/stanford-cs250-ecc-lec4/"/>
    <id>https://f7ed.com/2025/01/13/stanford-cs250-ecc-lec4/</id>
    <published>2025-01-12T16:00:00.000Z</published>
    <updated>2025-01-31T12:29:41.123Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>    In this <a href="/categories/Cryptography-ECCs">series</a>, I will be learning <strong>Algebraic Error Correcting Codes</strong>, lectured by Mary Wootters. The lecture videos are available <a href="https://youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr">here</a>. Feedback and sugguestions are always welcome! ^ - ^</div></article><p><b>Topics Covered:</b></p><ul><li>Singleton Bound</li><li>Plotkin Bound</li><li>Reed-Solomon Codes</li><li>Dual View of RS Codes</li><li>Generalized RS Code</li></ul><span id="more"></span><p>In the previous lecture, we explored the GV bound and the Hamming bound in the asymptotic setting where $n,k,d$ gets big. We would like to push the GV bound as much up as possible while at the same time push down the Hamming bound as much as possible.</p><img src="https://s21.ax1x.com/2025/01/13/pEPjyb4.png" alt="image.png" style="zoom:40%;" /><p>Today, we are going to learn two additional bounds, <strong>the Singleton and Plotkin bounds</strong>, which narrow down the yellow region a little bit.</p><p>Additionally, we will learn Reed Solomon codes, which meet the Singleton bound.</p><h1 id="Singleton-Bound"><a href="#Singleton-Bound" class="headerlink" title="Singleton Bound"></a><strong>Singleton Bound</strong></h1> <article class="message is-info"> <div class="message-header">  Singleton Bound:  </div> <div class="message-body">  If $\mathcal{C}$ is an $(n,k,d)_q$ code, then  $$ k\le n-d+1 $$   </div> </article> <p>Before proving the Singleton bound, let’s  examine what it looks like. The Singleton bound states that $k\le n-d+1$ for an $(n,k,d)_q$ code. In the asymptotic setting, this translates to<br>$$<br>R\le 1-\delta<br>$$</p><p>as illustrated below:</p><img src="https://s21.ax1x.com/2025/01/13/pEPj02V.png" alt="image.png" style="zoom:50%;" /><p>The purple line represents the <u>Singleton bound for binary codes</u> ($q=2$). This implies that all points above the Singleton bound are unachievable, which we already know from the Hamming bound. Hence, <u>for $q=2$, the Singleton bound is strictly weaker than the Hamming bound</u>, despite its simplicity. </p><p>However, <u>when $q\to \infty$, the Singleton bound can be better than the Hamming bound</u>. For $q&gt;2$, the Hamming bound shifts up while the Singleton bound stays the same as illustrated below:</p><img src="https://s21.ax1x.com/2025/01/13/pEPjcVJ.png" alt="image.png" style="zoom:30%;" /><p>This shows that the region to the bottom right (below the Hamming bound but above the Singleton bound) becomes unachievable. This is information we cannot infer from the Hamming bound alone.</p><p>Now, let’s prove the Singleton bound.</p><p><b><font color=blue><u><i> Proof of the Singleton Bound: </i></u></font></b></p><ul><li>For a codeword $c=(x_1,\dots, x_n)\in \mathcal{C}$, consider throwing out the last $d-1$ coordinates.      $$    c=(\underbrace{x_1, x_2, \dots, x_{n-d+1}}_{\text{call this }\varphi(c)\in \Sigma^{n-d+1}}, \underbrace{x_{n-d+2}, \dots, x_n}_{\text{get rid of these}})    $$        Define the first $n-d+1$ coordinates as $\varphi(c)\in \Sigma^{n-d+1}$.  </li><li><p>Now, we define <strong>a new code:</strong></p><p>  $$<br>  \tilde{\mathcal{C}}={\varphi(c):c\in \mathcal{C}}<br>  $$</p><p>   which is the set of all $\varphi(c)$ for every codeword $c\in \mathcal{C}$. Thus, $\tilde{\mathcal{C}}\subseteq \Sigma^{n-d+1}$.</p></li><li><p>We derive two key claims:</p><ul><li>Claim 1: $|\mathcal{C}|=|\tilde{\mathcal{C}}|$.<br>If not, then there must be a collision: $\exists c, c’\in \mathcal{C}, c\ne c’$ such that $\varphi(c)=\varphi(c’)$. This implies that $c$ and $c’$ differ only in their last $d-1$ coordinates, meaning  $\Delta(c, c’)\le d-1$, which contradicts the distance $d$ of the original code.</li><li>Claim 2: $|\tilde{\mathcal{C}}|\le q^{n-d+1}$.<br>This follows because $\tilde{\mathcal{C}}\subseteq \Sigma^{n-d+1}$.</li></ul></li><li><p>Combining these two claims, we have: $|\mathcal{C}|=q^k\le q^{n-d+1}$. Taking log base $q$ gives us $k\le n-d+1$.</p></li><li><p>$\blacksquare$</p></li></ul><h1 id="Plotkin-Bound"><a href="#Plotkin-Bound" class="headerlink" title="Plotkin Bound"></a>Plotkin Bound</h1><p>Recall that the GV bound only works up to the relative distance $\delta=d/n\le 1-1/q$. </p><p>Hence, as depicted below, there is a gap between $\delta\in (1-1/q, 1)$. We are wondering <u>if it is possible to have codes with relative distance greater than $1-1/q$ and rate greater than $0$.</u></p><img src="https://s21.ax1x.com/2025/01/13/pEPjw80.png" alt="image.png" style="zoom:70%;" /><p><strong>Unfortunately, the answer is no.</strong> This is what the Plotkin bound tells us:</p><article class="message is-info"> <div class="message-header"> Plotkin Bound: </div> <div class="message-body"> Let $\mathcal{C}$ be a $(n, k, d)_q$ code. <br> 1. If $d=(1-{1/q})\cdot n$, then $|\mathcal{C}|\le 2\cdot q\cdot n$. <br> 2. If $d>(1-1/q)\cdot n$, then $|\mathcal{C}|\le \frac{d}{d-(1-1/q)\cdot n}$. </div> </article><p>This implies that the asymptotic rate $R=\frac{\log_q|\mathcal{C}|}{n}$ approaches to $0$ as $n\to \infty$. </p><p>Thus, <u>in order to have a constant-rate code, we must have $d&lt;(1-1/q)\cdot n$.</u></p><blockquote><p>The proof of the Plotkin bound is omitted in class. For details, refer to “Essential Coding Theory”, Section 4.4.</p></blockquote><p>Instead, we prove a <strong>corollary</strong>, which extends the Plotkin bound and achieves a trade-off when distance $\delta &lt; 1 - 1/q$.</p><article class="message is-info"> <div class="message-header"> Corollary of the Plotkin Bound: </div> <div class="message-body"> Let $\mathcal{C}$ be a family of codes of rate $R$ and relative distance $\delta< 1-1/q$. Then: $$ R\le 1-\left({q\over q-1}\right )\cdot \delta + o(1) $$ where $o(1)$ can be dropped since we are considering a family of codes with $n,k,d\to \infty$. </div> </article><p>Before proving this corollary, let’s look at what the Plotkin bound looks like when $q=2$.</p><img src="https://s21.ax1x.com/2025/01/13/pEPjBvT.png" alt="image.png" style="zoom:50%;" /><p>The <strong>green line</strong> represents the Plotkin bound for binary codes ($q=2$). <u>When $\delta$ is small, the Plotkin bound is a little worse than the Hamming bound. But when $\delta$ gets larger, it is better than the Hamming bound.</u></p><p>When $q&gt;2$, the Plotkin bound is also a straight line ending at $\delta=1-1/q$, which is also  the endpoint of the GV bound.</p><img src="https://s21.ax1x.com/2025/01/13/pEPjga9.png" alt="image.png" style="zoom:47%;" /><p><font color=blue><u><b><i>Proof of Corollary of Plotkin Bound:</i></b></u></font></p><ul><li><p>Assuming the Plotkin Bound holds.</p></li><li><p>Choose $n’=\lfloor{dq \over q-1}\rfloor-1$ such that $d&gt;(1-1/q)\cdot n’$.<br>Notice that $n’&lt;{dq\over q-1}$, so $d&gt;(1-1/q)\cdot n’$. This will be useful when applying the Plotkin bound.</p></li><li><p>For all $x\in \Sigma^{n-n’}$, define <strong>a new code</strong> $\mathcal{C}_x$:       $$    \mathcal{C}_x=\{(\underbrace{c_{n-n'+1}, \dots, c_n}_{n'}): c\in \mathcal{C} \text{ with }(\underbrace{c_1, \dots, c_{n-n'}}_{n-n'})=x\}    $$        which is the set of <strong>ENDs</strong> (the last $n’$ symbols) of codewords that <strong>BEGIN</strong> with $x$. </p></li><li><p>Now $\mathcal{C}_x$ has distance $\ge d$ with block length $n’&lt;{d\over 1-1/q}$.</p><ul><li><p>Why is this true?</p></li><li><p>For any two different codeword $c, c’\in \mathcal{C}$, they must have distance from each other at least $d$. If they have the same first $n-n’$ symbols denoted by $x$, they corresponds to the codewords in $\mathcal{C}_x$.</p>  <img src="https://s21.ax1x.com/2025/01/13/pEPjsrF.png" alt="image.png" style="zoom:40%;" /></li><li><p>Thus, their distance must come from the end part, meaning that $\mathcal{C}_x$ also has the distance $\ge d$.</p></li></ul></li><li><p>Applying the Plotkin bound for $\mathcal{C}_x$ with $d&gt;(1-1/q)\cdot n’$, we have</p><p>  $$<br>  |\mathcal{C}_x|\le {d\over d-(1-1/q)\cdot n’}= {qd\over qd - (q-1)\cdot n’}\le qd<br>  $$</p><p>  The second inequality follows by the fact that the denominator $qd-(q-1)\cdot n’$ is an integer $&gt;0$. Thus, in particular, it is $\ge 1$.</p></li><li><p>We can plug this bound into the original code $\mathcal{C}$ since each codeword in $\mathcal{C}$ shows in only a certain $\mathcal{C}_x$.      $$    \begin{align}    |\mathcal{C}|    =\sum_{x\in \Sigma^{n-n'}}|\mathcal{C}_x|    &\le q^{n-n'} \cdot qd \\    &=q^{(n-\lfloor {qd\over q-1}\rfloor +1)}\cdot qd\\    &=\exp_q(n-{qd\over q-1} + o(n))\\    &=\exp_q(n (1-\delta\cdot ({q\over q-1})+o(1))    \end{align}    $$        </p><p>  where the third equality captures the floor operation and constant by $o(n)$.</p></li><li><p>Taking log base $q$ to the both sides, we have $R\le 1-\delta\cdot ({q\over q-1}+o(1))$ as desired.</p></li><li><p>$\blacksquare$</p></li></ul><h1 id="Discussion-of-Two-Bounds"><a href="#Discussion-of-Two-Bounds" class="headerlink" title="Discussion of Two Bounds"></a>Discussion of Two Bounds</h1><p>Both <u>the Singleton and Plotkin bounds indicate the impossible results.</u> They demonstrate that what trade-off between the distance and rate is impossible while the GV bound shows the possible trade-off.</p><p>As depicted below, the Plotkin bounds seems strictly better than the Singleton bound.</p><p>Why would we bother to discuss the Singleton bound?</p><img src="https://s21.ax1x.com/2025/01/13/pEPjga9.png" alt="image.png" style="zoom:47%;" /><p>On one hand, it is true.</p><p>But one the other hand, we’ll see <u>a family of codes that indeed achieve the Singleton bound.</u></p><p>Before we get there, you might think it impossible given what we’ve just stated that the Singleton bound shows the impossible results. We table the discussion in the next section.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  The trick here is the alphabet size $q$ will be growing with $n$. </div> </article><h1 id="Reed-Solomon-Codes"><a href="#Reed-Solomon-Codes" class="headerlink" title="Reed-Solomon Codes"></a>Reed-Solomon Codes</h1><p>Reed-Solomon code is a family of codes that achieve the Singleton bound while admitting efficient error-correcting algorithm. This code is widely used in practice.</p><p>Before getting into Reed-Solomon codes, let’s first discuss the polynomials over finite fields and the Vandermonde matrix.</p><p><strong><u>Polynomial:</u></strong></p><p>A (univariate) polynomial in variable $X$ over the finite field $\mathbb{F}_q$ of degree $d$ is of the form:</p><p>$$<br>f(X)=a_0+a_1\cdot X + \dots a_d\cdot X^d<br>$$</p><p>where each coefficient $a_i\in \mathbb{F}_q$ and the top coefficient $a_d\ne 0$.</p><p>The set of all univariate polynomials with coefficients in $\mathbb{F}_q$ is denoted by $\mathbb{F}_q[X]$.</p><p>Polynomials over finite fields behave similarly to those over $\mathbb{R}$.</p><p>There is a simple but super useful fact about polynomials that <u>low-degree polynomial do not have too many roots.</u></p><p><font color=blue><u><b>Fact:</b></u></font></p><p>A non-zero polynomial of $f$ of degree $d$ over $\mathbb{F}_q$ has at most $d$ roots.</p><p><font color=blue><u><b><i><em>Proof Sketch:</em></i></b></u></font></p><ul><li>If $f(\beta)=0$, then $(x-\beta)\mid f$.</li><li>If $f$ has $d+1$ (distinct) $\beta_1,\dots, \beta_{d+1}$ roots, then $(x-\beta_1)(x-\beta_2)\dots(x-\beta_{d+1})\mid f$ .</li></ul><p>This leads to a contradiction because the grand product on the left has degree $d+1$, while $f(X)$ has degree of $d$.</p><p><u>Example:</u></p><p>Consider the field  $\mathbb{F}_3=\{0, 1, 2\}$:</p><ul><li>$f(X)=X^2-1$ has two roots $[f(2)=f(1)=0]$</li><li>$f(X)=X^2+2X+1$ has one root $[f(2)=0]$</li><li>$f(X)=X^2+1$ has zero root</li></ul><p>Note: The polynomial $X^2+1$ <strong>DOES</strong> have a root over $\mathbb{F}_2$, showing that the choice of the field matters.</p><h2 id="Vandermonde-Matrix"><a href="#Vandermonde-Matrix" class="headerlink" title="Vandermonde Matrix"></a>Vandermonde Matrix</h2><p>Next, we will explore some useful facts about Vandermonde matrix.</p><p><font color=blue><u><b>Vandermonde matrix:</b></u></font></p>A Vandermonde matrix has the form$$V=\left[\begin{array}{c}1 & \alpha_1 & \alpha_1^2 & \dots & \alpha_1^m \\1 & \alpha_2 & \alpha_2^2 & \dots & \alpha_2^m \\1 & \alpha_3 \\\vdots \\1 & \alpha_n & \alpha_n^2 & \dots & \alpha_n^m \end{array}\right]$$where $\alpha_1, \dots, \alpha_n\in \mathbb{F}_q$ are distinct. Aka, $V_{ij}=\alpha_i^{j-1}$. <article class="message is-info"> <div class="message-header"> Theorem: </div> <div class="message-body"> A square Vandermonde matrix is invertible. </div> </article><p><font color=blue><u><i>Proof 1:</i></u></font></p><ul><li><p>Consider a square Vandermonde matrix $V\in \mathbb{F}^{n\times n}$.</p></li><li><p>Suppose $\vec{a}=(a_0, \dots, a_n)$ is a vector. Then $V\cdot \vec{a}$ can be expressed as      $$    V\cdot \vec{a}=\left (    \begin{array}{c}    \sum_{i=0}^{n-1} a_i\cdot \alpha_1^i \\    \sum_{i=0}^{n-1} a_i\cdot \alpha_2^i \\    \vdots \\    \sum_{i=0}^{n-1} a_i\cdot \alpha_n^i    \end{array}    \right )=\left (    \begin{array}{c}    f(\alpha_1)\\    f(\alpha_2)\\    \vdots \\    f(\alpha_n)    \end{array}    \right )    $$        where $f(X)=a_0+a_1X+\dots a_{n-1}X^{n-1}$.</p></li><li><p>To prove that the Vandermonde matrix is invertible, we’d like to show:<br>if $V\cdot \vec{a}=0$, then $\vec{a}$ itself must be $0$.</p></li><li><p>This is true because</p><ul><li><strong>Case 1</strong>: If $f$ is a zero polynomial (i.e., $\vec{a}$ itself is the zero vector), it is clear that $V\cdot \vec{a}=0$.</li><li><strong>Case 2:</strong> If $f$ is a non-zero polynomial (i.e., $\vec{a}$ is a non-zero vector), then $f(X)$ has degree at most $n-1$ and cannot have $n$ roots. So, we have $V\cdot \vec{a}\ne 0$.</li></ul></li><li><p>Hence, $\text{Ker}(V)=\emptyset$, so $V$ is invertible.</p></li><li><p>$\blacksquare$</p></li></ul><p><font color=blue><u><i>Proof 2:</i></u></font></p><p>It can be proven by its determinate.</p><p>The determinate of a Vandermonde matrix is</p><p>$$<br>\det(V)=\prod_{1\le i&lt;j\le n}(a_i-a_j)\ne 0<br>$$</p><p>since $a_i\ne a_j$ for any $i\ne j$. $\blacksquare$</p><article class="message is-info"> <div class="message-header"> Corollary: </div> <div class="message-body"> Any square <b>contiguous</b> submatrix of a Vandermonde matrix is invertible. </div> </article> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i>  <b>Caveat</b>: If one of the evaluation points is 0, then the submatrix must include part of the all-ones column for it to be invertible. Otherwise, it is not inveritble.  </div> </article> <p><font color=blue><u><i>Proof:</i></u></font></p><p>A $(r+1)\times (r+1)$   square submatrix takes the following form:$$\left[\begin{array}{c}\alpha_i^j & \alpha_i^{j+1} & \dots &\alpha_i^{j+r-1} \\ \alpha_{i+1}^j & \alpha_{i+1}^{j+1} & \dots &\alpha_{i+1}^{j+r-1} \\\vdots \\\alpha_{i+r}^j & \alpha_{i+r}^{j+1} & \dots & \alpha_{i+r}^{j+r}\end{array}\right]=D\cdot V $$where:</p><ul><li>$D$ is a diagonal matrix with <strong>non-zero</strong> diagonal entries $\alpha_i^j, \dots, a_{i+r}^j$,</li><li>$V$ is a $(r+1)\times (r+1)$ Vandermonde matrix.</li></ul><p>The invertibility of $D\cdot V$ depends on the invertibility of both $D$ and $V$:</p><ul><li>$D$ is invertible if all diagonal entries are non-zero. This is guaranteed if either $j=0$ or $a_i\ne 0$ for all $i$, as required in the caveat.</li><li>$V$ is invertible by the aforementioned theorem.</li></ul><p>$\blacksquare$</p><p>These facts about Vandermonde matrices will be useful.</p><p>First, they imply  that “polynomial interpolation works over $\mathbb{F}_q$”.</p><p><font color=blue><u><b>Theorem:</b></u></font></p><p>Given $(\alpha_i, y_i)\in \mathbb{F}_q\times \mathbb{F}_q$ for $i=1,\dots, d+1$, there is a unique degree-$d$ polynomial $f$ so that $f(\alpha_i)=y_i$ for $\forall i$.</p><p><font color=blue><u><i>Proof: </i></u></font></p><p>If $f(X)=a_0+a_1X+\dots+a_dX^d$, then the requirement that $f(\alpha_i)=y_i$ for all $i$ can be written as $V\cdot \vec{a}=\vec{y}$, where $V$ is a square Vandermonde matrix.</p><p>Hence, $\vec{a}=V^{-1}y$ is the unique solution because linear algebra “works” over $\mathbb{F}_q$.</p><p>$\blacksquare$</p><p>Moreover, the proof implies that we can find $f$ efficiently.</p><p>Actually, we can compute $f$ very efficiently by using <strong>NTT</strong> (Number Theoretic Transform), which allows for  fast multiplication by certain special Vandermonde matrix .</p><p><font color=blue><u><b>Fact:</b></u></font></p><p>All functions $f:\mathbb{F}_q\mapsto \mathbb{F}_q$ are polynomials of degree $\le q-1$.</p><p><font color=blue><u><i>Proof: </i></u></font></p><p>There are only $q$ points in $\mathbb{F}_q$, so we can interpolate a (unique) degree $\le q-1$ polynomial through any function.</p><p><u>Example:</u></p><p>$f(X)=X^q$ must have some representation as a degree $\le q-1$ polynomial over $\mathbb{F}_q$.</p><p>It is $X^q\equiv X$ because $\alpha^q=\alpha$ for all $\alpha\in \mathbb{F}_q$ (by <strong>Fermat’s little theorem)</strong>.</p><h2 id="RS-code"><a href="#RS-code" class="headerlink" title="RS code"></a>RS code</h2><p>Now, we are finally ready to define the <strong>Reed-Solomon codes</strong>.</p><article class="message is-info"> <div class="message-header"> Reed-Solomon Codes: </div> <div class="message-body"> Let $q\ge n \ge k$. The Reed-Solomon code of dimension $k$ over $\mathbb{F}_q$, with (distinct) evaluation points $\vec{\alpha}=(\alpha_1, \dots, \alpha_n)$ is  $$ \text{RS}_q(\vec{\alpha}, n, k)=\{(f(\alpha_1), f(\alpha_2), \dots, f(\alpha_n)):f\in \mathbb{F}_q[X], \deg(f)\le k-1\} $$  </div> </article><p>The <strong>basic idea</strong> is that <u>low-degree polynomial does not have too many roots</u> so that the RS code can achieve a fairly good trade-off between the rate and the distance.</p><p>The <strong>codeword</strong> of Reed-Solomon code <u>consists of evaluations of low-degree polynomials.</u></p><p>This implies that <u>these codewords don’t have too many zeros</u> so that the distance of the code is good.</p><p>Additionally, this definition implies a natural <strong>encoding map for RS code</strong>:</p><p>$$<br>\vec{x}=(x_0, \dots, x_{k-1})\mapsto (f_{\vec{x}}(\alpha_0),\dots,f_{\vec{x}}(\alpha_{n}))<br>$$</p><p>where $f_{\vec{x}}(X)=x_0+x_1\cdot X+\dots x_{k-1}\cdot X^{k-1}$.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Note that this isn't the only encoding map, but it’s the commonest one. </div> </article><p>This also implies the Reed-Solomon code is a linear code with Vandermonde matrix as  the generator matrix.</p><p><font color=blue><u><b> Property: </b></u></font></p><p>$\text{RS}_q(\vec{\alpha}, n, k)$ is <strong>a linear code</strong>, and the <strong>generator matrix</strong> is the $n\times k$ Vandermonde matrix with rows corresponding to $\alpha_1, \alpha_2, \dots, \alpha_n$.</p><p><font color=blue><u><i>Proof: </i></u></font></p><p>The proof is clear when we write down the generator matrix.</p><img src="https://s21.ax1x.com/2025/01/13/pEPjrKU.png" alt="image.png" style="zoom:50%;" /><p>In the view of generator matrix, we have $\dim(\text{RS}(n, k))=k$ since the Vandermonde matrix has rank $k$.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  When $\vec{\alpha }$ is clear from context, $\vec{\alpha}$ can be omitted. </div> </article><p>Then we can easily compute distance of a linear code.</p><article class="message is-info"> <div class="message-header"> Property: </div> <div class="message-body"> The distance of $\text{RS}_q(n,k)$ is $d=n-k+1$.  </div> </article><p><font color=blue><u><i>Proof: </i></u></font></p><ul><li>Since RS code is a linear code, $\text{dist}(\text{RS}_q(n, k)=\min_{c\in \text{RS}}\text{wt}(c)$.</li><li>It suffices to show that $\max$ #non-zeros of any non-zero codewords is $k-1$.</li><li>The #zeros of a non-zero codeword corresponds to the #roots of a non-zero polynomial of degree at most $k-1$.</li><li>$\blacksquare$ </li></ul><p>The distance of the RS codes achieves the Singleton bound, and it is optimal for any $n$ and $k$ we choose.</p><p><font color=blue><u><b> Corollary: </b></u></font></p><p>RS codes exactly meet the Singleton bound.</p><article class="message is-info"> <div class="message-header"> Maximum Distance Separable (MDS): </div> <div class="message-body"> A linear $(n, k, d)_q$ code with $d=n-k+1$ (aka, meeting the Singleton bound) is called Maximum Distance Separable. <br> Notice that <b>MDS-ness is equivalent</b> to the property: <u>“every $k\times k$ submatrix of the generator matrix is full rank”.</u> This property implies that if $\mathcal{C}$ is <b>MDS</b>, then <u>any $k$ positions of the codeword $c\in \mathcal{C}$ determine all of $c$.</u>  </div> </article><p><font color=blue><u><i>Proof Sketch: </i></u></font></p><ul><li>To prove the distance is $n-k+1$, it <u>suffices to show the code can correct any $n-k$ erasures.</u></li><li>For a codeword, any $n-k$ erasures leave us $k$ remaining non-erased positions, which <u>corresponds to $k$ rows of the generator matrix, forming a $k\times k$ submatrix.</u></li></ul><img src="https://s21.ax1x.com/2025/01/29/pEVwBAH.png" alt="image.png" style="zoom:97%;" /><ul><li>We can recover the message $x$ if and only if the submatrix is invertible/full rank.</li><li>$\blacksquare$</li></ul><p>We have seen that the RS code is MSD and it has the Vandermonde matrix as the generator matrix, so the property also holds for RS code.</p><h2 id="Discussion-of-Two-Bound-cont"><a href="#Discussion-of-Two-Bound-cont" class="headerlink" title="Discussion of Two Bound (cont.)"></a>Discussion of Two Bound (cont.)</h2><p>We previously showed that the distance-rate trade-off in the Plotkin Bound and the Singleton Bound.</p><img src="https://s21.ax1x.com/2025/01/29/pEVwDNd.png" alt="image.png" style="zoom:30%;" /><p>Both bounds indicate the impossible results, and the Plotkin bound is strictly better than the Singleton bound for any $q\ge 2$.</p><p>This suggests that the <u>Singleton bound should never be achievable</u>. However, the <u>MDS codes are defined as codes that achieves the Singleton bound.</u></p><p><strong>Why don’t MDS codes violate the Singleton bound?</strong></p><p>The key lies in the fact that the <u>above figure only applies to any <strong>fixed</strong> $q$.</u></p><p>However, <strong>in RS codes</strong>, the alphabet size $q$ is <strong>NOT fixed—</strong>it’s growing with $n$.</p><p>Thus, <u>to get a MDS code, $q$ must be growing with $n$.</u></p><p><strong>How big does $q$ have to be?</strong></p><p>It is an <strong>open question</strong> in general!</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  It was settled for prime field in 2012 by Ball. </div> </article><h1 id="Dual-View-of-RS-Codes"><a href="#Dual-View-of-RS-Codes" class="headerlink" title="Dual View of RS Codes"></a>Dual View of RS Codes</h1><p><strong>What is the parity-check matrix of an RS code?</strong></p><p>Before getting this, we need to recall some preliminary algebra.</p><p><strong>Multiplicative Group $\mathbb{F}_q^*$</strong>:</p> The set $\mathbb{F}_q^*$ is the <b>multiplicative group</b> of non-zero elements in $\mathbb{F}_q$, i.e. $\mathbb{F}_q^*=\mathbb{F}_q\backslash\{0\}$. $\mathbb{F}_q^*$  is <b>CYCLIC</b>, which means there’s some $\gamma\in \mathbb{F}_q^*$ so that $$\mathbb{F}_q^*=\{\gamma, \gamma^2, \dots, \gamma^{q-1}\}$$where $\gamma$  is called a <b>Primitive Element</b> of $\mathbb{F}_q$. <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i>  Note that the multiplicative group $\mathbb{F}_q^*$ is equipped with multiplication while the field $\mathbb{F}_q$ is equipped with both multiplication and addition. For example, $\mathbb{F}_5^*=\{1, 2,3,4\}$ and $2+3=0$ is not in the set.  </div> </article> <p><u>Example:</u></p><ul><li>2 is a primitive element of $\mathbb{F}_5$, and  $\mathbb{F}_5^*=\{2, 2^2=4, 2^3=3, 2^4=1\}$.</li><li>4 is NOT a primitive element, since $4^2=1,4^3=-1, 4^4=1, 4^5=-1, \dots$, and we will never generate 2 or 3 as a power of 4.</li></ul><p><font color=blue><u><b>Lemma:</b></u></font></p><p>For any integer $d$ with $0&lt;d&lt;q-1$,  the sum of all $d$-th powers in   $\mathbb{F}_q^*$  is zero,  i.e. </p><p> $$ \sum_{\alpha\in \mathbb{F}_q}\alpha^d =0$$</p><p><font color=blue><u><i>Proof: </i></u></font></p><ul><li>$\sum_{\alpha\in \mathbb{F}_q}\alpha^d=\sum_{\alpha\in \mathbb{F}_q^*}\alpha^d$<ul><li>We are leaving out $\alpha=0$ since $0^d$ contributes 0 to the sum.</li></ul></li><li><p>$=\sum_{j=0}^{q-2}(\gamma^j)^d$ for a primitive element $\gamma$.</p><ul><li>The index start from 0 since $\gamma^0=\gamma^{q-1}=1$.)</li></ul></li><li><p>$=\sum_{j=0}^{q-2}(\gamma^d)^j$ for a primitive element $\gamma$.</p><ul><li>We are switching the order of the exponents.</li></ul></li><li><p>$={1-(\gamma^d)^{q-1}\over 1-\gamma^d}$</p><ul><li><strong>Fact</strong>: For any $x\ne 1$, it follows that $(1-x)\cdot (\sum_{j=0}^{n-1}x^j)=1-x^n$, and so $\sum_{j=0}^{n-1}x^j={1-x^n\over 1-x}$ for any $n$.</li></ul></li><li><p>$={1-1\over 1-\gamma^d}=0$</p><ul><li>$(\gamma^d)^{q-1}\cdot \gamma^d = (\gamma^d)^q=\gamma^d$. (Fermat’s little theorem)</li><li>$\gamma^d\ne0$ since $0&lt;d&lt;q-1$.</li><li>So $(\gamma^d)^{q-1}=1$.</li></ul></li></ul><hr><p>Now, we can <u>view the RS code in a new perspective</u>, which allows us to capture what the parity-check matrix of the RS code looks like.</p> <article class="message is-info"> <div class="message-header">  Proposition:  </div> <div class="message-body">  Let $n=q-1$, and let $\gamma$ be a primitive element of $\mathbb{F}_q$.  $$\text{RS}_q((\gamma^0, \dots, \gamma^{n-1}),n, k)\\ =\{(c_0, c_1, \dots, c_{n-1})\in \mathbb{F}_q^n:c(\gamma^j)=0 \text{ for }j=1, 2, \dots, n-k\}$$ where $c(X)=\sum_{i=0}^{n-1}c_i\cdot X^i$.   </div> </article> <p>This proposition kinds of flipping things around.</p><p>In the <strong>original definition</strong>, the <u>messages are coefficients of some polynomial and the codewords are evaluations of that polynomial.</u></p><p><strong>In this view</strong>, the <u>codewords are coefficients of some polynomial.</u></p><p><font color=blue><u><i>Proof: </i></u></font></p><p>First, let’s prove one direction of this proposition.</p><ul><li>Let $f(X)=\sum_{i=0}^{k-1}f_i\cdot X^i$ be a message, so the RS codeword is $(f(\gamma^0), f(\gamma),\dots, f(\gamma^{n-1}))$.</li><li>$c(\gamma^j)=\sum_{\ell=0}^{n-1}c_{\ell}\cdot \gamma^{j\cdot \ell}$</li><li>$=\sum_{\ell=0}^{n-1}(\sum_{i=0}^{k-1}f_i\cdot \gamma^{i\cdot \ell})\cdot \gamma^{j\cdot \ell}$</li><li>$=\sum_{\ell=0}^{n-1}\sum_{i=0}^{k-1}f_i\cdot \gamma^{(i+j)\cdot \ell}$</li><li>$=\sum_{i=0}^{k-1}\sum_{\ell=0}^{n-1}f_i\cdot \gamma^{(i+j)\cdot \ell}$ (switching the order of summation)</li><li>$=\sum_{i=0}^{k-1}f_i\cdot \sum_{\ell=0}^{n-1}\gamma^{(i+j)\cdot \ell}$</li><li>$=\sum_{i=0}^{k-1}f_i\cdot \sum_{\ell=0}^{n-1}(\gamma^{\ell})^{i+j}$<ul><li>$0\le i\le k-1$ and $1\le j\le n-k$</li><li>Thus, $1\le i+j\le n-1&lt;q-1$. (strictly less than $q-1$)</li><li>Apply the aforementioned lemma to have $\sum_{\ell=0}^{n-1}(\gamma^{\ell})^{i+j}=0$</li></ul></li><li>$=0$.</li><li>$\text{RS}_q((\gamma^0, \dots, \gamma^{n-1}),n, k)\\\subseteq\{(c_0, c_1, \dots, c_{n-1})\in \mathbb{F}_q^n:c(\gamma^j)=0 \text{ for }j=1, 2, \dots, n-k\}$</li></ul><p>This shows one direction of the proposition—<u>RS code is contained in the above set.</u></p><p>To prove the equality, we need to <u>count dimension.</u></p><p>We will <u>show the set also has dimension $k$.</u></p><ul><li><p>Let $H$ be a matrix of this form:      $$    H=\left[    \begin{array}{c}    1 & \gamma & \gamma^2 & \dots & \gamma^{n-1} \\    1 & \gamma^2 & \gamma^4 & \dots & \gamma^{2(n-1)} \\    \vdots \\    1 & \gamma^{n-k} & \gamma^{2(n-k)} & \dots & \gamma^{(n-k)(n-1)} \\    \end{array}    \right]\in \mathbb{F}_q^{(n-k)\times n}    $$    </p></li><li><p>Consider $H\cdot c$ where $c$ is a vector in that set:</p><ul><li>The $j$’th entry is $\sum_{i=0}^{n-1}c_i\cdot \gamma^{ji}=c(\gamma^j)=0$.</li></ul></li><li><p>This implies that the set  $\{(c_0, c_1, \dots, c_{n-1})\in \mathbb{F}_q^n:c(\gamma^j)=0 \text{ for }j=1, 2, \dots, n-k\}$ is exactly $\text{Ker}(H)$.</p></li><li><p>This shows that the set has dimension $k$ since $H$ is a Vandermonde matrix of dimension $n-k$.</p></li></ul><p>The proposition also answer our question about the parity-check matrix of the RS-codes.</p> <article class="message is-info"> <div class="message-header">  Corollary:  </div> <div class="message-body">  The parity-check matrix of $\text{RS}_q((\gamma^0, \dots, \gamma^{n-1}),n,k)$ is $$ H=\left[ \begin{array}{c} 1 & \gamma & \gamma^2 & \dots & \gamma^{n-1} \\ 1 & \gamma^2 & \gamma^4 & \dots & \gamma^{2(n-1)} \\ \vdots \\ 1 & \gamma^{n-k} & \gamma^{2(n-k)} & \dots & \gamma^{(n-k)(n-1)} \\ \end{array} \right]\in \mathbb{F}_q^{(n-k)\times n} $$  </div> </article> <p>Notice that the <strong>dual code</strong> $\text{RS}(n, k)^\perp$ has a <u>generator matrix</u> $H^T$, which again looks a lot like a Vandermonde matrix. </p><p>It suggests that <u>the dual of the RS code is basically an RS code.</u></p><p>This particular derivation of the proposition used the choice of evaluation points heavily. However, a statement like this is true in general.</p><p>More precisely, we define <strong>a generalized RS codes</strong> with an additional parameter vector $\vec\lambda$.</p> <article class="message is-info"> <div class="message-header">  Generalized Reed-Solomon Code (GRS):  </div> <div class="message-body">  A generalized RS code $\text{GRS}_q(\vec{\alpha},n,k;\vec{\lambda})$ is  $\text{GRS}_q(\vec{\alpha},n,k;\vec{\lambda})\\=\{\lambda_0f(\alpha_0), \lambda_1f(\alpha_1),\dots, \lambda_nf(\alpha_n):f\in \mathbb{F}_q[X],\deg(f)\le k\}$  where $\vec{\lambda}=(\lambda_0, \dots, \lambda_n)\in (\mathbb{F}_q^*)^n$.  </div> </article> <p>The GRS code is almost identical to the RS code, except for the scaling factors $\lambda_i$ applied to each coordinate.</p><p>The introduction of $\vec{\lambda}$ enables the following <strong>property</strong> we’ve seen before: <u>if we take the transpose of the parity-check matrix of a GRS code, it yields a generator matrix for another GRS code.</u></p><p><strong>Theorem:</strong></p> <article class="message is-info"> <div class="message-header">  Theorem:  </div> <div class="message-body">  For any distinct evaluation points $\vec{\alpha}$ and any $\vec{\lambda}\in (\mathbb{F}_q^*)^n$, there exists some $\vec{\sigma}\in (\mathbb{F}_q^*)^n$ s.t. $$ \text{GRS}_q(\vec{\alpha},n,k;\vec{\lambda})^\perp=\text{GRS}_q(\vec{\alpha},n,n-k;\vec{\sigma}) $$   </div> </article> ]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
    In this &lt;a href=&quot;/categories/Cryptography-ECCs&quot;&gt;series&lt;/a&gt;, I will be learning &lt;strong&gt;Algebraic Error Correcting Codes&lt;/strong&gt;, lectured by Mary Wootters. The lecture videos are available &lt;a href=&quot;https://youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr&quot;&gt;here&lt;/a&gt;. Feedback and sugguestions are always welcome! ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;b&gt;Topics Covered:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Singleton Bound&lt;/li&gt;
&lt;li&gt;Plotkin Bound&lt;/li&gt;
&lt;li&gt;Reed-Solomon Codes&lt;/li&gt;
&lt;li&gt;Dual View of RS Codes&lt;/li&gt;
&lt;li&gt;Generalized RS Code&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-ECCs" scheme="https://f7ed.com/categories/Cryptography-ECCs/"/>
    
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="ECC" scheme="https://f7ed.com/tags/ECC/"/>
    
      <category term="Singleton Bound" scheme="https://f7ed.com/tags/Singleton-Bound/"/>
    
      <category term="Plotkin Bound" scheme="https://f7ed.com/tags/Plotkin-Bound/"/>
    
      <category term="RS Code" scheme="https://f7ed.com/tags/RS-Code/"/>
    
      <category term="GRS Code" scheme="https://f7ed.com/tags/GRS-Code/"/>
    
  </entry>
  
  <entry>
    <title>「Algebraic ECCs」: Lec3 GV Bound and q-ARY Entropy</title>
    <link href="https://f7ed.com/2024/12/26/stanford-cs250-ecc-lec3/"/>
    <id>https://f7ed.com/2024/12/26/stanford-cs250-ecc-lec3/</id>
    <published>2024-12-25T16:00:00.000Z</published>
    <updated>2024-12-26T06:54:08.680Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>    In this <a href="/categories/Cryptography-ECCs">series</a>, I will be learning <strong>Algebraic Error Correcting Codes</strong>, lectured by Mary Wootters. The lecture videos are available <a href="https://youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr">here</a>. Feedback and sugguestions are always welcome! ^ - ^</div></article><p><b>Topics Covered:</b></p><ul><li>The GV Bound:</li><li>Efficiency and Maximum-Likelihood Decoding</li><li>Application: McEliece Cryptosystem</li><li>Off to Asymptopia<ul><li>Family of Codes</li><li>q-ary Entropy</li><li>Trade-off Between Rate and Distance</li></ul></li></ul><span id="more"></span><h1 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h1><p>In the <a href="/2024/12/16/stanford-cs250-ecc-lec2/" title="last lecture">last lecture</a>, we saw linear codes over a finite field in two linear-algebraic ways. As figured below, we can view a code as the column-span of a generator matrix and as the kernel of the parity-check matrix. There can be many different generator matrices and parity-check matrices for the same code.</p><img src="https://s21.ax1x.com/2024/12/26/pAvnIaD.png" alt="image.png" style="zoom:50%;" /><p>Today, we are going to learn some useful things one can do with linear codes.</p><h1 id="The-GV-Bound"><a href="#The-GV-Bound" class="headerlink" title="The GV Bound"></a>The GV Bound</h1><p>In <a href="/2024/12/16/stanford-cs250-ecc-lec2/" title="Lecture 2">Lecture 2</a>, we explored the <strong>Hamming bound:</strong></p><p>$$<br>R\le 1 - \frac{\log_q\text{Vol}_q(\lfloor \frac{d-1}{2}\rfloor, n)}{n}<br>$$</p><p>which provides <u>an upper bound on the rate of code</u>. This bound highlights an <strong>impossible result</strong>, meaning that we <u>cannot construct a code with a large rate.</u></p><p>In contrast, the <strong>Gilbert-Varshamov (GV) bound</strong> offers a <strong>possible result</strong>. It demonstrates that <u>there exist codes with a decent rate.</u></p> <article class="message is-info"> <div class="message-header">  Gilber-Varshamov (GV) Bound:  </div> <div class="message-body">  For any <b>prime power</b> $q$, and for any $d\le n$, there exists a <b>linear</b> code $\mathcal{C}$ of length $n$, alphabet size $q$, distance $d$, and rate $$ R\ge 1 - \frac{\log_q {(\text{Vol}_q(d-1, n))-1}}{n} $$ Note: you can remove the words “prime power” and “linear” and the statements is till true.   </div> </article> <p>Aside from the fact that the GV bound provides a possibility result, another difference lies in the <u>quantity difference</u> in the $q$-ary volume of the Hamming ball.</p><p>We’ll explore the relationship between these two bounds later, but for now, keep in mind that the rate given by the GV bound must be less than that of the Hamming bound:</p><p>$$<br>R_{\text{GV}} &lt; R_{\text{Hamming}}<br>$$</p><p>otherwise, the math is broken.</p><p>Now, we’ll prove the GV bound — it’s pretty easy! </p><p><font color=blue><u><b><i>Proof of the GV Bound: </i></b></u></font></p><ul><li>The <strong>idea</strong> of the proof is to <u>choose a random linear code</u> $\mathcal{C}$ of specific dimension $k$, and show that <u>it has a distance at least $d$ with non-zero probability</u>—that is:</li></ul><p>$$<br>\text{Pr}[\mathcal{C} \text{ has distance}\ge d]&gt;0<br>$$</p><p>​    This implies that there exists a linear code with a decent rate.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  This approach is known as the <b>Probabilisitic Method</b>. </div> </article><ul><li><p>Let $\mathcal{C}$ be a <strong>random subspace</strong> of $\mathbb{F}_q^n$ with <u>dimension</u> $k = n -\log_q(\text{Vol}_q(d-1, n))-1$.<br>If this code achieves a good distance, the $k/n$ basically <u>corresponds to the rate given in the GV bound.</u><br>This construction works because a linear code is essentially a linear space. Since there are finitely many subspaces of dimension $k$ in $\mathbb{F}_q^n$, we can uniformly sample a random linear subspace.</p></li><li><p>Let $G\in \mathbb{F}_q^{n\times k}$ be a <strong>random generator matrix</strong> for $\mathcal{C}$.<br>As discussed before, each code can have many generator matrices. We can uniformly sample one by selecting a random basis for the subspace and using it as the columns of the generator matrix.</p></li><li><p>Now, since the distance is the minimum weight of all non-zero codewords in the linear code, we have $\text{dist}(\mathcal{C})=\min_{c\in \mathcal{C}\backslash{0}}\text{wt}(c)=\min_{x\in \mathbb{F}_q^k\backslash{0}}\text{wt}(G\cdot x)$.</p></li><li><p><font color=blue><u><b>Useful Fact:</b></u></font><br>For any fixed $x\ne 0$, $G\cdot x$ is <u>uniformly random</u> in $\mathbb{F}_q^n\backslash{0}$. </p></li><li><p>For any given $x\ne 0$, using the useful fact, the <strong>probability that the weight</strong> of $G\cdot x$ is less than $d$ is equal to <u>the probability of a random non-zero codeword lying within the Hamming ball</u> centered at 0 with radius $d-1$. It is basically the volume of the Hamming ball divided by the volume of the whole space as indicated in the following second equality.</p></li></ul><p>$$<br>\begin{aligned}<br>\text{Pr}_G{\text{wt}(G\cdot x)&lt;d} &amp;= \text{Pr}_G{G\cdot x\in B_q(0, d-1)}\<br>&amp;= \frac{\text{Vol}_q(d-1, n)-1}{q^n-1}\<br>&amp;\le \frac{\text{Vol}_q(d-1, n)}{q^n}<br>\end{aligned}<br>$$</p><ul><li>By the union bound, we have</li></ul><p>$$<br>\text{Pr}{\exists x\in \mathbb{F}_q^k:\text{wt}(G\cdot x)&lt;d}\le q^k\cdot \frac{\text{Vol}_q(d-1, n)}{q^n}<br>$$</p><ul><li><p>The <strong>complement of this event</strong> is that $\forall x\in \mathbb{F}_q^k$ , $\text{wt}(G\cdot x)\ge d$, which <u>implies that the distance of the code is at least $d$.</u> </p></li><li><p>Thus, we win as long as this probability is <strong>strictly less than</strong> $1$, which guarantees the existence of a code with a good distance with non-zero probability.</p></li><li><p>Taking logs of both sides, we win if </p></li></ul><p>$$<br>k-n+\log_q(\text{Vol}_q(d-1, n))&lt;0<br>$$</p><ul><li>This is true since we precisely choose $k = n-\log_q(\text{Vol}_q(d-1, n))-1$ before. $\blacksquare$</li></ul><h1 id="Efficiency-amp-Maximum-Likelihood-Decoding"><a href="#Efficiency-amp-Maximum-Likelihood-Decoding" class="headerlink" title="Efficiency &amp; Maximum-Likelihood Decoding"></a>Efficiency &amp; <strong>Maximum-Likelihood Decoding</strong></h1><p>The GV bound tells us there exists good codes with decent rates.</p><p>Next, we are going to discuss the extent to which linear codes admit efficient algorithms.</p><p>We have the following efficient algorithms for linear codes to encode, detect errors and correct erasures:</p><ul><li><p><strong>Efficient Encoding:</strong><br>If $\mathcal{C}$ is linear, we have an efficient encoding map $x\mapsto G\cdot x$.<br>The computational cost is one matrix-vector multiplication.</p></li><li><p><strong>Efficient Error Detection:</strong><br>If $\mathcal{C}$ is linear with distance $d$, we can detect $\le d - 1$ errors efficiently:<br>If $0&lt; \text{wt}(e)\le d-1$ and $c\in \mathcal{C}$, then $H(c+e)=H\cdot e \ne 0$. Thus, we can just simply check if $H\tilde{c}=0$.</p></li><li><p><strong>Efficient Erasure Correction:</strong><br>If $\mathcal{C}$ is linear with distance $d$, we can correct $\le d-1$ erasures efficiently:<br><strong>Erasing bits in the codeword</strong> $c$ corresponds to <u>removing the corresponding rows of the generator matrix</u> $G$.</p>  <img src="https://s21.ax1x.com/2024/12/26/pAvn2x1.png" alt="image.png" style="zoom:50%;" /><p>The remaining $n-(d-1)$ rows form <u>a new linear system</u> $G’\cdot x = c’$. Since we know a code with distance $d$ can handle up to $d-1$ erasures (albeit with a non-efficient algorithm), there <u>must be exactly one $x$ that is consistent with this linear system</u>, and hence $G’$is full rank. The remaining task is to solve this linear system, which can be done with Gaussian elimination.</p>  <img src="https://s21.ax1x.com/2024/12/26/pAvng2R.png" alt="image.png" style="zoom:50%;" /></li></ul><p>The above is leaving out one important thing—<strong>correcting errors.</strong></p><p>We know how to correct errors in the $(7, 4, 3)_2$-Hamming code, but what about in general?</p><p>If $\mathcal{C}$ is linear with distance $d$, can we correct up to $\lfloor \frac{d-1}{2} \rfloor$ errors efficiently?</p><p>The bad news is no.</p><p>Consider the following problem. If we would solve this problem, we can correct up to $\lfloor \frac{d-1}{2} \rfloor$ errors. </p> <article class="message is-info"> <div class="message-header">  Maximum-Likelihood Decoding for Linear Codes:  </div> <div class="message-body">  Given $\tilde{c}\in \mathbb{F}_q^n$, and $G\in \mathbb{F}_q^{n\times k}$, find $x\in \mathbb{F}_q^k$ such that $\Delta(G\cdot x, \tilde{c})$ is minimized. Aka, find the codeword closest to a received word $\tilde{c}$.  </div> </article> <p>This problem (called <strong>Maximum-likelihood decoding for linear codes</strong>) <strong>is NP-hard</strong> in general [Berlekamp-McEliece-Van Tilborg 1978], even if the code is known in advance and you have an arbitrary amount of preprocessing time [Bruck-Noar 1990, Lobstein 1990]. It is even NP-hard to approximate (within a constant factor)! [Arora-Babai-Stern-Sweedyk 1993].</p><p>Even <strong>computing the minimum distance of linear codes is NP-hard</strong> given the generator matrix.</p><p>The take-away here is that <u>we are  unlikely to find a polynomial-time algorithm for this task.</u> This may sounds discouraging, but remember that <strong>NP-hardness is a worst-case condition</strong>. <u>While there exist linear codes that are probably hard to decode, but this does not imply that that all of them are.</u></p><p>Going forward, we will focus on designing codes that admit efficiently-decodable algorithms. Before that, let’s look at a <strong>cryptography application</strong> that leverages this decoding hardness.</p><h1 id="Application-McEliece-Cryptosystem"><a href="#Application-McEliece-Cryptosystem" class="headerlink" title="Application: McEliece Cryptosystem"></a>Application: McEliece Cryptosystem</h1><p>McEliece cryptosystem is a public-key scheme based on the decoding hardness of binary linear codes.</p><p>Suppose that Alice and Bob want to talk securely. Now there is no noise, just an Eavesdropper Eve. </p><img src="https://s21.ax1x.com/2024/12/26/pAvnhqK.png" alt="image.png" style="zoom:40%;" /><p>In public key cryptography, everyone has a public key and a private key. To send a message to Bob, Alice encrypts it using Bob’s public key. Bob decodes it with his private key. We hope this process is secure as long as Bob’s private key stays private.</p><p>The <strong>McEliece Cryptosystem</strong> consists of three main algorithms:</p><ul><li><p><strong>Generate Private and Public Keys</strong></p><ol><li><p>Bob chooses $G\in \mathbb{F}_2^{n\times k}$, the generator matrix for an (appropriate) binaray linear code $\mathcal{C}$ that is efficiently decodable from $t$ errors.</p> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> Not all codes work for McEliece crytosystem; the chosen code at least must be efficiently decodable. In particular, McEliece cryptosystem uses a binary “<b>Goppa Code</b>”. </div> </article> </li><li><p>Bob chooses a random invertible $S\in \mathbb{F}_2^{k\times k}$ and a random permutation matrix $P\in \mathbb{F}_2^{n\times n}$. The permutation matrix $P$ has exactly one 1 in each column such that $Px$ permutes the coordinates of the vector $x$.</p></li><li><p>Bob’s private key is $(S, G, P)$.</p></li><li><p>Bob’s public key consists of $\hat{G}=PGS$ and the parameter $t$.</p></li></ol></li><li><p><strong>Encrypt with Bob’s Public key</strong><br>To send a message $x\in \mathbb{F}_2^k$ to Bob:</p><ol><li>Alice chooses a random vector $e\in \mathbb{F}_2^n$ with $\text{wt}(e)=t$.</li><li>Alice sends $\hat{G}x+e$ to Bob.</li></ol></li><li><p><strong>Decrypt with Bob’s Private Key</strong><br>To decrypt the message $G’x+e$:</p><ol><li><p>Bob computes $P^{-1}(\hat{G}x+e)=GSx + P^{-1} e =G(Sx) +e’$, where $\text{wt}(e’)=t$.</p><p>At this point, we write it as <u>a corrupted codeword</u> $G(Sx)+e’$ with exactly $t$ errors since the permutation matrix $P^{-1}$ only permutes the coordinates of $e$. Bob can use the fact that $G$ is the generator matrix for a code that is efficiently able to correct up to $t$ errors.</p></li><li><p>Bob uses the efficient decoding algorithm to recover $Sx$.</p></li><li><p>Bob can compute $x = S^{-1}\cdot Sx$.</p></li></ol></li></ul><p>Why might this be secure?</p><p>Suppose Eve sees $\hat{G}x+e$ and she knows $G’$ and $t$. Hence, this problem is <u>the same as decoding the code</u> $\hat{C}={\hat{G}x : x\in \mathbb{F}_2^k}$ from $t$ errors. </p><p>The <strong>security</strong> of the McEliece crytosystem relies the following assumptions:</p><ol><li><strong>The public key $\hat{G}$ looks random</strong>: By scrambling $G$ with $S$ and $P$, it is difficult for Eve to distinguish $\hat{G}$ from a random generator matrix.</li><li><strong>Decoding a random linear code is computationally hard:</strong>  While decoding the worst-case code is NP-hard, it is not too much of stretch that decoding a random linear code is also hard on average.</li></ol><p>If these assumptions hold true, decoding the code $\hat{C}={\hat{G}x : x\in \mathbb{F}_2^k}$ from $t$ errors is computationally hard for Eve.</p><p>This assumption that “Decoding $\hat{G}x+e$ is hard” (for an appropriate choice of $G$) is called the McEliece Assumption. Some people believe it and some don’t.</p> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i>  “Decoding $\hat{G}x+e$ is hard for Eve” is <b>NOT</b> the same as “Maximum likelihood decoding of linear codes is NP-hard”. <br>There are two main differences: <br>First, we have some promise that there were $\le  t$ errors in McEliece assumption.  <br>Second, <b>NP-harness is a worst-case assumption</b>. For cryptography, we need an average-case assumption.  </div> </article>  <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>   <b>Worst-case vs. Average-case:</b><br> Worst-case: The problem is considered hard on worst-case if it is difficult to solve for the most difficult case. If a solution is found for the worst-case instance, the problem is solvable for all instances, e.g. $P\ne NP$. <br><br>Average-case: The problem is considered hard if it is difficult to solve for a randomly chosen instance. This assumes that solving the problem is computationally hard for the “average” case rather than just the hardest instance. <br><br>In cryptography, <b>average-case hardness</b> is more relevant because security relies on the assumption that attackers cannot efficiently solve a typical random instance of the underlying problem (e.g., decoding a random linear code or factoring a randomly chosen large integer).  </div> </article> <h1 id="Off-to-Asymptopia"><a href="#Off-to-Asymptopia" class="headerlink" title="Off to Asymptopia"></a>Off to Asymptopia</h1><p>So far, we’ve seen the optimal rate  for a code with distance $d$ and $|\Sigma|=q$ is bounded above by the Hamming bound and bounded below by the GV bound.</p><p>$$<br>1-\frac{1}{n}\cdot \log_q (\text{Vol}_q(d-1, n))\le k/n \le 1 - \frac{1}{n} \cdot \log_q(\text{Vol}_q(\left\lfloor \frac{d-1}{2}\right \rfloor, n)<br>$$</p><p>Recall the <strong>combinational question</strong> we posed in <a href="/2024/12/11/stanford-cs250-ecc-lec1/" title="Lecture 1">Lecture 1</a>:</p><p><font color=red> <b>What is the best trade-off between rate and distance?</b></font></p><p>To address this in the asymptotical setting, we are going to think about the following limiting <strong>parameter regime</strong>: $n, k, d\rightarrow \infty$ so that the rate $k/n$ and the relative distance $\delta$ approaches constants.</p><img src="https://s21.ax1x.com/2024/12/26/pAvncG9.png" alt="image.png" style="zoom:50%;" /><p>The <strong>motivations</strong> for this parameter regime:</p><ol><li>It will allow us to better understand what’s possible and what’s not.</li><li>In many applications, $n, k, d$ are pretty large and $R, \delta$ are the things we want to be thinking about.</li><li>It will let us talk meaningfully about computational complexity.</li></ol><h2 id="Family-of-Codes"><a href="#Family-of-Codes" class="headerlink" title="Family of Codes"></a>Family of Codes</h2><p>Before that, let’s define a family of codes.</p> <article class="message is-info"> <div class="message-header">  A Family of Codes  </div> <div class="message-body">  A family of codes is a collection $\mathcal{C}=\{\mathcal{C}_i\}_{i=1}^{\infty}$, where $\mathcal{C}_i$ is an $(n_i, k_i, d_i)_{q_i}$ code. Given such a family of codes, we can define the rate and the relative distance: The <b>rate</b> of $\mathcal{C}$ is $R(\mathcal{C})=\lim_{i\to \infty} k_i/n_i$. The <b>relative distance</b> of $\mathcal{C}$ is $\delta{(\mathcal{C}})=\lim_{i\to \infty} d_i /n_i$.  </div> </article>  <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i>  We will frequently abuse notation and refer to $\mathcal{C}$ as a “code”, and we’ll drop the subscript $i$ and just think about $n, k, d\to \infty$  </div> </article> <p>Now, let’s look at an example of a family of codes—<strong>Hamming codes.</strong></p>The $i$-th code $\mathcal{C}_i$ is a $(2^i-1, 2^i-i-1, 3)_2$ code so $\mathcal{C}=\{\mathcal{C}_i\}_{i=1}^\infty$ represents a family of codes. $\mathcal{C}_i$ is defined by its parity-check matrix, where the columns corresponds to the binary vector representations of all non-zero elements of $\mathbb{F}_{2^i}$. <img src="https://s21.ax1x.com/2024/12/26/pAvn6PJ.png" alt="image.png" style="zoom:60%;" /><p>The <u>rate</u> of this family is: </p><p>$$<br>\lim_{i\to \infty }\frac{2^i-i-1}{2^i-1}=1<br>$$</p><p>This rate approaches to 1, which is a very good rate.</p><p>However, the <u>relative distance</u> is:</p><p>$$<br>\lim_{i\to \infty}\frac{3}{2^i-1}=0<br>$$</p><p>Hence, in the asymptotic setting, our question is: for any family of a code,</p><p><font color=red><b>What is the best trade-off between $R(\mathcal{C})$ and $\delta{(\mathcal{C})}$?</b></font></p><p>As we see in the family of Hamming codes, we cannot achieve good trade-off between rate and distance. While it has a phenomenal rate, its distance is poor.</p><p>An easier question to ask is:</p><p><font color=red><b>Can we obtain codes with $R(\mathcal{C})&gt;0$ and $\delta{(\mathcal{C})}&gt;0$?</b></font></p><p>We define a family of codes with the rate $R(\mathcal{C})&gt;0$ and relative distance $\delta(\mathcal{C})&gt;0$ (both strictly greater than 0) as <font color=red>asymptotically good</font>.</p><h2 id="q-ary-Entropy"><a href="#q-ary-Entropy" class="headerlink" title="q-ary Entropy"></a>q-ary Entropy</h2><p>Now we have an asymptotic parameter regime, how should we parse the GV and Hamming bounds? In particular, what do these bounds look like in terms of $\delta$?</p><p>$$<br>1-\frac{1}{n}\cdot \log_q (\text{Vol}_q(d-1, n))\le R(\mathcal{C}) \le 1 - \frac{1}{n} \cdot \log_q(\text{Vol}_q(\left\lfloor \frac{d-1}{2}\right \rfloor, n)<br>$$</p><p>We know that $\text{Vol}_q(\lfloor \frac{d-1}{2} \rfloor, n)=\sum_{j=0}^{\lfloor {d-1\over 2}\rfloor}{n\choose j}(q-1)^j$ but this expression is not very helpful for analysis. </p><p>To address this, we use the <strong>$q$-ary entropy function,</strong> which provides a concise way to <u>capture the volume of Hamming balls.</u></p> <article class="message is-info"> <div class="message-header">  q-ary Entropy Function:  </div> <div class="message-body">  The q-ary entropy function $H_q:[0,1]\to [0, 1]$ is defined as:  $$ H_q(x)=x\log_q(q-1)-x\log_q(x)-(1-x)\log_q(1-x) $$ This generalizes the binary entropy function $H_2(x)=-x\log x - (1-x)\log (1-x)$.  </div> </article> <p>Using q-ary entropy function, we can bound the volume of the Hamming ball with the following propositions, allowing us to replace the pesky volume expression with cleaner approximations.</p><p> <font color=blue><u><b>Propositions:</b></u></font>  </p><p>Let $q\ge 2$ be an integer, and let $0\le p \le 1 - {1\over q}$. Then:</p><ul><li>$\text{Vol}_q(pn, n)\le q^{n\cdot H_q(p)}$ </li><li>$\text{Vol}_q(pn, n)\ge q^{n\cdot H_q(p) - o(n)}$  </li></ul><p>Here, the $o(n)$ term is a function $f(n)$ such that $\lim_{n\to \infty}{f(n)\over n}\to 0$. We can consider this term as negligible compared to $n\cdot H_q(p)$.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>   <b>Intuitive Interpretation of q-ary entropy:</b> <br> The binary entropy function $H_2(p)$ is often described in terms of the number of bits needed to describe something. For example, a random string of length $n$, where each bit is 1 with probability $p$, can be described using  $n\cdot H_2(p)$ bits. <br><br>There is a similar interpretation for $q$-ary entropy. Suppose we choose $x\in \mathbb{F}_q^n$ s.t. each $x_i$ is 0 with probability $1-p$ and random in $\mathbb{F}_q^*$ with probability $p$. $$ x_i=\begin{cases}0 &\text{w/ prob. }1-p\\\text{random in }\mathbb{F}_q^* &\text{w/ prob. }p\end{cases} $$ <br>Then, the number of bits needed to describe $x$ is roughly $n\cdot H_2(p)$.  </div> </article> <p>Before proceeding, let’s examine how this q-ary entropy function behaves.</p><img src="https://s21.ax1x.com/2024/12/26/pAvnWKx.png" alt="image.png" style="zoom:50%;" /><ul><li>$H_2(x)$: This function is 0 at $x=0$ and $x=1$, with a maximum value of $1$ at $x={1\over 2}$.</li><li>$H_3(x)$: It resembles $H_2(x)$ but is slightly shoved over to the right. It’s maximum value occurs at $x={2\over 3}$.</li><li>$H_6(x)$: This function is shifted further to the right, with its maximum value occurring at $x=\frac{5}{6}$.</li><li>More generally, $H_q(x)$ has the maximum value of $1$ at $x={q-1\over q}$. As $q$ increases, curve of the function is shoved more and more over to the right.</li></ul><p>Here are some useful properties of $H_q(x)$:</p><ul><li><p>If $p\in [0, 1]$ is constant and $q\to \infty$, then</p>  $$    H_q(p)= \underbrace{p\cdot \log_q(q-1)}_{\text{basically 1}}+\underbrace{O(\log_q(\text{stuff}))}_{\text{really small}}\approx p  $$      <p>  So, eventually the plot looks like a line of $H_q(p)=p$ and a little hicky at the end.</p>  <img src="https://s21.ax1x.com/2024/12/26/pAvnfr6.png" alt="image.png" style="zoom:100%;" /></li><li><p>If $q$ is constant and $p\to 0$, then</p>  $$    \begin{align}    H_q(p)&=\underbrace{p\cdot \log_q(q-1)}_{O(p)}+\underbrace{p\log_q\left({1\over p}\right)}_{\text{This term is the largest}}+    \underbrace{(1-p)\log_q\left({1\over 1-p}\right )}_{\approx p/\ln(q) =O(p)}\\    &\approx p\log_q\left({1\over p}\right)    \end{align}  $$      <p>  So, near the origin, all those curves look like $x\ln(1/x)\over \ln q$.</p></li></ul><article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> The following is my <strong>own analysis</strong> for the last term $(1-p)\cdot \log_q\left ({1\over 1-p}\right)$ using Taylor expansion. </div>  </article><p><font color=blue><u><b><i>Analyze the last term using Taylor expansions: </i></b></u></font></p><ul><li><p>The Taylor expansion of $f(x)$ at $x=a$ is</p><p>  $$<br>  \begin{align}<br>  f(x)&amp;=\sum_{n = 0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n \<br>  &amp;=f(a)+f’(a)\cdot (x-a)+\frac{f’’(a)}{2!}\cdot (x-a)^2+\dots<br>  \end{align}<br>  $$</p></li><li><p>Derivatives of $\ln (x)$ are given as: $\ln’(x)=x^{-1}$, $\ln^{‘’}(x)=(-1)\cdot x^{-2}$, $\ln^{(3)}=(-1)\cdot (-2)\cdot x^{-3}$. By deduction, we have:  $\ln^{(n)}(x)=(-1)^{n-1}\cdot (n-1)!\cdot x^{-n}$.</p></li><li><p>The Taylor expansion of $\ln(x)$ at $x=1$ is:</p><p>  $$<br>  \begin{align}<br>  \ln(x) &amp;= \ln(1)+\sum_{n=1}^{\infty} \frac{\ln^{n}(1)}{n!}\cdot (x-1)^n\&amp;=\sum_{n=1}^{\infty}(-1)^{n-1}\cdot {1\over n}\cdot (x-1)^n\<br>  &amp;=(x-1) -{(x-1)^2\over 2}+{(x-1)^3\over 3}-\dots+{(-1)^{n-1}\over n}\cdot (x-1)^n<br>  \end{align}<br>  $$</p></li><li><p>Applying this expansion to the last term:</p><p>  $$<br>  \begin{align}<br>  \log_q\left ({1\over 1-p}\right)&amp;=-{\ln (1-p) \over \ln q}\<br>  &amp;=-{1\over \ln q}\cdot [(-p)-\frac{(-p)^2}{2}+\dots]\<br>  &amp;\approx {1\over \ln q}\cdot p<br>  \end{align}<br>  $$</p></li><li><p>This shows that $\lim_{p\to 0}(1-p)\cdot \log_q\left ({1\over 1-p}\right)\approx p/\ln (q)=O(p)$</p></li></ul><hr><p>Now, we can use them to <u>simplify our expression for GV and Hamming bounds</u>, both involving the the volume of the q-ary Hamming ball. The strategy is to take log base $q$ of the following approximation in terms of the $\delta$:</p><p>$$<br>\text{Vol}_q(\delta n, n)\approx q^{n\cdot H_q(\delta)}<br>$$</p><p>We can replace the pesky term $\log_q \text{Vol}_q(\delta n,n)$ with $n\cdot H_q(\delta)$.</p> <article class="message is-info"> <div class="message-header">  Hamming Bound:  </div> <div class="message-body">  For any family $\mathcal{C}$ of the q-ary codes, we have $$ R(\mathcal{C})\le 1 - H_q(\delta{\mathcal{(C)}}/2) $$  </div> </article> <p><strong>GV Bound:</strong></p> <article class="message is-info"> <div class="message-header">  GV Bound:  </div> <div class="message-body">  Let $q\ge 2$. For any $0\le \delta\le 1- \frac{1}{q}$, and for any $0< \epsilon \le 1 - H_q(\delta)$, there exists a q-ary family of codes $\mathcal{C}$ with $\delta(\mathcal{C})\ge \delta$ and  $$ R(\mathcal{C})\ge 1 - H_q(\delta)-\epsilon $$   </div> </article> <h2 id="Trade-off-Between-Rate-and-Distance"><a href="#Trade-off-Between-Rate-and-Distance" class="headerlink" title="Trade-off Between Rate and Distance"></a>Trade-off Between Rate and Distance</h2><p>Now, it’s easier to compare these two bounds:</p><p>The following plot the trade-off for $q=2$ in terms of only the rate $R$ and the relative distance $\delta$, without considering $n, k, d$.</p><img src="https://s2.loli.net/2024/12/26/wj7O5mn6hkHJPZQ.png" alt="image.png" style="zoom:50%;" /><ul><li>The <font color=red>red line</font> represents the Hamming bound for binary codes.<br>Notably,  <strong>no point above the Hamming bound is achievable</strong> by any binary codes.</li><li>The <font color=blue>blue line</font> represents the GV bound for binary codes.<br>Notably,  <strong>any point below the GV bound is achievable</strong> by some codes.</li><li>The <font color=yellow>yellow region</font> is an area of <strong>uncertainty</strong>.</li><li>We would like to push the GV bound as much up as possible while at the same time try and push down the Hamming bound as much as possible.</li></ul><p>Note that the GV bound answers our earlier question:</p><p> <strong>There do exist asymptotic good codes!</strong></p><p>But, can we find some explicit ones with efficient algorithms?</p><p>Regarding the yellow uncertain region, there are <strong>several other interesting questions:</strong></p><ul><li><font color=red><b> Are there family of codes that beat the GV bound? </b></font><br>The answer is both yes and no.<ul><li>Answer 1: Yes. For $q=49$, “Algebraic Geometry Codes” beat the GV bound.</li><li>Answer 2: For <strong>binary codes</strong>, we don’t know.<br>This remains an <font color=red><b> OPEN QUESTION! </b></font> The GV bound (which is relatively straighforward to prove) is the best-known possibility of result we have for binary codes.</li></ul></li><li><font color=red><b> Can we find explicit constructions of families of codes that meet the GV bound? </b></font><br>Recall that our <strong>proof for GV bound is non-constructive;</strong> it uses the probabilistic method to show the existence of a random linear codes with decent rates. However, we are looking for  explicit descriptions or efficient algorithms to construct such codes.<ul><li>Answer 1: Yes, for large alphabet. (We’ll see soon)</li><li>Answer 2: For <strong>binary codes</strong>, recent work [Ta-Shma 2017] gives something close in a very particular parameter regime…but in general, it’s still an <font color=red><b> OPEN QUESTION! </b></font></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
    In this &lt;a href=&quot;/categories/Cryptography-ECCs&quot;&gt;series&lt;/a&gt;, I will be learning &lt;strong&gt;Algebraic Error Correcting Codes&lt;/strong&gt;, lectured by Mary Wootters. The lecture videos are available &lt;a href=&quot;https://youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr&quot;&gt;here&lt;/a&gt;. Feedback and sugguestions are always welcome! ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;b&gt;Topics Covered:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The GV Bound:&lt;/li&gt;
&lt;li&gt;Efficiency and Maximum-Likelihood Decoding&lt;/li&gt;
&lt;li&gt;Application: McEliece Cryptosystem&lt;/li&gt;
&lt;li&gt;Off to Asymptopia&lt;ul&gt;
&lt;li&gt;Family of Codes&lt;/li&gt;
&lt;li&gt;q-ary Entropy&lt;/li&gt;
&lt;li&gt;Trade-off Between Rate and Distance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-ECCs" scheme="https://f7ed.com/categories/Cryptography-ECCs/"/>
    
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="ECC" scheme="https://f7ed.com/tags/ECC/"/>
    
      <category term="GV Bound" scheme="https://f7ed.com/tags/GV-Bound/"/>
    
      <category term="q-ary Entropy" scheme="https://f7ed.com/tags/q-ary-Entropy/"/>
    
  </entry>
  
  <entry>
    <title>「Algebraic ECCs」: Lec2 Linear Codes and Finite Fields</title>
    <link href="https://f7ed.com/2024/12/16/stanford-cs250-ecc-lec2/"/>
    <id>https://f7ed.com/2024/12/16/stanford-cs250-ecc-lec2/</id>
    <published>2024-12-15T16:00:00.000Z</published>
    <updated>2024-12-26T06:45:34.169Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>    In this <a href="/categories/Cryptography-ECCs">series</a>, I will be learning <strong>Algebraic Error Correcting Codes</strong>, lectured by Mary Wootters. The lecture videos are available <a href="https://youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr">here</a>. Feedback and sugguestions are always welcome! ^ - ^</div></article><b>Topics Covered:</b><ul><li><p>Linear Algebra over   $\{0, 1\}$ </p><ul><li>Generator Matrices</li><li>Parity-Check Matrices</li></ul></li><li><p>Linear Algebra not Working over  $\{0, 1, 2, 3\}$ </p></li><li><p>Finite Fields and Linear Codes</p></li></ul><span id="more"></span><h1 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h1><p>In last lecture, we are left with the open question:</p><p><strong>What is the best trade-off between rate and distance?</strong></p><p>We called the following example the <strong>Hamming Code</strong> of length 7 and we saw that it was optimal in that it met the <strong>Hamming Bound</strong>.</p><img src="https://s21.ax1x.com/2024/12/13/pAbxUjf.png" alt="image.png" style="zoom:44%;" /><p>We also came up with a decoding algorithm for this code.</p><p>Viewing the code as overlapping circles, we can identify which circles don’t sum to 0 (mod 2) and flip the unique bit that ameliorates the situation.</p><img src="https://s21.ax1x.com/2024/12/13/pAbxNgP.png" alt="image.png" style="zoom:50%;" /><p>However, this circle view seems a bit ad hoc.</p><p>In this Lecture, we are going to <strong>generalize this construction</strong> and <strong>generalize the decoding algorithm</strong> as well as the <strong>distance argument</strong>.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  <b>Who was Hamming?</b> <p>Richard Hamming (1915-1998) was working at Bell labs starting in the late 1940’s, where he was colleagues with Claude Shannan (of the “Shannon model” which we also mentioned). Hamming was working on old-school computers (calculating machines), and they would return an error if even on bit was entered in error. This was extremely frustrating, and inspired Hamming to study this rate-vs-distance question, and to come up with Hamming codes. </div> </article></p><h1 id="Linear-Algebra-over-0-1"><a href="#Linear-Algebra-over-0-1" class="headerlink" title="Linear Algebra over {0, 1}"></a>Linear Algebra over {0, 1}</h1><p>The Hamming code in the previous Example 3 has a really nice form of encoding:</p><img src="https://s21.ax1x.com/2024/12/13/pAbxt3t.png" alt="image.png" style="zoom:50%;" /><h2 id="Generator-Matrices"><a href="#Generator-Matrices" class="headerlink" title="Generator Matrices"></a>Generator Matrices</h2><p>We can see this encoding map as the <u>multiplication by a matrix modulo 2</u>, written as $x\mapsto Gx\pmod 2$, where $G$ is some matrix called the <strong>Generator Matrix</strong>. </p><img src="https://s21.ax1x.com/2024/12/13/pAbxY9I.png" alt="image.png" style="zoom:60%;" /><p>Viewing codes generated by a generator matrix is a very useful view to look at things.</p> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> Suppose for now “linear algebra works mod 2”. </div> </article> <p>For example, the codes in Example 3has a very nice property: <u>it is closed under addition modulo 2</u>, which means that if $c\in \mathcal{C}$, $c’\in \mathcal{C}$, then $c+c’\in \mathcal{C}$.</p><p>Writing it as the multiplication by a matrix makes it very clear:</p><img src="https://s21.ax1x.com/2024/12/13/pAbxdu8.png" alt="image.png" style="zoom:50%;" /><p>Moreover, we can view any <strong>codeword</strong> $c\in \mathcal{C}$ as <u>a linear combination of the column vectors of the generator matrix $G$.</u> </p><p>In other words, the <strong>code</strong> $\mathcal{C}$ is the column span of the generator matrix $G$, aka $\mathcal{C}=\text{span}(\text{cols}(G))$ is <u>a linear subspace of dimension 4.</u></p><h3 id="dist-C-min-wt-C"><a href="#dist-C-min-wt-C" class="headerlink" title="dist (C) = min wt (C)"></a>dist (C) = min wt (C)</h3><p>A key observation is that the minimum Hamming distance of the code is the same as the minimum weight of the non-zero codewords.</p> <article class="message is-info"> <div class="message-header">  Observation:  </div> <div class="message-body">  If $\mathcal{C}$  is linear, then $\text{distance}(\mathcal{C})=\min \text{wt}(C)$.  </div> </article> <p><font color=blue><u><i> Proof: </i></u></font></p><p>The proof is straightforward: </p><p>$$<br>\Delta(Gx, Gx’) = \Delta(G(x-x’), 0)=\text{wt}(G(x-x’))<br>$$</p><p> where the first equality holds follows from linearity and $G(x-x’)$ is itself a codeword by the definition. $\blacksquare$ </p><h2 id="Parity-check-Matrices"><a href="#Parity-check-Matrices" class="headerlink" title="Parity-check Matrices"></a>Parity-check Matrices</h2><p>The other way we looked at Example 3 was placing bits into circles as shown:</p><img src="https://s21.ax1x.com/2024/12/13/pAbxNgP.png" alt="image.png" style="zoom:50%;" /><p>Each <strong>circle constitutes a parity check,</strong> meaning the <u>sum of the circle must equal 0 mod 2</u>. This constrain is indeed the linear relation, which we can express as:</p><img src="https://s21.ax1x.com/2024/12/13/pAbxwDS.png" alt="image.png" style="zoom:45%;" /><p>where <u>each of the three rows corresponds to the linear constrains given by a circle.</u>  For example, the first row corresponds to the green circle, indicating that $c_2+c_3+c_4+c_5=0 \pmod 2$. </p><p>This implies that <u>multiplying any potential codeword by the matrix $H$ should results in 0.</u> </p><p>In other words, for any $c\in \mathcal{C}$, we have $Hc=0\pmod2$, which means that $\mathcal{C}$ is <u>contained in the kernel of the matrix $H$:</u></p><p>$$<br>\mathcal{C}\subseteq \text{Ker}(H)<br>$$</p><p>A raised <strong>question</strong> is: Does $\mathcal{C}=\text{Ker}(H)?$</p><p>The answer is YES!</p> <article class="message is-info"> <div class="message-header">  Lemma:  </div> <div class="message-body">  $\mathcal{C}=\text{Ker}(H)$  </div> </article> <p>We prove it by counting dimension.</p><p><font color=blue><u><i> Proof: </i></u></font></p><ul><li><p>$\text{dim}(\mathcal{C})=\text{dim}(\text{colspan}(G))=4$</p><p>  It’s easy to see $\text{dim}(\text{colspan}(G))=4$ since the identity matrix is just sitting up there.</p>  <img src="https://s2.loli.net/2024/12/16/aBWTGqlvfxDeU8A.png" alt="image.png" style="zoom:60%;" /></li><li><p>$\text{dim}(\text{Ker}(H))=n - \text{rank}(H) = 7 - \text{dim}(\text{rowspan}(H))=4$<br>Again, it’s easy to see $\text{dim}(\text{rowspan}(H))=3$ because of the identity matrix.</p><img src="https://s2.loli.net/2024/12/16/ZhUfqgj587BFndT.png" alt="image.png" style="zoom:60%;" /></li><li><p>Having $\mathcal{C}\subseteq \text{Ker}(H)$ with the same dimension, $\text{dim}(C)=\text{dim}(\text{Ker}(H))$, it follows that  $\mathcal{C}=\text{Ker}(H)$. $\blacksquare$ </p></li></ul><p>Informally, the matrix $H$ here is called a <strong>Parity-Check</strong> matrix of $\mathcal{C}$ so that <u>the code $\mathcal{C}$ is the kernel of this matrix.</u></p><h3 id="Easy-to-Capture-Distance"><a href="#Easy-to-Capture-Distance" class="headerlink" title="Easy to Capture Distance"></a>Easy to Capture Distance</h3><p>One <strong>benefit</strong> of the parity-check matrix is we can <u>read off the distance easily.</u></p><p>As mentioned in the previous lecture, we supposed that the distance of the code in Example 3 is 3. Now, we can provide a proof using parity-check matrices.</p><p><font color=blue><u><b> Claim: </b></u></font> $\text{dist}(\mathcal{C}) = 3$</p><p><font color=blue><u><i> Proof: </i></u></font></p><p>As before, it suffices to show  $\min_{c\in \mathcal{C}\backslash\{0, 1\} } \text{wt}(c) = 3$ that the minimum weight of all non-zero codewords is 3. </p><p>Firstly, we prove $\text{wt}(c)\ge 3$ for $\forall c\in \mathcal{C}$, meaning that <u>all non-zero codewords has weight of at least 3.</u> </p><p><strong>By contradiction</strong>, suppose there is some codeword vector $c\in \mathcal{C}$ with <u>has weight 1 or 2</u> so that $Hc = 0 \pmod 2$.</p><img src="https://s2.loli.net/2024/12/16/tdOJvFxC81go4BP.png" alt="image.png" style="zoom:50%;" /><ul><li><p>$\text{wt}(c)=1$<br>It implies one column of $H$ is 0 mod 2. (contradiction)</p></li><li><p>$\text{wt}(c)=2$</p><p>  It implies the sum of some two columns of $H$ is 0 mod 2, aka there is a repeated column. (contradiction)</p></li></ul><p>Now, the codeword <code>0101010</code> has weight exactly 3, so this bound is tight. Hence, the minimum weight is 3. $\blacksquare$ </p><h3 id="Easy-to-Decode"><a href="#Easy-to-Decode" class="headerlink" title="Easy to Decode"></a>Easy to Decode</h3><p>Furthermore, the parity-check matrix gives us <strong>a slick decoding algorithm.</strong></p><p>Recall the puzzle in Example 3:</p><p><font color=blue><u><b> Puzzle: </b></u></font></p><p>Given $\tilde{c}=0111010$, which bit has suffered one bit flip, and what is the original codeword $c$?</p><p>The goal of the solution is to find a codeword $c\in \mathcal{C}$ such that the Hamming distance $\Delta(c, \tilde{c})\le 1$.</p><p><font color=blue><u><i> Solution: </i></u></font></p><p>We write $\tilde{c}=c+z\pmod 2$ where $z$ is <u>an error vector with weight 1</u>. Next, we consider what the product of $H\cdot \tilde{c}$ actually is?</p><ul><li><p>One the one hand, compute $H\cdot \tilde{c} \mod 2$ :</p>  <img src="https://s2.loli.net/2024/12/16/4i8azDdmu6MLSOj.png" alt="image.png" style="zoom:40%;" /></li><li><p>On the other hand, write the product as $H\cdot (c+z) = Hc + Hz =Hz$ since $Hc=0$ for all $c\in \mathcal{C}$.</p>  <img src="https://s2.loli.net/2024/12/16/FLkWo1qvnANtyTZ.png" alt="image.png" style="zoom:40%;" /><p>  This shows that $z$ is just picking one column of $H$.</p></li></ul><p>From the computation, we see  $H\cdot \tilde{c}$ <u>corresponds to the 3rd column of $H$, indicating the error occurred in position 3.</u></p><p>This leads to an efficient decoding algorithm for $\mathcal{C}$: </p><ol><li>Compute $H\cdot \tilde{c}$ and identify which column of $H$ matches the result</li><li>Determine $z$ recover $\tilde{c}=c + z\mod 2$.</li></ol><p>This view with <u>parity-check matrix</u> gives us a much nicer way of seeing our <u>circle-based algorithm.</u> </p><p>“Which circles fail to sum 1” is the same as “which bits of $H\cdot \tilde{c}$ are 1”, and it picks out which bit we need to flip.</p><hr><p>Now, let’s summarize <strong>the moral of the story</strong> so far:</p><p>The Hamming code of length 7 $\mathcal{C}$, is a subspace over $\{0, 1\}^7$, which means we can view it in <strong>two different liner-algebraic ways:</strong></p><ol><li> $\mathcal{C}=\{G\cdot x: x\in \{0, 1\}^4\}=\text{span(cols}(G))$, as the <u>column-span of the generator matrix.</u></li><li> $\mathcal{C}=\{x\in \{0, 1\}^7:H\cdot x=0\} = \text{Ker}(H)$ , as the <u>kernel of the parity-check matrix.</u></li></ol><p>However, all of these are predicated on the <strong>assumption</strong> that the <u>linear algebra works over $\{0, 1\}$ .</u></p><h1 id="Linear-Algebra-not-Working-over-0-1-2-3"><a href="#Linear-Algebra-not-Working-over-0-1-2-3" class="headerlink" title="Linear Algebra not Working over {0, 1, 2, 3}"></a>Linear Algebra not Working over {0, 1, 2, 3}</h1><p>A natural question arises:</p><p>Does linear algebra “make sense” over $\{0,1\}$? </p><p>To explore this, let’s consider what happens for $\{0, 1,2,3\}$. </p> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> The statements below contains <b>falsehoods.</b> </div> </article> <p><font color=blue><u><b><i>Non-example: </i></b></u></font></p><p>Let  $G = \left[\begin{array}{cc}2 & 0 \\ 0 & 2 \\2 & 2\end{array}\right]$be a generator matrix, mod 4. </p><p>Using this generator matrix, we can define a code $\mathcal{C} = \{G\cdot x: x\in \{0, 1, 2,3\}^2\} = \text{colspan}(G)$, with message length $k = 2$, block length $n = 3$, over the alphabet $\Sigma = \{0, 1, 2,3\}$. </p><p>Thus, $\text{dim}(\mathcal{C})=2$, since the columns of $G$ are <u>not scalar multiples of each other, aka, they are linearly independent.</u></p><p>Now, consider the parity-check matrix  $H = G^{T} = \left[\begin{array}{cc}2 & 0 & 2 \\0 & 2 & 2\\\end{array} \right]$. </p><p>We observe $H\cdot G = 0$, which means $H$ is a legit parity-check matrix for the code $\mathcal{C}$. Specifically, for any $c\in \mathcal{C}$, we have: $H\cdot c = H\cdot G\cdot x = 0 \mod 4$.</p><p>However, when we calculate $\text{dim(Ker}(H))$, we find: $\text{dim}(\text{Ker}(H)) = 3 - \text{rank}(H) = 1$.</p><p>This leads to a <font color=red><b>contradiction</b></font> for the code $\mathcal{C}$ is a liner subspace such that $\mathcal{C} = \text{colspan}(G) = \text{Ker}(H)$.</p><p>Additionally, when we look at the distance of code using the parity-check matrix, we find that $\text{dist}(\mathcal{C})\ge 3$ since no two columns of $H$ are linearly dependent.</p><p>Yet, there exists a codeword  $c = G\cdot \left(\begin{array}{c}1\\1\end{array}\right) = \left(\begin{array}{c}2\\2 \\0\end{array}\right)$, which has weight $2$,  leading to another <font color=red><b>contradiction</b></font>.</p><hr><p>The main reason for contradiction is that the linear algebra does not “work” over  $\{0, 1, 2, 3\} \mod 4$. </p><p>In particular, the assertions that two vectors are linearly independent since they are not scalar multiples of each other are not working over  $\{0, 1, 2, 3\}$. </p><p>The following definitions are both for the <strong>linearly independent</strong>. </p><ul><li>Non-zero vectors $v$ and $w$ are linearly independent iff there is no non-zero $\lambda$ s.t. $v = \lambda \cdot w$.</li><li>Non-zero vectors $v$ and $w$ are linearly independent iff there is no non-zero $\lambda_1, \lambda_2$ s.t. $\lambda_1 \cdot v + \lambda_2 \cdot w = 0$.</li></ul><p>They are the same over the real field $\mathbb{R}$. The proof is straightforward</p><p><font color=blue><u><i> Proof: </i></u></font><br>Suppose $\exists \lambda_1, \lambda_2 \ne 0$ s.t. $\lambda_1 \cdot v + \lambda_2 \cdot w = 0$. Then $v = (\frac{-\lambda_2}{\lambda_1})\cdot w$.<br>Conversely, if $\exists \lambda$ s.t. $v = \lambda w$, then choose $\lambda_2 = \lambda, \lambda_1 = -1$ and $\lambda_1 v + \lambda_2 w = 0$. $\blacksquare$ </p><p>However, they not the same over  $\{0, 1,2, 3\}$. There exists $\lambda_1 =\lambda_2 = 2$ such that </p> $$2\cdot \left(\begin{array}{c}2 \\ 0 \\2\end{array}\right ) + 2\cdot \left(\begin{array}{c}0 \\ 2 \\2\end{array}\right ) = \left(\begin{array}{c}4 \\ 4 \\8\end{array}\right ) = \left(\begin{array}{c}0 \\ 0 \\0\end{array}\right ) \mod 4$$ <p>even though the two columns of $G$ are not scalar multiples of each other.</p><p>If you have the background of the finite field, the reason here is that $\{0, 1, 2, 3\}$ is not a finite field and we cannot divide by 2 mod 4. </p><p>This does not bode well for algebraic coding theory if even linear algebra doesn’t work.</p><h1 id="Finite-Field-and-Linear-Codes"><a href="#Finite-Field-and-Linear-Codes" class="headerlink" title="Finite Field and Linear Codes"></a>Finite Field and Linear Codes</h1> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> The second part of the lecture covers finite fields. I omit it here since we have already covered it in this blog. </div> </article> <p>The key takeaway is all the definitions we know for the linear algebra over $\mathbb{R}$ also make sense over finite fields.</p><p>Here, I list some essential points relevant to this lecture.</p><p>Let $\mathbb{F}$ be a finite field. Then:</p><ul><li> $\mathbb{F}^n = \{(x_1, \dots, x_n): x_i \in \mathbb{F}\}$ </li><li>A <strong>subspace</strong> $V\subseteq \mathbb{F}^n$ is a subset that is closed unde r addition and scalar multiplication. Specifically: $\forall v, w\in V, \forall \lambda\in \mathbb{F}, v+ \lambda w\in V$.</li><li>Vectors $v_1, \dots, v_t \in \mathbb{F}^n$ are <strong>linearly independent</strong> if $\forall \lambda_1, \dots, \lambda_t\in \mathbb{F}$ that are not all 0, $\sum_{i} \lambda_i \cdot v_i \ne 0$.</li><li>For $v_1, \dots, v_t \in \mathbb{F}^n$, their <strong>span</strong> is  defined as $\text{span}(v_1, \dots, v_t)=\{\sum_i \lambda_i v_i : \lambda_i \in \mathbb{F}\}$. </li><li>A <strong>basis</strong> for a subspace $V\subseteq \mathbb{F}$ is a collection of vectors $v_1, \dots, v_t\in V$ s.t.<ul><li>$v_1, \dots, v_t$ are linearly independent</li><li>$V = \text{span}(v_1, \dots, v_t)$</li></ul></li><li>The <strong>dimension</strong> of a subspace $V$ is the number of elements in any basis of $V$.</li></ul><p>Now, we can define a linear code over a finite field.</p> <article class="message is-info"> <div class="message-header">  Definition of Linear Codes:  </div> <div class="message-body">  A Linear Code $\mathcal{C}$ of length $n$ and dimension $k$ over a finite field $\mathbb{F}$ is a $k$-dimension linear subspace of $\mathbb{F}^n$. (The alphabet of $\mathcal{C}$ is $\Sigma = \mathbb{F}$)  </div> </article> <p>This definition implies that a linear code is essentially a linear space.</p><p>This definition aligns with the one we introduced in the previous lecture. </p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>   <b>(Recall) Definition of Code :</b>  A code with distance $d$, message length $k$, block length $n$, and alphabet $\Sigma$ is called a $(n, k, d)_{\Sigma}$ code.  </div> </article> <p>Here, we use overloaded $k$ for both message length and dimension. This is intentional, as it makes sense in this context.</p><p>If $\mathcal{C}$ is a $k$-dimensional subspace over $\mathbb{F}$, then $|\mathcal{C}|=|\mathbb{F}^k|$, hence $k = \log _{|\mathbb{F}|}|\mathcal{C}| = \log_{|\Sigma|}|\mathcal{C}|=$ message length. </p><p>Moreover, if $\mathcal{C}$ is a code of block length $n$, message length $k$, over alphabet $\mathcal{\Sigma = \mathbb{F}}$, then $\mathcal{C}$ is a linear code and also a $k$-dimensional subspace over $\mathbb{F}$.</p> <article class="message is-info"> <div class="message-header">  Definition of Generator Matrix:  </div> <div class="message-body">  Let $\mathcal{C}$ be a linear code of length $n$ and dimension $k$ over a finite field $\mathbb{F}$. A matrix $G\in \mathbb{F}^{n\times k}$ is a generator matrix  for $\mathcal{C}$ if $\mathcal{C} = \text{colspan}(G)=\{G\cdot x: x\in \mathbb{F}^k\}$.   </div> </article> <p><font color=blue><u><b> Observation: </b></u></font></p><p>Any linear code has a generator matrix. </p><p><font color=blue><u><i> Proof: </i></u></font></p><p>Choose the columns of $G$ to be  a basis of $\mathcal{C}$. $\blacksquare$</p><p>Note that the generator matrix for a code is <strong>not unique</strong>. There can be many generator matrices for the same code. They all describe the same code, but they implicitly describe different encoding maps. </p><p>For example, the following $G$ and $G’$ are both generator matrices for the same Hamming code.</p><img src="https://s2.loli.net/2024/12/16/Tqx8G4cNyBaFSIW.png" alt="image.png" style="zoom:50%;" /><p>In particular, we can always permute on rows since it does not change the space of column-span.</p><p>Some generator matrices may be more useful than others. For example, $G$ above corresponds to a <strong>systematic encoding map</strong>. This means that that $\text{Enc}_G(x_1, x_2, x_3, x_4) \mapsto (x_1, x_2, x_3, x_4, \text{stuff})$.</p><p>In particular, <strong>for linear codes</strong>, there is always a systematic encoding map defined by a generator matrix look like the above $G$, having an identity matrix sit up there and some other stuff down there.</p> <article class="message is-info"> <div class="message-header">  Definition of Dual Code:  </div> <div class="message-body">   If $\mathcal{C}\subseteq \mathbb{F}^n$ is a linear code over $\mathbb{F}$, then $\mathcal{C}^\perp=\{v\in \mathbb{F}^n: \langle v, c\rangle =0 \text{ for }\forall c\in \mathcal{C}\}$   </div> </article> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Dual code is the same definition as the dual subspace in linear algebra. </div> </article> <article class="message is-info"> <div class="message-header">  Note:  </div> <div class="message-body">  Let $\mathcal{C}\subseteq \mathbb{F}^n$ be a linear code of length $n$ over $\mathbb{F}$ with $\text{dim}(\mathcal{C})=k$, then $\text{dim}(\mathcal{C}^\perp) = n - k$. (Just like over $\mathbb{R}$).  </div> </article>  <article class="message is-info"> <div class="message-header">  Definition of Parity Check Matrix:  </div> <div class="message-body">  Let $\mathcal{C}$ be a linear code of length $n$ and dimension $k$ over a finite field $\mathbb{F}$. A matrix $H\in \mathbb{F}^{(n-k)\times n}$ so that  $\mathcal{C} = \text{Ker}(H)=\{c\in \mathbb{F}^n: H\cdot c = 0\}$ is  a <b>parity-check matrix</b> for $\mathcal{C}$.  The rows of $H$ (or any vector $v$ s.t. $\langle v, c\rangle =0 \;\forall c\in \mathcal{C}$ ) are called <b>parity checks</b>.  </div> </article> <p><font color=blue><u><b> Observation: </b></u></font></p><p>Any linear code has a parity-check matrix. </p><p><font color=blue><u><i> Proof: </i></u></font></p><p>Choose the rows of $H$ to be  a basis of $\mathcal{C}^\perp$. $\blacksquare$  </p><p>Similarly, the parity-check matrix for a code is <strong>not unique—</strong>there can be many parity-check matrices for the same code.</p><p>With these definitions under our belts, we can move to some facts about linear codes.</p><p><font color=blue><u><b> Facts About Linear Codes: </b></u></font></p><p>If $\mathcal{C}\subseteq \mathbb{F}^n$  is a linear code over $\mathbb{F}$ of dimension $k$ with the generator matrix $G$ and parity-check matrix $H$, then:</p><ul><li>$H\cdot G = 0$</li><li>$\mathcal{C}^\perp$ is a linear code of dimension $n-k$ with generator matrix $H^T$ and parity-check matrix $G^T$.<br>Note that the parity-check matrix $H$ for code $\mathcal{C}$ is taken as the generator matrix for $\mathcal{C}^\perp$.</li><li>The distance of $\mathcal{C}$ is the minimum weight of any nonzero codeword in $\mathcal{C}$: $\text{dist}(\mathcal{C})=\min_{c\in \mathcal{C}\backslash\{0\}}\sum_{i=0}1\{c_i \ne 0\}$. </li><li>The distance of $\mathcal{C}$ is the smallest number $d$ so that $H$ has $d$ linearly dependent columns.<br>Recall how we proved the distance of the code in Example 3: we argue that it suffices to show that no pairs of columns in $H$ are linearly dependent.<br>As shown below, the distance of the code corresponds to the smallest number $d$ so that there exists a codeword $c$ of weight $d$, which is picking up $d$ columns of $H$ that is linearly dependent. <img src="https://s2.loli.net/2024/12/16/k7lP8ozXf4aCTJZ.png" alt="image.png" style="zoom:60%;" /></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
    In this &lt;a href=&quot;/categories/Cryptography-ECCs&quot;&gt;series&lt;/a&gt;, I will be learning &lt;strong&gt;Algebraic Error Correcting Codes&lt;/strong&gt;, lectured by Mary Wootters. The lecture videos are available &lt;a href=&quot;https://youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr&quot;&gt;here&lt;/a&gt;. Feedback and sugguestions are always welcome! ^ - ^
&lt;/div&gt;
&lt;/article&gt;
&lt;b&gt;Topics Covered:&lt;/b&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Linear Algebra over   $&#92;{0, 1&#92;}$ &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generator Matrices&lt;/li&gt;
&lt;li&gt;Parity-Check Matrices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Linear Algebra not Working over  $&#92;{0, 1, 2, 3&#92;}$ &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finite Fields and Linear Codes&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-ECCs" scheme="https://f7ed.com/categories/Cryptography-ECCs/"/>
    
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="ECC" scheme="https://f7ed.com/tags/ECC/"/>
    
      <category term="Linear Code" scheme="https://f7ed.com/tags/Linear-Code/"/>
    
  </entry>
  
  <entry>
    <title>「Algebraic ECCs」: Lec1 Basics of ECCs</title>
    <link href="https://f7ed.com/2024/12/11/stanford-cs250-ecc-lec1/"/>
    <id>https://f7ed.com/2024/12/11/stanford-cs250-ecc-lec1/</id>
    <published>2024-12-10T16:00:00.000Z</published>
    <updated>2024-12-26T06:45:12.835Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>    In this <a href="/categories/Cryptography-ECCs">series</a>, I will be learning <strong>Algebraic Error Correcting Codes</strong>, lectured by Mary Wootters. The lecture videos are available <a href="https://youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr">here</a>. Feedback and sugguestions are always welcome! ^ - ^</div></article><p><b>Topics Covered:</b></p><ul><li>Basic problem in coding theory</li><li>Code and codeword</li><li>Hamming distance and minimum distance</li><li>Rate</li><li><strong>Hamming bound</strong> on trade-off of the rate and distance</li></ul><span id="more"></span><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This course is named “Algebraic Error Correcting Codes”, containing two parts:</p><ol><li>Error Correcting Codes (ECC): ECC are a fundamental tool for communication, storage, complexity theory, algorithm design, cryptography, pseudorandomness and etc.</li><li>Algebraic: algebraic techniques are a fundamental tool for designing ECCs.</li></ol><h1 id="Basic-Problem-in-Coding-Theory"><a href="#Basic-Problem-in-Coding-Theory" class="headerlink" title="Basic Problem in Coding Theory"></a>Basic Problem in Coding Theory</h1><p>Let’s start with the basic problem in coding theory:</p><p>We encode a message $x$ of length $k$ into a codeword $c$ of length $n$, which adds some redundancy when $n&gt;k$.  When we transmit or store this codeword, something bad might happen, i.e. some bits might be corrupted. Thus, we are left with a corrupted codeword $\tilde{c}$.</p><img src="https://s21.ax1x.com/2024/12/12/pAb1sKK.png" alt="image.png" style="zoom:50%;" /><p>The <strong>goal</strong> is to find (something about) the message $x$ given the corrupted codeword $\tilde{c}$.</p><p>This has many applications, including communication and storage:</p><p><u>Application1: Communication</u></p><img src="https://s21.ax1x.com/2024/12/11/pAHvPE9.png" alt="image.png" style="zoom:50%;" /><ol><li>Suppose Alice has a message  $x\in \{0,1\}^{k}$ and wants to sends it to Bob.</li><li>In order to add some redundancy, she encodes the message into a codeword $c$ and sends the codeword through a noisy channel.</li><li>After Alice passes $c$ through the noisy channel, Bob receives $\tilde{c}$ and tries to figure out the original message $x$ that Alice intended to send. </li></ol><p><u>Application2: Storage</u></p><img src="https://s21.ax1x.com/2024/12/11/pAHviNR.png" alt="image.png" style="zoom:50%;" /><ol><li>Suppose $x$ is a file we want to store.</li><li>Instead of storing $x$ directly, we encode it as a codeword $c$, introducing redundancy.</li><li>If $c$ is stored on a CD or in a RAID array, something bad, like a fire, might happen.</li><li>However, the owner still wants to recover the original file $x$.</li></ol><hr><p>Based on the above two applications, we summarize four things we care about in the coding schemes:</p><p><font color=blue><u><b> Things We Care About (Take 1): </b></u></font> </p><ol><li><p>We should be able to handle <strong>SOMETHING BAD</strong>.</p></li><li><p>We should be able to recover <strong>WHAT WE WANT TO KNOW</strong> about $x$.</p></li><li><p>We want to <strong>MINIMIZE OVERHEAD</strong>.</p><p> Jumping ahead, the overhead is defined as the quantity $k/n\in (0, 1]$, which should be as big as possible.</p></li><li><p>We want to do all this <strong>EFFICIENTLY</strong>.</p></li></ol><p>The question about these things are: what is the best <u><strong>trade-off</strong></u> between 1-4?</p><p>The trade-off depends on how we model things:</p><ol><li>What is something bad?</li><li>What exactly do we want to know about $x$?<br>All of $x$ or the first bit of $x$?</li><li>What counts as efficient? </li></ol><p>This lecture will gives one way of answering these questions and there are many legit ways.</p><p>Now, let’s give some formal definitions for a code.</p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p>Let $\Sigma$ be any finite set and let $n&gt;0$ be a positive integer. </p> <article class="message is-info"> <div class="message-header">  Code:   </div> <div class="message-body">   A <b>code</b> $\mathcal{C}$ of <u>block length</u> $n$ over an <u>alphabet</u> $\Sigma$ is a subset $\mathcal{C}\subseteq\Sigma^n$.  An element $c\in \mathcal{C}$ is called a <b>codeword</b>.   </div> </article> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Sometimes, block length is referred to simply as length. </div> </article><p>So far, this is not a very interesting definition. A descriptive example for this definition is provided below.</p><p><u><b>Example 1:</b></u></p><img src="https://s21.ax1x.com/2024/12/11/pAHvpB4.png" alt="image.png" style="zoom:50%;" /><p>Another example is slightly more interesting.</p><p><u><b>Example 2:</b></u></p><p>The following is a set of 7 vectors, which forms a binary code of length 4 over $\Sigma=\{0, 1\}$.</p><img src="https://s21.ax1x.com/2024/12/11/pAHvSuF.png" alt="image.png" style="zoom:50%;" /><p>We can relate this code to the communication between Alice and Bob mentioned earlier. </p><img src="https://s21.ax1x.com/2024/12/11/pAHv9HJ.png" alt="image.png" style="zoom:50%;" /><p>Thus, the code $\mathcal{C}$ is the image of this map, i.e. $\mathcal{C}=\mathrm{Im(ENC)}$. In other words, $\mathcal{C}$ is the <u>set of all codewords that can be obtained using this encoding map.</u></p><p>Example 2 can actually be used to <u>correct one erasure</u> (something bad). </p><p>An <strong>erasure</strong> means we know which bit got erased, but we don’t know its original value. Based on the map definition, the missing bit must be 1.</p><img src="https://s21.ax1x.com/2024/12/11/pAHvA9x.png" alt="image.png" style="zoom:50%;" /><p>Furthermore, it can also be used to <u>detect one error</u> (something bad).</p><p>An <strong>error</strong> means we know one bit my have been changed, but we don’t know which one.</p><img src="https://s21.ax1x.com/2024/12/11/pAHvF41.png" alt="image.png" style="zoom:50%;" /><p>To sum up, we say that the code in Example 2 can correct one erasure or detect one error. But it <strong>cannot</strong> correct one error.</p><p>Let’s see a code that can correct one error.</p><p><u><b>Example 3:</b></u></p><img src="https://s21.ax1x.com/2024/12/11/pAHvE36.png" alt="image.png" style="zoom:50%;" /><p>Given this encoding map, we can define a code $\mathcal{C}=\mathrm{Im(ENC)}$ . Hence, $\mathcal{C}\subseteq\{0,1\}^7$ is a binary code of length 7.</p><p>This code has a nice way to visualize as three overlapping circles.</p><img src="https://s21.ax1x.com/2024/12/12/pAb1a59.png" alt="image.png" style="zoom:50%;" /><p>We place the messages $x_1, x_2, x_3, x_4$ in the middle and the remaining spaces in the circles correspond to the <strong>parity bits</strong>, which are the sum of the other bits in each circle. This ensures that <u>the sum of the bits in each circle equals 0.</u></p><p>With these parity bits, this code can correct one error.</p><hr><p>With these circles in mind, we can solve the following puzzle more easily.</p><img src="https://s21.ax1x.com/2024/12/12/pAb1Bgx.png" alt="image.png" style="zoom:50%;" /><p>In a legit codeword, all three circles should sum to 0. Here, both the green and red circles are messed up while the blue circle is correct. Therefore,  $\tilde{c}_3$ must be flipped.</p><p>But this solution with the help of circles seems pretty ad hoc. In order to make the solution less ad hoc, we’ll introduce more definitions to formalize the solution and flesh out the four things we care about for ECCs mentioned before.</p><h1 id="Hamming-Distance"><a href="#Hamming-Distance" class="headerlink" title="Hamming Distance"></a>Hamming Distance</h1><p>We first give the definition of hamming distance, which equals to the number of positions on which two vectors $x, y\in \Sigma^n$ differ.</p> <article class="message is-info"> <div class="message-header">  Hamming Distance:  </div> <div class="message-body">  The <b>Hamming Distance</b> between $x,y\in \Sigma^n$ is  $$ \Delta(x,y)=\sum_{i=1}^n \mathbb{1}(x_i\ne y_i) $$   </div> </article> <p>Then we can define the relative hamming distance, which is the hamming distance normalized by $n$. It’s the fraction of positions on which two vectors differ.</p> <article class="message is-info"> <div class="message-header">  Relative Hamming Distance:  </div> <div class="message-body">  The <b>Relative Hamming Distance</b> between $x,y\in \Sigma^n$ is  $$ \delta(x,y)=\frac1 n \sum_{i=1}^n \mathbb{1}(x_i\ne y_i)=\frac{\Delta(x,y)}{n} $$    </div> </article> <p>Another useful definition is the minimum distance, which is the minimum hamming distance over all distinct pairs. Jumping ahead, we will see that the code with <u>large minimum distance can be used to correct errors.</u></p> <article class="message is-info"> <div class="message-header">  Minimum Distance:  </div> <div class="message-body">  The <b>Minimum Distance</b> of a code $\mathcal{C}\subseteq \Sigma^n$ is  $$ \min_{c\ne c'\in \mathcal{C}}\Delta(c,c') $$ Sometimes, the minimum distance is referred to simply as <b>distance</b>.   </div> </article> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Aside: The hamming distance is a metric, which obeys the triangle inequality $\Delta(x, z)\le \Delta(x, y) + \Delta(y, z)$. </div> </article><p><font color=blue><u><b> Claim: </b></u></font></p><p>The code in Example 3 has minimum distance 3.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Now we can convince ourselves the claim is true with the circles in mind. But we’ll see a much less ad hoc way to prove the distance in Lecture 2. </div> </article><p>If this claim is true, it explains why that code can correct one error using  the triangle inequality for hamming distance. </p><p>The minimum distance equal to 3 means that the hamming distance $\Delta(c,c’)\ge 3$ for every distinct pair $c, c’\in \mathcal{C}$. </p><p>As depicted in the following figure, suppose the corrupted codeword we received is $\tilde{c}$ and $c$ is the correct code should be $c$. There is another codeword $c’\in \mathcal{C}$.</p><img src="https://s21.ax1x.com/2024/12/12/pAb1081.png" alt="image.png" style="zoom:60%;" /><p>We know that the codeword $\tilde{c}$ has exactly one error so the hamming distance $\Delta(\tilde{c}, c)=1$. The triangle inequality tells us:</p><p>$$<br>3\le \Delta(c,c’)\le \Delta(c, \tilde{c}) + \Delta(\tilde{c}, c’)<br>$$</p><p>that the hamming distance $\Delta(\tilde{c}, c’)\ge 2$ for every codeword $c’$ other than the correct codeword $c$.</p><p>Thus, the correct codeword $c\in \mathcal{C}$ is uniquely defined by “the one that is closest to $\tilde{c}$”.</p><h1 id="Minimum-Distance-Proxy-for-Robustness"><a href="#Minimum-Distance-Proxy-for-Robustness" class="headerlink" title="Minimum Distance: Proxy for Robustness"></a>Minimum Distance: Proxy for Robustness</h1><p>The point of this discussion is that the minimum distance is a reasonable proxy for robustness.</p><p>To be more specific, in example 2, the code had <u>minimum distance 2</u> and could <u>correct 1 erasure and detect 1 error</u>.</p><p>In example 3, the code had <u>minimum distance 3</u> and could <u>correct 1 error</u>.</p><p>More generally, <strong>a code with minimum distance $d$</strong> can:</p><ol><li>correct $\le d-1$ erasures</li><li>detect $\le d-1$ errors</li><li>correct $\lfloor \frac{d-1}2\rfloor$ errors</li></ol><p>For point 1 and point 3, the (inefficient) <strong>algorithm</strong> is <u>“if you see $\tilde{c}$, return $c\in \mathcal{C}$ that’s closest to $\tilde{c}$.</u> </p><p>For point 2, the (inefficient) <strong>algorithm</strong> is <u>“if $\tilde{c}\notin \mathcal{C}$, say that something wrong.”</u></p><p>To understand why the algorithm works, we look at the following picture:</p><img src="https://s21.ax1x.com/2024/12/12/pAb1Dv6.png" alt="image.png" style="zoom:50%;" /><ol><li><p><font color = "red">Red Points</font> represent the codewords $c, c’\in \mathcal{C}$.</p></li><li><p><font color=orange> Orange Circle</font> correspond to the <strong>hamming balls</strong> of radius $\lfloor \frac{d-1}{2} \rfloor$ centered at codewords. These hamming balls are <u>disjoint</u> because the minimum distance is $d$.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  This <b>hamming ball</b> around a codeword $c$ is indeed <u>a set of points</u> $\{x\in \Sigma^n:\Delta(c, x)\le \lfloor\frac{d-1}{2} \rfloor\}$. </div> </article></li><li><p><font color=green>Green Circles</font> correspond to the hamming balls of radius $d-1$ centered at codewords. These hamming balls are not disjoint, but they each contain exactly one codeword.</p></li></ol><p>The hamming balls help explain the (inefficient) algorithm we mention before.</p><ul><li><p>Correct  $\lfloor \frac{d-1}2\rfloor$ errors:<br>If $c$ is the <font color=red>“correct” codeword</font> (the left red point) and $\le \lfloor \frac{d-1}{2}\rfloor$ errors are introduced, we may end up with <font color=purple> $\tilde{c}_1$ (the purple point)</font> within its <font color=orange> orange circle</font>. Since the <font color=orange> orange circle</font> are disjoint, the algorithm can correct codeword $c$ from $\tilde{c}_1$.</p></li><li><p>Detect  $d-1$ errors:</p><p>  If $\le d-1$ errors are introduced, we may end up with <font color=pink> $\tilde{c}_2$ (the pink point)</font> within its <font color=green>green circles</font>. Now it’s possible that $\tilde{c}_2$ came from $c$ or that it came from $c’$; we can’t tell. However, since each <font color=green>green circles</font> <u>only contain exactly one codeword, meaning other codewords has to live outside of this ball</u>, we can tell that something wrong.</p></li><li><p>Correct $d-1$ erasures:</p><p>  If $\le d-1$ erasures are introduced in $\tilde{c}$, suppose the candidate codewords are the correct codeword $c$ and some other $c’$. We erase the corresponding positions in both candidate codewords $c$ and $c’$. Since the hamming distance between $c$ and $c’$  is at least $d$,there must be at least one position where $c$ and $c’$ differ. This differing position allows us to resolve the ambiguity the identify the correct codeword $c$.</p></li></ul><p>Returning to the things we care about, we can now clarify the firs two things:</p><p><font color=blue><u><b> Things We Care About (Take 2): </b></u></font></p><ol><li><p>We should be able to handle $\lfloor \frac{d-1}{2}\rfloor$ <u>worst-case</u> errors or $d-1$ <u>worst-case</u> erasures.</p></li><li><p>We should be able to recover <u>all</u> of $x$ (aka correct the errors or erasures)</p></li><li><p>We want to <strong>MINIMIZE OVERHEAD</strong>.</p><p> Jumping ahead, the overhead is defined as the quantity $k/n\in (0, 1]$, which should be as big as possible.</p></li><li><p>We want to do all this <strong>EFFICIENTLY</strong>.</p></li></ol><p>Moreover, the first two things can be combined to the <u>minimum distance</u> $d$ and we want it as large as possible.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Aside:<br>  In this class, we only focus on the <b>worst-case model</b> (also called the “Hamming model” or “adversarial model”. We will discuss a little bit about the <b>random-error model</b> (also called the “Shannon model” or “Stochastic model”. </div> </article><h1 id="Rate"><a href="#Rate" class="headerlink" title="Rate"></a>Rate</h1><p>Moving on to the point 3, what do we mean by “overhead”?</p> <article class="message is-info"> <div class="message-header">  Dimension:  </div> <div class="message-body">  The <b>Message Length</b> (sometimes called <b>Dimension</b>) of a code $\mathcal{C}$ over an alphabet $\Sigma$ is defined to be  $$ k=\log_{|\Sigma|}|\mathcal{C}| $$   </div> </article> <p>This definition is in line of the the encoding operation</p><img src="https://s21.ax1x.com/2024/12/12/pAb1UUJ.png" alt="image.png" style="zoom:60%;" /><p>which encodes a message $x$ of length $k$ over $\Sigma$ into a codeword $c\in \mathcal{C}$.</p><p>This map assigns each possible message to a single codeword so we have $|\Sigma^k|=|\mathcal{C}|$ aka $k=\log_{|\Sigma|}|\mathcal{C}|$.</p> <article class="message is-info"> <div class="message-header">  Rate:  </div> <div class="message-body">  The <b>Rate</b> of a code $\mathcal{C}\subseteq \Sigma^n$ with block length $n$ over an alphabet $\Sigma$ is  $$ \mathcal{R}=\frac{\log_{|\Sigma|}|\mathcal{C}|}{n}=\frac{\text{message length }k}{\text{block length }n} $$    </div> </article> <p>The rate captures the notion of overhead so minimizing the overhead means to maximize the rate. </p><p>So if $\mathcal{R}$ is close to $1$, it means not too much overhead. And if $\mathcal{R}$ is close to 0, it means lots of overhead. We want the rate to be as close to 1 as possible.</p><p>Now we can denote a code by these parameters</p> <article class="message is-info"> <div class="message-header">  Code:  </div> <div class="message-body">  A code with distance $d$, message length $k$, block length $n$, and alphabet $\Sigma$ is called a $(n, k, d)_{\Sigma}$ code.    </div> </article> <p>After clarifying the things we care about, a <strong><u>question</u></strong> arises:</p><p><font color="red"><strong>What is the best trade-off between rate and distance?</strong></font></p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  This question is still open for binary codes! </div> </article><h1 id="Hamming-Bound-Rate-vs-Distance"><a href="#Hamming-Bound-Rate-vs-Distance" class="headerlink" title="Hamming Bound: Rate vs. Distance"></a>Hamming Bound: Rate vs. Distance</h1><p>The Hamming bound gives us one bound on the trade of rate and distance. It establishes some limitations on how good this trade-off can be.</p><p>Let’s return to the picture we had before, with $|C|$ disjoint Hamming balls of radius $\lfloor \frac{d-1}{2}\rfloor$ in the space $\Sigma^n$.</p><img src="https://s21.ax1x.com/2024/12/12/pAb1wCR.png" alt="image.png" style="zoom:50%;" /><p>The idea of the Hamming bound is: there can’t be too much of them or they wouldn’t all fit in space $\Sigma^n$.</p><p>To be more precise, we define the Hamming ball as follows.</p> <article class="message is-info"> <div class="message-header">  Hamming Ball:  </div> <div class="message-body">  The Hamming Ball in $\Sigma^n$ of radius $e$ about some point $x\in \Sigma^n$ is  $$ B_{\Sigma^n}(x, e)=\{y\in \Sigma^n: \Delta(x, y)\le e\} $$ The <b>Volume</b> of $B_{\Sigma^n}(x, e)$ is denoted by $\mathrm{Vol_{|\Sigma|}}(e, n)=|B_{\Sigma^n}(x, e)|$, representing the number of points in the Hamming ball.   </div> </article>  <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> Notice that the right hand side, $|B_{\Sigma^n}(x, e)|$, includes $x$ while the volume notation only refers to the radius $e$ and block length $n$. This is because the volume does not depend on $x$. </div> </article>  <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>   Note: the $\Sigma$ is sometimes dropped from the notation $B_{\Sigma^n}(x, e)$. Note: The notation $B_{\Sigma^n}(x, e/n)$ refers to the Hamming ball with relative distance.  </div> </article> <p>Say that $|\Sigma|=q$, then we can express the volume of a $q$-array Hamming ball as</p><p>$$<br>\mathrm{Vol}_q(e, n) = 1 + \binom{n}{1}\cdot (q-1) + \binom{n}{2}\cdot (q-1)^2 + \dots + \binom{n}{e}(q-1)^e<br>$$</p><p>where term $1$ refers to the $\vec{0}$, term $\binom{n}{1}\cdot (q-1)$ refers to all the elements of $\Sigma^n$ of weight 1.</p><p>Armed with these definitions, we can formalize the bound. </p><p>If a code $\mathcal{C}\subseteq \Sigma^n$ has distance $d$ and message length $k$, where $|\Sigma|=q$, we have</p><p>$$<br>|\mathcal{C}|\cdot \mathrm{Vol}_q\left(\left\lfloor \frac{d-1}{2}\right \rfloor, n\right) \le q^n<br>$$</p><p>The LHS corresponds to the total volume taken up by all disjoint Hamming balls while the RHS corresponds to the total volume in the whole space $\Sigma^n$.</p><p>Taking logs base $q$ of both sides, it can be written as</p><p>$$<br>\log_q |\mathcal{C}|+\log_q \left(\mathrm{Vol}_q\left(\left\lfloor \frac{d-1}{2}\right \rfloor, n\right)\right)\le n<br>$$</p><p>It gives the bound of the rate in terms of the distance.</p> <article class="message is-info"> <div class="message-header">  Hamming Bound:  </div> <div class="message-body">   $$ \text{Rate}=\frac{k}{n} \le 1-\frac{\log_q \left(\mathrm{Vol}_q\left(\left\lfloor \frac{d-1}{2}\right \rfloor, n\right)\right)}{n} $$   </div> </article> <p>Back to Example 3, which was a $(n=7, k=4, d=3)_{q=2}$ code.</p><p>We have $\lfloor\frac{d-1}{2}\rfloor=1$, $\mathrm{Vol}_2(1, 7) = 1 + \binom{7}{1}\cdot 1 = 8$.</p><p>So we have rate bounded by $\frac k n \le 1 - \frac{\log_2(8)}{n} = 1 - \frac 3 7 = \frac 4 7$.</p><p>In fact, the rate $\frac k n= \frac 4 7$, so in this case the Hamming bound is tight!</p><p>It means this code achieves the <u>best trade-off between the minimum distance and rate.</u></p><p>When the Hamming bound is tight, we say the code is <strong>perfect</strong>.<br>Example 3 (which is perfect) is a special case of something called a <strong>Hamming Code.</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
    In this &lt;a href=&quot;/categories/Cryptography-ECCs&quot;&gt;series&lt;/a&gt;, I will be learning &lt;strong&gt;Algebraic Error Correcting Codes&lt;/strong&gt;, lectured by Mary Wootters. The lecture videos are available &lt;a href=&quot;https://youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr&quot;&gt;here&lt;/a&gt;. Feedback and sugguestions are always welcome! ^ - ^
&lt;/div&gt;
&lt;/article&gt;


&lt;p&gt;&lt;b&gt;Topics Covered:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic problem in coding theory&lt;/li&gt;
&lt;li&gt;Code and codeword&lt;/li&gt;
&lt;li&gt;Hamming distance and minimum distance&lt;/li&gt;
&lt;li&gt;Rate&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hamming bound&lt;/strong&gt; on trade-off of the rate and distance&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-ECCs" scheme="https://f7ed.com/categories/Cryptography-ECCs/"/>
    
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="ECC" scheme="https://f7ed.com/tags/ECC/"/>
    
      <category term="Hamming Bound" scheme="https://f7ed.com/tags/Hamming-Bound/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-ZKP」: Lec7 Poly-commit based on ECC</title>
    <link href="https://f7ed.com/2023/08/02/zkp-lec7/"/>
    <id>https://f7ed.com/2023/08/02/zkp-lec7/</id>
    <published>2023-08-01T16:00:00.000Z</published>
    <updated>2023-08-02T12:11:14.601Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>    In this <a href="/categories/Cryptography-ZKP">series</a>, I will learn <strong>Zero Knowledge Proofs (ZKP)</strong> on this <a href="https://zk-learning.org/">MOOC</a>, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and <strong>Yupeng Zhang</strong>. <br>Any corrections and advice are welcome. ^ - ^</div></article><p><strong>Topics:</strong></p><ul><li>Poly-commit based on Error-correcting Codes</li><li><strong>Argument for Vector-Matrix Product</strong><ul><li>Proximity Test</li><li>Consistency Test</li></ul></li><li><strong>Linear-time encodable code based on expanders</strong><ul><li>Lossless Expander</li><li>Recursive Encoding with constant relative distance</li></ul></li></ul><span id="more"></span><p>An important component in the common paradigm for efficient SNARK is the polynomial commitment scheme. </p><img src="https://s1.ax1x.com/2023/08/02/pPPPYb4.png" alt="Paradigm for SNARKs" style="zoom:34%;" /><p>In Lecture 6 we introduced the KZG polynomial commitment based on bilinear pairing and other polynomial commitments based on discrete-log.</p><p>It is worth noting that prover time is dependent on $O(d)$ exponentiations, which is not strictly linear-time.</p><img src="https://s1.ax1x.com/2023/08/02/pPPPNVJ.png" alt="Poly-commit based on pairing and discrete-log" style="zoom:40%;" /><p>Today we are going to present a new class of <strong>polynomial commitments based on error-correcting codes.</strong></p><p>Here are the motivations and drawbacks.</p><p><u><b>Motivations:</b></u> </p><ul><li>Plausibly post-quantum secure</li><li>No group exponentiations<br>Prover only uses hashes, additions and multiplications.</li><li>Small global parameters</li></ul><p><u><b>Drawbacks:</b></u> </p><ul><li>Large proof size</li><li>Not homomorphic and hard to aggregate</li></ul><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Error-correcting-Code"><a href="#Error-correcting-Code" class="headerlink" title="Error-correcting Code"></a>Error-correcting Code</h2><p>Let’s briefly introduce the error-correcting code, which is allowed to correct errors.</p><p>A $[n,k,\Delta]$ code is defined below:</p><ul><li>$Enc(m)$: Encode a message of size $k$ to a codeword of size $n$.</li><li><strong>Minimum distance</strong> (Hamming distance) between any two codewords is $\Delta$.</li></ul><p>Note that we omit the alphabet $\Sigma$ (binary or field) here, which is another important parameter in code. </p><img src="https://s1.ax1x.com/2023/08/02/pPPP3vT.png" alt="encoding" style="zoom:33%;" /><p>A simple example is the <strong>repetition code</strong>, which just repeat each symbol three times.</p><p>Consider the binary alphabet with $k=2$ and $n=6$.</p><p>The codewords are Enc(00)=000000, Enc(01)=000111, Enc(10)=111000 and Enc(11)=111111 with minimum distance $\Delta=3$.</p><p>This repetition code with minimum distance $\Delta=3$ can correct 1 error duting the transmission.</p><p>E.g., suppose the transmission can induce at most 1 error, then the message 010111 received from the sender can be decoded to 01.</p><p>It is worth mentioning that we are going to <u>build poly-commit using error-correcting code <strong>without</strong> efficient decoding algorithm.</u></p><p>The truth is that we don’t use the decoding algorithm at all.</p><p>We define <u>rate</u> and <u>relative distance</u> over a $[n,k,\Delta]$ code.</p><p>The <strong>rate</strong> $\frac{k}{n}$  represents the ratio of the minimal message in the codeword of size $n$. We want the rate close to 1.</p><p>The <strong>relative distance</strong> $\frac{\Delta}{n}$ represents the ratio of the different locations between any two codewords.</p><p>E.g., repetition code with rate $\frac 1 a$ repeats each symbol $a$ times with $n=ak$, and has $\Delta=a$ and relative distance $\frac 1 k$.</p><p>We want rate and relative distance as big as possible, but increasing rate could decrease the relative distance.</p><p>The <u>trade-off between the rate</u> and the distance of a code is well studied in code theory.</p><h2 id="Linear-Code"><a href="#Linear-Code" class="headerlink" title="Linear Code"></a>Linear Code</h2><p>The most common type of code is <strong>linear code</strong>.</p><p>An important <strong>property</strong> <u>is any linear combination of codewords is also a codeword.</u></p><p>It is equivalent to say that <u>encoding can always be represented as vector-matrix multiplication</u> between $m$ (of size $k$) and the generator matrix (of size $k\times n$).</p><p>Moreover, the <u>minimum (Hamming) distance</u> is the same as the <u>codeword with the least number of non-zeros</u> (<strong>weight</strong>).</p><p>(The weight of a codeword indicats the number of non-zeros.)</p><p>The subtraction of any two codewords is also a codeword so the number of the different locations directly implies the weight of another non-zero codeword.</p><h3 id="Reed-Solomon-Code"><a href="#Reed-Solomon-Code" class="headerlink" title="Reed-Solomon Code"></a>Reed-Solomon Code</h3><p>A classical construction of linear code is <strong>Reed-Solomon Code</strong>.</p><p>It encodes messages in $\mathbb{F}_p^k$ to codewords in $\mathbb{F}_p^n$.</p><p>The <strong>idea of encoding</strong> is <u>veiwing the message of size $k$ as a unique degree $k-1$ univariate polynomial</u> and the <strong>codeword</strong> is the evaluations at $n$ points.</p><p>It <u>treats each symbol of the message as an evaluation at a pre-defined point</u> so the polynomial can be uniquely defined by interpolation on the fixed set of public points.</p><p>Then we can evaluate $n$ pre-defined public points as the codeword. E.g., $(\omega,\omega^2,\dots, \omega^n)$ for $n$-th root-of-unity $\omega^n=1 \mod p$.</p><p>The <strong>minimal distance</strong> is $\Delta=n-k+1$ (indicating the least number of non-zeros) <u>since a degree $k-1$ polynomial has at most $k-1$ roots</u> (indicating the most number of zero evaluations).</p><p>E.g., when $n=2k$, rate is $\frac 1 2$ and relative distance is $\frac 1 2$.</p><p>It is pretty good in practice and is almost the best we can achieve.</p><p>RS code is a linear code that the encoding algorithm can be represented as vector-matrix multiplication where the vector is the message and the <u>generator matrix can be derived from Fourier matrix.</u></p><p>The <strong>encoding time</strong> is $O(n\log n)$ <u>using the fast Fourier transform (FFT).</u></p><h1 id="Poly-commit-based-on-error-correcting-codes"><a href="#Poly-commit-based-on-error-correcting-codes" class="headerlink" title="Poly-commit based on error-correcting codes"></a>Poly-commit based on error-correcting codes</h1><p>Recall the polynomial commitment scheme we discussed in previous lectures. </p><img src="https://s1.ax1x.com/2023/08/02/pPPPJrF.png" alt="poly-commit" style="zoom:43%;" /><ol><li>keygen generates global parameters for $\mathbb{F}_p^{(\le d)}$.</li><li>Prover commits to a univariate polynomial of degree $\le d$ .</li><li>Later verifier requests to evaluate at point $u$.</li><li>Prover opens $v$ with proof that $v=f(u)$ and $f\in \mathbb{F}_p^{(\le d)}$.</li></ol><h2 id="Reduce-PCS-to-Vec-Max-Product"><a href="#Reduce-PCS-to-Vec-Max-Product" class="headerlink" title="Reduce PCS to Vec-Max Product"></a>Reduce PCS to Vec-Max Product</h2><p>In the poly-commit based on error-correcting codes, we <u>write the polynomial coefficients in a matrix</u> of size $\sqrt{d}$ by $\sqrt{d}$.</p><p>For simplicity, we assume $d$ is an exact power.</p><img src="https://s1.ax1x.com/2023/08/02/pPPP12V.png" alt="Coefficient Matrix" style="zoom:53%;" /><p>Note that the <strong>vectorization of the above matrix</strong> forms the original vector of polynomial coefficients, that is:</p>$$[f_{1,1},f_{2,1},\dots, f_{\sqrt{d},1},\dots,f_{1,\sqrt{d}},\dots,f_{\sqrt{d},\sqrt{d}}]^{T}$$<p>Hence, the <strong>polynomial behind the matrix</strong> can be written with two indices:</p>$$f(u)=\sum_{i=1}^{\sqrt{d}}\sum_{j=1}^{\sqrt{d}} f_{i,j}u^{i-1+(j-1)\sqrt{d}}$$<p>Then the <strong>evaluation</strong> of $f(u)$ can be <u>viewed as two steps</u> as follows.</p><img src="https://s1.ax1x.com/2023/08/02/pPPPGKU.png" alt="evaluation as two steps" style="zoom:50%;" /><p><font color=blue><u><b>Two steps of evaluation:</b></u></font> </p><ol><li><p><strong>(Vecor-Matrix Product)</strong> <u>Multiply a vector defined by point $u$</u> with the matrix of coefficients to get a vector of size $\sqrt{d}$.</p><p> <img src="https://s1.ax1x.com/2023/08/02/pPPPUa9.png" alt="Vec-Mat Product"></p></li><li><p><strong>(Inner Product)</strong> Multiply the vector of size $\sqrt{d}$ <u>with another vector defined by point $u$</u> to obtain the final evaluation.</p></li></ol><p>With this nice observation, we actually <strong>reduce the poly-commit to an argument for vector-matrix product.</strong></p><p>Roughly speaking, <strong>prover</strong> can <u>only evaluate the first step</u> and sends a vector of size $\sqrt{d}$ as proof.</p><p><strong>Verifier</strong> <u>checks whether the Vec-Mat product is correct</u> using proof system and <u>evaluates the second step locally</u>, which is an inner product of the Vec-Mat product and the vector defined by point $u$.</p><p>As a result, the argument for Vec-Mat product gives us a polynomial commitment with $\sqrt{d}$ proof size.</p><h2 id="Argument-for-Vec-Mat-Product"><a href="#Argument-for-Vec-Mat-Product" class="headerlink" title="Argument for Vec-Mat Product"></a>Argument for Vec-Mat Product</h2><p>Now our <strong>goal</strong> is to design a scheme to <u>test the Vec-Mat product without sending the matrix directly.</u></p><h2 id="Commit"><a href="#Commit" class="headerlink" title="Commit"></a>Commit</h2><p>As usual, we need to commit to the polynomial.</p><p>Here we instead <u>commit to an encoded matrix defined by the polynomial.</u></p><p>We first use a linear code to encode the original matrix defined by the coefficients of polynomial.</p><p>Concretely speaking, we <strong>encode each row with a linear code to compute an encoded matrix</strong> of size $\sqrt{d}\times n$ where $n$ is the size of the codeword.</p><img src="https://s1.ax1x.com/2023/08/02/pPPPa5R.png" alt="Encode Rowwise" style="zoom:50%;" /><p>We will <u>use a linear code with constant rate</u> so that the <u>size of the encoded matrix is asymptotical to $d$.</u></p><p>Then we can <strong>commit to each column of the encoded matrix using Merkle tree.</strong></p><p>Recall the Merkle tree commitment introduced in <a href="/2023/07/18/zkp-lec4/" title="[Lecture 4]">[Lecture 4]</a>.</p><img src="https://s1.ax1x.com/2023/08/02/pPPPwP1.png" alt="Commit Colwise" style="zoom:50%;" /><p>The root hash is served as the <u>commitment to the encoded matrix.</u></p><p>Then verifier can request to <u>open each column individually</u> and checks whether the opened column is altered.</p><p>It is worth noting that the <strong>key generation</strong> for this Merkle tree commitment is only <u>sampling a hash function</u>, resulting in a <u>constant size global parameters with no trusted setup.</u></p><h2 id="Eval-and-Verify"><a href="#Eval-and-Verify" class="headerlink" title="Eval and Verify"></a>Eval and Verify</h2><p>We actually perform the evaluation together with verification.</p><p>It consists of two tests, <strong>proximity test</strong> and <strong>consistency check</strong>.</p><p>We fisrt consider <u>how a malicious prover could cheat in the commitment.</u></p><p>A malicious prover can commit to a matrix of inappropriate size but it can be recognized easily by the Merkle tree proof.</p><p>A <strong>malicious prover</strong> can <u>commit to an abitrary matrix</u> of specified size in which <u>each row is not a valid codeword.</u> </p><p>E.g., a valid RS code is a vector of evaluations of a polynomial specified by the message.</p><p>Hence, verifier uses the <strong>proxomity test</strong> to <u>test if the committed matrix indeed consists of $\sqrt{d}$ codewords.</u></p><p>Having checked this proximity test, verifier is convinced that the committed matrix is nearly close to the encoded matrix defined by the original matrix of coefficients.</p><p>Then verifier can move to the <strong>consistency check</strong> to <u>compute (and verifiy) the actual evaluation.</u></p><h3 id="Step1-Proximity-Test"><a href="#Step1-Proximity-Test" class="headerlink" title="Step1: Proximity Test"></a>Step1: Proximity Test</h3><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Ligero <a href="https://eprint.iacr.org/2022/1608">[AHIV’2017]</a> and <a href="https://eprint.iacr.org/2017/872">[BCGGHJ’2017]</a> are two independent works to introduce the proximity test. <br> Ligero proposed the so-called interleaved test using Reed-Solomon code with quasi-linear prover time.<br> [BCGGHJ’2017] instead used a linear-time encodable code to build the ideal linear commitment model, which is the first work to build SNARK with strictly linear prover time.<br> Note that the proximity test in these two works are proposed to build general-purpose SNARKs. Here we use it to build poly-commit as a specified protocol. </div> </article><p>Ligero [<a href="https://eprint.iacr.org/2022/1608">AHIV’2017</a>] and [<a href="https://eprint.iacr.org/2017/872">BCGGHJ’2017</a>] are two independent works to introduce the proximity test. </p><p>Ligero proposed the so-called interleaved test using Reed-Solomon code with quasi-linear prover time. </p><p>[BCGGHJ’2017] instead used a linear-time encodable code to build the ideal linear commitment model, which is the first work to build SNARK with strictly linear prover time. </p><p>Note that the proximity test in these two works are proposed to build general-purpose SNARKs. Here we use it to build poly-commit as a specified protocol.</p><p>We first present the description of the proximity test as below.</p><img src="https://s1.ax1x.com/2023/08/02/pPPP08x.png" alt="Proximity Test" style="zoom:50%;" /><ol><li><strong>Verifier</strong> <u>samples a random vector</u> $r$ of size $\sqrt{d}$ and sends it to prover.</li><li><strong>Prover</strong> returns the vector-matrix product of the random vector $r$ and the <u>encoded matrix.</u></li><li><strong>Verifier</strong> requests to <u>open several random columns</u> and <strong>prover</strong> reveals them with Merkle tree proof.</li><li><strong>Verifier</strong> performs <u>3 checks</u><ol><li>The returned vector is a <u>codeword</u></li><li>The <u>opened columns are consistent</u> with the committed Merkle tree.</li><li>The <u>inner product</u> between $r$ and the opened column is consistent with the corresponding element of the returned vector.</li></ol></li></ol><p>The <strong>completeness</strong> is evident.</p><p>The vec-mat product computed by the <strong>honest prover</strong> is indeed the <u>linear combination of rows (codewords) specified by the random vector chosen by the verifier.</u></p><p>Recall the propery of the linear codes that a linear combination of codewords is a codeword.</p><p>So these 3 checks will be passed by verifier.</p><p>Let’s intuitively give the <strong>proof of soundness.</strong></p><p>Assume for the <u>contradiction</u> that the malicious prover <u>commits to a fake matrix</u>, and computes the vec-mat product by this fake matrix.</p><p><font color=blue><u><b>Soundness (Intuition):</b></u></font> </p><ul><li>If the <u>vector is correctly computed</u>, under our assumption, the product is <u>not a codeword</u>. → check 1 will be failed.</li><li>If the <u>vector is false</u> meaning that the prover just returns an arbitrary codeword,  there are <u>many different locations from the correct answer.</u><ul><li>By check 2, columns are as committed.</li><li>Probability of passing check 3 is <u>extreamly small.</u></li></ul></li></ul><p>Let’s discuss <strong>the second case</strong> where the <u>vector sent by the prover is false</u> and $w=r^TC$ denotes the correct answer.</p><img src="https://s1.ax1x.com/2023/08/02/pPPntld.png" alt="vector is false" style="zoom:50%;" /><p>In the <font color=blue><u><b>formal proof for soundness:</b></u></font>  [<a href="https://eprint.iacr.org/2022/1608">AHIV’2017</a>], it defines a parameter $e&lt;\frac{\Delta}{4}$, which is related to the minimal distance $\Delta$, to <u>measure the distance between the committed matrix and the codeword space.</u></p><p>Concretely speaking, $e$ measures the minimal distance between any row (of the committed matrix) and any codeword (in the codeword space).</p><p>If the <u>committed (fake) matrix is $e$-far from any codeword</u> for $e&lt;\frac{\Delta}{4}$, then the <strong>probability</strong> that the <u>vec-mat product $w=r^T C$ is $e$-close to any codeword</u> is $\le \frac{e+1}{\mathbb{F}}$, which is extreamly small.</p>$$\operatorname{Pr}[w=r^TC\text{ is }e\text{-close to any codeword}]\le \frac{e+1}{\mathbb{F}}$$<p>Then we can rule out this case, and the <strong>remaining case</strong> is that the <u>correct answer $w=r^TC$ is $e$-far from any codeword.</u></p><p><u>Under this condition, we know there are at least $e$ different positions</u> between the codeword sent by prover and the correct answer $w$.</p><p>Then the probability that check 3 is true for $t$ random columns is bounded by $(1-\frac e n)^t$ where $\frac e n$ is <u>constant for the linear code with constant relative distance</u>, e.g. RS code. </p>$$\operatorname{Pr}[\text{check 3 is true for }t \text{ random columns}] \le (1-\frac{e}{n})^t$$<p>Hence, soundness probability can be reduced to <u>negligible</u> probability. That’s why we want linear codes with constant relative distance.</p><h3 id="Optimization-for-Proximity-Test"><a href="#Optimization-for-Proximity-Test" class="headerlink" title="Optimization for Proximity Test"></a>Optimization for Proximity Test</h3><p>There is one optimization for the proximity test.</p><p>Instead of sending the codeword, <strong>prover</strong> can <u>send the message behind the codeword to verifier.</u></p><p>Note that the message is computed by the random vector and the original matrix defined by the polynomial coefficients.</p><p>Then <strong>verifier</strong> can <u>encode the message to obtain the corresponding codeword</u> that is supposed to be sent by prover.</p><p>This nice optimization <u>reduces the proof size</u> from $n$ to $k$.</p><p>Moreover, there is <strong>no need</strong> for verifier to <u>perform the first check explicitly that the vector is a codeword</u> since the encoding is done by the verifier.</p><p>We depict the optimized proximity test as below.</p><img src="https://s1.ax1x.com/2023/08/02/pPPnN6A.png" alt="Optimization for Proximity Test" style="zoom:50%;" /><ol><li><strong>Verifier</strong> samples a random vector $r$ of size $\sqrt{d}$ and sends it to prover.</li><li><strong>Prover</strong> returns the vector-matrix product of the random vector $r$ and the <u>original matrix of coefficients.</u></li><li><strong>Verifier</strong> <u>encodes the message to compute the codeword.</u></li><li><strong>Verifier</strong> requests to open several random columns and <strong>prover</strong> reveals them with Merkle tree proof.</li><li><strong>Verifier</strong> performs <u>2 checks</u><ol><li><del>The returned vector is a codeword</del></li><li>The opened columns are consistent with the committed Merkle tree.</li><li>The inner product between $r$ and the opened column is consistent with the corresponding element of the returned vector.</li></ol></li></ol><h3 id="Step2-Consistency-Check"><a href="#Step2-Consistency-Check" class="headerlink" title="Step2: Consistency Check"></a>Step2: Consistency Check</h3><p>With the <strong>proximity test</strong>, the <u>verifier knows the committed matrix is close to an encoded matrix with overwhelming probability.</u></p><p>Next we can <strong>perform the consistency check</strong> to really <u>test the evaluation of vec-mat product</u> between the vector defined by point $u$  and the original matrix of size $\sqrt{d}\times \sqrt{d}$.</p><p>The consistency check is almost the same as the proximity test with the optimization mentioned above excetp that the vector is defined by point $u$ rather than a random vector $r$.</p><p>Likewise, the verifier encodes the message to compute the codeword so the first check can be removed.</p><p>Futhermore, the <strong>verifier</strong> can <u>use the same opened columns in the proximity test to perform the third check.</u></p><p>The cosistency check is depicted as below where the first two checks can be removed.</p><img src="https://s1.ax1x.com/2023/08/02/pPPndmt.png" alt="Consistency Check" style="zoom:50%;" /><p><font color=blue><u><b><strong>Knowledge Soundness (Intuition):</strong></b></u></font> </p><p>In the consistency test, we actually need to prove the <strong>knowledge soundness.</strong></p><p>By the proximity test, the committed matrix $C$ is close to an encoded matrix that can be uniquely decoded  to a matrix $F$ defined by  polynomial coefficients.</p><p>Intuitively speaking, <u>there exists an extractor that extracts $F$ by Merkle tree commitment and decoding $C$,</u> s.t. $\vec{u}\times F=m$ with probability $1-\epsilon$.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>To put everything together, the poly-commit scheme based on linear code is described as below.</p><p><font color=blue><u><b>PCS based on Linear Code:</b></u></font> </p><ul><li><strong>Keygen</strong>: sample a hash function</li><li><strong>Commit</strong>:<ol><li>encode the coefficient matrix $F$ of $f$ row-wise with a linear code</li><li>compute the Merkle tree commitment col-wise</li></ol></li><li><strong>Eval and Verify:</strong><ul><li><strong>Proximity test</strong>: random linear combination of all rows, check its consistency with $t$ random columns</li><li><strong>Consistency test</strong>: compute $\vec{u}\times F=m$, encode $m$ and check its consistency with $t$ random columns</li><li><strong>Evaluate</strong> $f(u)=&lt;m,u’&gt;$ by verifier where $u’$ is another vector defined by point $u$.</li></ul></li></ul><p>An important thing to point out is that the <strong>proximity test is necessary</strong> for evaluation and verification although it is almost the same as the consistency test.</p><p><strong>Suppose</strong> we only perform the consistency test, then the verifier checks consistency of the <u>inner-product of vector $\vec{u}$ and the random columns.</u></p><p>But it<font color=red> <strong>dose not work</strong> </font>since vector $\vec{u}$ is defined in a very structured way.</p><p>Intuitively speaking, it has to <strong>use random challenges</strong> chosen by the verifier to guarantee the consistency.</p><p><font color=blue><u><b>Properties:</b></u></font> </p><ul><li>Keygen: $O(1)$, transparent setup with constant size $gp$.</li><li>Commit:<ul><li>Encoding: $O(d\log d)$ field multiplications using RS code, $O(d)$ using linear-time encodable codes.</li><li>Merkle tree: $O(d)$ hashes, $O(1)$ commitment size.</li></ul></li><li>Eval: $O(d)$ field multiplications</li><li>Proof size: $O(\sqrt{d})$ (several vectors of size $\sqrt{d}$ )</li><li>Verifier time: $O(\sqrt{d})$</li></ul><p>Look at the <strong>concrete performance</strong> in [<a href="https://eprint.iacr.org/2021/1043">GLSTW’21</a>] with degree $d=2^{25}$ and linear-time encodable code.</p><ul><li>Commit: 36s</li><li>Eval: 3.2s</li><li>Proof size: 49MB</li><li>Verifier time: 0.7s</li></ul><p>It is <u>excellent in practice</u> and <u>significantly faster than PCS based on pairing or discrete-log</u> (such as KZG, Bulletproofs) because it only uses linear operations without any exponentiations.</p><h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><p>Let’s disscuss the following up-to-date works based on error-correcting codes.</p><img src="https://s1.ax1x.com/2023/08/02/pPPnUOI.png" alt="Untitled" style="zoom:35%;" /><ul><li>[<a href="https://eprint.iacr.org/2020/1426">Bootle-Chiesa-Groth’20</a>]</li></ul><p>It <strong>proposed</strong> the <u>tensor query IOP</u> $&lt;f,(\vec{u}\otimes \vec{u}’)&gt;$, which evaluates inner-product of vector $f$ of size $\sqrt{d}$ and another vector generated by tensor product between two sub-vectors of size $\sqrt{d}$. (dimentsion 2)</p><p>Note that this IOP only works for the product of specific form.</p><p>Moreover, it <strong>generalizes to multiple dimentsions</strong> and performs the proximity test and consistency test dimension by dimension <u>with smaller proof size</u> $O(n^\epsilon)$ for constant $\epsilon&lt;1$.</p><ul><li>Brakedown [<a href="https://eprint.iacr.org/2021/1043">GLSTW’21</a>]</li></ul><p>This work <strong>proposed</strong> the <u>polynomial commitment based on tensor query.</u></p><p>As described above, we don’t use decoding algorithm at all in the poly-commit. The prover just sends the message and the verifier encodes the message to get the corresponding codeword.   It <u>gives relaxation on the design of the poly-commit which allows to use linear codes without efficient decoding algorithm.</u></p><p><strong>Unfortunately</strong>, when we <u>prove the knowledge soundness</u>, it has to extract the matrix of polynomial coefficients from the committed encoded matrix in which <u>the efficient decoding is required</u>. If the decoding algorithm is not efficient, the extractor is not polynomial as well.</p><p>Back to this work, the <strong>another contribution</strong> <u>is showing an alternative way to prove knowledge soundness without efficient decoding algorithm.</u></p><p>As a result, it enables us to build poly-commit using any linear codes without efficient decoding algorithm.</p><ul><li>[<a href="https://eprint.iacr.org/2020/1527">Bootle-Chiesa-Liu’21</a>]</li></ul><p>It improves proof size to $\text{poly}\log (n)$ with a proof composition of tensor IOP and PCP of proximity. [<a href="https://link.springer.com/article/10.1007/s10472-009-9169-y">Mie’09</a>]</p><ul><li>Orion [<a href="https://eprint.iacr.org/2022/1010">Xie-Zhang-Song’22</a>]</li></ul><p>It improves the proof size to $O(\log^2 n)$ with a proof composition of the code-switching technique [<a href="https://eprint.iacr.org/2019/1062">Ron-Zewi-Rothblum’20</a>]</p><p>Concretely, the proof size is 5.7MB for $d=2^{25}$, which is quite large in practice.</p><h1 id="Linear-time-encodable-code-based-on-expanders"><a href="#Linear-time-encodable-code-based-on-expanders" class="headerlink" title="Linear-time encodable code based on expanders"></a>Linear-time encodable code based on expanders</h1><p>It is noteworthy that the following line of works all <strong>build SNARKs with linear prover</strong> <u>using the linear-time encodable code with constant relative distance.</u></p><img src="https://s1.ax1x.com/2023/08/02/pPPnUOI.png" alt="Untitled" style="zoom:35%;" /><p>In the last segment, we are going to present the construction of linear-time encodable code based on expanders.</p><p><strong>Linear-time encodable code</strong> is proposed by [<a href="https://www.cs.yale.edu/homes/spielman/Research/ITsuperc.pdf">Spielman</a>’96] and generalized from binary to field by [<a href="https://dl.acm.org/doi/10.1145/2554797.2554815">Druk-Ishai’14</a>], which <u>relies on the expander graph.</u></p><h2 id="Expander-Graph"><a href="#Expander-Graph" class="headerlink" title="Expander Graph"></a>Expander Graph</h2><p>Look at the following <u>bipartite graph</u> where each node on the left set has <u>3 outgoing edges</u> connecting to the nodes on the right edges and <u>every two nodes on the left set connect at least 5 nodes on the right set.</u></p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Bipartite Graph (from wiki) <br> A bipartite graph is a graph whose vertices can be divided into two disjoint and independent sets U and V, that is, every edge connects a vertex in U to one in V. Vertex sets U and V are usually called the parts of the graph. </div> </article><p>It is a <strong>good expander</strong> since <u>every two nodes on the left set have at most 6 outgoing edges.</u></p><img src="https://s1.ax1x.com/2023/08/02/pPPnYSH.png" alt="Expander Graph" style="zoom:30%;" /><p>In terms of <u>encoding</u>, consider the <u>left nodes as message</u> where each symbol is put on a node and the <u>right nodes is the codeword</u>.</p><p>The encoding of the message is to <u>compute for each right-side node the sum of nodes connected from the left set,</u> which can be represented as the vector-matrix multiplication between the message and the adjacent matrix of the graph so it is <u>a linear code.</u></p><p>A <strong>nature question</strong> is <u>why we need such an expander graph to achieve linear codes with a good relative distance.</u></p><p>Intuitively speaking, with such a good expander, <u>even a single non-zero node on the left set can be expanded to many non-zero nodes on the right set</u>, that is, it amplies the number of non-zeros (weight) from the message to the codeword, enabling us to achieve good relative distance.</p><h2 id="Lossless-Expander"><a href="#Lossless-Expander" class="headerlink" title="Lossless Expander"></a>Lossless Expander</h2><article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> Note that the relative distance in the above simple example is not constant. </div> </article><p>We are going to describe the lossless expander in a formal way.</p><img src="https://s1.ax1x.com/2023/08/02/pPPnYSH.png" alt="Expander Graph" style="zoom:30%;" /><p>Let $|L|$ denote <u>the number of left nodes</u> and <u>the number of the right nodes</u> is $\alpha |L|$ for a constant $\alpha\in (0,1)$.</p><p>The <u>degree of a left node</u> is denoted by $g$.</p><p>Consider a good (almost perfect) expander that for every subset $S$ of nodes on the left, the <u>number of neighers</u> $|\Tau(S)|=g|S|$ for $|S|\le \frac{\alpha |L|}{g}$, which is bounded by the number of right nodes.</p><p>But it is too good to achieve.</p><p>We need to <u>relax</u> the equality and the boundary.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Lossless Expander:</strong></p> </div> <div class="message-body">  For every subset $S$ of nodes on the left, the number of neighbors $|\Tau(S)|\ge (1-\beta)g|S|$ for $|S|\le \frac{\delta |L|}{g}$.( $\beta\rightarrow 0$, $\delta \rightarrow \alpha$ ) </div> </article> <p>Likewise, the encoding on the lossless expander is to sum up the connected nodes from the left nodes for each right node to compute the codeword.</p><h2 id="Recursive-Encoding"><a href="#Recursive-Encoding" class="headerlink" title="Recursive Encoding"></a>Recursive Encoding</h2><p>Then we move to the construction of linear-time encodable codes, which uses the recursive encoding with the lossless expander.</p><p>The encoding algorithm is depicted as below.</p><img src="https://s1.ax1x.com/2023/08/02/pPPhWWt.png" alt="Overview of Recursive Encoding" style="zoom:43%;" /><p>Let’s elaborate on the detailed procedure of encoding a message $m$ of size $k$ to a codeword of size $4k$ with rate $1/4$.</p><p><font color=blue><u><b>Recursive Encoding:</b></u></font> </p><ol><li><u>Copy the message</u> as the first part of the final codeword.</li><li><u>Apply the lossless expander</u> with $\alpha=\frac 1 2$ to compute the codeword $m_1$ of size $k/2$.</li><li><u>Assume we already had an encoding algorithm</u> for message $m_1$ of size $k/2$ with rate $1/4$ and <u>good relative distance</u> $\Delta$, then we apply it to compute the codeword $c_1$ of size $2k$ as the second part of the final codeword.</li><li><u>Apply another lossless expander</u> with $\alpha =\frac 1 2$ for messages of size $2k$ to compute the codeword $c_2$ of size $k$ as the third part.</li><li>The final codeword is the concatenation $c=m|| c_1||c_2$</li></ol><p>The <strong>remaining thing</strong> is <u>how we get the encoding algorithm for messages of size $k/2$ with rate $1/4$ and good relative distance.</u></p><p>That’s exactly the <strong>recursiving encoding</strong> that we just use the entire encoding algorithm for the message of size $k/2$.</p><p>Hence, we <u>repeate the entire encoding algorithm</u> in recursion from $k/2,k/4,\dots$ until a constant size.</p><p>Note that the <u>lossless expanders used in each iteration are different</u> since the size of message are different.</p><p>Finally we can <u>use any code with good distance for a constant-size message</u>. E.g., Reed-Solomon code.</p><h2 id="Distance-of-the-Code"><a href="#Distance-of-the-Code" class="headerlink" title="Distance of the Code"></a>Distance of the Code</h2><p>The recursive way of encoding enables to <u>achieve a constant relative distance:</u></p>$$\Delta'=\min \{\Delta,\frac{\delta}{4g}\}$$<p>where $\Delta$ is the relative distance of the <u>code used in the middle</u> from $k/2$ to $2k$ and $\frac{\delta}{4g}$ depends on the <u>expander graph.</u></p><p><font color=blue><u><b>Proof of relative distance (case by case):</b></u></font> [<a href="https://dl.acm.org/doi/10.1145/2554797.2554815">Druk-Ishai’14</a>]</p><ol><li>If weight of $m$ is larger than $4k\Delta’$, then the relative distance is larger than $\frac{4k\Delta’}{4k}=\Delta’$.<br>→ Done. It means that for all messages with large weight, we automatically get codewords with large weight.</li><li> If weight of $m\le 4k\Delta'\le \frac{\delta k}{g}$, the condition of the first lossless expander  holds. (since $\Delta'\le \frac{\delta}{4g}$ )<ol><li>Let $S$ be the set of non-zero nodes in $m$, then we have $|\Tau(S)|\ge (1-\beta)g|S|$.</li><li>We can set $g\ge 10$ and $\beta &lt; 0.1$, then at least $(1-2\beta)g|S| &gt; 8|S|&gt;0$ vertices in Hamming ball have a unique neighbor in $S$.</li><li>Hence, $m_1$ (the output of this lossless expander) is <u>non-zero</u>.</li><li>After applying the encoding for $m_1$ of size $k/2$ with relative distance $\Delta$, the wight of $c_1$ $\ge 2k\Delta\ge 2k\Delta’$.<br>(The second inequality holds by the definition of min).</li></ol></li><li>If the weight of $c_1$ is larger than $4k\Delta’$, then the relative distance is larger than $\Delta’$.<br>→Done</li><li>Else, weight of $c_1$ is $\le 4k\Delta’&lt;\frac{\delta2k}{g}$, the condition of the second lossless expander holds. <ol><li>Let $S’$ be the set of non-zero nodes in $c_1$, then we can show the weight of $c_2$ is at least $|\Tau(S’)|\ge (1-\beta)g|S’|&gt;8|S’|&gt;16k\Delta’   &gt;(4k)\Delta’$.</li></ol></li></ol><h2 id="Sampling-of-the-Lossless-Expander"><a href="#Sampling-of-the-Lossless-Expander" class="headerlink" title="Sampling of the Lossless Expander"></a>Sampling of the Lossless Expander</h2><p>With lossless expander, we can build the linear-time encodable codes with constant relative distance.</p><p>The last piece of the puzzle is <u>how to construct the lossless expander.</u></p><p>[<a href="https://dash.harvard.edu/bitstream/handle/1/3330492/Vadhan_CondLosslessExpanders.pdf">Capalbo-Reingold-Vadhan-Widgerson’2002</a>] proposed an <u>explicit construction</u> of lossless expander.</p><p>Note that being explicit is <u>deterministic</u>.</p><p><u>Unfortunately, it has large hidden constant</u> so the concrete efficiency is not good.</p><p><strong>An alternative way</strong> is <u>random sampling</u> since a random graph is supposed to have good expansion.</p><p>Since the sample space is polynomial, there <u>is a $1/\text{poly}(n)$ failure probability</u> instead of negligible probability.</p><h2 id="Improvements-of-the-Code"><a href="#Improvements-of-the-Code" class="headerlink" title="Improvements of the Code"></a>Improvements of the Code</h2><ul><li>Brakedown [<a href="https://eprint.iacr.org/2021/1043">GLSTW’21</a>]</li></ul><p>Instead of the plain summations when encoding, it uses <u>random summations</u>, which assign a random weight for each edge and performs the weighted summation,  to significantly boost the distance.</p><ul><li>Orion  [<a href="https://eprint.iacr.org/2022/1010">Xie-Zhang-Song’22</a>]</li></ul><p>It proposes <u>an expander testing with a negligible failure probability via maximum density of the graph.</u></p><hr><p>Let’s sum up the pros and cons of the polynomial commitment (and SNARK) based on linear code.</p><ul><li>Pros<ul><li>Transparent setup: $O(1)$</li><li>Commit and Prover time: $O(d)$ field additions and multiplications</li><li>Plausibly post-quantum secure</li><li>Field agnostic<br>It means that we can use any field.</li></ul></li><li>Cons<ul><li>Proof size: $O(\sqrt{d})$, MBs</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
    In this &lt;a href=&quot;/categories/Cryptography-ZKP&quot;&gt;series&lt;/a&gt;, I will learn &lt;strong&gt;Zero Knowledge Proofs (ZKP)&lt;/strong&gt; on this &lt;a href=&quot;https://zk-learning.org/&quot;&gt;MOOC&lt;/a&gt;, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and &lt;strong&gt;Yupeng Zhang&lt;/strong&gt;. 
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;



&lt;p&gt;&lt;strong&gt;Topics:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Poly-commit based on Error-correcting Codes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Argument for Vector-Matrix Product&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Proximity Test&lt;/li&gt;
&lt;li&gt;Consistency Test&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linear-time encodable code based on expanders&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Lossless Expander&lt;/li&gt;
&lt;li&gt;Recursive Encoding with constant relative distance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-ZKP" scheme="https://f7ed.com/categories/Cryptography-ZKP/"/>
    
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="ECC" scheme="https://f7ed.com/tags/ECC/"/>
    
      <category term="Poly-commit" scheme="https://f7ed.com/tags/Poly-commit/"/>
    
      <category term="Bulletproofs" scheme="https://f7ed.com/tags/Bulletproofs/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-ZKP」: Lec6 Poly-commit based on Pairing and Discret-log</title>
    <link href="https://f7ed.com/2023/07/26/zkp-lec6/"/>
    <id>https://f7ed.com/2023/07/26/zkp-lec6/</id>
    <published>2023-07-25T16:00:00.000Z</published>
    <updated>2023-07-27T07:53:23.274Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>    In this <a href="/categories/Cryptography-ZKP">series</a>, I will learn <strong>Zero Knowledge Proofs (ZKP)</strong> on this <a href="https://zk-learning.org/">MOOC</a>, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and <strong>Yupeng Zhang</strong>. <br>Any corrections and advice are welcome. ^ - ^</div></article><p><strong>Topics:</strong></p><ul><li><strong>KZG poly-commit based on bilinear pairing</strong><ul><li>KZG scheme</li><li>Powers-of-tau Ceremony</li><li>Security Analysis</li><li>Knowledge of exponent assumption</li><li>Variants: multivariate; ZK; batch openings</li></ul></li><li><strong>Bulletproofs poly-commit based on discrete logarithm</strong></li></ul><span id="more"></span><p>Before proceeding to today’s topic, let’s recall the common recipe for building an efficient SNARK.</p><img src="https://s1.ax1x.com/2023/07/25/pCXSG3q.png" alt="Paradigm of building SNARKs" style="zoom:33%;" /><p>The common way of building SNARK is to combine a poly-IOP with a functional commitment scheme.</p><p>Lecture 4 uses Plonk (poly-IOP) combined with KZG (a univariate polynomial commitment) to build SNARK for general circuits.</p><img src="https://s1.ax1x.com/2023/07/25/pCXS8Cn.png" alt="Plonk poly-IOP combined with KZG" style="zoom:33%;" /><p>Lecture 3 uses Sumcheck protocol combined with a multivariate polynomial commitment to build SNARK.</p><img src="https://s1.ax1x.com/2023/07/25/pCXSQEQ.png" alt="Sumcheck protocol combined with multivariate poly-commitment" style="zoom:33%;" /><p>In this lecture, we are going to introduce polynomial commitments based on bilinear pairing and discrete log.</p><p>When <strong>building polynomial commitment schemes</strong>, we first choose a family of polynomial $\mathcal{F}$, then prover commits to a function $f(x)\in \mathcal{F}$.</p><p>Verifier receives $\text{com}_f$ as the commitment, then  verifier is able to query $f$ at point $u$.</p><p>Finally, prover sends the evaluation $v$ and the proof $\pi$ that $f(u)=v$ and $f\in \mathcal{F}$, and verifier accepts if proof is valid.</p><p>The above procedure is depicted as follows.</p><img src="https://s1.ax1x.com/2023/07/25/pCXS14s.png" alt="Poly-commitment Scheme" style="zoom:33%;" /><p>For ease of use, we give a formal definition for polynomial commitment schemes (PCS).</p><p>It consists of four algorithms as follows.</p><ul><li>$\text{keygen}(\lambda, \mathcal{F})\rightarrow gp$<br>In setup, this algorithm takes the family as inputs and outputs the global parameters used in the commitment and proof.</li><li>$\text{commit}(gp,f)\rightarrow \text{com}_f$<br>Prover calls this algorithm to commit to a function.</li><li>$\text{eval}(gp,f,u)\rightarrow v,\pi$<br>Prover calls this algorithm to compute the evaluation at the point $u$ and the corresponding proof.</li><li>$\text{verify}(gp,\text{com}_f,u,v,\pi)\rightarrow \text{accept or reject}$<br>Verifier calls this algorithm to check the validity of the proof and accept the answer if valid.</li></ul><p>It is complete if an honest prover can convince the verifier to accept the answer.</p><p>It is sound if a verifier can catch a lying prover with high probability.</p><p>We compare the soundness and knowledge soundness in Lecture 3. To put it simply, <strong>soundness</strong> establishes the <u>existence</u> of the witness while <strong>knowledge soundness</strong> establishes that the prover <u>necessarily knows the witness</u>. As a result, knowledge soundness is stronger.</p><p>We give a formal definition of knowledge soundness.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Knowledge Sound:</strong></p> </div> <div class="message-body"> <p>For every poly. time adversary $A(A_0, A_1)$ such that </p>$$\text{keygen}(\lambda,\mathcal{F})\rightarrow gp, A_0(gp)\rightarrow \text{com}_f, A_1(gp, u)\rightarrow v, \pi: \\ \operatorname{Pr}[V(vp, x,\pi)=\text{accept}]=1$$<p>there is an efficient <strong>extractor</strong> $E$ (that uses $A$) such that </p>$$\text{keygen}(\lambda,\mathcal{F})\rightarrow gp, A_0(gp)\rightarrow \text{com}_f, E(gp, \text{com}_f)\rightarrow f:\\ \operatorname{Pr}[f(u)=v \text{ and } f(x)\in \mathcal{F}]> 1-\epsilon $$<p>where $\epsilon$  is negligible.</p> </div> </article> <h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Let’s quickly go through the basic conceptions in number theory, which is widely used in the following sections.</p><p>I refer readers to  This Blog for a detailed description.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Group:</strong> A set $\mathbb{G}$ and an operation $*$</p> </div> <div class="message-body"> <ol><li>Closure: For all $a,b\in \mathbb{G}$, $a* b \in \mathbb{G}$</li><li>Associativity: For all $a,b,c\in \mathbb{G}$, $(a*b)*c =a*(b*c)$ </li><li>Identity: There exists a unique element $e\in \mathbb{G}$ s.t. for every $a\in \mathbb{G}$, $e*a=a*e=a$.</li><li>Inverse: For each $a\in \mathbb{G}$, there exists $b\in \mathbb{G}$ s.t. $a*b=b*a=e$. </li></ol> </div> </article> A simple example is the group that contains integers $\{\dots, -2,-1,0,1,2,\dots\}$ under addition operation $+$.The common Groups considered in Cryptography are the group that contains positive integers mod prime $p:\{1,2,\dots, p-1\}$ under multiplication operation $\times$ and the groups defined by elliptic curves. <article class="message is-info"> <div class="message-header"> <p><strong>Generator of a group:</strong></p> </div> <div class="message-body"> <p>An element $g$ that generates all elements in the group by taking all powers of $g$.</p> </div> </article> <p>For example, $3$ is a generator of the group $\mathbb{Z}_7^*={1,2,3,4,5,6}$.</p><p>We can write every group element in the power of $3$.</p><p>$3^1=3;3^2=2;3^3=6;3^4=4;3^5=5;3^6=1 \mod 7$</p><h2 id="Discrete-log"><a href="#Discrete-log" class="headerlink" title="Discrete-log"></a>Discrete-log</h2>A group $\mathbb{G}$ has an alternative representation as the powers of the generator $g:\{g,g^2, g^3,\dots,g^{p-1}\}$. <article class="message is-info"> <div class="message-header"> <p><strong>Discrete logarithm problem:</strong></p> </div> <div class="message-body"> <p>Given $y\in \mathbb{G}$, find $x$ such that $g^x=y$.</p> </div> </article> <p>The quantum computer can actually solve the discrete logarithm problem in polynomial time.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Discrete logarithm assumption:</strong></p> </div> <div class="message-body"> <p>Discrete-log problem is computationally hard.</p> </div> </article> <p>Note that the DL assumption <u>does not hold in all groups</u> but it is believed to hold in certain groups.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Computational Diffie-Hellman assumption:</strong></p> </div> <div class="message-body"> <p>Given $\mathbb{G},g,g^x,g^y$, cannot compute $g^{xy}$.</p> </div> </article> <p>It is worth noting that <u>a stronger assumption means the underlying problem is easier.</u></p><p>Hence, the CDH assumption is a stronger assumption than the DL assumption since the CDH problem is reducible to the DL problem.</p><h2 id="Bilinear-Pairing"><a href="#Bilinear-Pairing" class="headerlink" title="Bilinear Pairing"></a>Bilinear Pairing</h2><p>The bilinear pairing is defined over $(p,\mathbb{G},g,\mathbb{G}_T,e)$.</p><ul><li>$\mathbb{G}$: the base group of order $p$, a multiplicative cyclic group</li><li>$\mathbb{G}_T$: the target group of order $p$, a multiplicative cyclic group</li><li>$g$: the generator of $\mathbb{G}$</li><li>$e:\mathbb{G}\times \mathbb{G} \rightarrow \mathbb{G}_T$, the pairing operation</li></ul><p>The <strong>pairing</strong> possesses the following <u>bilinear property:</u></p>$$\forall P,Q\in \mathbb{G}: e(P^x,Q^y)=e(P,Q)^{xy}$$<p>The pairing takes two elements in the base group $\mathbb{G}$ as inputs, and outputs an element of the target group.</p><p>For example, $e(g^x,g^y)=e(g,g)^{xy}=e(g^{xy},g)$.</p><p>By the CDH assumption, we know computing $g^{xy}$ is hard given $g^x$ and $g^y$. It means that computing the product in the exponent is hard.</p><p>But with pairing, we can <u>check</u> that some element $h=g^{xy}$ without knowing $x$ and $y$. </p><p>Note that with pairing, we <strong>cannot</strong> break the CDH assumption with pairing.</p><p>It actually gives us a tool to <u>verify the product relationship in the exponent</u> rather than computing the product in the exponent.</p><p>A pairing example is the BLS signature proposed by Boneh, Lynn, and Shacham in 2001. [<a href="https://www.iacr.org/archive/asiacrypt2001/22480516.pdf">Bonth-Lynn-Shacham’2001</a>]</p><ul><li>$\text{Keygen}(p,\mathbb{G},g, \mathbb{G}_T,e):$ private key $x$ and public key $g^x$.</li><li>$\text{Sign}(sk,m)\rightarrow \sigma:H(m)^x$ where $H$ is a cryptographic hash that maps the message space to $\mathbb{G}$.</li><li>$\text{Verify}(\sigma, m):e(H(m),g^x)=e(\sigma,g)$</li></ul><p>The verification is to check the pairing equation.</p><p>The LHS is the pairing of the hash of the message and the public key. The verifier cannot compute $H(m)^x$ without knowing $x$.</p><p>The RHS is the pairing of the signature and the generator.</p><p><font color=blue><u><b><em>Security Analysis:</em></b></u></font> </p><p>The <strong>correctness</strong> holds since the verifier will pass if the signer honestly computes $H(m)^x$.</p><p>The idea of proving <strong>soundness</strong> is by contradiction.</p><p>Assuming there is an adversary that can forge a signature to pass the verification, then we can break CDH assumption using this bilinear group.</p><p>For your information, <strong>not all groups</strong> in which DL is hard are believed to <u>support efficient computable pairing,</u> but some groups especially those <u>defined by elliptic curves.</u></p><h1 id="KZG-based-on-Bilinear-Pairing"><a href="#KZG-based-on-Bilinear-Pairing" class="headerlink" title="KZG based on Bilinear Pairing"></a>KZG based on Bilinear Pairing</h1><p>Lecture 4 has introduced KZG polynomial commitment scheme [<a href="https://www.iacr.org/archive/asiacrypt2010/6477178/6477178.pdf">Kate-Zaverucha-Goldberg’2010</a>] with the multiplication notation but omits the details of pairing.</p>In this section, we use the exponent notation and consider a bilinear group defined by $p, \mathbb{G},g,\mathbb{G}_T,e$ and the univariate polynomials $\mathcal{F}=\mathbb{F}_p^{(\le d)}[X]$ with degree $\le d$.<p>Note that the degree $d$ is public to the verifier.</p><h2 id="KZG-scheme"><a href="#KZG-scheme" class="headerlink" title="KZG scheme"></a>KZG scheme</h2><p>Let’s elaborate on the four algorithms one by one.</p><p><font color=blue><u><b>keygen</b></u></font> $(\lambda, \mathcal{F})\rightarrow gp$ : compute the global parameters</p><ul><li>$\text{keygen}(\lambda, \mathcal{F})\rightarrow gp$<ul><li>Sample random $\tau\in \mathbb{F}_p$</li><li>$gp=(g,g^\tau,g^{\tau^2},\dots, g^{\tau^d})$</li><li><font color=red> delete $\tau$ !! </font> (trusted setup)</li></ul></li></ul><p>Intuitively, it uses a group where <u>DL assumption is hard so that no one can compute $\tau$.</u></p><p>It suffices to use $gp$ to <u>commit and generate the proof</u> for the <strong>prover</strong>, and to <u>check the pairing equation</u> for the <strong>verifier</strong>, without knowing the secret $\tau$.</p><p>Once the prover learns the secret $\tau$, the prover can generate fake proof to fool the verifier and break the security of the polynomial commitment scheme.</p><p>It is worth noting that the secret $\tau$ should be deleted so that it <u>requires a trusted setup</u>, which is the main drawback of KZG.</p><p>For some practical applications, it is actually hard to find a trusted party to run a trusted setup.</p><p><font color=blue><u><b>commit</b></u></font> $(gp,f)\rightarrow \text{com}_f$</p><ul><li><p>$f(x)=f_0+f_1x+f_2x^2+\dots+f_d x^d$</p></li><li><p>$gp=(g,g^\tau,g^{\tau^2},\dots, g^{\tau^d})$</p></li><li><p>Compute the commitment using $gp$</p>$$\begin{aligned}\text{com}_f&=g^{f(\tau)} \\ &=g^{f_0+f_1\tau +f_2\tau^2+\dots+f_d \tau^d} \\ &=(g)^{f_0} \cdot (g^{\tau})^{f_1} \cdot (g^{\tau^2})^{f_2}\dots (g^{\tau^d})^{f_d} \end{aligned}$$​</li></ul><p><font color=blue><u><b>eval</b></u></font> $(gp,f,u)\rightarrow v,\pi$</p><ul><li>$f(x)-f(u)=(x-u)q(x)$ as $u$ is a root of $f(x)-f(u)$</li><li>Compute $q(x)$ and $\pi=g^{q(\tau)}$ using $gp$</li></ul><p>Note that the proof can be computed without accessing $\tau$ and the proof size is only one group element.</p><p>Finally, we are going to introduce the <strong>verification part</strong>, which is the highlight of KZG.</p><p>The <u>equation that the verifier wants to check</u> is $f(x)-f(u)=(x-u)q(x)$.</p><p>A <strong>naive idea</strong> is to <u>verify the equation at the point $\tau$ in the exponent on the base $g$.</u></p>$$g^{f(\tau)-f(u)}=g^{(\tau-u)q(\tau)}$$<p>Verify has <u>received</u> $\text{com}_f=g^{f(\tau)}$ as commitment to $f$, $\pi=g^{q(\tau)}$ as eval proof, and $v=f(u)$ as evaluation from an honest prover.</p><p>Verifier can <u>compute</u> $g^{(\tau-u)}$ and $g^{q(\tau)}$ using $gp$.</p><p>Unfortunately, <u>under CDH assumption,</u> the verifier <strong>cannot</strong> compute $g^{(\tau-u)q(\tau)}$, which is the product in the exponent.</p><p>The solution is <strong>pairing</strong>, which gives us a way to <u>check the relation in the exponent of the equation instead of computing it.</u></p><p><font color=blue><u><b>verify</b></u></font> $(gp,\text{com}_f,u,v,\pi)$</p><ul><li><p>Idea: check the equation at point $\tau$</p></li><li><p>Challenge: only know $g^{\tau-u}$ and $g^{q(\tau)}$</p></li><li><p>Solution: pairing!</p></li><li><p>Pairing!</p>$$\begin{aligned}e(\text{com}_f/g^v,g)&=e(g^{\tau-u},\pi) \\ e(g,g)^{f(\tau)-f(u)}&=e(g,g)^{(\tau-u)q(\tau)}\end{aligned}$$​</li></ul><p>With pairing, the verifier can <u>check the equation at the point $u$ in the exponent.</u></p><p>The complete protocol is depicted as follows.</p><img src="https://s1.ax1x.com/2023/07/25/pCXSlNj.png" alt="KZG" style="zoom:50%;" /><h2 id="Properties-of-KZG"><a href="#Properties-of-KZG" class="headerlink" title="Properties of KZG"></a>Properties of KZG</h2><p>Let’s sum up the properties of KZG poly-commit.</p><p><font color=blue><u><b>Properties of KZG:</b></u></font></p><ul><li>Keygen: trusted setup!</li><li>Commit: $O(d)$ group exponentiations, $O(1)$ commitment size.</li><li>Eval: $O(d)$ group exponentiations where $q(x)$ can be <u>computed efficiently in linear time!</u><br>Note: The polynomial division algorithm with nearly linear time is referred to <a href="http://people.csail.mit.edu/madhu/ST12/scribe/lect06.pdf">this Lecture</a>.</li><li>Proof size: $O(1)$, 1 group element.</li><li>Verifier time: $O(1)$, 1 pairing.</li></ul><h2 id="Powers-of-tau-Ceremony"><a href="#Powers-of-tau-Ceremony" class="headerlink" title="Powers-of-tau Ceremony"></a>Powers-of-tau Ceremony</h2><p>The main drawback of KZG is the requirement of a trusted setup.</p><p>A way to relax the trusted setup is <strong>Ceremony</strong> which <u>uses a distributed generation of $gp$</u> so that no one can reconstruct the trapdoor if at least one of the participants is honest and discards their secrets.</p><p>The main idea of distributed generation is using the product of secrets from all parties.</p><p>The first party generates global parameters $gp=(g^\tau,g^{\tau^2},\dots, g^{\tau^d})=(g_1,g_2,\dots, g_d)$.</p><p>Then the next party samples random $s$, and update</p>$$\begin{aligned}gp' &=(g_1',g_2',\dots, g_d')  \\ &=(g_1^s,g_2^{s^2},\dots, g_d^{s^d}) \\ &= (g^{\tau s},g^{(\tau s)^2},\dots, g^{(\tau s)^d})\end{aligned}$$<p>with secret $\tau\cdot s$.</p><p>It introduces a secret $s$ from updating /ma</p><p>Finally, if all parties are honest, then the above procedure can generate the global parameters with the product of secrets from all parties.</p><p>Meanwhile, it is required to <u>check the correctness of $gp’$.</u></p><p><font color=blue><u><b>Correctness of :</b></u></font></p><p>(See [<a href="https://eprint.iacr.org/2022/1592">Nikolaenko-Ragsdale-Bonneau-Boneh’22</a>])</p><ol><li>The contributor knows $s$ s.t. $g_1’=(g_1)^s$.<br>It can be verified by the $\Sigma$ protocol.</li><li>$gp’$ consists of <u>consecutive powers</u> $e(g_i’,g_1’)=e(g’_{i+1},g)$, and $g_1’\ne 1$.</li></ol><p>Note that the check of $g_1’\ne 1$ guarantees that the next party <u>cannot remove the product of the preceding secrets and change it to 0.</u></p><h2 id="Security-Analysis"><a href="#Security-Analysis" class="headerlink" title="Security Analysis"></a>Security Analysis</h2><p>The <strong>completeness</strong> of KZG is evident.</p><p>The <strong>soundness</strong> of KZG is <u>based on the following assumption.</u></p> <article class="message is-info"> <div class="message-header"> <p><b> $q$-Strong Bilinear Diffie-Hellman ($q$-SBDH) assumption:</b></p> </div> <div class="message-body"> <p>Given $(p,\mathbb{G},g,\mathbb{G}_T,e)$ and $(g,g^\tau,g^{\tau^2},\dots, g^{\tau^d})$, cannot compute $e(g,g)^{\frac{1}{\tau-u}}$ for any $u$.</p> </div> </article> <p>There is an <a href="https://eprint.iacr.org/2010/215.pdf">exposition</a> [Tanaka-Saito] of reductions among the $q$-strong Diffie-Hellman problem and related problems.</p><p>The $q$-SBDH problem is reducible to the CDH problem, so $q$-SBDH is a strictly stronger assumption.</p><p><font color=blue><u><b>Proof of Soundness:</b></u></font> (By Contradiction)</p>Assume for contradiction that an adversary returns a wrong answer $v^*\ne f(u)$ but the fake proof $\pi^*$ pass the verification.<p>Then we can break $q$-SBDH assumption, which arrives to a contradiction.</p><ul><li> $e(\text{com}_f/g^{v^*})=e(g^{\tau-u},\pi^*)$ <ul><li>The pairing equation of verification holds by assumption.</li></ul></li><li> $e(g^{f(\tau)-v^{*}},g)=e(g^{\tau-u},\pi^*)$ <ul><li><p><strong>Knowledge of Exponent (KoE) assumption</strong></p><p>  Later we are going to introduce the KoE assumption, which <u>proves that the prover necessarily knows</u> $f$ s.t. $\text{com}_f=g^{f(\tau)}$ rather than a random element.</p></li><li><p>Because a random element as the commitment cannot be written in $g^{f(\tau)}$ for some $f$.</p></li><li><p>By KoE assumption, it means the prover necessarily knows an explicit $f$ so prover can compute $f(u)$.</p></li></ul></li><li><p>Define $\delta=f(u)-v^*$, which is $\ne 0$ by assumption.</p><ul><li>This is the <strong>key idea</strong> of the proof that <u>decomposes $v^*$ to the correct answer $f(u)$ and $\delta$.</u></li></ul></li><li>$\Leftrightarrow e(g^{\color{red}{f(\tau)-f(u)+f(u)-v^*}},g)=e(g^{\tau-u},\pi ^*)$</li><li><p>$\Leftrightarrow e(g^{\color{red}{(\tau-u)q(\tau)+\delta}},g)=e(g^{\tau-u},\pi ^*)$</p></li><li><p>$\Leftrightarrow e(g,g)^{(\tau-u)q(\tau)+\delta}=e(g,\pi ^*)^{\tau-u}$</p></li><li><p>Then we can <u>extract the common factor $\tau-u$ and put them outside the pairing</u> to achieve our goal of computing$e(g,g)^{\frac{1}{\tau-u}}$.</p></li><li><p>$\Leftrightarrow e(g,g)^{\delta}=(e(g,\pi ^*)/e(g,g)^{q(\tau)})^{\tau-u}$</p></li><li><p>$\Leftrightarrow e(g,g)^{\frac{\delta}{\tau -u}}=e(g,\pi^*)/e(g^{q(\tau)},g)$</p><ul><li>which breaks $q$-SBDH assumption.</li></ul></li></ul><p>Then we are going to prove knowledge soundness by the knowledge of exponent assumption.</p><h2 id="Knowledge-of-Exponent-assumption"><a href="#Knowledge-of-Exponent-assumption" class="headerlink" title="Knowledge of Exponent assumption"></a>Knowledge of Exponent assumption</h2><p>In the above security proof, we assume that the prover necessarily knows $f$ such that $\text{com}_f=g^{f(\tau)}$ rather than a random element.</p><p>We make use of the Knowledge of Exponent (KoE) assumption to refine KZG protocol, achieving  knowledge soundness.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  I excerpt the following descriptions from <a href="https://eprint.iacr.org/2004/008.pdf">[Bellare-Palacio’04]</a> to intuitively introduce the knowledge of exponent assumption. </div> </article><p>Let $q$ be a prime such that $2q+1$ is also prime (safe prime), and let $g$ be a generator of the order $q$ subgroup of $Z_{2q+1}^*$.</p><p>Suppose we are given input $q,g,g^a$ and <u>want to output a pair $(C,Y)$ such that $Y=C^a$.</u> </p><p><strong>One way</strong> to do this is to pick some $c\in \mathbb{Z}_q$, let $C=g^c$, and let $Y=(g^a)^c$. </p><p>Intuitively, it can be viewed as saying that <u>this is the “<strong>only</strong>” way to produce such a pair.</u> </p><p>The assumption captures this by saying that <u>any adversary outputting such a pair must “know” an exponent $c$ such that $g^c=C$.</u> </p><p>The formalization asks that there be an <strong>“extractor”</strong> that can return $c$.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Knowledge of Exponent assumption :</strong> </p> </div> <div class="message-body"> <p>For any adversary $A$ that takes input $q,g,g^a$ and returns $(C,Y)$ with $Y=C^a$, there exists an “extractor” $\bar{A}$, which given the same inputs as $A$ returns $c$ such that $g^c=C$.</p> </div> </article> <h2 id="KZG-with-Knowledge-Soundness"><a href="#KZG-with-Knowledge-Soundness" class="headerlink" title="KZG with Knowledge Soundness"></a>KZG with Knowledge Soundness</h2><p>Having this assumption, we can refine the KZG protocol.</p><p>The <strong>goal</strong> is to prove that <u>prover necessarily “knows” an exponent $f(\tau)$</u> such that $\text{com}_f=g^{f(\tau)}$.</p><p>We’d like to <u>ask the prover to generate such a pair $g^{f(\tau)}$ and $g^{\alpha f(\tau)}$ given “ $gp$ and $(gp)^{\alpha}$”.</u></p><p>The sketch of design is as follows.</p><p><font color=blue><u><b>Sketch:</b></u></font></p><ul><li>$gp=(g,g^\tau,g^{\tau^2},\dots, g^{\tau^d})$</li><li>Sample random $\alpha$, compute $g^\alpha,g^{\alpha \tau},g^{\alpha \tau^2},\dots, g^{\alpha \tau^d}$</li><li>$\text{com}_f=g^{f(\tau)}$ and $\text{com}’_f=g^{\alpha f(\tau)}$.</li><li>If $e(\text{com}_f,g^\alpha)=e(\text{com}_f’,g)$, there exists an extractor $E$ that extracts $f$ s.t. $\text{com}_f=g^{f(\tau)}$.</li></ul><p>Let’s elaborate on the details.</p>In addition to the original global parameters $gp$, we need to sample random $\alpha$ and compute $(gp)^\alpha:\{g^\alpha, g^{\alpha \tau}, g^{\alpha \tau^2},\dots, g^{\alpha\tau^d}\}$, which is raising each element of the original $gp$ to random $\alpha$.<p>Note that the random $\alpha$ is <u>secret</u> as $\tau$.</p><p>The prover commits to $f$ by computing such a pair, $\text{com}_f=g^{f(\tau)}$ and $\text{com}_f’=g^{\alpha f(\tau)}$.</p><p>Finally, the verifier can <u>check the relation of these two commitments in the exponent by pairing.</u></p><p>If the pairing equation $e(\text{com}_f,g^\alpha)=e(\text{com}_f’,g)$ holds, by KoE assumption, there exists an extractor $E$ that extracts $f$ such that $\text{com}_f=g^{f(\tau)}$.</p><p>Here is the KZG scheme with knowledge soundness.</p><p><font color=blue><u><b>KZG with Knowledge Soundness:</b></u></font></p><ul><li>$\text{Keygen}$: $gp$ includes both $g,g^\tau,g^{\tau^2},\dots, g^{\tau^d}$ and $g^\alpha,g^{\alpha\tau},g^{\alpha\tau^2},\dots, g^{\alpha\tau^d}$ where $\tau$ and $r$ are both secret and required to be deleted. (trusted setup)</li><li>$\text{Commit}$: $\text{com}_f=g^{f(\tau)}$ and $\text{com}_f’=g^{\alpha f(\tau)}$.</li><li>$\text{Verify}$: additionally checks $e(\text{com}_f,g^\alpha)=e(\text{com}_f’,g)$.</li></ul><p>The idea of proving knowledge soundness is to extract $f$ in the first step by the KoE assumption.</p><p>But it brings an <strong>overhead</strong> that the <u>prove size</u> is two group elements and the <u>verifier time</u> involves two pairing equations.</p><p><strong>Generic Group Model (GGM)</strong> [Shoup’97, Maurer’05] can <u>replace the KoE assumption and reduce the commitment size in KZG.</u></p><p>Informally speaking, the adversary <u>is only given an oracle to compute the group operation.</u> E.g., given $g,g^\tau,g^{\tau^2},\dots, g^{\tau^d}$, the adversary can only compute their linear combinations.</p><p>As a result, the adversary cannot sample a random element that happens to be the power of another group element.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> See book “A Graduate Course in Applied Cryptography” by Dan Boneh and Victor Shoup, section 16.3 for more details. </div> </article><h2 id="Variants-of-KZG"><a href="#Variants-of-KZG" class="headerlink" title="Variants of KZG"></a>Variants of KZG</h2><h3 id="Multivariate-poly-commit"><a href="#Multivariate-poly-commit" class="headerlink" title="Multivariate poly-commit"></a>Multivariate poly-commit</h3><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Reference:  <a href="https://eprint.iacr.org/2011/587"> [Papamanthou-Shi-Tamassia’13]</a> </div> </article><p>KZG can be generalized to the multivariate polynomial commitment.</p><p>The <strong>key idea</strong> is the following equation:</p>$$f(x_1,\dots, x_k)-f(u_1,\dots,u_k)=\sum_{i=1}^k (x_i-u_i) q_i(\vec{x})$$<p>where $q(\vec{x})$ is a multivariate polynomial.</p><ul><li><strong>Keygen</strong>: sample $\tau_1,\tau_2,\dots, \tau_k$, each representing one variable, and compute $gp$ as $g$ raised to all possible monomials of $\tau_1,\tau_2,\dots, \tau_k$.<br>e.g. $2^k$ monomials for multilinear polynomials.</li><li><strong>Commit</strong>: $\text{com}=g^{f(\tau_1,\tau_2,\dots, \tau_k)}$</li><li><strong>Eval</strong>: compute $\pi_i=g^{q_i(\vec{\tau})}$.<br>Note that the proof consists of multiple elements.</li><li><strong>Verify</strong>: $e(\text{com}_f/g^v,g)=\prod_{i=1}^ke(g^{\tau_i-u_i},\pi_i)$ </li></ul><p>Let $N\le 2^k$ denote the total size of the polynomial.</p><p>The <u>proof size and verifier time</u> are $O(\log N)$.</p><h3 id="Achieving-zero-knowledge"><a href="#Achieving-zero-knowledge" class="headerlink" title="Achieving zero-knowledge"></a>Achieving zero-knowledge</h3><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Reference: [ZGKPP’2018] </div> </article><p>We say a poly-commit scheme is ZK if there is a simulator without knowing the polynomial can simulate the view of the verifier.</p><p>The <u>plain KZG is not ZK.</u> E.g. the commit algorithm $\text{com}_f=g^{f(\tau)}$ is <u>deterministic</u>. </p><p>The <strong>solution to achieve zero-knowledge</strong> is <u>masking the commitment and proof with randomizers.</u> </p><ul><li><strong>Commit</strong>: $\text{com}_f=g^{f(\tau)+r\eta}$</li><li><strong>Eval</strong>: the main idea is to check <u>whether a polynomial with randomizers is of a certain form.</u><ul><li><strong>bivariate polynomial with randomizer:</strong> $\begin{aligned} f(x)+ry-f(u) =(x-u)(q(x)+r’y)+y(r-r’(x-u))\end{aligned}$</li><li><strong>proof</strong>: $\pi=g^{q(\tau)+r’\eta},g^{r-r’(\tau-u)}$</li></ul></li></ul><p>Let’s elaborate on the details.</p><p>First look at the <strong>commitment</strong> $\text{com}_f=g^{f(\tau)+r\eta}$.</p><p>The commitment to $f$ is <u>randomized by $r$ randomly chosen by the prover.</u></p><p>Note that  $\eta$ is another <u>secret sampled in the trusted setup</u> so $g^\eta$ is <u>included in the global parameters</u>, which enables the prover to compute it.</p><p>With random $r$ in the commitment, the <strong>idea of evaluation</strong> is to <u>check the randomized bivariate polynomial is of a certain form:</u> </p><p>$$<br>\begin{aligned} f(x)+ry-f(u) =(x-u)(q(x)+r’y)+y(r-r’(x-u))\end{aligned}<br>$$</p><p>Likewise the check $f(x)-f(u)=(x-u)q(x)$ in the univariate polynomial, we can <u>check the above relation in the exponent with pairing</u> yet it is split into two terms of products.</p><p>Consequently, the proof consists of <u>two elements,</u> the first evaluating $q(x)+r’y$ and the second evaluating $r-r’(x-u)$ at point $(x=\tau,y=\eta)$.</p><p>Note that the verifier can compute $g^{\tau-u}$ and $g^\eta$ using $gp$.</p><p>Finally, the verifier can check the relation in the pairing equation.</p><h3 id="Batch-opening-single-polynomials"><a href="#Batch-opening-single-polynomials" class="headerlink" title="Batch opening: single polynomials"></a>Batch opening: single polynomials</h3><p>Another variant of KZG is <strong>batch opening</strong> or <strong>batch proofs</strong>.</p><p>Let’s consider the <u>batch proofs for a single polynomial</u> in which the prover wants to prove $f$ at $u_1,\dots, u_m$ for $m&lt;d$.</p><p>Note that $m&lt;d$ is necessary since $m (&gt;d)$ points can interpolate the polynomial in clear.</p><p>The key idea is to extrapolate $f(u_1),\dots, f(u_m)$ to get $h(x)$ such that $h(u_i)=f(u_i)$.</p><p>Recall that in Lecture 5 we introduce a vanishing polynomial in <u>ZeroTest on the set $\Omega$.</u> </p><p>It’s actually the same that we can <u>prove $f(x)-h(x)=0$ on the set ${u_1,\dots, u_m}$ using ZeroTest:</u></p><p>$$<br>f(x)-h(x)=\prod_{i=1}^m (x-u_i) q(x)<br>$$</p><p>$f(x)-h(x)$ is zero over the set ${u_1,\dots, u_m}$ if and only if it is divisible by the vanishing polynomial $\prod_{i=1}^m (x-u_i)$.</p><p>The <strong>prover</strong> needs to compute the quotient polynomial $q(x)$ and generates the proof $\pi =g^{q(\tau)}$, a single group element as the batch proofs.</p><p>The <strong>verifier</strong> checks the pairing equation</p> $$e(\text{com}_f/g^{h(\tau)},g)=e(g^{\prod_{i=1}^m(\tau-u_i)},\pi)$$<p>where $g^{h(\tau)}$ and $g^{\prod_{i=1}^m(\tau-u_i)}$ can be computed using $gp$.</p><p>Note that the <u>proof size is only one group element</u> but the <u>verifier time grows linearly in the number of evaluations.</u></p><h3 id="Batch-opening-multiple-polynomials"><a href="#Batch-opening-multiple-polynomials" class="headerlink" title="Batch opening: multiple polynomials"></a>Batch opening: multiple polynomials</h3><p>Then we extend <u>batch opening for multiple polynomials (and multiple evaluations)</u> where the prover wants to prove $f_i(u_{i,j})=v_{i,j}$ for $i\in [n]$, $j\in [m]$.</p><p>The <strong>key idea</strong> kind of similar to the single polynomial case that <u>extrapolates multiple polynomials</u>. Specifically, the extrapolates $f_i(u_1),\dots, f_i(u_m)$ to get $h_i(x)$ for each $i\in [n]$.</p><p>For each polynomial, we have $f_i(x)-h_i(x)=\prod_{i=1}^m (x-u_m)q_i(x)$.</p><p>The prover needs to <u>compute every quotient polynomial $q_i(x)$ combine them via a random linear combination.</u></p><p>Then prover can compute the <strong>proof</strong> <u>as $g$ to the equation of the random linear combination, which</u> is a <u>single element</u>.</p><p>The verifier can check the relation in the exponent using bilinear pairing.</p><hr><p>KZG and its variants play an important role in building SNARKs.</p><p>Plonk poly-IOP is combined with the univariate version of KZG to build SNARK for general circuits.</p><img src="https://s1.ax1x.com/2023/07/26/pCj8ax1.png" alt="Plonk" style="zoom:33%;" /><p>vSQL and Libra both combine the Sumcheck protocol (Lecture 4) and the multivariate KZG.</p><img src="https://s1.ax1x.com/2023/07/26/pCj8U2R.png" alt="vSQL, Libra" style="zoom:33%;" /><hr><p>Before ending up discussing the poly-commit scheme based on the bilinear pairing, let’s sum up the <strong>pros and cons</strong> of KZG poly-commit.</p><p>The <strong>pros</strong> contains that the <u>commitment and proof size</u> is $O(1)$, 1 group element and the <u>verifier time</u> involves $O(1)$ pairing.</p><p>The main <strong>cons</strong> is that KZG <u>requires a trusted setup</u> to generate $gp$.</p><p>The <u>trusted setup is a fundamental problem</u> to solve although the ceremony process relaxes trust a little bit and it is good for many applications in practice.</p><h1 id="Bulletproofs-based-on-discrete-log"><a href="#Bulletproofs-based-on-discrete-log" class="headerlink" title="Bulletproofs based on discrete-log"></a>Bulletproofs based on discrete-log</h1><p>Bulletproofs is proposed by [<a href="https://eprint.iacr.org/2017/1066.pdf">BCCGP’16</a>] and refined by [BBBPWM’18] to build SNARKs using a transparent setup.</p><p>Moreover, they proposed an inner product protocol and a special protocol for range proof that can be generalized to build SNARKs for a general arithmetic circuit.</p><h2 id="Poly-commit-based-on-Bulletproofs"><a href="#Poly-commit-based-on-Bulletproofs" class="headerlink" title="Poly-commit based on Bulletproofs"></a>Poly-commit based on Bulletproofs</h2><p>In this section, we <u>rephrase the Bulletproofs as a poly-commit</u> because it still shows the key idea of the reduction but significantly simplifies the protocol.</p><p>The <strong>transparent setup</strong> samples random $gp=(g_0,g_1,g_2,\dots, g_d)\in \mathbb{G}$ <u>without the trapdoor</u> $\tau$ whose size is still linear to the degree $d$.</p><p>The prover <strong>commits</strong> to $f(x)=f_0+f_1x+f_2x^2+\dots +f_dx^d$ as usual, which <u>raises each element of $gp$ to the corresponding</u> coefficients of the polynomials and multiply them together to get a <u>single element.</u></p><p>$$<br>\text{com}_f=g_0^{f_0}g_1^{f_1}g_2^{f_2}\dots g_d^{f_d}<br>$$</p><p>Note that the random term is omitted.</p><p>Let’s describe the <strong>high-level idea of Bulletproofs reduction</strong> using <u>an example of a degree-3 polynomial.</u></p><p>After receiving the commitment to $f$ from the prover, verifier queries at $u$. Prover replies with the evaluation $v$.</p><p>The <strong>key idea</strong> is to <u>reduce the claim of evaluating $v$ at point $u$ for the polynomial $f$</u> inside the commitment $\text{com}_f$ to <u>a new claim about a new polynomial of only half of the size.</u></p><p>In our example, we reduce the original polynomial of degree 3 to a new polynomial of degree only 1 with only two coefficients $f_0’$ and $f_1’$.</p><p>Furthermore, the verifier will receive a new instance of the commitment $\text{com}_{f’}$ to this new polynomial of only half of the size.</p><p>Then we <u>keep doing recursively</u> to reduce the polynomial of degree $d/2$ to a new polynomial of degree $d/4,d/8,\dots,$  to a constant degree.</p><p>Finally, <u>in the last round,</u> the <strong>prover</strong> can <u>just send a polynomial of constant size to the verifier directly</u>, and the <strong>verifier</strong> <u>opens the polynomial and checks the evaluation</u> of the last round is indeed true.</p><p>It completes the entire reduction and guarantees that the claim of the evaluation of $v$ for the original polynomial is correct.</p><p>The main <strong>challenge of the reduction</strong> is how to <u>go from the original polynomial to a new polynomial of half of the size.</u></p><p>We<font color=red> <b>can’t have</b> </font> the <u>prover commit to a random polynomial of half of the size without any relationship.</u> Otherwise, the prover can cheat and lie about the evaluation since this new polynomial has no relation to the original polynomial.</p><p>It <u>has to check the relationship between the two polynomials.</u></p><img src="https://s1.ax1x.com/2023/07/26/pCj8J54.png" alt="High-level idea" style="zoom:43%;" /><p>Let’s elaborate on the details of the reduction.</p><p><strong>Prover</strong> first sends the evaluation $v=f_0+f_1u+f_2u^2+f_3u^3$ at point $u$.</p><p>A <strong>common way of reduction for the prover</strong> is reducing the polynomial to two polynomials of half of size. </p><p>Then <strong>prover</strong> <u>evaluates these two reduced polynomials</u> at point $u$ to get $v_L=f_0+f_1u$ and $v_R=f_2+f_3u$ such that $v=v_L+v_Ru^2$.</p><p>It is safe for the <strong>verifier</strong> to <u>believe that $v$ is the correct evaluation if and only if $v_L$ and $v_R$ are correctly evaluated for the reduced polynomials.</u></p><p>So <strong>prover</strong> also <u>commits to the two reduced polynomials</u> with two cross terms $L=g_2^{f_0}g_3^{f_1}$ and $R=g_0^{f_2}g_1^{f_3}$, which uses $g_2,g_3$ as bases to commit to the left reduced polynomial and $g_0,g_1$ to commit to the right reduced polynomial. </p><p>As depicted follows, prover sends two commitments $L$ and $R$  and two evaluations $v_L$ and $v_R$ on the two reduced polynomials.</p><img src="https://s1.ax1x.com/2023/07/26/pCj8NG9.png" alt="Poly-commitment based on Bulletproofs" style="zoom:45%;" /><p>But these two polynomials are <u>actually temporary reduced polynomials.</u></p><p>The <strong>actual reduced polynomial</strong> is <u>a single polynomial with two coefficients</u> $rf_0+f_2$ and $rf_1+f_3$, which is <u>a randomized linear combination</u> of the original coefficients where the <u>randomness $r$ is sampled by the verifier.</u></p><p>This <strong>new claim</strong> <strong>about this new reduced polynomial</strong> actually <u>combines two claims about the old temporary polynomials</u> through randomized linear combinations.</p><p>And the <strong>claim</strong> <strong>about the evaluation</strong> <u>in the next round</u> is to altered to $v’=rv_L+v_R$.</p><p>The only <strong>remaining challenge</strong> is to <u>generate the commitment</u> of this randomized reduced polynomial.</p><p>We <font color=red> can’t </font> let the prover commit $g_0^{rf_0+f_2}g_1^{rf_1+f_3}$ with the original global parameters because the <u>transparent setup doesn’t know the relationship ( defined by $r$ )</u> between the two polynomials.</p><p>In order to address the issue, Bulletproofs proposed a clever design to allow the verifier to compute the new commitment from the old commitments with the help of the commitments to the temporary polynomials.</p><p>Recall that $\text{com}_f=g_0^{f_0}g_1^{f_1}g_2^{f_2}\dots g_d^{f_d}$, $L=g_2^{f_0}g_3^{f_1}$ and $R=g_0^{f_2}g_1^{f_3}$. Then <strong>verifier</strong> can <u>compute the commitment</u> $\text{com}’$ from $L$ and $R$ such that </p>$$\begin{aligned} \text{com}' &=L^r\cdot \text{com}_f \cdot R^{r^{-1}}  \\ &=g_0^{f_0+r^{-1}f_2}g_2^{rf_0+f_2}\cdot g_1^{f_1+r^{-1}f_3}g_3^{rf_1+f_3} \\ &= (g_0^{r^{-1}}g_2)^{rf_0+f_2} \cdot (g_1^{r^{-1}}g_3)^{rf_1+f_3}\end{aligned}$$<p>where the global parameter is updated to $gp’=(g_0^{r^{-1}}g_2,g_1^{r^{-1}}g_3)$.</p><p>The <u>last equation holds by extracting the common factor</u> to commit to the new polynomial with coefficients $rf_0+f_2$ and $rf_1+f_3$.</p><p>And the verifier can compute the new global parameters related to the new commitment, which allows the verifier to check some pairing equations. </p><p>Let’s sum up the reduction procedure.</p><p><font color=blue><u><b>Poly-commitment based on Bulletproofs:</b></u></font></p><p><u><b><i>Recurse $\log d$ rounds:</i></b></u> </p><ul><li>Eval: (Prover)<ol><li>Compute $L,R,v_L,v_R$</li><li>Receive $r$ from the verifier, reduce $f$ to $f’$ of degree $d/2$</li><li>Update the bases $gp’$</li></ol></li><li>Verify: (Verifier)<ol><li>Check $v=v_L+v_Ru^{d/2}$</li><li>Generate $r$ randomly</li><li>Update $\text{com}’=L^r\cdot \text{com}_f\cdot R^{r^{-1}}$, $gp’$, and $v’=rv_L+v_R$</li></ol></li></ul><p><u><b><i>In the last round:</i></b></u> </p><ol><li>The prover sends the constant-size polynomial to the verifier.</li><li>The verifier checks the commitment and the evaluation is correct.</li></ol><p>Note that the above protocol can be rendered non-interactive via Fiat Shamir.</p><h2 id="Properties-of-Bulletproofs"><a href="#Properties-of-Bulletproofs" class="headerlink" title="Properties of Bulletproofs"></a>Properties of Bulletproofs</h2><p>Let’s sum up the properties of poly-commitment based on Bulletproofs.</p><p><font color=blue><u><b>Properties of Bulletproofs:</b></u></font> </p><ul><li>Keygen: $O(d)$, transparent setup!</li><li>Commit: $O(d)$ group exponentiations, $O(1)$ commitment size.</li><li>Eval: $O(d)$ group exponentiations</li><li>Proof size: $O(\log d)$<br>In each round, the prover sends 4 elements as proof.</li><li>Verifier time: $O(d)$<br>The verifier has to recursively update the global parameters the number of which falls geometrically so the verifier time depends on the first round that is nearly linear in $d$.</li></ul><h1 id="Other-works"><a href="#Other-works" class="headerlink" title="Other works"></a>Other works</h1><h2 id="Hyrax"><a href="#Hyrax" class="headerlink" title="Hyrax"></a>Hyrax</h2><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Reference: [Wahby-Tzialla-shelat-Thaler-Walfish’18] </div> </article><p>The <strong>main drawback</strong> of Bulletproofs is the <u>linear verifier time.</u></p><p>Hyrax improves the <u>verifier time</u> to $O(\sqrt{d})$ by representing the coefficients as a 2-D matrix with <u>proof size</u> $O(\sqrt{d})$.</p><p>In fact, the product of verifier time and the proof size is linear in $d$ so the proof size can be reduced to $\sqrt[n]{d}$ while the verifier time is $d^{1-1/n}$.</p><h2 id="Dory"><a href="#Dory" class="headerlink" title="Dory"></a>Dory</h2><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Reference: [Lee’2021] </div> </article><p>Dory improves the <u>verifier time</u> to $O(\log d)$ <u>without any asymptotic overhead on other parts.</u></p><p>It’s a nice improvement over the poly-commitment based on Bulletproofs.</p><p>The <strong>key idea</strong> is <u>delegating the structured verifier computation to the prover</u> using inner pairing product arguments. [BMMTV’2021]</p><p>It also improves the prover time to $O(\sqrt{d})$ exponentiations plus $O(d)$ field operations.</p><h2 id="Dark"><a href="#Dark" class="headerlink" title="Dark"></a>Dark</h2><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Reference: [Bünz-Fisch-Szepieniec’20] </div> </article><p>Dark achieves $O(\log d)$ proof size and verifier time based on the cryptographic primitive of group of unknown order.</p><hr><p>Let’s end up with a summary of all works mentioned in this lecture.</p><img src="https://s1.ax1x.com/2023/07/26/pCj8tPJ.png" alt="Summary of PCS" style="zoom:33%;" />]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
    In this &lt;a href=&quot;/categories/Cryptography-ZKP&quot;&gt;series&lt;/a&gt;, I will learn &lt;strong&gt;Zero Knowledge Proofs (ZKP)&lt;/strong&gt; on this &lt;a href=&quot;https://zk-learning.org/&quot;&gt;MOOC&lt;/a&gt;, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and &lt;strong&gt;Yupeng Zhang&lt;/strong&gt;. 
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;



&lt;p&gt;&lt;strong&gt;Topics:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KZG poly-commit based on bilinear pairing&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;KZG scheme&lt;/li&gt;
&lt;li&gt;Powers-of-tau Ceremony&lt;/li&gt;
&lt;li&gt;Security Analysis&lt;/li&gt;
&lt;li&gt;Knowledge of exponent assumption&lt;/li&gt;
&lt;li&gt;Variants: multivariate; ZK; batch openings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bulletproofs poly-commit based on discrete logarithm&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-ZKP" scheme="https://f7ed.com/categories/Cryptography-ZKP/"/>
    
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="ZKP" scheme="https://f7ed.com/tags/ZKP/"/>
    
      <category term="Poly-commit" scheme="https://f7ed.com/tags/Poly-commit/"/>
    
      <category term="KZG" scheme="https://f7ed.com/tags/KZG/"/>
    
      <category term="Bulletproofs" scheme="https://f7ed.com/tags/Bulletproofs/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-ZKP」: Lec5-The Plonk SNARK</title>
    <link href="https://f7ed.com/2023/07/21/zkp-lec5/"/>
    <id>https://f7ed.com/2023/07/21/zkp-lec5/</id>
    <published>2023-07-20T16:00:00.000Z</published>
    <updated>2023-08-15T07:51:12.471Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>    In this <a href="/categories/Cryptography-ZKP">series</a>, I will learn <strong>Zero Knowledge Proofs (ZKP)</strong> on this <a href="https://zk-learning.org/">MOOC</a>, lectured by <strong>Dan Boneh</strong>, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. <br>Any corrections and advice are welcome. ^ - ^</div></article><p><strong>Topics:</strong> </p><ul><li>Preprocessing SNARK</li><li><strong>KZG Poly-Commit Scheme</strong></li><li>Proving Properties of committed polys<ul><li>ZeroTest</li><li>Product Check</li><li>Permutation Check</li><li>Prescribed Permutation Check</li></ul></li><li><strong>Plonk IOP for General Circuits</strong></li></ul><span id="more"></span><h1 id="What-is-SNARK"><a href="#What-is-SNARK" class="headerlink" title="What is SNARK?"></a>What is SNARK?</h1><p>Before proceeding to today’s topic, let’s review what is SNARK.</p><p>Note that this part is well explained in Lecture 2.</p><h2 id="preprocessing-NARK"><a href="#preprocessing-NARK" class="headerlink" title="preprocessing NARK"></a>preprocessing NARK</h2><p>NARK is <strong>Non-interactive ARgument of Knowledge.</strong></p><p>Given a public arithmetic circuit $C(x,w)\rightarrow \mathbb{F}$ where $x$ is the public statement in $\mathbb{F}^n$ and $w$ is the secret witness in $\mathbb{F}^m$, a <strong>preprocessing (setup) algorithm</strong> </p><p>$$<br>S(C)\rightarrow \text{ public parameters } (pp,vp)<br>$$</p><p> <u>takes a description of the circuit as inputs</u>, and <u>outputs public parameters</u> $(pp,vp)$ for prover and verifier.</p><p>NARK works as follows.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZmOx.png" alt="NARK" style="zoom:33%;" /><p>A <strong>preprocessing NARK</strong> is a triple of algorithms $(S,P,V)$.</p><ul><li>$S(C)\rightarrow$ public parameters $(pp,vp)$ for prover and verifier.</li><li>$P(pp,x,w)\rightarrow$  proof $\pi$.</li><li>$V(vp,x,\pi)\rightarrow$ accept or reject.</li></ul><p>Note that all algorithms and the adversary have access to a random oracle.</p><p>The informal requirements of NARK are completeness and adaptively knowledge soundness. </p><p><strong>Completeness</strong> means that <u>an honest prover always convinces the verifier to accept the answer,</u> i.e.</p>$$\forall x,w: C(x,w)=0 \text{ then } \operatorname{Pr}[V(vp,x,P(pp,x,w))=\text{accept}]=1$$<p>Adaptively <strong>knowledge soundness</strong> means that <u>if the verifier accepts the proof, then the prover must know a witness such that $C(x,w)=0$.</u>  In other words, there exists an extractor $E$ that can extract a valid $w$ from $P$.</p><p><strong>Zero-knowledge</strong> suggests that $(C,pp,vp,x,\pi)$ reveals nothing new about $w$. Note that the privacy requirement, i.e. zero knowledge, in NARK is optional. So there is a <u>trivial NARK</u> in which the prover just sends $w$ as proof and the verifier just rerun the circuit and check if $C(x,w)=0$.</p><h2 id="SNARK-a-Succinct-ARgument-of-Knowledge"><a href="#SNARK-a-Succinct-ARgument-of-Knowledge" class="headerlink" title="SNARK: a Succinct ARgument of Knowledge"></a>SNARK: a Succinct ARgument of Knowledge</h2><p>SNARK is a <u>NARK (complete and knowledge sound) that is succinct.</u> </p><p>zk-SNARK is a SNARK that is also zero knowledge, meaning that the <u>SNARK proof reveals nothing about the witness.</u></p><p>A <strong>preprocessing SNARK</strong> is a triple of algorithms $(S,P,V)$.</p><ul><li>$S(C)\rightarrow$ public parameters $(pp,vp)$ for prover and verifier.</li><li>$P(pp,x,w)\rightarrow$  <strong>short</strong> proof $\pi$; $\text{len}(\pi)=\text{sublinear}(|w|)$.</li><li>$V(vp,x,\pi)\rightarrow$ <strong>fast to verify</strong>; $\text{time}(V)=O_\lambda(|x|,\text{sublinear}(|C|))$.</li></ul><p>SNARK requires the <strong>length of proof</strong> to be <u>sublinear in the length of the witness,</u> and the <strong>verify runtime</strong> to be <u>linear in the statement and sublinear in the size of the circuit.</u></p><p>Being linear in the statement $x$ means that the verifier at least read the statements.</p><p>Furthermore, a <strong>strongly succinct preprocessing NARK</strong> is not only sublinear <u>but logarithmic in the size of the circuit.</u></p><ul><li>$S(C)\rightarrow$ public parameters $(pp,vp)$ for prover and verifier.</li><li>$P(pp,x,w)\rightarrow$  <strong>short</strong> proof $\pi$; $\text{len}(\pi)=O_\lambda(\log(|C|))$.</li><li>$V(vp,x,\pi)\rightarrow$ <strong>fast to verify</strong>; $\text{time}(V)=O_\lambda(|x|,\log (|C|))$.</li></ul><p>Note that the verifier runtime is logarithmic in the size of the circuit, which implies that the <u>verifier even does not know what the underlying circuit is.</u></p><p>An <strong>insight</strong> is that the <u>verifier parameter $vp$ is the short “summary” of the circuit</u> so the verifier is able to verify the evaluations of the circuit at an arbitrary input $x$.</p><p>That’s the reason why we need the preprocessing or setup.</p><p>It is worth noting that the trivial SNARK is not a SNARK.</p><h2 id="Types-of-preprocessing-Setup"><a href="#Types-of-preprocessing-Setup" class="headerlink" title="Types of preprocessing Setup"></a>Types of preprocessing Setup</h2><p>The setup for a circuit $C$ is an algorithm $S(C;r)\rightarrow \text{ public parameters}(pp,vp)$, which takes the circuit and random bits as inputs and generates public parameters for the prover and the verifier.</p><p>The types of setup are more detailed.</p><ul><li><strong>trusted setup per circuit:</strong> random $r$ must be <u>kept secret from the prover</u>.<ul><li>$S(C;r)$</li></ul></li><li><strong>trusted but universal (updatable) setup:</strong> secret $r$ is <u>independent of $C$.</u><ul><li>$S=(S_{init},S_{index})$</li><li>$S_{init}(\lambda;r)\rightarrow gp$: onetime setup, secret $r$</li><li>$S_{index}(gp,C)\rightarrow (pp,vp)$: deterministic algorithm</li></ul></li><li><strong>transparent setup:</strong> does not use secret data<ul><li>$S(C)$</li></ul></li></ul><p>In the <strong>trusted setup</strong>, random $r$ must be kept secret from the prover, meaning that it has to be run for every circuit.</p><p>Once the <u>prover learns $r$, it allows the prover to prove false statements.</u> In practice, $r$ will be destroyed so that nobody can ever know $r$. Sometimes, it is called the <strong>trusted setup ceremony</strong>.</p><p>In the <strong>trusted but universal setup</strong>, it splits the setup into two parts. $S_{init}$ is a <strong>one-time setup</strong> that <u>does not take the circuit as input</u> and generates <u>global parameters</u> $gp$. Note that $r$ is required to be secret as well.</p><p>Then $S_{index}$ is a <u>deterministic algorithm executed for every circuit</u> so it <u>takes the circuit and $gp$</u> as inputs and generates public parameters for the prover and the verifier.</p><p>In the <strong>transparent setup</strong>, $S(C)$ does not use secret data.</p><h1 id="General-Paradigm-for-Building-SNARK"><a href="#General-Paradigm-for-Building-SNARK" class="headerlink" title="General Paradigm for Building SNARK"></a>General Paradigm for Building SNARK</h1><p>The general paradigm for building SNARK is made up of two steps.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZMTO.png" alt="General paradigm for building SNARK" style="zoom:33%;" /><p>One is the <strong>functional commitment scheme</strong>, which is a cryptographic object that depends on some assumptions.</p><p>The other is the compatible <strong>interactive oracle proof (IOP)</strong>. IOP is an information-theoretic object, which <u>provides unconditional security without any assumptions.</u></p><p>To be precise, they are not separate steps. </p><p>For example, <strong>using poly-IOP</strong>, we can <u>boost polynomial functional commitment for $\mathbb{F}_p^{(\le d)}[X]$ to build SNARK for any circuit $C$ where $|C|&lt;d$.</u></p><h2 id="Polynomial-Commitments"><a href="#Polynomial-Commitments" class="headerlink" title="Polynomial Commitments"></a>Polynomial Commitments</h2><p>Review the polynomial commitments introduced in the last lecture.</p><p>Prover commits to a univariate polynomial $f(X)$ in $\mathbb{F}_p^{(\le d)}[X]$ where the variable $X$ has degree at most $d$.</p><p>Later the verifier can request to know the evaluation of this polynomial at a specific point. </p><p>In other words, for public $u,v\in \mathbb{F}_p$, the <u>prover can convince the verifier that the committed polynomial satisfies</u> </p><p>$$<br>f(u)=v \text{ and deg}(f)\le d<br>$$</p><p>Note that the verifier has the upper bound of the degree $d$, the commitment received from the prover, and $u,v$.</p><p>In SNARK, the evaluation proof size and verifier time should be logarithmic in the degree, i.e. $O_\lambda(\log d)$.</p><h3 id="Equality-Test-Protocol"><a href="#Equality-Test-Protocol" class="headerlink" title="Equality Test Protocol"></a>Equality Test Protocol</h3><p>Recall the observation that if $p,q$ are univariate polynomials of degree at most $d$, then  $\operatorname{Pr}_{r\overset{\$}\leftarrow \mathbb{F}}[p(r)=q(r)]\le d/p$. </p>   If $f(r)=g(r)$ for a random $r \overset{\$}\leftarrow \mathbb{F}_p$, then $f=g$ w.h.p.Having the polynomial commitments, we can construct the equality test protocol as follows.  <img src="https://s1.ax1x.com/2023/07/21/pCbZK0K.png" alt="Equality Test Protocol" style="zoom:33%;" /><ol><li>Prover has committed to two polynomial $f,g$, so verifier has two commitments depicted in two boxes.</li><li>Verifier samples a random coin $r$ in $\mathbb{F}_p$ and sends to prover.<br>Hence, it is a public coin.</li><li>Prover sends the evaluations $y,y’$ and proof that $y=f(r)$ and $y’=g(r)$.</li><li>Verifier accepts if $y=y’$ and the proof is valid.</li></ol><h3 id="Fiat-Shamir-Transform"><a href="#Fiat-Shamir-Transform" class="headerlink" title="Fiat-Shamir Transform"></a>Fiat-Shamir Transform</h3><p>Note that the equality teat protocol is interactive but the verifier just sends coins to prover.</p><p>A solution to making it a SNARK (non-interactive) is the <strong>Fiat-Shamir transform</strong>, which <u>can render a public-coin interactive protocol non-interactive.</u></p><p>Let $H:M\rightarrow R$ be a cryptographic hash function.</p><p>The <strong>main idea</strong> is <u>having prover generates verifier’s random bits on its own using $H$,</u> i.e. $r\leftarrow H(x)$.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZum6.png" alt="Fiat-Shamir Transform" style="zoom:43%;" /><p>Prover and verifier can compute $r\leftarrow H(x)$, making it <strong>non-interactive.</strong></p><p>The <strong>completeness</strong> is evident and one has to <u>prove knowledge soundness.</u></p><p><font color=blue><u><b>Thm via Fiat-Shamir Transform:</b></u></font> </p><p>This is a SNARK if ( i ) $d/p$ is negligible where $f,g\in \mathbb{F}_p^{(\le d)}[X]$, and ( ii ) $H$ is modeled as a random oracle.</p><p>In practice, we use SHA256 as $H$.</p><p>The <strong>succinctness</strong> holds by a succinct poly commitment scheme.</p><p>Note that it is <strong>not zk</strong> since verifier learns the value of $f(r),g(r)$ that he didn’t know before.</p><hr><p>In this blog, we’ll introduce a specific polynomial commitment scheme — <strong>KZG’10</strong> with a trusted setup. </p><p>KZG requires <strong>a trusted but universal setup</strong> that generates global parameters once, then it can commit to an arbitrary polynomial of bounded degree $d$.</p><h2 id="mathcal-F-IOP"><a href="#mathcal-F-IOP" class="headerlink" title="$\mathcal{F}$-IOP"></a>$\mathcal{F}$-IOP</h2><p>Having the functional commitments, we can build SNARKs for some circuits, e.g. zero test, equality test.</p><p>But with $\mathcal{F}$-IOP, we can <u>boost functional commitment to build SNARK for any circuit $C(x,w)$.</u></p><p>Hence, $\mathcal{F}$-IOP is <strong>a proof system</strong> that proves $\exist w:C(x,w)=0$ as follows. </p><ul><li><p>Setup: The setup algorithm outputs public parameters for prover and verifier where $vp$ <u>contains a number of oracles for functions in $\mathcal{F}$.</u><br>Verifier can ask the oracle for evaluations at some points.<br>The <u>oracles will be replaced to functional commitment schemes in SNARKs.</u></p>  <img src="https://s1.ax1x.com/2023/07/21/pCbZe61.png" alt="Setup" style="zoom:33%;" /></li><li><p>IP proving $C(x,w)=0$: In each round, prover sends an oracle for a function $f_i$ which later on the verifier can evaluate at any point of his choice.<br>Likewise, the oracles will be <u>compiled to the commitment scheme when instantiating actual SNARKs.</u></p>  <img src="https://s1.ax1x.com/2023/07/21/pCbZ1te.png" alt="Public-coin IP" style="zoom:33%;" /></li></ul><p><font color=blue><u><b><i>Properties:</i></b></u></font></p><ul><li>Complete:  if $\exist w:C(x,w)=0$ then $\operatorname{Pr}[\text{verifier accepts}]=1$. </li><li>(Unconditional) knowledge sound (as an IOP): extractor is given $(x,f_1, r_1, \dots, r_{t-1},f_t)$ and outputs $w$.<br>Note that the <strong>given functions are in clear</strong> since the <u>functional commitments are SNARKs so the extractor can extract the functions from the commitments.</u></li><li>Optional: zero knowledge for a zk-SNARK</li></ul><h1 id="KZG-Poly-Commit-Scheme"><a href="#KZG-Poly-Commit-Scheme" class="headerlink" title="KZG Poly-Commit Scheme"></a>KZG Poly-Commit Scheme</h1><p>Let’s introduce today’s highlight, KZG polynomial commitment scheme [Kate-Zaverucha-Goldberg’2010].</p><h2 id="KZG-A-Binding-PSC"><a href="#KZG-A-Binding-PSC" class="headerlink" title="KZG: A Binding PSC"></a>KZG: A Binding PSC</h2><p>Fixed a group $\mathbb{G}={0,G,2\cdot G, 3\cdot G,\dots, (p-1)\cdot G}$ of order $p$.</p><h3 id="Commitment"><a href="#Commitment" class="headerlink" title="Commitment"></a>Commitment</h3><p>It requires a <strong>trusted but universal setup.</strong></p><ul><li>$\text{setup}(1^\lambda)\rightarrow gp$<ul><li>Sample random $\tau\in \mathbb{F}_p$</li><li>$gp=(H_0=G,H_1=\tau\cdot G, H_2=\tau^2\cdot G, \dots, H_d=\tau^d \cdot G)\in \mathbb{G}^{d+1}$.</li><li>delete $\tau$!!</li></ul></li><li>$\text{commit}(gp,f)\rightarrow \text{com}_f$ where $\text{com}_f=f(\tau)\cdot G \in \mathbb{G}$<ul><li>$f$ as prover parameter</li><li>$\text{com}_f$ as verifier parameter</li></ul></li></ul><p>The setup generates global parameters $gp$ in which the random $\tau$ used must be destroyed. </p><p>Then prover <u>can use $gp$ to generate the commitment for any specific polynomial</u> $f\in \mathbb{F}_p^{(\le d)}[X]$.</p><p>It is worth noting the <u>prover can compute $f(\tau)\cdot G$ without knowing $\tau$.</u></p><p>It can be evaluated by $gp$:</p><p>$$<br>\begin{aligned}f(X)&amp; =f_0+f_1X+\dots+f_d X^d \ \text{com}_f &amp;= f_0 \cdot H_0 +\dots f_d\cdot H_d \ &amp;=f_0\cdot G + f_1\tau \cdot G +f_2 \tau^2\cdot G +\cdots \ &amp;= f(\tau)\cdot G\end{aligned}<br>$$</p><p>where $f_0,\dots, f_d$ are coefficients of the polynomial.</p><p>This is a binding commitment but <strong>not hiding</strong> since it reveals $f(\tau)\cdot G$.</p><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>After committing to $f$, verifier can request for evaluations at a specific point.</p><p>For public $u,v\in \mathbb{F}_p$, the prover can convince the verifier that the committed polynomial satisfies $f(u)=v$.</p><p>The proof hinges on some <u>cute algebraic properties:</u> </p><ul><li>$f(u)=v$ iff</li><li>$u$ is a root of $\hat{f}=f-v$ iff</li><li>$(X-u)$ divides $\hat{f}$ iff</li><li>exists $q\in \mathbb{F}_p[X]$ s.t. $q(X)\cdot (X-u)=f(X)-v$</li></ul><p>As a result, we can construct <strong>an equality test</strong> on two polynomials to verify the original claim $f(u)=v$.</p><p>The whole idea is depicted as follows.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZlkD.png" alt="Evaluation at a specific point" style="zoom:33%;" /><p><font color=blue><u><b>Eval:</b></u></font> </p><ul><li><strong>Prover</strong><ul><li>compute the quotient polynomial $q(X)$ and $\text{com}_q=q(\tau)\cdot G$ as the commitment.</li><li>send the proof $\pi:=\text{com}_q\in \mathbb{G}$</li><li>Note that the proof is <u>just one group element</u>, which is const size better than logarithmic in $d$.</li></ul></li><li><strong>Verifier</strong><ul><li>accept if $(\tau-u)\cdot \text{com}_q=\text{com}_f -v\cdot G$</li><li>The equality test for $(X-u)q(X)\cdot G=(f(X)-v)\cdot G$ can be checked by the random $\tau$.</li></ul></li></ul><p>It is worth noticing that $\tau$ is secret.</p><p> But verifier can use a “pairing” to do the above computation with only $H_0,H_1$ from $gp$, which is <u>a fast computation</u> for <strong>verifier</strong>.</p><p>As for the <strong>prover</strong>, <u>computing the quotient is indeed an expensive computation</u> for large $d$.</p><h2 id="Generalizations"><a href="#Generalizations" class="headerlink" title="Generalizations"></a>Generalizations</h2><p>There are two ways to generalize KZG.</p><ul><li>[PST’13] Can use KZG to <u>commit to $k$-variate polynomials.</u></li><li><strong>Batch proofs</strong>: Can <u>commit to $n$ polynomials</u> and provide a batch proof for multiple evaluations.<ul><li>suppose verifier has commitments $\text{com}_{f_1},\dots, \text{com}_{f_n}$ </li><li>prover wants to prove $f_i(u_{i,j})=v_{i,j}$ for $i\in [n],j\in[m]$</li><li>→ batch proof $\pi$ is <u>only one group element !</u></li></ul></li></ul><h2 id="Properties-linear-time-commitment"><a href="#Properties-linear-time-commitment" class="headerlink" title="Properties: linear time commitment"></a>Properties: linear time commitment</h2><p>One wonderful property of KZG is the <u>prover’s runtime for commitment is linear in the degree $d$.</u></p><p>There are two ways to represent a polynomial $f(X)$ in $\mathbb{F}_p^{(\le d)}[X]$:</p><ul><li><strong>Coefficient representation:</strong><br>$f(X)=f_0+f_1X+\dots +f_d X^d$.<ul><li>computing $\text{com}_f=f_0\cdot H_0 +\dots +f_d\cdot H_d$ <u>takes linear time in $d$.</u></li></ul></li><li><strong>Point-value representation:</strong><br>$((a_0,f(a_0)),\dots,(a_d,f(a_d))$<ul><li>computing $\text{com}_f$ naively: <u>construct coefficients $(f_0,f_1,\dots, f_d)$ takes time $O(d\log d)$</u> using Number Theoretic Transform (NTT).</li></ul></li></ul><p>Using the point-value representation, the naive way of constructing coefficients takes time $O(d\log d)$ yet <u>we want it linear in $d$.</u></p><p>A better way to compute the commitment with point-value representation is the <strong>Lagrange interpolation.</strong></p>  $$f(\tau)=\sum_{i=0}^d \lambda_i (\tau)\cdot f(a_i) \\ \text{where } \lambda_i(\tau)=\frac{\prod_{j=0,j\ne i}^d (\tau -a_j)}{\prod _{j=0,j\ne i}^{d}(a_i-a_j)}\in \mathbb{F}_p$$  <p>One can <u>transform $gp$ into Lagrange form $\hat{gp}$ by a linear map</u>, not involving any secrets so that anyone can fulfill this transformation.</p>  $$\hat{gp}=(\hat{H}_0=\lambda_0(\tau)\cdot G,\hat{H}_1=\lambda_1(\tau)\cdot G,\dots, \hat{H_d}=\lambda_d(\tau)\cdot G)$$  <p>Now, prover can <strong>in linear time</strong> computes the commitment from the point-value representation as follows.</p>  $$\text{com}_f=f(\tau)\cdot G=f(a_0)\cdot \hat{H}_0+\dots +f(a_d)\cdot \hat{H}_d$$  <p>To sum up, prover can compute the commitment in linear time from the coefficient representation or the point-value representation.</p><h3 id="Multi-point-Proof-Generation"><a href="#Multi-point-Proof-Generation" class="headerlink" title="Multi-point Proof Generation"></a>Multi-point Proof Generation</h3><p>Let $\Omega\subseteq \mathbb{F}_p$ and $|\Omega|=d$.</p><p>Consider such a case in which  prover has some $f(X)$ in $\mathbb{F}_p^{(\le d)}[X]$ and needs evaluation proofs $\pi_a\in G$ <strong>for all</strong> $a\in \Omega$.</p><p>When it comes to generate evaluation proofs for multi-points, prover <u>naively takes time $O(d^2)$ for $d$ proofs</u>, each takes time $O(d)$.</p><p><u><b>Feist-Khovratovich (FK2020) algorithm</b></u> optimize to</p><ul><li>time $O(d\log d)$ if $\Omega$ is <u>a multiplicative subgroup</u></li><li>time $O(d\log^2 d)$ otherwise.</li></ul><h3 id="The-Dory-polynomial-commitment"><a href="#The-Dory-polynomial-commitment" class="headerlink" title="The Dory polynomial commitment"></a>The Dory polynomial commitment</h3><p>The difficulties with KZG lies in two parts</p><p>One has to require trusted setup for $gp$, and $gp$ size is linear in $d$.</p><p>Dory (eprint/2020/1274) is proposed to get over the trusted setup.</p><ul><li><strong>transparent setup:</strong> no secret randomness in the setup</li><li>$\text{com}_f$ is a single group element (independent of degree $d$ )</li><li>eval proof size for $f\in \mathbb{F}_p^{(\le d)}[X]$ is $O(\log d)$ group elements.<br>Note the eval proof size is <u>constant</u> size in original KZG.</li><li>eval verify time is $O(\log d)$<br>Note the eval verify time is <u>constant</u>.</li><li>prover time is $O(d)$</li></ul><h2 id="Applications-vector-commitment"><a href="#Applications-vector-commitment" class="headerlink" title="Applications: vector commitment"></a>Applications: vector commitment</h2><p>Polynomial Commitment Scheme (PCS) have many applications.</p><p>One is to <u>perform a drop-in replacement for Merkle trees.</u></p><p>The idea is to <u>view the vector $(u_1,\dots, u_k)\in \mathbb{F}_p^{(\le d)}$ as a function</u> $f$ such that $f(i)=u_i$ for $i=1,\dots, k$, then prover can commits to this polynomial.</p><p>Instead of proving the revealed entry is consistent with the committed vector, prover can <u>generate evaluation proof</u> that $f(2)=a,f(4)=b$ as depicted follows.</p><p>The proof $\pi$ is a single group element (using batch proof) that is shorter than a Merkle proof.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZ3fH.png" alt="PCS as vector commitment" style="zoom:33%;" /><h1 id="Proving-properties-of-committed-polynomials"><a href="#Proving-properties-of-committed-polynomials" class="headerlink" title="Proving properties of committed polynomials"></a>Proving properties of committed polynomials</h1><p>Having PCS, not only verifier can query evaluations of a committed polynomial, but <u>prover can convince verifier that the committed polynomials $f,g$ satisfy some properties</u>, e.g. equality test.</p><p>It can be summed up in the following process.</p><ul><li>Start: Prover has functions $f,g$ in clear and verifier has the corresponding commitments via PCS.</li><li>Verifier samples a random $r\in \mathbb{F}_p$.</li><li>Prover computes a related polynomial $q$ and commits to it.</li><li>Verifier can query $f,g,q$ at point $x$ and accept if valid.<br>Note that when we say verifier query a committed polynomial $f(x)$, it means verifier sends $x$ to prover who responds with $f(x)$ and eval proof $\pi$. (described in here[link])</li></ul><img src="https://s1.ax1x.com/2023/07/21/pCbZGpd.png" alt="Untitled" style="zoom:33%;" /><h2 id="Polynomial-Equality-Testing-with-KZG"><a href="#Polynomial-Equality-Testing-with-KZG" class="headerlink" title="Polynomial Equality Testing with KZG"></a>Polynomial Equality Testing with KZG</h2><p>As described above, we can construct equality test for two committed polynomials.</p><p>But for KZG, $f=g$ <strong>if and only if</strong> $\text{com}_f=\text{com}_g$, resulting that <u>verifier can tell if $f=g$ on its own.</u></p><p>But prover is needed to <strong>test equality of computed polynomials.</strong></p><p>For example, verifier has four individual commitments to $f,g_1,g_2,g_3$  where all four are in $\mathbb{F}_p^{(\le d)}[X]$ to test $f=g_1g_2g_3$.</p><p>Then verifier queries all four polynomials at a random point $r\overset{$}\leftarrow \mathbb{F}_p$ and tests equality.</p><p>It is complete and sound assuming $3d/p$ is negligible since $\text{deg}(g_1g_2g_3)\le 3d$.</p><h2 id="Summary-of-Proof-Gadgets-for-Univariates"><a href="#Summary-of-Proof-Gadgets-for-Univariates" class="headerlink" title="Summary of Proof Gadgets for Univariates"></a>Summary of Proof Gadgets for Univariates</h2><p>In order to construct Poly-IOPs for an arbitrary circuit.</p><p>In this section, we’ll introduce some important <strong>proof gadgets for univariates.</strong></p><p>Let $\Omega$ be some subset of $\mathbb{F}_p$ of size $k$.</p><p>Let $f\in \mathbb{F}_p^{(\le d)}[X]$ where $d\ge k$ and verifier has the commitment to $f$.</p><p>We can construct <u>efficient Poly-IOPs</u> for the following tasks.</p><ul><li><strong>Zero Test</strong>: prove that $f$ is identically zero on $\Omega$.</li><li><strong>Sum Check</strong>: prove that $\sum_{a\in \Omega}f(a)=0$.</li><li><strong>Prod Check</strong>: prove that $\prod_{a\in \Omega}f(a)=1$.<ul><li>→ prove for rational functions that $\prod_{a\in \Omega}f(a)/g(a)=1$</li></ul></li><li><strong>Permutation Check</strong>: prove that $g(\Omega)$ is the same as $f(\Omega)$, just permuted.</li><li><strong>Prescribed Permutation Check</strong>: prove that $g(\Omega)$ is the same as $f(\Omega)$, permuted by the prescribed $W$.</li></ul><h2 id="Vanishing-Polynomial"><a href="#Vanishing-Polynomial" class="headerlink" title="Vanishing Polynomial"></a>Vanishing Polynomial</h2><p>Before staring, let’s introduce the vanishing polynomial.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Def: Vanishing Polynomial of $\Omega$:</strong></p> </div> <div class="message-body"> <p>The vanishing polynomial of $\Omega$ is </p><p>$$<br>Z_\Omega(X):=\prod_{a\in \Omega}(X-a)<br>$$</p><p>such that $\text{deg}(Z_\Omega)=k$.</p> </div> </article> <p>By definition, the <u>vanishing polynomial is a univariate polynomial to be $0$ everywhere on subset $\Omega$.</u> </p><p>We can construct <strong>a cute vanishing polynomial</strong> by constructing a special subset $\Omega$.</p><p>Let $\omega\in \mathbb{F}_p$ be a primitive $k$-th root of unity so that $\omega ^k=1$.</p>  If $\Omega=\{1,\omega, \omega^2, \dots, \omega^{k-1}\}\subseteq \mathbb{F}_p$ then $Z_\Omega(X)=X^k-1$.  <p>Then for $r\in \mathbb{F}_p$, <u>evaluating $Z_\Omega(r)$ takes $\le 2\log_2{k}$ field operations</u> by repeated squaring algorithm.</p><p>It’s super fast.</p>  In the following tasks, we fix $\Omega=\{1,\omega, \omega^2, \dots, \omega^{k-1}\}$.  <h2 id="ZeroTest-on-Omega"><a href="#ZeroTest-on-Omega" class="headerlink" title="ZeroTest on $\Omega$"></a>ZeroTest on $\Omega$</h2><p>In zero test, prove wants to convince verifier that $f$ is identically zero on $\Omega$.</p><p>We build zero test by the following lemma.</p><p><font color=blue><u><b>Lemma:</b></u></font> </p><p>$f$ is zero on $\Omega$ if and only if $f(X)$ is divisible by $Z_{\Omega}(X)$.</p><p>The IOP of zero test is depicted as follows.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZY6I.png" alt="IOP for zero test." style="zoom:33%;" /><ol><li><p><strong>Prover</strong> computes the quotient polynomial $q(X)=f(X)/Z_{\Omega}(X)$ and commits to this polynomial.</p><p> Note that with KZG prover can only commits to a polynomial in $\mathbb{F}_p^{(\le d)}$ rather than a rational functions.</p></li><li><p><strong>Verifier</strong> samples a random $r\in \mathbb{F}_p$.</p></li><li><p><strong>Verifier</strong> query $q(X)$ and $f(X)$ at point $r$ to learn $q(r)$ and $f(r)$. And verifier evaluates $Z_\Omega(r)$ by itself.</p></li><li><p><strong>Verifier</strong> accepts if $f(r)=q(r)\cdot Z_\Omega(r)$ since it implies $f(X)=q(X)\cdot Z_\Omega(X)$ w.h.p.</p></li></ol><p><font color=blue><u><b>Theorem:</b></u></font> </p><p>This protocol is complete and sound assuming $d/p$ is negligible.</p><p><font color=blue><u><b>Costs:</b></u></font> </p><ul><li><strong>Verifier time</strong>: $O(\log k)$ for evaluating $Z_\Omega(r)$ plus two poly queries (that can be batch into one)</li><li><strong>Prover time</strong>: dominated by the time to compute $q(X)$ and then commit to $q(X)$.</li></ul><h2 id="Product-Check-on-Omega"><a href="#Product-Check-on-Omega" class="headerlink" title="Product Check on $\Omega$"></a>Product Check on $\Omega$</h2><p>We omit the details of sum check and jump to the product check since they are nearly the same.</p><p>Product check is a useful gadget to construct the permutation check introduced later.</p><p>In product check, prover wants to convince verifier that the products of all evaluations over $\Omega$ equals to $1$, i.e. </p><p>$$<br>\prod_{a\in \Omega}f(a)=1<br>$$</p><p>We construct a degree-$k$ polynomial to prove it.</p><p>Set $t\in \mathbb{F}_p^{(\le k)}[X]$ to be the <u>degree-$k$ polynomial</u> such that</p><p>$$<br>\begin{aligned}t(1)&amp;=f(1), \ t(\omega^s)&amp;=\prod_{i=0}^s f(\omega^i) \text{ for }s=1,\dots, k \end{aligned}<br>$$</p><p>Note that a degree-$k$ polynomial can be uniquely specified by $k+1$ points.</p><p>Then $t(\omega^i)$ <u>evaluates the prefix-products as follows.</u></p><ul><li>$t(\omega)=f(1)\cdot f(\omega)$,</li><li>$t(\omega^2)=f(1)\cdot f(\omega)\cdot f(\omega^2)$</li><li>… …</li><li>$t(\omega^{k-1})=\prod_{a\in \Omega}f(a)=1$</li></ul><p>We can represent prefix-product <u>in a iterative way:</u></p><p>$$<br>t(\omega\cdot x)=t(x)\cdot f(\omega \cdot x) \text{ for all }x\in \Omega<br>$$</p><p>As a result, we can do the product check by the following lemma, which can be <u>proved with the evaluation proof and a zero test.</u></p><p><font color=blue><u><b>Lemma:</b></u></font>  If </p><p>( i ) $t(\omega^{k-1})=1$ and </p><p>( ii ) $t(\omega\cdot x)-t(x)\cdot f(\omega\cdot x)=0$ for all $x\in \Omega$</p><p>then $\prod_{a\in \Omega}f(a)=1$.</p><p>The IOP for product check is depicted as follows.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZtXt.png" alt="IOP for product check" style="zoom:33%;" /><p>We can split it two parts:</p><ul><li><strong>Evaluation proof</strong> to prove $t(\omega^{k-1})=1$<ul><li>Prover construct $t(X)\in \mathbb{F}_p^{(\le k)}$ and <u>commits</u> to it.</li><li>Verifier <u>queries</u> $t(X)$ at $\omega^{k-1}$.</li><li><u>check1:</u> Verifier checks that if $t(\omega^{k-1})=1$.</li><li><u>proof size:</u> one commit, one evaluation.</li></ul></li></ul><p>Let  $t_1(X)=t(\omega\cdot X)-t(X)\cdot f(\omega\cdot X)$.</p><ul><li><strong>Zero test</strong> to prove $t_1$ is zero on $\Omega$.<br>Recall the lemma that $t_1$ is zero on $\Omega$ iff $Z_{\Omega}(X)$ divides $t_1(X)$.<ul><li>Prover computes the quotient polynomial $q(X)=t_1(X)/(X^{k}-1)\in \mathbb{F}_p^{(\le d)}$ and <u>commits</u> to it.</li><li>Verifier samples a random $r\in \mathbb{F}_p$ and need to learn $t_1(r)$ and $q(r)$.<ul><li>Verifier <u>queries</u> $q(X)$ at $r$.</li><li>Verifier <u>queries</u> $t(X)$ at $\omega r$, and $r$.</li><li>Verifier <u>u</u> $f(X)$ at $\omega r$.</li></ul></li><li>Verifier computes $r^{k}-1$ in time $O(\log k)$.</li><li><u>check2:</u> Verifier checks if $t(\omega\cdot r)-t(\omega)\cdot f(\omega\cdot r)=q(r)\cdot (r^k-1)$.</li><li><u>proof size:</u> one commit, four evaluations.</li></ul></li></ul><p>Note that it is a public-coin interactive protocol that can be rendered non-interactive via Fiat-Shamir Transform.</p><p>To sum up, the <strong>proof size</strong> is made up of <u>two commits and five evaluations</u> (can be batched into a single group element).</p><p><font color=blue><u><b>Theorem:</b></u></font> </p><p>This protocol is complete and sound assuming $2d/p$ is negligible.</p><p>( $t(X)\cdot f(\omega\cdot X)$ has degree at most $2d$ ).</p><p>It takes <strong>verifier</strong> $O(\log k)$ time to compute $r^{k-1}-1$.</p><p>It takes <strong>prover</strong> $O(k\log k)$ time to compute $t(X)$ and $q(X)$ using the naive way that constructs the coefficients from the point-value representation. </p><hr><p>Likewise, it works to <strong>prove the products on rational functions:</strong></p><p>$$<br>\prod_{a\in \Omega}(f/g)(a)=1<br>$$</p><p>We construct a similar degree-$k$ polynomial to prove it.</p><p>Set $t\in \mathbb{F}_p^{(\le k)}[X]$ to be the <u>degree-$k$ polynomial</u> such that</p><p>$$<br>\begin{aligned}t(1)&amp;=f(1)/g(1), \ t(\omega^s)&amp;=\prod_{i=0}^s f(\omega^i)/g(\omega^i) \text{ for }s=1,\dots, k \end{aligned}<br>$$</p><p>We write the prefix-product in an iterative way:</p><p>$$<br>t(\omega\cdot x)=t(x)\cdot \frac{f(\omega \cdot x)}{g(\omega\cdot x)} \text{ for all }x\in \Omega<br>$$</p><p>Then we can prove the following two parts to fulfill the product check.</p><p><font color=blue><u><b>Lemma:</b></u></font> If </p><p>( i ) $t(\omega^{k-1})=1$ and </p><p>( ii ) $t(\omega\cdot x)\cdot g(\omega \cdot x)-t(x)\cdot f(\omega\cdot x)=0$ for all $x\in \Omega$</p><p>then $\prod_{a\in \Omega}f(a)/g(a)=1$.</p><p>Note that the <strong>proof size</strong> is <u>two commits and six evaluations.</u></p><p><font color=blue><u><b>Theorem:</b></u></font> </p><p>This protocol is complete and sound assuming $2d/p$ is negligible.</p><p>Compared to the original prod-check, the <strong>one extra evaluation</strong> comes from the query to $g(X)$ at $\omega\cdot r$. </p><h2 id="Permutation-Check"><a href="#Permutation-Check" class="headerlink" title="Permutation Check"></a>Permutation Check</h2><p>Let $f,g$ be polynomials in $\mathbb{F}_p^{(\le d)}[X]$.</p><p>Verifier has commitments to $f$ and $g$.</p><p>In permutation check, that <strong>goal</strong> is that prover wants to prove that $(f(1),f(\omega),f(\omega^2),\dots, f(\omega^{k-1})\in \mathbb{F}_p^k$ is a <strong>permutation</strong> of $(g(1),g(\omega),g(\omega^2),\dots, g(\omega^{k-1}))\in \mathbb{F}_p^k$.</p><p>It means to prove that $g(\Omega)$ is the same as $f(\Omega)$, just permuted.</p><p>The <strong>main idea</strong> is to construct auxiliary polynomials that have the evaluations as its root.</p><p>Let $\hat{f}(X)=\prod_{a\in \Omega}(X-f(a))$ and $\hat{g}(X)=\prod_{a\in \Omega}(X-g(a))$.</p><p><u>Then $\hat{f}(X)=\hat{g}(X)$ if and only if $g$ is a permutation of $f$ on $\Omega$.</u></p><p>The <strong>thing to notice</strong> is that prover <strong>cannot</strong> just commits to $\hat{f}$ and $\hat{g}$, then verifier checks if $\hat{f}(r)=\hat{g}(r)$.</p><p>Because there is a missed proof that $\hat{f}$ is honestly constructed by the committed $f$. </p><p>Instead, prover is needed to prove $\hat{f}(r)=\hat{g}(r)$ by <u>performing a product check on a rational function</u> $\frac{r-f(X)}{r-g(X)}$.</p><p>The IOP for permutation check is depicted as follows.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZUnP.png" alt="IOP for permutation check" style="zoom:33%;" /><p>Let’s elaborate on the details.</p><ul><li><u>Start</u>: Prover has functions $f,g$ in clear and verifier has commitments to $f,g$.</li></ul><ol><li><p>Verifier samples a random $r$.</p></li><li><p>Prover constructs $\hat{f}$ using the evaluations of $f$, so is $\hat{g}$.</p></li><li><p>Then prover wants to prove $\hat{f}(r)=\hat{g}(r)$.</p><ol><li><p>It can be transformed to prove $\frac{\hat{f}(r)}{\hat{g}(r)}=1$ where $r$ is <u>fixed</u>.</p></li><li><p>They can <strong>perform prod-check</strong> to prove </p><p> $$<br> \frac{\hat{f}(r)}{\hat{g}(r)}=\prod_{a\in \Omega}\left(\frac{r-f(a)}{r-g(a)}\right)=1<br> $$</p><p> where the <u>rational function</u> is defined as $\frac{r-f(X)}{r-g(X)}$ on $\Omega$.</p></li></ol></li></ol><ul><li><u>Proof size:</u> two commits and six evaluations, same as prod-check on rational functions.</li></ul><p>It’s a public-coin protocol that can be rendered non-interactive.</p><p><font color=blue><u><b>Theorem:</b></u></font> </p><p>This protocol is complete and sound assuming $2d/p$ is negligible.</p><h2 id="Prescribed-Permutation-Check"><a href="#Prescribed-Permutation-Check" class="headerlink" title="Prescribed Permutation Check"></a>Prescribed Permutation Check</h2><p>Let’s look into an embellished permutation check where the <u>permutation is prescribed by a specific permutation $W$.</u></p><p>We say $W:\Omega\rightarrow \Omega$ is a permutation of $\Omega$ if $\forall i\in [k]$: $W(\omega^i)=\omega^j$ is <u>bijection</u>.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  <p>A bijection means <strong>one-to-one correspondence.</strong></p><p>A bijection function is <strong>injective and surjective.</strong></p> </div> </article> <p>Let $f,g$ be polynomials in $\mathbb{F}_p^{(\le d)}[X]$.</p><p>Verifier has three individual commitments to $f, g,$ and $W$.</p><p>In prescribed permutation check, the <strong>goal</strong> of prover is to prove that $f(y)=g(W(y))$ for all $y\in \Omega$.</p><p>In other works, it proves <u>that $g(\Omega)$ is the same as $f(\Omega)$, permuted by the prescribed $W$.</u></p><hr><p>At first sight, we try to use a zero-test to prove $f(y)-g(W(y))=0$ on $\Omega$.</p><p>But the <strong>problem</strong> is the <u>polynomial $f(y)-g(W(y))$ has degree $k^2$</u> since $g(W(y))$ is a composition of $f$ and $W$.</p><p>Then prover would need to manipulate polynomials of degree $k^2$, <u>resulting a quadratic time prover !!</u> </p><p>Yet we want <strong>a linear time prover.</strong></p><hr><p>Let’s reduce this to a <strong>prod-check</strong> <u>on a polynomial of degree $2k$.</u></p><p><font color=blue><u><b>Observation:</b></u></font> </p><p>If $(W(a),f(a))_{a\in \Omega}$ is a permutation of $(a, g(a))_{a\in \Omega}$,</p><p>then $f(y)=g(W(y))$ for all $y\in \Omega$.</p><p>By the definition of permutation, for $a\in \Omega$, there exists a $a’\in \Omega$ $W(a’)=a$ and $f(a’)=g(a)$ hold. Then we have $f(a’)=g(W(a’))$.</p><p>The following example illustrates the proof.</p><img src="https://s1.ax1x.com/2023/07/21/pCbZJ1A.png" alt="proof by an example" style="zoom:43%;" /><p>Likewise, we construct auxiliary polynomials that have the <u>evaluations as its root yet the evaluations are listed in form of the tuple.</u></p><p>The <strong>intuition</strong> is to <u>encode the tuple to a variable</u>, then use a similar way to <u>construct a bivariate polynomial that has the variables as its root.</u></p><p>Hence, the tuple is encoded as variables $Y\cdot W(a)+f(a)$ and $Y\cdot a+g(a)$, respectively.</p><p>And the <u>bivariate polynomials</u> of total degree $k$ is constructed as follows.</p>$$\begin{cases} \hat{f}(X,Y)&= \prod _{a\in \Omega}(X-Y\cdot W(a)-f(a)) \\ \hat{g}(X,Y) &= \prod_{a\in \Omega}(X-Y\cdot a -g(a))\end{cases}$$<p>The following lemma shows the correctness.</p><p><font color=blue><u><b>Lemma:</b></u></font> </p>$\hat{f}(X,Y)=\hat{g}(X,Y)$ if and only if $(W(a),f(a))_{a\in \Omega}$ is a permutation of $(a,g(a))_{a\in \Omega}$.<p>To prove, use the fact that $\mathbb{F}_p[X,Y]$ is a unique factorization domain. Yet I’m not familiar with this fact.</p><p>The complete protocol is depicted as follows, which <strong>composes a prod-check on the rational function</strong> $\frac{r-s\cdot W(X)-f(X)}{r-s\cdot X-g(X)}$ where $r,s$ are fixed and randomly chosen by the verifier.</p><img src="https://s1.ax1x.com/2023/07/21/pCb11tx.png" alt="Complete protocol for prescribed permutation check" style="zoom:33%;" /><p><font color=blue><u><b>Theorem:</b></u></font> </p><p>This protocol is complete and sound assuming $2d/p$ is negligible.</p><h1 id="The-PLONK-IOP-for-General-Circuits"><a href="#The-PLONK-IOP-for-General-Circuits" class="headerlink" title="The PLONK IOP for General Circuits"></a>The PLONK IOP for General Circuits</h1><p><a href="eprint/2019/953">PLONK</a></p><p>Finally, let’s introduce PLONK IOP, a widely used proof system in practice.</p><p>It is a poly-IOP for a general circuit $C(x,w)$.</p><p>We can think the <strong>PLONK IOP</strong> as <u>an abstract IOP that can be combined with different polynomial commitment schemes to construct actual SNARK system.</u></p><img src="https://s1.ax1x.com/2023/07/21/pCb1lA1.png" alt="PLONK IOP " style="zoom:33%;" /><h2 id="Step1-compile-circuit-to-a-computation-trace"><a href="#Step1-compile-circuit-to-a-computation-trace" class="headerlink" title="Step1: compile circuit to a computation trace"></a>Step1: compile circuit to a computation trace</h2><p>The first step is to <u>compile a general circuit to a computation trace</u> that can <u>be encoded by a polynomial.</u></p><p>Considering a general circuit $(x_1+x_2)(x_2+w_1)$ with two public inputs and one witness input, we can write the computation trace into a table as follows. This compilation is also called <strong>arithmetization</strong>.</p><img src="https://s1.ax1x.com/2023/07/21/pCb1M7R.png" alt="arithmetization: compile the circuit to a computation trace" style="zoom:33%;" /><p>The example is illustrated as above.</p><p>Let $|C|$  denote the total # of gates in $C$. </p><p>Let $|I|=|I_x|+|I_w|$ denote the # inputs to $C$. </p>  Let $d=3|C|+|I|$ and $\Omega=\{1,\omega, \omega^2, \dots, \omega^{d-1}\}$ where  $\omega\in \mathbb{F}_p$ is the primitive $k$-th root of unity so that $\omega ^d=1$.  <p>In the above example, $|C|=3$, $|I|=3$, and $d=12$.</p><p>The <strong>plan</strong> is <u>prover can interpolates a polynomial $T\in \mathbb{F}_p^{(\le d)}[X]$ that encodes the entire trace.</u></p><ol><li>$T$ <strong>encodes all inputs</strong>:   $T(\omega^{-j})=\text{input }\# j \text{ for } j=1,\dots, |I|$. </li><li>$T$ <strong>encodes all wires</strong>: $\forall l=0,\dots, |C|-1$:<br>For each gate labeled by $l$,<ol><li>$T(\omega^{3l})$: left input to gate  $\#l$  </li><li>$T(\omega^{3l+1})$: right input to gate  $\#l$  </li><li>$T(\omega^{3l+2})$: output to gate  $\#l$  </li></ol></li></ol><p>In our example, prover interpolates $T(X)$ of degree 11 such that:</p><img src="https://s1.ax1x.com/2023/07/21/pCbZd78.png" alt="interpolate T with 12 points" style="zoom:33%;" /><p>Note that prover can use FFT / NTT to compute the coefficients of $T$ in time $O(d\log d)$.</p><h2 id="Step2-proving-validity-of-T"><a href="#Step2-proving-validity-of-T" class="headerlink" title="Step2: proving validity of $T$"></a>Step2: proving validity of $T$</h2><p>Then prover <u>commits to the polynomial</u> $T$ encoded by the computation trace, and <u>needs to prove that $T$ is a correct computation trace.</u></p><img src="https://s1.ax1x.com/2023/07/21/pCbZa0f.png" alt="commits to T" style="zoom:33%;" /><p><font color=blue><u><b>Proving Validity of :</b></u></font> </p><ol><li>$T$ encodes the correct <u>inputs</u>.</li><li>Every <u>gate</u> is evaluated correctly.</li><li>The <u>wiring</u> is implemented correctly.</li><li>The <u>output</u> of last gates is 0.<br>(In our example, the output is 77)</li></ol><p><u>Proving (4)</u> is easy that only proves $T(\omega^{3|C|-1})=0$.</p><p>The <u>wiring constraints</u> contains that the second input $6$ is connected with the left wire of the gate 0 and the right wire of the gate 1 as depicted as follows.</p><img src="https://s1.ax1x.com/2023/07/21/pCblQW8.png" alt="wiring constraints" style="zoom:33%;" /><h3 id="1-T-encodes-the-correct-input"><a href="#1-T-encodes-the-correct-input" class="headerlink" title="(1) $T$ encodes the correct input"></a>(1) $T$ encodes the correct input</h3><p>Note that the statement $x$ is public.</p><p>Both <u>prover and verifier</u> interpolate a polynomial $v(X)\in \mathbb{F}_p^{(\le |I_x|)}[X]$ that encodes the $x$-inputs to the circuit:</p>$$\text{for }j=1,\dots, |I_x|: v(\omega^{-j})=\text{input }\# j$$<p>In our example, $v(\omega^{-1})=5,v(\omega^{-2})=6$, hence $v$ is linear.</p><p>Note that constructing $v(X)$ takes time proportional to the size of input $x$ so that verifier has time to do this.</p>  Let $\Omega_{\text{inp}}=\{\omega^{-1},\omega^{-2},\dots, \omega^{-|I_x|}\}\subseteq \Omega$ that <u>contains the points encoding the inputs.</u>  <p>Prover proves (1) by using <strong>ZeroTest</strong> on $\Omega_{\text{inp}}$ to prove that </p><p>$$<br>T(y)-v(y)=0 ; \forall y\in \Omega_{\text{inp}}<br>$$</p><p>Note that verifier can  construct $v(X)$ explicitly so verifier only query $T(X)$ at randomly chosen $r$.</p><h3 id="2-every-gate-is-evaluated-correctly"><a href="#2-every-gate-is-evaluated-correctly" class="headerlink" title="(2) every gate is evaluated correctly"></a>(2) every gate is evaluated correctly</h3><p>Suppose that the circuit only composes the additional gates and multiplication gates.</p><p>The <strong>idea of differentiating</strong> is to <u>encode gate types using a selector polynomial $S(X)$.</u></p><p>Define $S(X)\in\mathbb{F}_p^{(\le d)}[X]$ such that $\forall l=0, \dots, |C|-1$:</p>$$\begin{cases}S(\omega^{3l})&= 1 \text{ if gate }\# l \text{ is an addition gate} \\ S(w^{3l})&=0\text{ if gate }\# l \text{ is an multiplication gate}\end{cases}$$<p>In our example, the selector polynomial is interpolated as follows.</p><img src="https://s1.ax1x.com/2023/07/21/pCbl1SS.png" alt="Interpolation for selector polynomial S(X)" style="zoom:33%;" /><p>The selector polynomial will be <strong>committed in the preprocessing phase</strong> because it is <u>a function of the circuit,</u> which just encodes what the gates represent in the circuit.</p><img src="https://s1.ax1x.com/2023/07/21/pCblMJf.png" alt="S can be preprocessed in the setup" style="zoom:33%;" /><p>With the selector polynomial, we can <u>encode the addition gates and the multiplication gates into a single polynomial.</u></p>  $$\forall y\in \Omega_{\text{gates}}=\{1,\omega^3,\omega^6,\omega^9,\dots, \omega^{3(|C|-1)}\}: \\ S(y)\cdot [T(y) + T(\omega y)] + (1-S(y))\cdot T(y)\cdot T(\omega y)=T(\omega^2 y)$$  <p>If the above equality holds, it means that all the addition gates and multiplication gates are evaluated correctly.</p><ul><li> $\#l$ is an **addition gate** → $S(y=\omega^{3l})=1$<ul><li>Prove that the sum of the left input and right input equals to the output, i.e. $T(y)+T(\omega y)=T(\omega^2 y)$.</li></ul></li><li> $\#l$  is a **multiplication gate** → $S(y=\omega^{3l})=0$<ul><li>Prove that the product of the left input and right input equals to the output, i.e. $T(y)\cdot T(\omega y)=T(\omega^2 y)$.</li></ul></li></ul><p>Then prover uses ZeroTest to prove that for $\forall y\in \Omega_{\text{gates}}$:</p><p>$$<br>S(y)\cdot [T(y) + T(\omega y)] + (1-S(y))\cdot T(y)\cdot T(\omega y)-T(\omega^2 y)=0<br>$$</p><h3 id="3-the-wiring-is-correct"><a href="#3-the-wiring-is-correct" class="headerlink" title="(3) the wiring is correct"></a>(3) the wiring is correct</h3><p>The last thing is to prove the wiring is correct.</p><p>First we  <strong>construct the wiring constraints</strong> to <u>encode the wires of $C$.</u></p><p>In our example, the(incomplete) wiring constraints are listed as follows. The first constraint means that the second input is connected to the right input of the gate 0 and the left input of the gate 1.</p><img src="https://s1.ax1x.com/2023/07/21/pCbl8yQ.png" alt="wiring constraints" style="zoom:33%;" /><p>Then <u>define a polynomial $W:\Omega\rightarrow \Omega$ that implements a rotation:</u></p><ul><li>$W(\omega^{-2},\omega^{1},\omega^3)=(\omega^{1},\omega^3,\omega^{-2})$</li><li>$W(\omega^{-1},\omega^0)=(\omega^{0},\omega^{-1})$</li><li>… …</li></ul><p>The rotation means $W$ maps  $\omega^{-2}$ → $\omega^{1}$, $w^1$ → $\omega^3$, and  $\omega^3$→ $\omega^{-2}$.</p><p>It means the polynomial $T$ is invariant under this rotation.</p><p>Note that $W$ actually defines a <u>prescribed permutation.</u></p><p>Finally, the following lemma tells us we can prove the wiring constraints <strong>using a prescribed permutation check</strong>.</p><p><font color=blue><u><b>Lemma:</b></u></font> </p><p>$\forall y\in \Omega$: if $T(y)=T(W(y))$, then wire constraints are satisfied.</p><p>It’s a clever way of encoding all the wiring constraints.</p><p>Note that the polynomial $W$ doesn’t depend on the inputs so it represents <u>an intrinsic property of the circuit itself</u>, which can be committed in the setup.</p><h2 id="Complete-Plonk-Poly-IOP"><a href="#Complete-Plonk-Poly-IOP" class="headerlink" title="Complete Plonk Poly-IOP"></a>Complete Plonk Poly-IOP</h2><p>The complete Plonk poly-IOP (and SNARK) is depicted as follows.</p><img src="https://s1.ax1x.com/2023/07/21/pCbl3Qg.png" alt="PLONK Poly-IOP" style="zoom:33%;" /><p>Let’s elaborate on the details.</p><ul><li>The setup preprocesses the circuit $C$ and outputs the commitmens to the <u>selector polynomial $S$ and the wiring polynomial $W$.</u> It is <strong>untrusted</strong> that anyone can check these commitments were done correctly.</li><li>Prover compiles the circuit to a computation trace, and encodes the entire trace into a polynomial $T(X)$.</li><li>Verifier can construct $v(X)$ explicitly from the public inputs $x$.</li><li>Then prover proves validity of $T$:<ul><li>gates: evaluated correctly by <strong>ZeroTest</strong></li><li>inputs: correct inputs by <strong>ZeroTest</strong></li><li>wires: correct wirings by <strong>Prescribed Permutation Check</strong></li><li>output: correct output by <strong>evaluation proof</strong></li></ul></li></ul><p><font color=blue><u><b>Theorem:</b></u></font> </p><p>The Plonk Poly-IOP is complete and knowledge sound, assuming $7|C|/p$ is negligible.</p><ul><li>$7|C|$ bounds the degree of the polynomial of $S\cdot T \cdot T$.</li><li>constant proof size: a short proof with $O(1)$ commitments.</li><li>fast verifier: runs in logarithmic time $O(\log |C|)$</li><li>quasi-linear prover: $O(|C|\log |C|)$</li><li>SNARK: rendered via Fiat-Shamir Transform</li></ul><p>Note that the SNARK is <u>not necessarily zk</u> since the commitments are not zk and the openings are not as well.</p><p>But there are generic transformations that can efficiently convert any Poly-IOP into a zk Poly-IOP, rendering a zk-SNARK.</p><h2 id="Extensions"><a href="#Extensions" class="headerlink" title="Extensions"></a>Extensions</h2><h3 id="Hyperplonk-linear-prover"><a href="#Hyperplonk-linear-prover" class="headerlink" title="Hyperplonk: linear prover"></a>Hyperplonk: linear prover</h3><p>The main challenge in PLONK is the prover runs in quasi-linear time.</p><p>Hyperplonk replaces $\Omega$ with ${0,1}^t$ where $t=\log _2 |\Omega|$ to achieve a linear prover.</p><p>The polynomial $T$ is now <u>a multilinear polynomial in $t$ variables</u>, and the computation trace is encoded on the vertices of the $t$-dim hypercube.</p><p>ZeroTest is replaced by a multilinear SumCheck.</p><p>Recall that the prover time in SumCheck (<a href="/2023/07/18/zkp-lec4/" title="[Lecture 4]">[Lecture 4]</a>) has a factor $2^t$, which is <u>linear</u> to $|C|$.</p><p>It turns out that all tools to build for <u>proving facts about committed univariate polynomials can be generalized to work and prove properties of multilinear polynomials.</u></p><h3 id="Plonkish-Arithmetization"><a href="#Plonkish-Arithmetization" class="headerlink" title="Plonkish Arithmetization"></a>Plonkish Arithmetization</h3><p>Another extension is about the arithmetization, including the <strong>custom gates</strong> and <strong>Plookup.</strong></p><p>Having these extension allows to <u>shrink the size of the computation traces, which speed up the prover runtime.</u></p><p>It supports custom gates other than addition gates and multiplication gates.</p><p>The plonkish computation trace can be illustrated as follows:</p><img src="https://s1.ax1x.com/2023/07/21/pCblGLj.png" alt="plonkish computation trace" style="zoom:33%;" /><p>It is defined by a <strong>custom gate</strong> that computes $v_4+w_3\cdot t_3$ and outputs $t_4$.</p><p>Likewise, we can encode it into the following polynomial</p><p>$$<br>\forall y\in \Omega_{\text{gates}}: v(y\omega)+w(y)\cdot t(y)-t(y\omega)=0<br>$$</p><p>Prover uses a ZeroTest check to prove that <u>the custom gate is evaluated correctly.</u></p><p>All such gate checks are included in the gate check <u>by multiplying a selector polynomial.</u></p><p>Furthermore, <strong>Plookup</strong> can <u>ensure some values in the computation trace are in pre-defined list.</u></p>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
    In this &lt;a href=&quot;/categories/Cryptography-ZKP&quot;&gt;series&lt;/a&gt;, I will learn &lt;strong&gt;Zero Knowledge Proofs (ZKP)&lt;/strong&gt; on this &lt;a href=&quot;https://zk-learning.org/&quot;&gt;MOOC&lt;/a&gt;, lectured by &lt;strong&gt;Dan Boneh&lt;/strong&gt;, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. 
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;strong&gt;Topics:&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preprocessing SNARK&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KZG Poly-Commit Scheme&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Proving Properties of committed polys&lt;ul&gt;
&lt;li&gt;ZeroTest&lt;/li&gt;
&lt;li&gt;Product Check&lt;/li&gt;
&lt;li&gt;Permutation Check&lt;/li&gt;
&lt;li&gt;Prescribed Permutation Check&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plonk IOP for General Circuits&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-ZKP" scheme="https://f7ed.com/categories/Cryptography-ZKP/"/>
    
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="ZKP" scheme="https://f7ed.com/tags/ZKP/"/>
    
      <category term="SNARKs" scheme="https://f7ed.com/tags/SNARKs/"/>
    
      <category term="KZG" scheme="https://f7ed.com/tags/KZG/"/>
    
      <category term="Plonk" scheme="https://f7ed.com/tags/Plonk/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-ZKP」: Lec4-SNARKs via IP</title>
    <link href="https://f7ed.com/2023/07/18/zkp-lec4/"/>
    <id>https://f7ed.com/2023/07/18/zkp-lec4/</id>
    <published>2023-07-17T16:00:00.000Z</published>
    <updated>2024-01-26T09:07:11.181Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>    In this <a href="/categories/Cryptography-ZKP">series</a>, I will learn <strong>Zero Knowledge Proofs (ZKP)</strong> on this <a href="https://zk-learning.org/">MOOC</a>, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, <strong>Justin Thaler</strong> and Yupeng Zhang. <br>Any corrections and advice are welcome. ^ - ^</div></article><p><strong>Topics:</strong> </p><ul><li>Differences between Interactive Proofs and SNARKs</li><li>Outline of SNARKs from IP</li><li>Brief intro to Functional Commitments</li><li>SZDL Lemma</li><li>Multilinear Extensions</li><li><strong>Sum-check Protocol and its application.</strong><ul><li>Counting Triangles</li><li>SNARK for Circuit-satisfiability</li></ul></li></ul><span id="more"></span><p>Before proceeding to today’s topic, let’s briefly recall what is a SNARK?</p><ul><li>SNARK is a <u>succinct proof that a certain statement is true.</u><br>For example, such a statement could be “I know an m such that SHA256(m)=0”.</li><li>SNARK indicates that <u>the proof is “short” and “fast” to verify.</u><br>Note that if m is 1GB then the trivial proof, i.e. the message m, is neither short nor fast to verify.</li></ul><h1 id="Interactive-Proofs-Motivation-and-Model"><a href="#Interactive-Proofs-Motivation-and-Model" class="headerlink" title="Interactive Proofs: Motivation and Model"></a>Interactive Proofs: Motivation and Model</h1><p>In traditional outsourcing, the Cloud Provider stores the user’s data and the user can ask the Cloud Provider to run some program on its data. The user just blindly trusts the answer returned by the Cloud Provider.</p><img src="https://s1.ax1x.com/2023/07/18/pCozkAf.png" alt="Traditional Outsourcing" style="zoom:33%;" /><p>The <strong>motivation</strong> for <strong>Interactive Proofs (IP)</strong> is that the above procedure can be turned into the following Challenge-Response procedure. The user is allowed to send a challenge to the Cloud Provider and the Cloud Provider is required to respond for such a challenge.</p><p>The user has to accept if the response is valid or reject as invalid.</p><img src="https://s1.ax1x.com/2023/07/18/pCozZ9g.png" alt="Challenge-Response Process" style="zoom:33%;" /><p>Hence, the Challenge-Response procedure or IP can be <strong>modeled</strong> as follows.</p><ul><li>P solves a problem and tells V the answer.<ul><li>Then they have a <u>conversation</u>.</li><li><strong>P’s goal</strong> is to <u>convince V that the answer is correct.</u></li></ul></li><li>Requirements:<ol><li><strong>Completeness</strong>: an honest P can convince V to accept the answer.</li><li><strong>(Statistical) Soundness</strong>: V will catch a lying P with high probability.</li></ol></li></ul><p>Note that statistical soundness is <strong>information-theoretically soundness</strong> so IPs are <u>not based on cryptographic assumptions.</u></p><p>Hence, the soundness must hold even if P <u>is computationally unbounded</u> and trying to trick V into accepting the incorrect answer.</p><p>If soundness holds <u>only against polynomial-time provers</u>, then the protocol is called an <strong>interactive argument</strong>.</p><p>It is worth noting that <u>SNARKs are arguments so it is not statistically sound.</u></p><h2 id="IPs-v-s-SNARKs"><a href="#IPs-v-s-SNARKs" class="headerlink" title="IPs v.s. SNARKs"></a>IPs v.s. SNARKs</h2><p>There are <strong>three main differences</strong> between Interactive Proofs and SNARKs. We’ll list them first and elaborate in the section.</p><ol><li>SNARKs are <u>not statistically sound.</u></li><li>SNARKs have <u>knowledge soundness.</u></li><li>SNARKs are <u>non-interactive.</u></li></ol><h3 id="Not-Statistically-Sound"><a href="#Not-Statistically-Sound" class="headerlink" title="Not Statistically Sound"></a>Not Statistically Sound</h3><p>The first one is mentioned above that SNARKs are arguments so the soundness is only against polynomial-time provers.</p><h3 id="Knowledge-Soundness-v-s-Soundness"><a href="#Knowledge-Soundness-v-s-Soundness" class="headerlink" title="Knowledge Soundness v.s. Soundness"></a>Knowledge Soundness v.s. Soundness</h3><p>The second one is that <strong>SNARKs has knowledge soundness.</strong></p><p><u>SNARKs that don’t have knowledge soundness</u> are called <strong>SNARGs</strong>, they are studied too.</p><p>Considering a public arithmetic circuit such that $C(x,w)=0$ where $x$ is the <strong>public statement</strong> and $w$ is the <strong>secret witness</strong>.</p><img src="https://s1.ax1x.com/2023/07/18/pCozPBt.png" alt="Arithmetic Circuit" style="zoom:33%;" /><p><u>Compare soundness to knowledge soundness</u> for such a circuit-satisfiability.</p><ul><li><strong>Sound</strong>: V accepts → There <strong>exist</strong> $w$ s.t. $C(x,w)=0$.</li><li><strong>Knowledge sound</strong>: V accepts → P actually “<strong>knows</strong>” $w$ s.t. $C(x,w)=0$.<br>The prover is establishing that he necessarily knows the witness.</li></ul><p>As for the soundness, the prover is only establishing the existence of such a witness. </p><p>The knowledge soundness is establishing that the prover necessarily knows the witness.</p><p>Hence, <u>knowledge soundness is stronger.</u></p><p>But sometimes standard soundness is meaningful even in contexts where knowledge soundness isn’t, and vice versa.</p><ul><li><strong>Standard soundness</strong> is meaningful.<ul><li>Because there’s <u>no natural “witness”.</u></li><li>E.g., P claims the output of V’s program on $x$ is 42.</li></ul></li><li><strong>Knowledge soundness</strong> is meaningful.<ul><li>E.g., P claims to <u>know the secret key</u> that controls a certain Bitcoin wallet.</li><li>It is actually claimed that the prover knows a pre-image such that the hash is 0.</li><li>The hash function is surjective so a witness for this claim always exists. In fact, there are many and many witnesses for this claim. It turns to a trivial sound protocol.</li><li>Hence, it needs to establish that the prover necessarily knows the witness.</li></ul></li></ul><h3 id="Non-interactive-and-Public-Verifiability"><a href="#Non-interactive-and-Public-Verifiability" class="headerlink" title="Non-interactive and Public Verifiability"></a>Non-interactive and Public Verifiability</h3><p>The final difference is that <strong>SNARKs are non-interactive.</strong></p><p>Interactive proof and arguments <u>only convince the party that is choosing or sending the random challenges.</u></p><p>This is bad if there are <u>many verifiers</u> as in most blockchain applications. P would <u>have to convince each verifier separately.</u></p><p>For <strong>public coin protocols</strong>, we have a solution, <u>Fiat-Shamir, which renders the protocol non-interactive and publicly verifiable.</u></p> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> <p>In quiz 4, it is a false statement that non-interactive implies publicly verifiable.</p><p>In my perspective, it only holds for non-interactive protocols rendered from the public coin protocols where the verifier only sample random coins and send the sampled coins to the provers.</p> </div> </article> <h1 id="SNARKs-from-Interactive-Proofs-Outline"><a href="#SNARKs-from-Interactive-Proofs-Outline" class="headerlink" title="SNARKs from Interactive Proofs: Outline"></a>SNARKs from Interactive Proofs: Outline</h1><p>We’ll describe the outline to build SNARKs from interactive proofs in this section.</p><h2 id="Trivial-SNARKs"><a href="#Trivial-SNARKs" class="headerlink" title="Trivial SNARKs"></a>Trivial SNARKs</h2><p>The first thing to point out is that the <u>trivial SNARK is not a SNARK.</u></p><p>The <strong>trivial SNARK</strong> is as follows:</p><ol><li>Prover sends $w$ to verifier.</li><li>Verifier checks if $C(x,w)=0$ and accepts if so.<br>The verifier is required to <u>rerun the circuit.</u></li></ol><p>The above trivial SNARK has <strong>two problems:</strong></p><ul><li>The witness $w$ might be long.<ul><li>We want a “short” proof  $\pi$ → $\text{len}(\pi)=\text{sublinear}(|w|)$ </li></ul></li><li>Computing  $C(x,w)$ may be hard.<ul><li>We want a “fast” verifier →  $\text{time}(V)=O_\lambda(|x|, \text{sublinear(|C|)})$. </li></ul></li></ul><p>As described in Lecture 2, <strong>succinctness</strong> means the proof length is sublinear in the length of the witness and the verification time is linear to the length of public statement $x$ and sublinear to the size of the circuit $C$. Note that the verification time linear to $|x|$ means that the verifier at least read the statement $x$.</p><h2 id="Less-Trivial"><a href="#Less-Trivial" class="headerlink" title="Less Trivial"></a>Less Trivial</h2><p>We can make it <strong>less trivial</strong> as follows:</p><ol><li>Prover sends $w$ to verifier.</li><li>Prover <u>uses an IP to prove</u> that $w$ satisfies the claimed property.</li></ol><p>It gives a fast verifier, but the proof is still too long. </p><h2 id="Actual-SNARKs"><a href="#Actual-SNARKs" class="headerlink" title="Actual SNARKs"></a>Actual SNARKs</h2><p>In actual SNARKs, instead of sending $w$, the prover <strong>commits cryptographically to $w$.</strong></p><p>Consequently, the actual SNARKs is described as follows:</p><ol><li>Prover commits cryptographically to $w$.</li><li>Prover uses an IP to prove that $w$ satisfies the claimed property.</li></ol><p>The IP procedure reveals just enough information about the committed witness $w$ to allow the verifier to run its checks.</p><p>Moreover, the IP procedure can be rendered non-interactive via Fiat-Shamir.</p><h1 id="Functional-Commitments"><a href="#Functional-Commitments" class="headerlink" title="Functional Commitments"></a>Functional Commitments</h1><p>There are several important functional families introduced in Lecture 2 we want to build commitment schemes.</p><ul><li><p><strong>Polynomial commitments</strong>: commit to a univariate  $f(X)$ in $\mathbb{F}_p^{(\le d)}[X]$. <br>$f(X)$ is a univariate polynomial in the variable $X$ that has a degree at most $d$.</p><ol><li><p>The <strong>prover</strong> <u>commits to a univariate polynomial</u> of degree $\le d$ .</p></li><li><p>Later the <strong>verifier</strong> <u>requests to know the evaluation of this polynomial at a specific point</u> $r$.</p></li><li><p>The <strong>prover</strong> can <u>reveal $f(r)$ and provide proof</u> that the revealed evaluation is consistent with the committed polynomial.</p><p>Note that the proof size and verifier time should be $O_\lambda(\log d)$ in SNARKs.</p></li></ol></li><li><p><strong>Multilinear commitments</strong>: commit to multilinear  $f$ in $\mathbb{F}_p^{(\le 1)}[X_1,\dots, X_k]$. <br>$f$ is a multilinear polynomial in variables $X_1,\dots, X_k$ where each variable has a degree at most  1. E.g., $f(x_1,\dots, x_k)=x_1x_3 + x_1 x_4 x_5+x_7$.</p></li><li><p><strong>Vector commitments</strong> (e.g. <strong>Merkle trees</strong>): commit to a vector   $\vec{u}=(u_1,\dots, u_d)\in \mathbb{F}_p^d$.   </p><ol><li>The prover commits to a vector of $d$ entries.</li><li>Later the verifier requests the prover to <u>open a specific entry of the vector</u>, e.g. the $i$th entry $f_{\vec{u}}(i)$.</li><li>The prover can open the $i$th entry $f_{\vec{u}}(i)=u_i$ and provide a short proof that the revealed entry is consistent with the committed vector.</li></ol></li><li><p><strong>Inner product commitments</strong> (inner product arguments - IPA): commit to a vector   $\vec{u}=(u_1,\dots, u_d)\in \mathbb{F}_p^d$.  </p><ol><li>The prover commits to a vector $\vec{u}$.</li><li>Later the verifier requests the prover to <u>open an inner product</u> $f_{\vec{u}}(\vec{v})$ that takes a vector $\vec{v}$ as input.</li><li>The prover can open the inner product $f_{\vec{u}}(\vec{v})=(\vec{u},\vec{v})$ and provide a short proof that the revealed inner product is consistent with the committed vector.</li></ol></li></ul><h2 id="Vector-Commitments-Merkle-Trees"><a href="#Vector-Commitments-Merkle-Trees" class="headerlink" title="Vector Commitments: Merkle Trees"></a>Vector Commitments: Merkle Trees</h2><p>In vector commitments, the prover wants to commit a vector.</p><p>We can pair up the values in the vector and hash them to form a binary tree.</p><p>A <strong>Merkle tree</strong> is a binary tree where the <strong>leaf node</strong> stores the <u>values of the vector that we want to commit</u> and the other <strong>internal nodes</strong> calculate the <u>hash value of its two children nodes.</u></p><img src="https://s1.ax1x.com/2023/07/18/pCoziHP.png" alt="Merkle Tree" style="zoom:33%;" /><p>The <strong>root hash</strong> is the <u>commitment of the vector</u> so the prover just sends the root hash to the verifier as the commitment.</p><p>Then the verifier wants to know the 6th entry in the vector. </p><p>The prover <u>provides the 6th entry (T) and proof</u> that the revealed entry is consistent with the committed vector. The proof is also called the authentication information.</p><p>The <strong>authentication information</strong> is the <u>sibling hashes of all nodes on the root-to-leaf path</u> that includes $C, m_4, h_1$. Hence, the proof size is $O(\log n)$ hash values.</p><p>The verifier can check these hashes are consistent with the root hash.</p><p>Under the assumption that $H$ is a collision-resistant hash family, the vector commitment has the <strong>binding</strong> property that <u>once the root hash is sent, the committer is bound to a fixed vector.</u></p><p>Because opening any leaf to two different values requires finding a hash collision along the root-to-leaf path.</p><h2 id="Poly-Commitments-via-Merkle-Trees"><a href="#Poly-Commitments-via-Merkle-Trees" class="headerlink" title="Poly Commitments via Merkle Trees"></a>Poly Commitments via Merkle Trees</h2><p>A natural way of constructing polynomial commitments is to use the Merkle trees. </p><p>For example, we can commit to a univariate $f(X)$ in $\mathbb{F}_7^{(\le d)}[X]$ with the following Merkle tree.</p><img src="https://s1.ax1x.com/2023/07/18/pCozAN8.png" alt="Root Hash is the Commitment" style="zoom:33%;" /><p>When the verifier requests to reveal $f(4)$, the prover can provide $f(4)$ and the following sibling hashes as proof.</p><img src="https://s1.ax1x.com/2023/07/18/pCozKun.png" alt="Authentication Information" style="zoom:33%;" /><p>In summary, if we want to commit a univariate $f(X)$ in $\mathbb{F}^{(\le d)}[X]$, the prover needs to Mekle-commit to all evaluations of the polynomial $f$.</p><p>When the verifier requests $f(r)$, the prover reveals the associated leaf along with opening information.</p><p>However, it has two <strong>problems</strong>.</p><ul><li>The number of leaves is $|\mathbb{F}|$ which means the time to compute the commitment is at least $|\mathbb{F}|$. It is a big problem when working over large fields, e.g., $|\mathbb{F}|\approx 2^{64}$ or $|\mathbb{F}|\approx 2^{128}$.<br>→ We want the time proportional to the degree bound $d$.</li><li>The verifier does not know if $f$ has a degree at most $d$ !.</li></ul><p>In lecture 5, we will introduce <strong>KZG polynomial commitment scheme</strong> using bilinear groups, which addresses both issues.</p><h1 id="Tech-Preliminaries"><a href="#Tech-Preliminaries" class="headerlink" title="Tech Preliminaries"></a>Tech Preliminaries</h1><h2 id="SZDL-Lemma"><a href="#SZDL-Lemma" class="headerlink" title="SZDL Lemma"></a>SZDL Lemma</h2><p>The heart of IP design is based on a simple observation.</p><p>For a non-zero   $f\in \mathbb{F}_p^{(\le d)}[X]$, <u>if we sample a random</u> $r$ from the field $\mathbb{F}_p$, the probability of $f(r)=0$ is at most $d/p$.  </p><p>Suppose $p\approx 2^{256}$ and $d\le 2^{40}$, then $d/p$ is <u>negligible</u>.</p><p>If $f(r)=0$ for a random $r\in \mathbb{F}_p$, then $f$ is identically zero w.h.p.</p><p>It gives us a simple <strong>zero test for a committed polynomial</strong>.</p><p>Moreover, we can achieve a simple equality test for two committed polynomials.</p><p>Let $p,q$ be univariate polynomials of degree at most $d$. Then   $\operatorname{Pr}_{r\overset{\$}\leftarrow \mathbb{F}}[p(r)=q(r)]\le d/p$. </p><p> If   $f(r)=g(r)$ for a random $\overset{\$}\leftarrow \mathbb{F}_p$, then $f=g$ w.h.p. </p><p>The Schwartz-Zippel-Demillo-Lipton lemma is a multivariate generalization of the above facts.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Schwartz-Zippel-Demillo-Lipton Lemma (SZDL Lemma):</strong></p> </div> <div class="message-body"> <p>Let $p,q$ be $\ell$-variate polynomials of total degree at most $d$. Then   $\operatorname{Pr}_{r\in \mathbb{F}^{\ell}}[p(r)=q(r)]\le d/{|\mathbb{F}|}$. </p><p>”Total degree” refers to the maximum sum of degree of all variables in any term. </p> </div> </article> <h2 id="Low-Degree-and-Multilinear-Extensions"><a href="#Low-Degree-and-Multilinear-Extensions" class="headerlink" title="Low-Degree and Multilinear Extensions"></a>Low-Degree and Multilinear Extensions</h2><p>Using many variables, we are <u>able to keep the total degree of polynomials quite low</u>, which ensures the proof is short and fast to verify.</p><p><font color=blue><u><b>Definition of Polynomial Extensions:</b></u></font></p><p>Given a function   $f:\{0,1\}^{\ell}\rightarrow \mathbb{F}$, a $\ell$-variate polynomial $g$ over $\mathbb{F}$ is said to extend $f$ if $f(x)=g(x)$ for all $x\in \{0,1\}^{\ell}$. </p><p>Note that the original domain of $f$ is  $\{0,1\}^{\ell}$  and the domain of extension $g$ is much bigger, that’s $\mathbb{F}^{\ell}$.</p><p><font color=blue><u><b>Definition of Multilinear Extensions:</b></u></font></p><p>Any function   $f:\{0,1\}^{\ell}\rightarrow \mathbb{F}$ has a unique multilinear extension (MLE) denoted by $\tilde{f}$. </p><p>The total degree of the multilinear extension can be vastly smaller than the degree of the original univariate polynomial.</p><p>Consider a univariate polynomial   $f:\{0,1\}^2\rightarrow \mathbb{F}$ as follows. It maps $00$ to $1$, maps $01$ to 2, and so on. </p><img src="https://s1.ax1x.com/2023/07/18/pCozE4S.png" alt="A univariate poly" style="zoom:33%;" /><p>The <strong>multilinear extension</strong>   $\tilde{f}:\mathbb{F}^2\rightarrow \mathbb{F}$ is defined as $\tilde{f}(x_1,x_2)=(1-x_1)(1-x_2)+2(1-x_1)x_2+8x_1(1-x_2)+10x_1x_2$.  </p><p>Its domain is field by field and it’s easy to check that   $\tilde{f}(0,0)=1,\tilde{f}(0,1)=2,\tilde{f}(1,0)=8$ and $\tilde{f}(1,1)=10$. </p><img src="https://s1.ax1x.com/2023/07/18/pCoze3Q.png" alt="The multilinear extension" style="zoom:33%;" /><p>Another non-multilinear extension of $f$ could be defined as   $g(x_1,x_2)=-x_1^2+x_1x_2+8x_1+x_2+1$. </p><img src="https://s1.ax1x.com/2023/07/18/pCoznjs.png" alt="Non-multilinear extension" style="zoom:33%;" /><h3 id="Evaluating-multilinear-extensions-quickly"><a href="#Evaluating-multilinear-extensions-quickly" class="headerlink" title="Evaluating multilinear extensions quickly"></a>Evaluating multilinear extensions quickly</h3><p>The sketch of evaluating the multilinear extension is <u>Lagrange interpolation.</u></p><p><font color=blue><u><b>Fact:</b></u></font></p><p>Given as input all $2^{\ell}$ evaluations of a function   $f:\{0,1\}^\ell \rightarrow \mathbb{F}$, for any point $r\in \mathbb{F}^{\ell}$, there is an $O(2^{\ell})$-time algorithm for evaluating $\tilde{f}(r)$. </p><p><font color=blue><u><b>Algorithm:</b></u></font></p><ul><li><p>Define   $\tilde{\delta}_w(r)=\prod_{i=1}^\ell (r_iw_i+(1-r_i)(1-w_i)).$ </p><p>  This is called the multilinear Lagrange basis polynomial corresponding to $w$. </p><p>  For any input   $r$, $\tilde{\delta}_w(r)=1$ if $r=w$, and $0$ otherwise.  </p></li><li><p>Hence, we can evaluate the multilinear extension of any input $r$ as follows.</p>        $$    \tilde{f}(r)=\sum_{w\in \{0,1\}^\ell}f(w)\cdot \tilde{\delta}_w(r)    $$    </li><li><p><strong>Complexity</strong>: For each   $w\in \{0,1\}^{\ell}$, $\tilde{\delta}_w(r)$ can be computed with $O(\ell)$ field operations, which yields an $O(\ell 2^\ell)$-time algorithm. </p><p>  It can be reduced to time $O(2^\ell)$ via dynamic programming.</p></li></ul>  If we feed this algorithm with the description of $f$ whose domain is $\{0,1\}^\ell$ as inputs and the description consists of all $2^\ell$ evaluations of $f$, then it is possible to evaluate the multilinear extension of $f$ at any desired point. <p>This fact means that evaluating multilinear extension is essentially as fast as $O(2^\ell)$, <u>which is constantly slower than reading the whole description</u> of $f$.</p><h1 id="The-Sum-Check-Protocol"><a href="#The-Sum-Check-Protocol" class="headerlink" title="The Sum-Check Protocol"></a>The Sum-Check Protocol</h1><p>In this part, we’ll introduce the sum-check protocol [Lund-Fortnow-Karloff-Nissan’90].</p><p>We <u>have a verifier with an oracle access</u> to a $\ell$-variate polynomial $g$ over field $\mathbb{F}$. The <strong>verifier’s goal</strong> is to compute the following quantity:</p>  $$\sum_{b_1\in\{0,1\}}\sum_{b_2\in\{0,1\}}\dots \sum_{b_\ell\in\{0,1\}} g(b_1,\dots, b_\ell)$$<blockquote><p>As described above (functional commitments), the prover <strong>commit a multilinear polynomial</strong>, later the verifier can request the prover to evaluate at some specific points. Then the prover provide the evaluation and the proof that the revealed evaluation is consistent with the committed polynomial.</p><p>In this part, we consider this process as <strong>a black box or an oracle</strong>. The verifier <u>can go to the oracle and requests the evaluation</u> of $g$ at some points.</p></blockquote><p>Note that this sum is the sum of all $g$’s evaluations over inputs   $\{0,1\}^\ell$ so the verifier can compute it on his own by just asking the oracle for the evaluations. But it costs the verifier $2^\ell$ oracle queries. </p><h2 id="Protocol"><a href="#Protocol" class="headerlink" title="Protocol"></a>Protocol</h2><p>Instead, we can <strong>offload the work of the verifier to the prover</strong> where <u>the prover computes the sum and convince the verifier that the sum is correct.</u></p><p>It turns out that the verifier only have  to run $\ell$-rounds to check the prover’s answer with only $1$ oracle query.</p><p>Denote $P$ as prover and $V$ as verifier.</p><p>Let’s dive into the start phase and the first round.</p><ul><li><p><u><b>Start</b></u>: $P$ sends claimed answer $C_1$. The protocol must check that:</p>        $C_1=\sum_{b_1\in\{0,1\}}\sum_{b_2\in\{0,1\}}\dots \sum_{b_\ell\in\{0,1\}} g(b_1,\dots, b_\ell)$     </li><li><p><u><b>Round 1:</b></u> $P$ sends a univariate polynomial $s_1(X_1)$ claimed to equal:</p>        $H_1(X_1):=\sum_{b_2\in\{0,1\}}\dots \sum_{b_\ell\in\{0,1\}} g(X_1,\dots, b_\ell)$     <ul><li>$V$ checks that $C_1=s_1(0)+s_1(1)$.<ul><li>If this check passes, it is safe for $V$ to <u>believe that $C_1$ is the correct answer as long as $V$ believes that $s_1=H_1$.</u></li><li>It can be checked that $s_1$ and $H_1$ agree at a random point $r_1\in \mathbb{F}_p$ by SZDL lemma.</li></ul></li><li>$V$ picks $r_1$ at random from $\mathbb{F}$ and sends $r_1$ to $P$.</li></ul></li></ul><p>In round 1, $s_1(X_1)$ is the univariate polynomial that prover actually sends while $H_1(X_1)$ is what the prover claim to send if the prover is honest.</p><p>Note that $H_1(X_1)$ is the true answer except that we cut off the first sum, which leave the first variable free. It reduce $2^\ell$ terms to $2^{\ell-1}$ terms.</p><p>$g$ is supposed to have low degree (2 or 3) in each variable so the univariate polynomial $H_1$ derived from $g$ has low degree.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  <b>Note:</b> We can <u>benefit from the low degree of the univariate polynomial.</u> One is that specifying $H_1$ can be done by just sending 2 or 3 coefficients. Moreover, the low degree gives us acceptable or negligible sound error. </div> </article><p>After receiving the $s_1$, $V$ can compute $s_1(r_1)$ directly, but not $H_1(r_1)$.</p><p>It turns out that $P$ can compute $H_1(r_1)$ and sends claimed $H_1(r_1)$ where $H_1(r_1)$ is the sum of $2^{\ell-1}$ terms where $r_1$ is <u>fixed</u> so the first variable in $g$ is bound to $r_1\in \mathbb{F}$.</p>  $$H_1(r_1):=\sum_{b_2\in\{0,1\}}\dots \sum_{b_\ell\in\{0,1\}} g(r_1,b_2,\dots, b_\ell)$$ <p>Hence, the round 2 is indeed a recursive sub-protocol that checks $s_1(r_1)=H_1(r_1)$ where $s_1(r_1)$ is computed on $V$’s own.</p><ul><li><p><u><b>Round 2:</b></u>  They recursively check that $s_1(r_1)=H_1(r_1)$, i.e. that</p>        $$    s_1(r_1)=\sum_{b_2\in\{0,1\}}\dots \sum_{b_\ell\in\{0,1\}} g(r_1,b_2,\dots, b_\ell)    $$     <ul><li><p>$P$ sends univariate polynomial $s_2(X_2)$ claimed to equal:</p>            $$        H_2(X_1):=\sum_{b_3\in\{0,1\}}\dots \sum_{b_\ell\in\{0,1\}} g(r_1,X_2,\dots, b_\ell)        $$         </li><li><p>$V$ checks that $s_1(r_1)=s_2(0)+s_2(1)$.</p><ul><li>If this check passes, it is safe for $V$ to <u>believe that $s_1(r_1)$ is the correct answer as long as $V$ believes that $s_2=H_2$.</u></li><li>It can be checked that $s_2$ and $H_2$ agree at a random point $r_2\in \mathbb{F}_p$ by SZDL lemma.</li></ul></li><li><p>$V$ picks $r_2$ at random from $\mathbb{F}$ and sends $r_2$ to $P$.</p></li></ul></li><li><p><strong><u><b>Round $i$:</b></u></strong>  They recursively check that</p>        $$    s_{i-1}(r_{i-1})=\sum_{b_i\in\{0,1\}}\dots \sum_{b_\ell\in\{0,1\}} g(r_1,\dots,r_{i-1},b_i,\dots b_\ell)    $$    <ul><li><p>$P$ sends univariate polynomial $s_i(X_i)$ claimed to equal:</p>            $$        H_i(X_i):=\sum_{b_{i+1}\in\{0,1\}}\dots \sum_{b_\ell\in\{0,1\}} g(r_1,\dots,r_{i-1},X_i,\dots b_\ell)        $$         </li><li><p>$V$ checks that $s_{i-1}(r_{i-1})=s_i(0)+s_i(1)$.</p></li><li><p>$V$ picks $r_i$ at random from $\mathbb{F}$ and sends $r_i$ to $P$.</p></li></ul></li><li><p><strong><u><b>Round $\ell$:</b></u></strong> (Final round): They recursively check that</p>        $$    s_{\ell-1}(r_{\ell-1})= \sum_{b_\ell\in\{0,1\}} g(r_1,\dots,r_{\ell-1},b_\ell)    $$     <ul><li><p>$P$ sends univariate polynomial $s_{\ell}(X_\ell)$ claimed to equal :</p>            $$        H_\ell(X_\ell):= g(r_1,\dots,r_{\ell-1},X_\ell)        $$         </li><li><p>$V$ checks that $s_{\ell-1}(r_{\ell-1})=s_\ell(0)+s_\ell(1)$.</p></li><li><p>$V$ picks $r_\ell$ at random, and needs to check that $s_\ell(r_\ell)=g(r_1,\dots,r_\ell)$.</p><ul><li>No need for more rounds. $V$ <u>can perform this check with one oracle query.</u></li></ul></li></ul></li></ul><p>Consequently, the final claim that the verifier is left to check $s_\ell(r_\ell)=g(r_1,\dots,r_\ell)$ <u>where $s_\ell(r_\ell)$ can be computed on its own and $g(r_1,\dots, r_\ell)$ can be computed with just a single query to the oracle.</u></p><p>If the final checks passes, then the verifier is convinced that $s_\ell(r_\ell)=g(r_1,\dots,r_\ell)$ and recursively convinced the claims left in the previous rounds, i.e. $s_i=H_i$. Finally, the verifier accepts the first claim that $C_1$ is the correct sum.</p><h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p><font color=blue><u><b>Completeness:</b></u></font></p><p>Completeness holds by design. If $P$ sends the prescribed message, then all of $V$’s checks will pass.</p><p><font color=blue><u><b>Soundness:</b></u></font></p><p>If $P$ dose not send the prescribed messages, then $V$ rejects with probability at least   $1-\frac{\ell\cdot d}{|\mathbb{F}|}$, where $d$ is the maximum degree of $g$ in any variable. </p><p><strong><font color=blue><u><b><i>Proof of Soundness (Non-Inductive):</i></b></u></font></strong></p><p>It is conducted by the <strong>union bound</strong>. </p><p>Specifically,   if $C_1\ne \sum_{(b_1,\dots, b_\ell)\in \{0,1\}^{\ell}}g(b_1,\dots, b_\ell)$,   then the only way the prover convince the verifier to accept is <u>if there at least one round</u> $i$ such that the prover sends a univariate polynomial $s_i(X_i)$ that dose not equal the prescribed polynomial </p>  $$H_i(X_i)=\sum_{(b_{i+1},\dots, b_\ell)}g(r_1, r_2,\dots, X_i,b_{i+1},\dots, b_\ell)$$<p>yet $s_i(r_i)=H_i(r_i)$.</p><p>For every round $i$, $s_i$ and $H_i$ both have degree at most $d$, and hence if $s_i\ne H_i$, then probability that $s_i(r_i)=H_i(r_i)$ is at most $d/|\mathbb{F}|$. <u>By a union bound over all $\ell$ rounds</u>, the probability that there (is a bad event) is any round $i$ such that the prover send a polynomial $s_i\ne H_i$ yet $s_i(r_i)=H_i(r_i)$ is at most $\frac{\ell\cdot d}{|\mathbb{F}|}$</p><p><font color=blue><u><b><i>Proof  of Soundness by Induction:</i></b></u></font></p><ul><li><p>Base case: $\ell=1$.</p><ul><li>In this case, $P$ sends a single message $s_1(X_1)$ claimed to equal $g(X_1)$.</li><li>$V$ picks $r_1$ at random, and checks $s_1(r_1)=g(r_1)$.</li><li>If $s_1\ne g$, then the probability that $s_1(r_1)=g(r_1)$ is at most $d/|\mathbb{F}|$.</li></ul></li><li><p>Inductive case: $\ell &gt;1$.</p><ul><li>Recall that $P$’s first message $s_1(X_1)$ is claimed to equal $H_1(X_1)$.</li><li>Then $V$ picks a random $r_1$ and sends $r_1$ to $P$. They recursively invoke sum-check to confirm $s_1(r_1)=H_1(r_1)$.</li><li>If $s_1\ne H_1$, then then probability that $s_1(r_1)=H_1(r_1)$ is at most $d/|\mathbb{F}|$.</li><li>Conditioned on $s_1(r_1)=H_1(r_1)$, $P$ is <u>left to prove a false claim in the recursive call.</u><ul><li>The recursive call applies sum-check to $g(r_1, X_2, \dots, X_\ell)$, which is $\ell-1$ variate.</li><li>By induction hypothesis, $P$ convinces $V$ in the recursive call with probability at most $\frac{d(\ell-1)}{|\mathbb{F}|}$.</li></ul></li></ul></li><li><p>In summary, if $s_1\ne H_1$, the probability $V$ accepts is at most</p>        $$    \begin{aligned}    \le &\operatorname{Pr}_{r_1\in \mathbb{F}}[s_1(r_1)=H_1(r_1)]+\operatorname{Pr}_{r_2,\dots, r_\ell\in \mathbb{F}}[V \text{ accepts}\mid s_1(r_1)\ne H_1(r_1)] \\  \le & \frac{d}{|\mathbb{F}|}+ \frac{d(\ell-1)}{|\mathbb{F}|}\le \frac{d\ell}{|\mathbb{F}|}\end{aligned}    $$    </li></ul><h2 id="Costs"><a href="#Costs" class="headerlink" title="Costs"></a>Costs</h2><p>Let $\mathrm{deg}_i(g)$ denote the degree of variable $X_i$ in $g$ and each variable has degree at most $d$.</p><p>$T$ denotes the <u>time required to evaluate $g$ at one point.</u></p><ul><li><p><strong>Total communication</strong> is $O(d\cdot \ell)$ field elements.</p><ul><li>The total prover-to-verifier communication is   $\sum_{i=1}^\ell(\mathrm{deg}_i(g)+1)=\ell+\sum_{i=1}^\ell \mathrm{deg}_i(g)=O(d\cdot \ell)$ field elements. </li><li>The total verifier-to-prover communication is $\ell-1$ field elements.</li></ul></li><li><p><strong>Verifier’s runtime</strong> is $O(d\ell+T)$.</p><ul><li>The running time of the verifier over the entire execution of the protocol is <u>proportional to the total communication</u>, plus the <u>cost of a single oracle query</u> to $g$ to compute $g(r_1,r_2, \dots, r_\ell)$.</li></ul></li><li><p><strong>Prover’s runtime</strong> is $O(d\cdot 2^\ell\cdot T)$.<br>Counting the number of evaluations over $g$ required by the prover is less straightforward.</p><ul><li><p>In round $i$, $P$ is required to send a univariate polynomial $s_i$, which can be <u>specified by $\mathrm{deg}_i(g)+1$ points.</u></p></li><li><p>Hence, $P$ can specify $s_i$ by sending for each $j\in {0, \dots, \mathrm{deg}_i(g)}$ the value:</p>            $$        s_i(j)=\sum_{(b_{i+1},\dots, b_\ell)}g(r_1,\dots,r_{i-1},j,b_{i+1},\dots, b_\ell)        $$         </li><li><p>An important insight is that <u>the number of the terms defining $s_i(j)$ falls geometrically</u> with $i$: in the $i$th sum, there are only   $(1+\mathrm{deg}_i(g))\cdot 2^{\ell-i}\approx d\cdot 2^{\ell-i}$ terms, with the $2^{\ell-i}$ factor due to the number of vectors in $\{0,1\}^{\ell-i}$.  </p></li><li><p>Thus, the total number of terms that must be evaluated is   $\sum_{i=1}^\ell d\cdot 2^{\ell-i}=O(d\cdot 2^{\ell})$. </p></li></ul></li></ul><h1 id="Application-of-Sum-check-Protocol"><a href="#Application-of-Sum-check-Protocol" class="headerlink" title="Application of Sum-check Protocol"></a>Application of Sum-check Protocol</h1><h2 id="An-IP-for-counting-triangles-with-linear-time-verifier"><a href="#An-IP-for-counting-triangles-with-linear-time-verifier" class="headerlink" title="An IP for counting triangles with linear-time verifier"></a>An IP for counting triangles with linear-time verifier</h2><p>The sum-check protocol can be applied to <u>design an IP for counting triangles in a graph with linear-time verifier.</u></p><p>The <strong>input</strong> is an <u>adjacent matrix</u> of a graph   $A\in \{0,1\}^{n\times n}$. </p><p>The <strong>desired output</strong> is   $\sum_{(i,j,k)\in [n]^3}A_{ij}A_{jk}A_{ik}$, which <u>counts the number of triangles in the graph</u>. </p><p>The <strong>fastest known algorithm</strong> runs in matrix-multiplication time, currently about $n^{2.37}$, which is <u>super linear time in the size of the matrix.</u></p><p>Likewise, we can <strong>offload the work to the prover to have a linear-time verifier.</strong></p><p>To design an IP derived from sum-check protocol, we need to <u>view the matrix $A$ to a function</u> mapping   $\{0,1\}^{\log n}\times \{0,1\}^{\log n}$ to $\mathbb{F}$. </p><p>It can be done easily by Lagrange interpolation. As for the following matrix   $A\in \mathbb{F}^{4\times 4}$, we can interpret the entry location $(i,j)\in \{0,1\}^{\log n}\times \{0,1\}^{\log n}$ as input and maps to the corresponding value $A_{i,j}\in \mathbb{F}$. E.g., $A(0,0,0,0)=1,A(0,0,0,1)=3$ and so on.  </p><img src="https://s1.ax1x.com/2023/07/18/pCozmcj.png" alt="View a matrix as a function" style="zoom:33%;" /><p>Note that the <u>domain of function</u>   $A$ is $\{0,1\}^{2\log n}$, which has $2\log n$ variables as inputs. It make sense to extend function $A$ to its multilinear polynomial $\tilde{A}$ with domain $\mathbb{F}^{2\log n}$, <u>each variable having degree at most 1.</u> </p><p>Hence, we can define a polynomial   $g(X,Y,Z)=\tilde{A}(X,Y)\tilde{A}(Y,Z),\tilde{A}(X,Z)$ that <u>has $3\log n$ variables, each variable having degree at most 2.</u> </p><p>Having defined the function $g$ with domain   $\{0,1\}^{3\log n}$,   the <u>prover and the verifier simply apply the sum-check protocol</u> to $g$ to compute: </p> $$\sum_{(a,b,c)\in \{0,1\}^{3\log n}}g(a,b,c)$$<p>In summary, the design of the protocol is as follows.</p><p><font color=blue><u><b>Protocol:</b></u></font></p><ul><li>View $A$ as a function mapping   $\{0,1\}^{\log n}\times \{0,1\}^{\log n}$ to $\mathbb{F}$. </li><li>Extend $A$ to obtain its multilinear extension denoted by $\tilde{A}$.</li><li>Define the polynomial $g(X,Y,Z)=\tilde{A}(X,Y)\tilde{A}(Y,Z),\tilde{A}(X,Z)$.</li><li>Apply the sum-check protocol to $g$ to compute   $\sum_{(a,b,c)\in \{0,1\}^{3\log n}}g(a,b,c)$. </li></ul><p><font color=blue><u><b>Costs:</b></u></font></p><p>Note that $g$ has $3\log n$ variables and it <u>has degree at most 2 in each variable.</u></p><ul><li><strong>Total communication</strong> is $O(\log n)$.</li><li><strong>Verifier runtime</strong> is $O(n^2)$.<ul><li>The total communication is logarithmic.</li><li>Hence, the verifier runtime is dominated by <u>evaluating $g$ at one point</u>  $g(r_1,r_2,r_3)=\tilde{A}(r_1,r_2)\tilde{A}(r_2,r_3)\tilde{A}(r_1,r_3)$, which <u>amounts to evaluating $\tilde{A}$ at three points.</u></li><li>The matrix $A$ gives the lists of all $n^2$ evaluations of the multilinear extension   $\tilde{A}:\{0,1\}^{2\log n}\rightarrow \mathbb{F}$. As described above, the **verifier** can <u>in linear time evaluate the multilinear extension function at any desired point </u> in $\mathbb{F}^{2\log n}$. </li><li>Note that the verifier runtime is <u>linear to the size of the input/matrix</u>, that’s $O(n^2)$.</li></ul></li><li><strong>Prover runtime</strong> is $O(n^3)$.<ul><li>The prover’s runtime is clearly at most $O(n^5)$ since there are $3\log n$ rounds and $g$ can be evaluated at any point in $O(n^2)$ time.</li><li>But more sophisticated algorithm insights can bring the prover runtime down to $O(n^3)$. We recommend reader to refer to Chapter 4 and Chapter 5 in <a href="https://people.cs.georgetown.edu/jthaler/ProofsArgsAndZK.pdf">Thaler</a></li></ul></li></ul><h2 id="A-SNARK-for-circuit-satisfiability"><a href="#A-SNARK-for-circuit-satisfiability" class="headerlink" title="A SNARK for circuit-satisfiability"></a>A SNARK for circuit-satisfiability</h2><p>We can apply the sum-check protocol to design <strong>SNARKs for circuit satisfiability.</strong></p><p>Given an arithmetic circuit $C$ over $\mathbb{F}$ of size $S$ and output $y$. $P$ claims to know a $w$ such that $C(x,w)=y$.</p><p>For simplicity, let’s take $x$ to be the empty input.</p><img src="https://s1.ax1x.com/2023/07/18/pCozMBq.png" alt="An arithmetic circuit with no public input" style="zoom:33%;" /><p>A <strong>transcript</strong> $T$ for $C$ is <u>an assignment of a value to every gate as follows.</u></p><img src="https://s1.ax1x.com/2023/07/18/pCoz1EV.png" alt="Transcript of circuit" style="zoom:33%;" /><p>$T$ is a <strong>correct transcript</strong> if it assigns the gate values obtained by evaluating $C$ on a valid witness $w$.</p><p>The <strong>main idea</strong> is to <u>assign each gate in $C$ a $(\log S)$-bit label</u> and <u>view such a transcript as a function</u> with domain   $\{0,1\}^{\log S}$ mapping to $\mathbb{F}$. </p><img src="https://s1.ax1x.com/2023/07/18/pCoz3NT.png" alt="View transcript as a function" style="zoom:33%;" /><p>Hence, $P$’s first message is <u>a $(\log S)$-variate polynomial $h$ claimed to extend a correct transcript</u> $T$, which means </p>  $$h(x)=T(x) \text{ }\forall x\in \{0,1 \}^{\log S}$$<p>As usual, $T$ is defined over the hypercube   $\{0,1\}^{\log S}$   and $h$ is multilinear extension of $T$ with domain   $\mathbb{F}^{\log S}$. </p><p>$V$ can check this claim by evaluating all $S$ evaluations on $h$.</p><p>Like the sum-check protocol, suppose the <strong>verifier</strong> is <u>only able to learn a few evaluations of $h$ rather than $S$ points.</u></p><h3 id="Intuition-of-extension-function"><a href="#Intuition-of-extension-function" class="headerlink" title="Intuition of extension function"></a>Intuition of extension function</h3><p>Before describing the design details, let’s dig the <strong>intuition</strong> for <u>why we use the extension polynomial $h$ of the transcript $T$ for $P$ to send.</u></p><p>Intuitively, we think $h$ as a <strong>distance-amplified encoding</strong> of the transcript $T$.</p><p>The domain of $T$ is   $\{0,1\}^{\log S}$. The domain of $h$ is $\mathbb{F}^{\log S}$, <u>which is vastly bigger</u>. </p><img src="https://s1.ax1x.com/2023/07/18/pCozQH0.png" alt="Distance-amplying nature of extension poly" style="zoom:43%;" /><p>By Schwart-Zippel lemma, if two <strong>transcripts</strong> <u>disagree at even a single gate value</u>, their <strong>extension polynomial</strong> $h,h’$ <u>disagree at almost all points</u> in   $\mathbb{F}^{\log S}$. Specifically, a $1-\log (S)/|\mathbb{F}|$ fraction. </p><p>The distance-amplifying nature of the encoding will enable $V$ to detect even a single “inconsistency” in the entire transcript.</p><p>As a result, it kind of <u>blows up the tiny difference in transcripts</u> by <strong>the extension polynomials</strong> <u>into easily detectable difference</u> so that  it can be detectable even by the verifier that is only allowed to evaluate the extension polynomials at a single point or a handful points.</p><h3 id="Two-step-plan-of-attack"><a href="#Two-step-plan-of-attack" class="headerlink" title="Two-step plan of attack"></a>Two-step plan of attack</h3><p>The <strong>original claim</strong> the prover makes is that <u>the $(\log S)$-variate polynomial $h$ extends the correct transcript.</u> </p><p>In order to offload work of the verifier and apply the sum-check protocol, the prover instead <strong>claims</strong> a <u>related $(3\log S)$-variate polynomial $g_h=0$ at every single boolean input</u>, i.e. $h$ extends a correct transcript $T$ ↔   $g_h(a,b,c)=0$ $\forall (a,b,c)\in \{0,1\}^{3\log S}$. </p><p>Moreover, to evaluate $g_h(r)$ at any input $r$, suffices to evaluate $h$ at only 3 inputs. Specifically, the first step is as follows.</p><p><font color=blue><u><b>Step 1:</b></u></font> Given any $(\log S)$-variate polynomial $h$, identify a related $(3\log S)$-variate polynomial $g_h(a,b,c)$ via</p>  $$\widetilde{add}(a,b,c)\cdot (h(a)-(h(b)+h(c))+\widetilde{mult}(a,b,c)\cdot (h(a)-h(b)\cdot h(c))$$<ol><li>$\widetilde{add},\widetilde{mult}$ are multilinear extension called <strong>wiring predicates of the circuit</strong>. $\widetilde{add}(a,b,c)$ splits out 1 iff $a$  is assigned to an addition gate and its two input neighbors are $b$ and $c$. Likewise, $\widetilde{mult}(a,b,c)$ splits out 1 iff $a$ is assigned to the product  of values assigned to $b$ and $c$.</li><li>$g_h(a,b,c)=h(a)-(h(b)+h(c))$ if $a$ is the label of a gate that computes the <strong>sum</strong> of gates $b$ and $c$.</li><li>$g_h(a,b,c)=h(a)-(h(b)\cdot h(c))$ if $a$ is the label of a gate that computes the <strong>product</strong> of gates $b$ and $c$.</li><li>$g_h(a,b,c)=0$ otherwise.</li></ol><hr><p>Then we need to <strong>design an interactive proof</strong> to check that   $g_h(a,b,c)=0 \text{ } \forall (a,b,c)\in \{0,1\}^{3\log S}$ in which $V$ <u>only needs to evaluate $g_h(r)$ at one random point</u> $r\in \mathbb{F}^{3\log S}$. </p><p>It is very <strong>different</strong> from the zero test. </p><p>Using zero test, we are able to check   $g_h=0$ for any input in $\mathbb{F}^{3\log S}$ by evaluating a random point $r$, but now we need to check $g_h=0$ over a hypercube $\{0,1\}^{3\log S}$. </p><p><b><font color=red>Imagine</font></b> for a moment that $g_h$ <u>were a univariate polynomial</u> $g_h(X)$.</p>  And rather than needing to check that $g_h$ vanishes over input set $\{0,1\}^{3\log S}$, we <u>need to check that $g_h$ vanishes over some set</u> $H\subseteq \mathbb{F}$. <p>We can design the polynomial IOP based on following fact. </p><p><font color=blue><u><b>Fact:</b></u></font></p><p> $g_h(x)=0$ for all $x\in H$ ↔ $g_h$ is divisible by $Z_H(x)=\prod_{a\in H}(x-a)$.</p><p>We call $Z_H$ the vanishing polynomial for $H$.</p><p>The polynomial IOP works as follows. More details can be referred to the next Lecture.</p><p><font color=blue><u><b>Polynomial IOP:</b></u></font></p><ol><li>$P$ sends a polynomial $q$ such that $g_h(X)=q(X)\cdot Z_H(X)$.</li><li>$V$ checks this by picking a random $r\in \mathbb{F}$ and checking that $g_h(r)=q(r)\cdot Z_H(r)$.</li></ol><p>However, it <font color=red>dosen’t work</font> when $g_h$ is <u>not a univariate polynomial</u>. Moreover, <u>having $P$ find and send the quotient polynomial is expensive</u> for high-degree polynomial.</p><p>In the final SNARK, this would mean applying polynomial commitment to additional polynomials. This is what Marlin, Plonk and Groth16 do. In the next lecture, we will elaborate on the Plonk. </p><hr><p>Instead, the <strong>solution</strong> is to <u>use the sum-check protocol.</u></p><p>Concretely speaking, the sum-check protocol is <u>able to handle multivariate polynomials</u> and <u>dosen’s require $P$ to send additional large polynomials.</u></p><p>For simplicity, <u>imagine working over the integers</u> instead of $\mathbb{F}$.</p><p>The general idea is as follows.</p><p>(Note that it is not a full version of solution.)</p><p><font color=blue><u><b>Step2: General Idea of IP</b></u></font></p><ul><li><p>$V$ checks this by running <strong>sum-check protocol</strong> with $P$ to compute:</p>        $$    \sum_{a,b,c\in \{0,1\}^{\log S}}g_h(a,b,c)^2    $$     <ul><li>If all terms in the sum are 0, the sum is 0.</li><li>If working over the integers, <u>any non-zero term in the sum will cause the sum to be strictly positive.</u></li></ul></li><li><p>At end of sum-check protocol, $V$ needs to evaluate $g_h(r_1, r_2, r_3)$.</p><ul><li>Suffices to evaluate $h(r_1),h(r_2),h(r_3)$.</li><li>Outside of these evaluations, $V$ runs in time $O(\log S)$ with $3\log S$ rounds.</li><li>$P$ performs $O(S)$ field operations given a witness $w$.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
    In this &lt;a href=&quot;/categories/Cryptography-ZKP&quot;&gt;series&lt;/a&gt;, I will learn &lt;strong&gt;Zero Knowledge Proofs (ZKP)&lt;/strong&gt; on this &lt;a href=&quot;https://zk-learning.org/&quot;&gt;MOOC&lt;/a&gt;, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, &lt;strong&gt;Justin Thaler&lt;/strong&gt; and Yupeng Zhang. 
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;



&lt;p&gt;&lt;strong&gt;Topics:&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Differences between Interactive Proofs and SNARKs&lt;/li&gt;
&lt;li&gt;Outline of SNARKs from IP&lt;/li&gt;
&lt;li&gt;Brief intro to Functional Commitments&lt;/li&gt;
&lt;li&gt;SZDL Lemma&lt;/li&gt;
&lt;li&gt;Multilinear Extensions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sum-check Protocol and its application.&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Counting Triangles&lt;/li&gt;
&lt;li&gt;SNARK for Circuit-satisfiability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-ZKP" scheme="https://f7ed.com/categories/Cryptography-ZKP/"/>
    
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="ZKP" scheme="https://f7ed.com/tags/ZKP/"/>
    
      <category term="IP" scheme="https://f7ed.com/tags/IP/"/>
    
      <category term="SNARKs" scheme="https://f7ed.com/tags/SNARKs/"/>
    
      <category term="Sum-check" scheme="https://f7ed.com/tags/Sum-check/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 17</title>
    <link href="https://f7ed.com/2022/09/03/mit6875-lec17/"/>
    <id>https://f7ed.com/2022/09/03/mit6875-lec17/</id>
    <published>2022-09-02T16:00:00.000Z</published>
    <updated>2022-09-11T03:57:50.290Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Definition of IND-CCA Security</li><li>Application of NIZK: Construction of CCA-secure encryption scheme</li></ul><span id="more"></span><p>In last post, we saw a NIZK for 3SAT, and then we can <u>get a NIZK for all of NP in the CRS model.</u></p><p>Moreover, we mentioned that <u>if we weaken the notion of proofs into arguments</u>, then we can <u>construct perfect zk for all of NP.</u></p><p>Today, we will discuss the <strong>application of NIZK</strong>, non-malleable and chosen ciphertext secure encryption scheme.</p><h1 id="Active-Attacks-against-CPA-secure-Encryption"><a href="#Active-Attacks-against-CPA-secure-Encryption" class="headerlink" title="Active Attacks against CPA-secure Encryption"></a>Active Attacks against CPA-secure Encryption</h1><p>Recall the public encryption schemes.</p><img src="https://s1.ax1x.com/2022/09/03/vorxiQ.png" alt="CPA-secure Encryption Scheme" style="zoom:33%;" /><ol><li>Bob generate a pair of keys, a public key $pk$, and a private (or secret) key $sk$.</li><li>Bob “publishes” $pk$ and keeps $sk$ to himself.</li><li>Alice encrypts $m$ to Bob using $pk$.</li><li>Bob decrypts using $sk$.</li></ol><p>In <a href="/2022/07/23/mit6875-lec8/" title="Lecture 8">Lecture 8</a>, we gave the definition of <strong>IND-CPA security</strong> for public encryption scheme and mentioned the <u>IND-CPA-secure is achievable with randomness.</u></p><p>But there are two active attacks against the public encryption scheme above.</p><h2 id="Malleability"><a href="#Malleability" class="headerlink" title="Malleability"></a>Malleability</h2><p>The first active attack is <strong>malleability</strong>.</p><p>Consider the <strong>scenario for bidding.</strong></p><p>Alice wants to bid  ¥100 and she encrypts her bid with public key.</p><p>There is a malicious attacker that wants to win the bidding and he <u>can modify the encryption of  ¥100​ into an encryption of ¥101 as his bidding.</u></p><p>The attack can always modify the encryption, which is always one more dollar than Alice’s although the attack <u>dose not know what Alice’s bid is.</u></p><img src="https://s1.ax1x.com/2022/09/03/vorTxI.png" alt="Malleability" style="zoom:33%;" /><p>So the adversary could <strong>modify (“maul”) an encryption</strong> of $m$ <u>into an encryption of a related message</u> $m’$.</p><h2 id="Chosen-Ciphertext-Attack"><a href="#Chosen-Ciphertext-Attack" class="headerlink" title="Chosen-Ciphertext Attack"></a>Chosen-Ciphertext Attack</h2><p>Another active attack is <strong>chosen-ciphertext attack.</strong></p><p>If the first bit of the message is 0, we define it’s the encryption of a valid message.</p><p>If the first bit of the message is 1, we define it’s the encryption of an invalid message.</p><img src="https://s1.ax1x.com/2022/09/03/vorIGd.png" alt="CCA" style="zoom:33%;" /><p>Then the adversary may <strong>have access to a decryption “oracle”</strong> and can <u>use it to break security of a “target” ciphertext $c^*$ or even extract the secret key!</u></p><p>In fact, <strong>Bleichenbacher</strong> showed <u>how to extract the entire secret key</u> <strong>given only a “ciphertext verification” oracle.</strong></p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  <a href="http://archiv.infsec.ethz.ch/education/fs08/secsem/bleichenbacher98.pdf">Bleichenbacher</a> </div> </article><h1 id="IND-CCA-Security"><a href="#IND-CCA-Security" class="headerlink" title="IND-CCA Security"></a>IND-CCA Security</h1><p>After defining the stronger active attackers, we can define the Indistinguishable Chosen-Ciphertext Attack Security, or <strong>IND-CCA.</strong></p><p>Recall the game in IND-CPA secure definition.</p><p><font color=blue><u><b> Game in IND-CPA (one-message): </b></u></font> </p><img src="https://s1.ax1x.com/2022/09/03/vorHMt.png" alt="Game in IND-CPA" style="zoom:33%;" /><ol><li>The <strong>Challenger</strong> generates a pair of key $(pk, sk)$ and publishes the $pk$.</li><li><strong>Eve</strong> sends two <strong>single messages</strong>, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$.</li><li>The challenger samples $b$ from ${0,1}$ and encrypts the message $m_b$ using $pk$.<br>And send the ciphertext to Eve.</li><li>Eve <u>guesses which message is encrypted</u> and output $b’$.</li><li>Eve wins if $b’=b$.</li></ol><p>Let’s move to the game in IND-CCA.</p><p>As has been said, the adversary has the <u>power of accessing to a decryption “oracle”.</u></p><p>The adversary can <strong>query for the decryption in polynomial many times.</strong></p><p>It can <u>happen both before and after the challenge.</u></p><p><strong>Note</strong> that the adversary can query for the decryption, <strong>before the challenge</strong>, of any ciphertext.</p><p>But <strong>after the challenge</strong>, he <u>cannot query for the decryption of the challenge.</u></p><p>Otherwise, the challenge is meaningless.</p><p>So there are additional two phases in the game of IND-CCA.</p><p><font color=blue><u><b> Game in IND-CCA: </b></u></font> </p><img src="https://s1.ax1x.com/2022/09/03/voroRA.png" alt="Game in IND-CCA" style="zoom:33%;" /><ol><li>The <strong>Challenger</strong> generates a pair of key $(pk, sk)$ and publishes the $pk$.</li><li><font color="blue">Eve <strong>asks for the decryption of any ciphertext</strong> in poly. many times.</font> </li><li><strong>Eve</strong> sends two <strong>single messages</strong>, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$.</li><li>The challenger samples $b$ from ${0,1}$ and encrypts the message $m_b$ using $pk$.<br>And send the ciphertext to Eve.</li><li><font color="blue">Eve <strong>asks for the decryption</strong> of any ciphertext (<strong>except the challenge</strong> $c^\star$) in poly. many times.</font> </li><li>Eve guesses <u>which message is encrypted</u> and output $b’$.</li><li>Eve wins if $b’=b$.</li></ol> <article class="message is-info"> <div class="message-header"> <p><strong>IND-CCA Security Definition</strong></p> </div> <div class="message-body"> <p>The encryption scheme is <strong>IND-CCA</strong> secure if no PPT Eve can win with probability $&gt;1/2+negl(\lambda)$.</p> </div> </article> <h1 id="Constructing-CCA-Secure-Encryption"><a href="#Constructing-CCA-Secure-Encryption" class="headerlink" title="Constructing CCA-Secure Encryption"></a>Constructing CCA-Secure Encryption</h1><p> In the light of the CCA-secure definition, we can construct the CCA-secure encryption scheme.</p><p><strong>Our goal</strong> is that the <u>adversary is hard to modify an encryption</u> of $m$ into an encryption of a related message, say $m+1$.</p><p>Intuitionally, the <strong>proof of knowledge</strong> and the <strong>signatures</strong> should help <u>against the malleability for CAP-secure encryption.</u></p><p>With <strong>NIZK proofs of knowledge</strong>, the idea is that the <u>encryption party attaches an NIZK proof of knowledge of the underlying message to the ciphertext.</u></p><p>Therefore, the encryption consists of CPA-encryption and the NIZK proof of knowledge of the message.</p><p>$C:(c=\text{CPAEnc}(m;r),\text{ proof }\pi \text{ that “I know }m\text{ and }r\text{ “})$.</p><p>This idea will turns out to be useful, but <strong>NIZK proofs themselves can be malleable.</strong></p><p>So the <strong>active attack</strong>  turns to create the CPA-encryption of $(m+1,r)$ and the NIZK proof of $(m+1,r)$.</p><h2 id="Start-with-Digital-Signatures"><a href="#Start-with-Digital-Signatures" class="headerlink" title="Start with Digital Signatures"></a>Start with Digital Signatures</h2><p>Let’s start with digital signatures.</p><p>We construct the CCA-secure encryption, which contains the signature of the CPA-secure encryption.</p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>  Note that it is malleable. </div> </article><p>$C:(c=\text{CPAEnc}(pk,m;r),\text{Sign}_{sgk}(c),vk)$</p><p>where the encryption produces a signing/verification key pair by running $(sgk,vk)\gets \text{Sign.Gen}(1^n)$.</p><p>It is actually <strong>not CCA-secure.</strong> (or it’s malleable)</p><p>If the adversary changes $vk$, all bets are off.</p><p>The picture below explains the reason vividly.</p><img src="https://s1.ax1x.com/2022/09/03/vorbsP.png" alt="NOT CCA-Secure" style="zoom:43%;" /><p>Consequently, the <strong>lesson</strong> is that <u>we need to “tie” the ciphertext $c$ to $vk$ in a “meaningful” way.</u></p><h2 id="IND-CPA-→-“Different-Key-Non-malleability”"><a href="#IND-CPA-→-“Different-Key-Non-malleability”" class="headerlink" title="IND-CPA → “Different-Key Non-malleability”"></a>IND-CPA → “Different-Key Non-malleability”</h2><p>One observation is we can <strong>reduce IND-CPA to Different-Key Non-malleability(NM).</strong></p><p>The <strong>Different-Key NM</strong> predicates that <u>given (independent) $pk,pk’$ and the encryption $\text{CPAEnc}(pk,m;r)$,</u> the adversary <strong>cannot produce</strong> $\text{CPAEnc}(pk’,m+1;r)$.</p><p>It’s a <strong>reduction</strong> that suppose the adversary could produce the different-key encryption, then she can break the IND-CPA security of $\text{CPAEnc}(pk,m;r)$.</p><p><font color=blue><u><b>Proof:</b></u></font> </p><p>Suppose for contradiction that the <strong>adversary</strong> has the <u>power of producing the different-key encryption</u>, $\text{CPAEnc}(pk’,m+1;r)$.</p><p>The interaction with the Diff-Key NM adversary is as follows.</p><img src="https://s1.ax1x.com/2022/09/03/vorqqf.png" alt="Diff-Key NM Game" style="zoom:43%;" /><p><font color=blue><u><i> Interaction with Diff-Key NM Adv.</i></u></font> </p><ol><li>Pick a random pair $(pk’, sk’)$ and <u>give the two public keys $pk,pk’$ .</u></li><li>Give the CPA-secure encryption of $m$ using $pk$.</li><li>The Diff-Key NM adv. <u>promises to produce the CPA-secure encryption</u> of $m+1$ using the different key $pk’$. (w.r.t. contradiction)</li></ol><p>Then we can <strong>construct a CPA adversary</strong> by using the Diff-Key NM adversary, as shown below.</p><img src="https://s1.ax1x.com/2022/09/03/vorXdS.png" alt="Reduction = CPA adversary" style="zoom:33%;" /><p><font color=blue><u><i> Break CPA-secure Encryption </i></u></font> </p><ul><li>The <strong>challenge</strong> is to <u>decrypt the message given CPA-secure encryption.</u><br>(as shown on the left of the picture)</li></ul><ol><li>Given $pk$, then we <u>pick a random pair</u> $(pk’,sk’)$ and send $pk,pk’$ to the adversary.</li><li>Give the CPA-secure encryption from CPA challenge.</li><li>The adversary promises to <u>produce an encryption using the different key</u> $pk’$.</li><li>Then we can decrypt it with $sk’$ and subtract 1 to get $m$.</li></ol><p>Hence, if the adversary can <strong>break the Different-Key NM game</strong>, then she can <strong>break CPA security.</strong></p><h2 id="CCA-Secure-Encryption-Scheme"><a href="#CCA-Secure-Encryption-Scheme" class="headerlink" title="CCA-Secure Encryption Scheme"></a>CCA-Secure Encryption Scheme</h2><p>We can get <strong>non-malleable and CCA-secure encryption</strong> putting <em>CPA-secure encryption</em>, <em>digital signature</em> and <em>NIZK proofs</em> together.</p><p>As has been said, we <u>need to tie the ciphertext $c$ to the verification key $vk$.</u></p><h3 id="NM-Encryption-Scheme"><a href="#NM-Encryption-Scheme" class="headerlink" title="NM Encryption Scheme"></a>NM Encryption Scheme</h3><p><font color=blue><u><b> CCA Encryption (only non-malleable): </b></u></font> </p><ul><li><p>We <u>define $2n$ public keys of the CPA scheme</u> as the <strong>CCA public key.</strong></p></li><li><p><strong>CCA Public Key:</strong></p><p>  $\left[\begin{array}{llll}p k_{1,0} &amp; p k_{2,0} &amp; \dots &amp; p k_{n, 0} \ p k_{1,1} &amp; p k_{2,1} &amp; \dots &amp; p k_{n, 1}\end{array}\right]$ where $n=|vk|$.</p></li></ul><ol><li><p>First, pick a signing/verification key pair $(sgk, vk)$.</p></li><li><p>Then <u>use the CCA public key, based on the bits of</u> $vk$, to produce the ciphertext.</p><p> $CT=[ct_{1,vk_1},ct_{2,vk_2},\dots,ct_{n,vk_n}]$ where $ct_{i,j}\gets \text{CPAEnc}(pk_{i,j},m)$.</p><ul><li><p>This ties the ciphertext $CT$ to the verification key $vk$ in meaningful way that the <u>encryption is under the public key indexed by the bits of $vk$.</u></p><p>For each ciphertext in slot $i$ of $CT$:</p></li><li><p>If the $i$-th bit of $vk$ is 0, then use $pk_{i,0}$ to produce the ciphertext.</p></li><li><p>If the $i$-th bit of $vk$ is 1, then use $pk_{i,1}$ to produce the ciphertext.</p></li></ul></li><li><p>Output $(CT,vk,\sigma=\text{Sign}_{sgk}(CT))$</p></li></ol><p>The encryption scheme above is <strong>non-malleable.</strong></p><p><font color=blue><u><i> Non-malleability rationale: </i></u></font> </p><ul><li>If the adversary <u>keeps the $vk$ the same,</u> she needs to produce $(CT’,vk,\sigma_{sgk}(CT’))$ for the related message $m’$, which <strong>has to break the signature scheme.</strong><ul><li>$CT’$ is encrypted <u>under the same public key</u> as $CT$.</li></ul></li><li>If the adversary <u>changes the $vk$,</u> she has to break the Different-Key Non-malleability game, and therefore <strong>CPA security.</strong><ul><li>The adversary needs the produce $(CT’,vk’,\sigma_{sgk’}(CT’))$ for the related message $m’$.</li><li>$CT’$ is encrypted <u>under the different public key</u>, which is indexed by $vk’$.</li><li>Hence, for <strong>each different bit</strong> of $vk’$:<ul><li>The original  $ct_{i,j}\gets \text{CPAEnc}(pk_{i,j},m)$.</li><li>The adversary needs to produce $ct_{i,j}’\gets \text{CPAEnc}(pk_{i,1\oplus j},m’)$.</li><li>Turns out the Different-Key NM, which can <strong>be reduced to CPA security.</strong></li></ul></li></ul></li></ul><h3 id="CCA-Secure-Encryption-Scheme-1"><a href="#CCA-Secure-Encryption-Scheme-1" class="headerlink" title="CCA-Secure Encryption Scheme"></a>CCA-Secure Encryption Scheme</h3><p>We are not done!!<br>Adversary could <strong>create ill-formed ciphertexts</strong>, e.g. <u>different $ct$s encrypt different messages</u>, and uses it for Bleichenbacher-like attack.</p><p>Hence, it has to <strong>prove that the ciphertext is well-formed.</strong></p><p><font color=blue><u><b> CCA Encryption (non-malleable and CCA-secure): </b></u></font> </p><ul><li><p>We <u>define $2n$ public keys of the CPA scheme</u> as the <strong>CCA public key.</strong></p></li><li><p><strong>CCA Keys:</strong></p><p>  PK = $\left[\begin{array}{llll}p k_{1,0} &amp; p k_{2,0} &amp; \dots &amp; p k_{n, 0} \ p k_{1,1} &amp; p k_{2,1} &amp; \dots &amp; p k_{n, 1}\end{array}\right]$, <font color="blue"> <strong>CRS</strong>  </font> where $n=|vk|$.</p><p>  SK = $\left[\begin{array}{c}sk_{1,0} \ sk_{1,1}\end{array}\right]$</p><ul><li>To achieve NIZK proof, it has to have (public) <strong>CRS</strong>.</li><li>The secret key contains <strong>only one pair</strong> since it needs to be proven well-formed.</li></ul></li></ul><ol><li><p>First, pick a signing/verification key pair $(sgk, vk)$.</p></li><li><p>Then <u>use the CCA public key, based on the bits of</u> $vk$, to produce the ciphertext.</p><p> $CT=[ct_{1,vk_1},ct_{2,vk_2},\dots,ct_{n,vk_n}]$ where $ct_{i,j}\gets \text{CPAEnc}(pk_{i,j},m)$.</p></li><li><p>Generate<font color="blue"> <strong>NIZK proof</strong> $\pi$ = “CT is well-formed”.</font> </p></li><li><p>Output $(CT,{\color{blue}\pi},vk,\sigma=\text{Sign}_{sgk}({\color{blue}{CT,\pi}}))$</p></li></ol><p><font color=blue><u><b> CCA Decryption: </b></u></font> </p><ol><li>Check the signature</li><li>Check the NIZK proof</li><li>Decrypt with $sk_{1,vk_1}$</li></ol><p>Now, this encryption scheme is CCA-secure and non-malleable.</p><p><font color=blue><u><b> Proof of CCA-security: </b></u></font> </p><ul><li><p>Proof Sketch</p><ul><li>Suppose for contradiction that there is an <strong>CCA adversary.</strong></li><li><strong>Play the CCA game with the adversary.</strong><br>Note that we will <u>define several hybrid distributions to argument</u> and <u>play the CCA game in one Hybrid.</u></li><li>Then we can use her to <strong>break either the NIZK soundness/ZK, the signature scheme or the CPA-secure scheme.</strong></li></ul></li><li><p>CCA game is as follows.</p>  <article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> I drew this picture <strong>on my own</strong>. since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article>  <img src="https://s1.ax1x.com/2022/09/03/vorjIg.png" alt="CCA Game" style="zoom:43%;" /></li><li><p><strong>Hybrid 0</strong>: Play the CCA game as prescribed.</p></li><li><p><strong>Hybrid 1</strong>: Observe that $vk_i\ne vk^*$.  <font color="blue"> (<strong>Otherwise break signature.</strong> ) </font> </p><ul><li>Observe that this means <strong>each query ciphertext-tuple</strong> <u>involves a different public-key from the challenge ciphertext.</u></li><li>Then we <u>can use the “different private-key” to decrypt.</u></li><li><font color="blue"><strong>(If the adversary sees a difference, she broke NIZK soundness.)</strong> </font><br>It means that the adversary <u>produces a ill-formed ciphertext</u> and she cheats successfuly.</li></ul></li><li><p>Hybrid 2: Now change the CRS/$\pi$ into <strong>simulated</strong> CRS/$\pi$.  <font color="blue"><strong>(It’s OK by ZK)</strong> </font> </p>  <article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> The following proof is my <strong>own deduction</strong> since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article><ul><li><p>If we want to use the adversary to break CPA security, it <u>has to win the CPA game</u> as follows.</p>  <img src="https://s1.ax1x.com/2022/09/03/vorOZ8.png" alt="CPA Game" style="zoom:33%;" /></li><li><p>We can plop $pk$, given in CPA game, into <u>one slot</u> of $PK$.</p></li><li><p>We plop ciphertext, given in CPA game, into $c^*$  <u>with the corresponding slot.</u></p></li><li><p>For <strong>other slots</strong> in $c^*$, we can <u>generate the ciphertext for random message.</u></p></li><li><p>But we cannot generate NIZK proof $\pi$ since we don’t have the witness.<br>It says that the $c^*$ is ill-formed.</p></li><li><p>But we  <font color="blue"><strong>cannot since we don’t have the witness</strong> </font>, that is $m^*$, the challenge of CPA game.</p></li><li><p>Hence, in Hybrid 2, we change the CRS/$\pi$ into <strong>simulated</strong> CRS/$\pi$.</p></li><li><p>It’s zero-knowledge.</p></li><li><p>But more importantly, we <u>can generate simulated proof.</u></p></li><li><p>Consequently, <u>if the adversary wins in this hybrid</u>, she <strong>breaks IND-CPA</strong> as shown below.</p><article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> I drew this picture <strong>on my own</strong> since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article><img src="https://s1.ax1x.com/2022/09/03/vorzGj.jpg" alt="Reduction = CPA adversary" style="zoom:14%;" /></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Definition of IND-CCA Security&lt;/li&gt;
&lt;li&gt;Application of NIZK: Construction of CCA-secure encryption scheme&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="Application of NIZK" scheme="https://f7ed.com/tags/Application-of-NIZK/"/>
    
      <category term="IND-CCA Security" scheme="https://f7ed.com/tags/IND-CCA-Security/"/>
    
      <category term="CCA-Secure Encryption" scheme="https://f7ed.com/tags/CCA-Secure-Encryption/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 16</title>
    <link href="https://f7ed.com/2022/08/23/mit6875-lec16/"/>
    <id>https://f7ed.com/2022/08/23/mit6875-lec16/</id>
    <published>2022-08-22T16:00:00.000Z</published>
    <updated>2022-09-11T04:01:36.495Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>IP for Quadratic Non-Residuosity</li><li>Non-interactive ZK<ul><li>NIZK in The Common Random String(CRS) Model</li><li>Construction in CRS Model: Blum-Feldman-Micali’88 (quadratic residuosity)</li><li>NIZK for QNR</li><li>NIZK for 3SAT</li></ul></li><li>Proofs vs. Argument</li></ul><span id="more"></span><p>At the end of last blog, we said that Non-Interactive Zero-Knowledge (NIZK) is <u>achievable in random oracle model.</u>  </p><p>Today, we move to the <u>NIZK in the common random string model.</u></p><h1 id="The-Power-of-Interactive-Proofs"><a href="#The-Power-of-Interactive-Proofs" class="headerlink" title="The Power of Interactive Proofs"></a>The Power of Interactive Proofs</h1><p>Before proceeding to NIZK, let’s focus on <strong>the power of Interaction Proofs(IP).</strong></p><p><strong>Is IP more powerful than NP ?</strong></p><p>We have been <u>using interaction to get zero knowledge proofs for NP.</u></p><p>Indeed, <u>interaction is necessary.</u></p><p>But, never mind zero knowledge for a moment.</p><p>Can you <strong>prove more stuff with interactive proofs</strong> than with traditional (i.e. NP) proofs ?</p><p>The thing to point is that we know <strong>there is NP proof for quadratic residues</strong>, i.e. proof = the square root, but <strong>there is no NP proof for quadratic non-residues.</strong></p><p>In Lecture 14, we gave <u>interactive proofs for quadratic residuosity.</u></p><p>Indeed, we can also <u>give an (honest-verifier perfect ZK) interactive for</u> <strong>quadratic non-residuosity.</strong></p><h3 id="IP-for-QNR"><a href="#IP-for-QNR" class="headerlink" title="IP for QNR"></a>IP for QNR</h3><p>The prover wants to <strong>convince</strong> the verifier <u>that the $y$ is a quadratic non-residuosity.</u></p><p>The interactive protocol is as shown below. </p><img src="https://s1.ax1x.com/2022/08/24/vgA3DK.png" alt="IP for QNR" style="zoom:33%;" /><p><font color=blue><u><b> ZK Proof for Quadratic Non-residue: </b></u></font> </p><ul><li>Verifier:<ol><li>pick a random $r$ </li><li>pick a random $b\gets{0,1}$</li><li>send $s=r^2y^b$</li></ol></li><li>Prover:<ul><li>guess $b’$</li></ul></li><li>Verifier Check: $b=b’$</li></ul><p>The protocol is <strong>complete</strong>, <strong>sound</strong> and <strong>(honest-verifier perfect) zero-knowledge.</strong></p><p><font color=blue><u><b><i>Completeness: </i></b></u></font> </p><p>Recall that the completeness is the property of the protocol when the prover and the verifier are both honest.</p><p>If $y$ is <u>non-square, the prover should be able to win.</u></p><p>The thing to point is that the prover knows $y$ is non-square, so <u>maybe the she is unbounded or she knows the factorization</u> of $N$ since there is no NP proof for QNR.</p><p>Hence, if $y$ is <strong>non-square</strong>, then the <u>prover can tell $s$ is square or non-square to answer $b’$.</u></p><p><font color=blue><u><b><i>Soundness: </i></b></u></font> </p><p>Recall that the soundness is the property of the protocol against malicious prover.</p><p>If $y$ is a square, then $r^2y^b$ is also a square.</p><p>Hence, the prover <strong>cannot cheat the verifier</strong> with probability <u>better than</u> $1/2$.</p><p>Then we can <u>use sequential repetition or parallel repetition</u> to reduce the soundness error.</p><p><font color=blue><u><b><i>Honest-Verifier Perfect Zero Knowledge: </i></b></u></font> </p><p>Recall that the <strong>zero-knowledge</strong> is the property of the protocol against verifier.</p><p>The view of $V$ is $(r,b,b’)$, so the simulator can do exactly what $V$ dose.</p><p>The simulated view is $(r,b,b’=b)$ for a random $r$ and a random $b$.</p><hr><p>Similarly, although there is <u>no NP proof</u> for graph non-isomorphism, there is <u>an (honest-verifier perfect) interactive proof</u> for <strong>graph non-isomorphism.</strong></p><p>It turns out that <strong>IP can prove more stuff than NP.</strong></p><p>Indeed, the <strong>IP = PSPACE &gt;&gt; NP.</strong> </p><p><strong>PSPACE</strong> is the <u>set of things that we can decide</u> if we have polynomial memory to work <u>with but potential exponential time.</u></p><p>It’s surprising that we can prove such a language, i.e. QNR, using interactive proofs.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>IP = PSPACE >> NP[Lund-Fortnow-Karloof-Nisan’89, Shamir’90] <p><a href="https://www.cs.princeton.edu/courses/archive/spring09/cos522/BabaiEmail.pdf">https://www.cs.princeton.edu/courses/archive/spring09/cos522/BabaiEmail.pdf</a></p> </div> </article> <h1 id="NIZK-The-Common-Random-String-Model"><a href="#NIZK-The-Common-Random-String-Model" class="headerlink" title="NIZK: The Common Random String Model"></a>NIZK: The Common Random String Model</h1><p>In last blog is introduced that we can achieve <strong>NIZK in the random oracle model.</strong></p><p>In this section, we will introduce another way, <strong>the common random string model.</strong></p><p>In this model, there is <u>an angle coming up with random sequence of polynomial bits.</u></p><p>The angle could be the sunspots.</p><img src="https://s1.ax1x.com/2022/08/24/vgAuC9.png" alt="CRS Model" style="zoom:33%;" /><ul><li><p><strong>Completeness</strong>: For  every $G\in 3COL$, $V$ accepts $P$’s proof.</p></li><li><p><strong>Soundness</strong>: For every $G\notin3COL$ and any “proof” $\pi^\star$, $V(CRS,\pi^\star)$ accepts with probability $\le neg(n)$.</p></li><li><p><strong>Zero Knowledge</strong>: There is a PPT simulator such that for every $G\in 3COL$, $S$ simulates the view of the verifier $V$.</p><p>  $$<br>  S(G)\approx(CRS\gets D, \pi \gets P(G,\text{colors})<br>  $$</p><ul><li>The view of verifier is $(CRS, \pi)$.</li><li>The <strong>simulator</strong> has to produce the simulated view <strong>without knowing the witness and the CRS.</strong> So the simulator has to <u>fake potentially the common random and the proof.</u></li><li>If we sort of <strong>change the definition</strong> that the <u>simulator gets a common random string</u> and has to <strong>produce the view with a fixed CRS</strong>, then this definition is impossible to achieve.<br>For the same reason that we prove the NIZK is impossible (Lecture 15), it dose <u>not satisfy the soundness.</u></li></ul></li></ul><p>Similarly, the <strong>Common Reference String Model</strong> is that there is an <u>angle coming up the random product of two primes</u>, not a sequence of random bits. And it cannot be achieved by the sunspots.</p><img src="https://s1.ax1x.com/2022/08/24/vgAK3R.png" alt="CRS(Reference) Model" style="zoom:33%;" /><h1 id="Construct-NIZK-in-the-CRS-Model"><a href="#Construct-NIZK-in-the-CRS-Model" class="headerlink" title="Construct NIZK in the CRS Model"></a>Construct NIZK in the CRS Model</h1><p>There are several constructions of NIZK in the CRS model.</p><ol><li><strong>Blum-Feldman-Micali’88 (quadratic residuosity)</strong></li><li>Feige-Lapidot-Shamir’90 (factoring)</li><li>Groth-Ostrovsky-Sahai’06 (bilinear maps)</li><li>Canetti-Chen-Holmgren-Lombardi-Rothblum^2-Wichs’19 and Peikert-Shiehian’19 (learning with errors)</li></ol><p>In this Lecture, we will introduce the first construction.</p><h2 id="Blum-Feldman-Micali’88-quadratic-residuosity"><a href="#Blum-Feldman-Micali’88-quadratic-residuosity" class="headerlink" title="Blum-Feldman-Micali’88 (quadratic residuosity)"></a>Blum-Feldman-Micali’88 (quadratic residuosity)</h2><ol><li>Review our number theory hammers &amp; polish them.</li><li>Construct <strong>NIZK for</strong> a special NP language, namely <strong>quadratic non-residuosity.</strong></li><li>Bootstrap to <strong>NIZK for 3SAT</strong>, an NP-complete language.</li></ol><h3 id="Quadratic-Residuosity"><a href="#Quadratic-Residuosity" class="headerlink" title="Quadratic Residuosity"></a>Quadratic Residuosity</h3> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>More introduction is referred in Lecture 9  </div> </article> <p>Let $N=pq$ be a product of two large primes.</p><p>Jacobi symbol ($Jac$) divides $\mathbb{Z}_N^*$ evenly unless $N$ is a perfect square.</p><ul><li>$Jac_{-1}$: non-squares</li><li>$Jac_{+1}$: pseudo-squares</li></ul><img src="https://s1.ax1x.com/2022/08/24/vgAMg1.png" alt="Jac divides ZN*" style="zoom:23%;" /><p>A <strong>surprising fact</strong> is Jacobi symbol $\left(\frac{x}{N}\right)=\left(\frac{x}{P}\right) \left(\frac{x}{Q}\right)$ is <u>computable in poly. time without knowing $p$ and $q$.</u> (using Law of Quadratic Reciprocity.[* 二次互反定理])</p><p>$x$ is square mod $N$ iff $x$ is square mod $p$ and square mod $q$, so we can even $Jac_{+1}$.</p><ul><li>$QR_N$: the set of squares mod $N$.</li><li>$QNR_N$: the set of non-squares mod $N$. (but with Jacobi symbol $+1$)</li></ul><img src="https://s1.ax1x.com/2022/07/29/vPBJkq.png" alt="QR and QNR with Jac+1" style="zoom:33%;" /> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i>Note: The following are new claims:  </div> </article> <p>We call $N$ <font color="blue">good</font> if exactly <u>half the elements</u> of $\mathbb{Z}_N^*$ <u>with Jacobi symbol</u> $+1$ are squares.</p><p>Exactly half residues <strong>even</strong> iff $N=p^iq^j$ is odd, and $i,j\ge 1$, not both even.</p><p>If $N$ is good, there is <font color="blue">an important property</font> is if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$.</p><p>(This property will be used in the following NIZK for QNR)</p><img src="https://s1.ax1x.com/2022/08/24/vgAEHU.png" alt="N is good: exactly half residues even" style="zoom:23%;" /><p>But if $N$ has three or more prime factors, <strong>the fraction of residues is smaller.</strong></p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>Analysis of $N=pqr$:  <p>$x$ is <strong>square</strong> mod $N$ iff $x$ is square mod $p$, mod $q$ and mod $r$.</p><p>If $N=pqr$, the <u>fraction</u> of residues in $Jac_{+1}$ is $1/4$ as shown in table below.</p><table><thead><tr><th align="center">Jac</th><th align="center">(x/p)</th><th align="center">(x/q)</th><th align="center">(x/r)</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">1</td><td align="center">1</td><td align="center">-1</td><td align="center">-1</td></tr><tr><td align="center">1</td><td align="center">-1</td><td align="center">1</td><td align="center">-1</td></tr><tr><td align="center">1</td><td align="center">-1</td><td align="center">-1</td><td align="center">1</td></tr><tr><td align="center">-1</td><td align="center">1</td><td align="center">1</td><td align="center">-1</td></tr><tr><td align="center">-1</td><td align="center">1</td><td align="center">-1</td><td align="center">1</td></tr><tr><td align="center">-1</td><td align="center">1</td><td align="center">1</td><td align="center">-1</td></tr><tr><td align="center">-1</td><td align="center">-1</td><td align="center">-1</td><td align="center">-1</td></tr></tbody></table> </div> </article> <p>Besides, the property, “ if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$”, is<font color=blue> <strong>no longer satisfied.</strong> </font></p><img src="https://s1.ax1x.com/2022/08/24/vgAABT.png" alt="The fraction of residues is smaller" style="zoom:23%;" /><hr><p><font color=blue><u><b> Quadratic Residuosity Assumption (QRA): </b></u></font> </p><p>Let $N=pq$ be a product of two large primes.</p><p>No PPT algorithm can <strong>distinguish</strong> between a random element of $QR_N$ from a random element of $QNR_N$ <strong>given only</strong> $N$.</p><h3 id="NIZK-for-QNR"><a href="#NIZK-for-QNR" class="headerlink" title="NIZK for QNR"></a>NIZK for QNR</h3><p>Define the NP language<font color="blue"> <strong>GOOD</strong> </font>with instance $(N,y)$ where </p><ul><li>$N$ is <strong>good</strong>; and</li><li>$y\in QNR_N$ (that is, $y$ has Jacobi symbol $+1$ but is not a square mod $N$)</li></ul><p>The non-interactive protocol is as follows.</p><p>The prover wants to convince the verifier that $(N,y)$ is good.</p><img src="https://s1.ax1x.com/2022/08/24/vgAZEF.png" alt="NIZK for QNR" style="zoom:23%;" /><p><font color=blue><u><b> NIZK for QNR: </b></u></font> </p><ul><li>$CRS = (r_1,r_2,\dots,r_m)$ where each $r_i$ is sampled from $Jac_N^{+1}$.</li><li>The <strong>proof</strong> is $\forall i: \sqrt{r_i}$ or $\sqrt{yr_i}$</li><li>The <strong>verifier check</strong><ul><li>$N$ is odd</li><li>$N$ is not a prime power</li><li>$N$ is not a perfect square</li><li>(<strong>Fact</strong>: If the preceding three passes, then <u>at most half</u> of $Jac_N^{+1}$ are squares.)</li><li>I received either <u>a mod-$N$ square root</u> of $r_i$ or $yr_i$.</li></ul></li></ul><p><font color=blue><u><b><i>Completeness: </i></b></u></font> </p><p>If $N$ is good and $y\in QNR_N$, the prover can compute either $\sqrt{r_i}$ or $\sqrt{yr_i}$. </p><ul><li>The prover has a knowledge that $y$ is <strong>non-residuosity.</strong> So, maybe <u>she is unbounded or knows the factorization of $N$.</u></li><li>If $r_i\in QR_N$: the prover can compute $\sqrt{r_i}$.</li><li>If $r_i\in QNR_N$: $yr_i$ is <strong>square</strong> by the property above so the prover can compute $\sqrt{yr_i}$.</li></ul><p><font color=blue><u><b><i>Soundness: </i></b></u></font> </p><p>What if $N$ has more than $2$ prime factors ?</p><p>No matter what $y$ is, <font color=blue>for <strong>half</strong> the $r_i$ </font>, both  $r_i$ and $yr_i$ are <strong>not</strong> quadratic residues.</p><p>So the prover <u>cannot compute either $\sqrt{r_i}$ or $\sqrt{yr_i}$ for half the $r_i$.</u></p><ul><li>Suppose $N=pqr$</li><li>The fraction of quadratic residues in $Jac_{+1}$ is $1/4$.</li><li>Besides, the property, “ if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$”, is <strong>no longer satisfied.</strong></li></ul> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i>The thing to notice is that the proof only proves $N$ is good, not $N=pq$.  </div> </article> <p><font color=blue><u><b><i>Perfect Zero Knowledge Simulator S: </i></b></u></font> </p><p>First pick the proof $\pi_i$ to be random in $\mathbb{Z}_N^*$.</p><p>Then <strong>reverse-engineer the CRS</strong>, letting $r_i=\pi_i^2$ or $r_i=\pi_i^2/y$ randomly.</p><p>The distribution of simulated view is <strong>identical</strong> to the real view.</p> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> Warning:We define $CRS = (r_1,r_2,\dots,r_m)$ where each $r_i$ is sampled from $Jac_N^{+1}$.<p>The CRS <strong>depends on the instance $N$.</strong> Not good.<br><strong>An alternative solution</strong> is</p><ol><li>Let CRS <u>be random numbers.</u></li><li>Interpret them as elements of $\mathbb{Z}_N^*$ and <u>both the prover and the verifier filter out $Jac_N^{-1}$.</u></li></ol> </div> </article> <h3 id="NIZK-for-3SAT"><a href="#NIZK-for-3SAT" class="headerlink" title="NIZK for 3SAT"></a>NIZK for 3SAT</h3> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i><b>From Wiki: </b> <p><strong>SAT:</strong><br>In logic and computer science, the <strong>Boolean satisfiability problem</strong> (sometimes called propositional satisfiability problem and abbreviated SATISFIABILITY, <strong>SAT</strong> or B-SAT) is the <u>problem of determining if there exists an interpretation that satisfies a given Boolean formula.</u><br>SAT is the <strong>first problem that was proven to be NP-complete.</strong><br>This means that <u>all problems in the complexity class NP</u>, which includes a wide range of natural decision and optimization problems, are <u>at most as difficult to solve as SAT.</u></p><p><strong>3SAT:</strong><br>Like the satisfiability problem for arbitrary formulas, determining the satisfiability of a formula in conjunctive normal form where <strong>each clause is limited to at most three literals</strong> is NP-complete also; this problem is called <strong>3-SAT</strong>, 3CNFSAT, or 3-satisfiability.</p><p><strong>Literal:</strong><br>In mathematical logic, a literal is <u>an atomic formula (atom) or its negation.</u></p> </div> </article> <ul><li><strong>Boolean Variables</strong>: $x_i$ can be either true(1) or false(0).</li><li>A <strong>Literal</strong> is either $x_i$ or $\overline{x_i}$</li><li>A <strong>Clause</strong> is a <u>disjunction of literals.</u><ul><li>A Clause is <strong>true</strong> if any one of the literals is true.</li><li>E.g. $x_1\vee x_2\vee \overline{x_5}$ is true as long as $(x_1,x_2,x_5)\ne(0,0,1)$</li></ul></li><li>A <strong>3-SAT formula</strong> is a <u>conjunction of many 3-clauses.</u><ul><li>A 3-SAT formula $\Psi$ is <strong>satisfiable</strong> if there is an assignment of values to the variables $x_i$ that makes all its clauses true.</li><li>$\Psi=\left(x_{1} \vee x_{2} \vee \overline{x_{5}}\right) \wedge\left(x_{1} \vee x_{3} \vee x_{4}\right)\wedge\left(\overline{x_{2}} \vee x_{3} \vee \overline{x_{5}}\right)$</li></ul></li></ul> <article class="message is-info"> <div class="message-header"> <p><strong>Cook-Levin Theorem:</strong> </p> </div> <div class="message-body"> <p>It is <strong>NP-complete</strong> to <u>decide whether a 3-SAT formula $\Psi$ is satisfiable.</u></p> </div> </article> <p>Recall NIZK for QNR.</p><p>We saw a way to show that a pair $(N,y)$ is GOOD. That is:</p><ul><li>the following is the picture of $\mathbb{Z}_N^*$ and</li><li>for every $r\in Jac_{+1}$, either $r$ or $ry$ is a quadratic residue.</li></ul><img src="https://s1.ax1x.com/2022/08/24/vgAkuV.png" alt="N is good" style="zoom:23%;" /><p>The prover wants to <u>convince the verifier that $\Psi$ is satisfiable.</u></p><p>The input $\Psi=\left(x_{1} \vee x_{2} \vee \overline{x_{5}}\right) \wedge\left(x_{1} \vee x_{3} \vee x_{4}\right)\wedge\left(\overline{x_{2}} \vee x_{3} \vee \overline{x_{5}}\right)…$ with $n$ variables and $m$ clauses.</p><p>The non-interactive protocol is as follows.</p><img src="https://s1.ax1x.com/2022/08/24/vgAPcq.png" alt="NIZK for 3SAT" style="zoom:33%;" /><p><font color=blue><u><b> NIZK for 3SAT: </b></u></font> </p><ul><li>$CRS=(r_1,r_2,…)$ where each $r_i$ is sampled from $Jac_N^{+1}$.<br>Note: Similarly, we can Let CRS be random numbers and interpret them as elements of $\mathbb{Z}_N^*$, and both the prover and the verifier filter out $Jac_N^{-1}$.</li></ul><ol><li>Prover picks an $(N,y)$ and <strong>proves that it is GOOD.</strong> (as defined above)</li><li>Prover <strong>encodes the satisfying assignment</strong> and sends the <strong>encode variables</strong> $(y_1,\dots,y_n)$ to $V$.<br>We should <u>hide the values to achieve zero knowledge</u> so the encoding is indeed a commitment.<ol><li>$y_i\gets QR_N$ if $x_i$ is false (encryption of $0$ is a residue)</li><li>$y_i \gets QNR_N$ if $x_i$ is true (encryption of $1$ a non-residue)</li><li>For the literal $x_i$, <ol><li>$Enc(x_i)=y_i$ </li><li>$Enc(\overline{x_i})=yy_i$ </li></ol></li><li><strong>exactly one</strong> of the $Enc(x_i)$ or $Enc(\overline{x_i})$ is a <strong>non-residue.</strong><ol><li>A <strong>residue</strong> is indeed <u>the encryption of 0(false)</u></li><li>A <strong>non-residue</strong> is indeed <u>the encryption of 1(true)</u></li></ol></li></ol></li><li><strong>Prove</strong> that <u>(encoded) assignment satisfies each clause.</u><ol><li>For each clause, say $x_1\vee x_2\vee \overline{x_5}$, we <strong>WANT to SHOW</strong>: $x_1$ OR $x_2$ OR $\overline{x_5}$ is <strong>true</strong>.</li><li>Let $(a_1=y_1,b_1=y_2,c_1=yy_5)$ denote the <u>encoded variables.</u></li><li>So, each of them is either $y_i$ (if the literal is a var) or $yy_i$ (if the literal is a negated var).</li><li>Now, we <strong>WANT to SHOW</strong>: $a_1$ OR $b_1$ OR $c_1$ is a <strong>non-residue</strong>.</li></ol></li></ol><p>A <strong>terrible way</strong> is to prove $a_1$ is non-residue or $b_1$ is non-residue. </p><p>If the prover convinces the verifier that $a_1$ is non-residue, $P$  indeed tells $V$ that $x_1$ is true(1).</p><p>Hence, we want to <u>prove one of them is a non-residue</u> <font color=blue> <strong>without telling the verifier that which one is non-residue.</strong> </font></p><p> <font color=blue><u><b> WANT to SHOW: </b></u></font> $a_1$ <strong>OR</strong> $b_1$ <strong>OR</strong> $c_1$ is <u>a non-residue</u></p><ul><li><p>Equivalently, the <strong>signature</strong> of $(a_1,b_1,c_1)$ is <strong>NOT</strong> (QR, QR, QR).</p></li><li><p>A <strong>clever idea</strong> is to generate seven additional triples.</p>  <img src="https://s1.ax1x.com/2022/08/24/vgAm4J.png" alt="Proof of Coverage" style="zoom:33%;" /><ul><li>The original triple is $(a_1,b_1,c_1)$.</li><li>This <strong>8 triples</strong> <u>span all possible QR signatures.</u></li><li>There is <strong>one triple is (QR, QR, QR)</strong>, e.g. the second triple, and <u>reveal the square roots of this triple.</u></li><li>Then we can <u>prove the origin triple is NOT (QR, QR, QR).</u></li></ul></li><li><p>Proof consists of two parts</p><ul><li><font color = blue><strong>Proof of Coverage</strong> </font>: show that the <strong>8 triples span all possible QR signatures.</strong><ul><li>For <u>each</u>  of poly. many triples $(r,s,t)$ from CRS, show <u>one of the 8 triples has the same signature.</u></li><li>That is, there is a triple $(a_i,b_i,c_i)$ s.t. $(ra_i,sb_i,tc_i)$ is (QR, QR, QR)</li></ul></li><li>Show <strong>one triple</strong> (except the original triple) is <strong>(QR, QR, QR) and reveal the square roots.</strong></li></ul></li></ul><p>Hence, <strong>the full proof</strong> is as follows.</p><img src="https://s1.ax1x.com/2022/08/24/vgAeN4.png" alt="NIZK for 3SAT(full version)" style="zoom:33%;" /><ol><li><p>Prover picks an $(N,y)$ and <strong>proves that it is GOOD.</strong></p></li><li><p>Prover <strong>encodes the satisfying assignment</strong> and sends the <strong>encode variables</strong> $(y_1,\dots,y_n)$ to $V$.</p></li><li><p>Prove that (encoded) assignments <strong>satisfy each clause.</strong></p><p> For each clause, construct the proof $\rho$ = <u>(7 additional triples, square root of the second triples, proof of coverage).</u></p></li></ol><p>Completeness and Soundness can be inherited from NIZK for QNR.</p><p><font color=blue><u><b><i>Computational Zero Knowledge Simulator S: </i></b></u></font> </p><p>Simulator picks $(N,y)$ where $y$ is a <strong>quadratic residue.</strong></p><article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> The following proof is my <strong>own deduction</strong> since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article><ul><li>Simulated view of $(N,y, \pi)$<ol><li>Pick the proof $\pi_i$ to be random in $\mathbb{Z}_N^*$.</li><li>Reverse-engineer the CRS, letting $r_i=\pi_i^2$ or $r_i=\pi_i^2/y$ randomly.</li></ol><ul><li>Then  $r_i=\pi_i^2$ and $r_i=\pi_i^2/y$ are both <u>residues</u>.</li><li>The distribution of simulated view is <strong>computationally indistinguishable</strong> to the real view.</li></ul></li><li>Simulated view of encoded assignments $(y_1,\dots, y_n)$<ul><li>$y_i \gets QNR_N$ whether $x_i$ is true or false (encryptions are <strong>both non-residues</strong>)<ul><li>$Enc(x_i)=y_i$ (encryption of $1$ is a non-residue)</li><li>$Enc(\overline{x_i})=yy_i$ (encryption of $0$ is a non-residue)</li></ul></li><li><strong>Encoding of ALL literals can be set to true.</strong></li></ul></li></ul><h1 id="Proofs-vs-Argument"><a href="#Proofs-vs-Argument" class="headerlink" title="Proofs vs. Argument"></a>Proofs vs. Argument</h1><p>Before proceeding to the evolution of proofs, let’s discuss the <strong>difference</strong> between <u>proofs and arguments.</u></p><p>The main difference is the <strong>definition in soundness.</strong></p><p>In <a href="/2022/08/11/mit6875-lec14/" title="Lecture 14">Lecture 14</a>, we gave the <strong>definition of soundness in Proofs</strong> that <u>nobody can convince you a false statement</u>. We mentioned that the soundness holds <strong>agains unbounded provers.</strong></p><p><strong>Arguments</strong> is where the soundness holds <strong>against only computational bounded provers.</strong></p><p>It’s actually weakening the notion of proof.</p><p>Why do we still introduce the the argument ?</p><ol><li>It allows us to get <strong>perfect zero knowledge for all of NP.</strong><br>Recall <u>perfect ZK proofs do not exist for NP-complete languages,</u> unless the polynomial hierarchy collapses. It turns out that if we weaken the notion of proofs, then we can construct the perfect zero knowledge systems.</li><li>It allows us to <strong>get very short proof</strong> = succinct arguments or SNARGs.<br>SNARKs : succinct arguments of knowledge.</li></ol><h1 id="The-Evolution-of-Proofs"><a href="#The-Evolution-of-Proofs" class="headerlink" title="The Evolution of Proofs"></a>The Evolution of Proofs</h1><p>Proofs have been evolving over generations.</p><ul><li><strong>CLASSIC Proofs</strong> (Complexity class: NP) Prover writes down a string (proof); And Verifier checks.</li><li><strong>INTERACTIVE Proofs</strong> (Complexity class: IP = PSPACE &gt;&gt; NP) Prover and verifier talk back and forth. In last blog, we introduced the zk proof for <strong>quadratic non-residue.</strong></li><li><strong>PROBABILISTICALLY CHECKABLE Proofs</strong> (Complexity class: NEXP &gt;&gt; PSPACE) Non-interactive, but verifier only looks at 3 bits of proof.</li><li>MULTIPROVER interactive proofs</li><li>Proofs in the wild: ZCash</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IP for Quadratic Non-Residuosity&lt;/li&gt;
&lt;li&gt;Non-interactive ZK&lt;ul&gt;
&lt;li&gt;NIZK in The Common Random String(CRS) Model&lt;/li&gt;
&lt;li&gt;Construction in CRS Model: Blum-Feldman-Micali’88 (quadratic residuosity)&lt;/li&gt;
&lt;li&gt;NIZK for QNR&lt;/li&gt;
&lt;li&gt;NIZK for 3SAT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Proofs vs. Argument&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="ZK" scheme="https://f7ed.com/tags/ZK/"/>
    
      <category term="PoK" scheme="https://f7ed.com/tags/PoK/"/>
    
      <category term="NIZK" scheme="https://f7ed.com/tags/NIZK/"/>
    
      <category term="CRS Model" scheme="https://f7ed.com/tags/CRS-Model/"/>
    
      <category term="QNR" scheme="https://f7ed.com/tags/QNR/"/>
    
      <category term="3SAT" scheme="https://f7ed.com/tags/3SAT/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 15</title>
    <link href="https://f7ed.com/2022/08/16/mit6875-lec15/"/>
    <id>https://f7ed.com/2022/08/16/mit6875-lec15/</id>
    <published>2022-08-15T16:00:00.000Z</published>
    <updated>2022-08-24T12:15:03.027Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Sequential vs Parallel Repetition: reduce soundness error</li><li>Proof of Knowledge<ul><li>PoK of DLOG</li></ul></li><li>Non-Interactive ZK(NIZK)<ul><li>NIZK in The Random Oracle Model<ul><li>NIZK for 3COL</li></ul></li><li>NIZK in The Common Random String Model (<a href="/2022/08/23/mit6875-lec16/" title="Lecture 16">Lecture 16</a>)</li></ul></li></ul><span id="more"></span><h1 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h1><p>Recap <strong>NP Proofs.</strong></p><p>Give NP Proofs for the NP-complete problem of <u>graph 3-coloring.</u></p><img src="https://s1.ax1x.com/2022/08/24/vcbuUU.png" alt="NP Proof for 3COL" style="zoom:33%;" /><ul><li>Prover $P$: has a <u>witness</u>, the 3-coloring of $G$.</li><li>$P$ gives the proof, <u>the solution to 3-coloring</u> of $G$, to $V$.</li><li>Verifier $V$ checks:<ul><li>only 3 colors are used</li><li>any two vertices connected by an edge are colored differently.</li></ul></li></ul><p>The verify learned <u>the graph $G$ is 3-colorable and the 3-coloring solution.</u></p><p>So NP proofs reveal too much information.</p><p>With <strong>Zero-knowledge (Interactive) Proofs</strong>, the verifier can only learns the graph $G$ is 3-colorable without knowing the solution.</p><img src="https://s1.ax1x.com/2022/08/24/vcbAvn.png" alt="ZK Proof for 3COL" style="zoom:33%;" /><ul><li>Prover $P$:<ol><li><u>permute</u> the colors</li><li><u>commit</u> to each color</li><li>send all the commitments to the verifier.</li></ol></li><li>Verifier $V$: pick a random edge</li><li>Prover $P$: open the vertices of the edge.</li><li>Verifier $V$ checks the openings &amp; the colorings of two vertices are different.</li></ul><p>Besides, we proved the 3COL Protocol is completeness, soundness and zero-knowledge in previous <a href="/2022/08/11/mit6875-lec14/" title="blog">blog</a>.</p><ul><li><strong>Completeness</strong>: For every $G\in 3COL$, $V$ accepts $P$’s proof.</li><li><strong>Soundness</strong>: For every $G\notin 3COL$ and any cheating $P^*$, $V$ rejects $P^*$’s proof with probability $\ge 1-neg(n)$. </li><li><strong>Zero-knowledge</strong>: For every cheating $V^*$, there is a PPT simulator $S$ such that for every $G\in 3COL$, $S$ simulates the view of $V^*$.</li></ul><h1 id="Sequential-vs-Parallel-Repetition"><a href="#Sequential-vs-Parallel-Repetition" class="headerlink" title="Sequential vs Parallel Repetition"></a>Sequential vs Parallel Repetition</h1><p>The 3COL protocol <u>has a large soundness error</u> of $1-1/|E|$, the probability that $V$ accepts even though $G\notin 3COL$.</p><h2 id="Reducing-Soundness-Error"><a href="#Reducing-Soundness-Error" class="headerlink" title="Reducing Soundness Error"></a>Reducing Soundness Error</h2> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem:</strong></p> </div> <div class="message-body"> <p><strong>Sequential Repetition</strong> reduces soundness error for interactive proofs, and <u>preserves the ZK property.</u></p> </div> </article> <p>But it brings about the <strong>problem</strong> that <u>it costs a lot of rounds.</u></p><p>An alternative way is parallel repetition.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem [Goldreich-Krawczyk’90]:</strong></p> </div> <div class="message-body"> <p><strong>Parallel Repetition</strong> also reduces soundness error for interactive proofs.</p><p>It is also <strong>honest-verifier ZK</strong>, but <u>dose not</u>, in general, <u>preserve the ZK property.</u></p> </div> </article> <p>Note: Preserving the ZK property in general means that it is ZK <u>against malicious verifier.</u></p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>There is <b>an intuitive interpretation</b> to the theorem.[Goldreich-Krawczyk’90] <p>The interaction in parallel repetition is $P$ sends all first message in parallel and $V$ <u>response at once with all second messages …</u></p><ul><li><p>If $V$ is <strong>honest verifier</strong>, he indeed dose not look at the commitments, and just <u>picks the random edges independently</u>, which is the same with the sequential repetition.</p></li><li><p>But when $V^*$ is <strong>malicious verifier</strong>, there is no reason that $V^*$ picks the edge independently.<br>$V^*$ can apply a giant hash function and <u>do some bizarre thing to pick these dependent edges.</u></p></li></ul><p>Intuitively, it’s harder to simulate such a thing.</p><p>The <strong>simulator’s strategy in parallel repetition:</strong></p><ol><li><p>$S$ feeds some made up first messages to $V^*$.</p></li><li><p>$V^*$ picks the edges in bizarre manners.</p></li><li><p>$S$ only can answer exactly one challenge.</p></li></ol><p>The <strong>key reason</strong> is the <u>challenge space is exponentially large</u> and the <u>probability of hitting that made up challenge is negligible.</u><br>So this simulation strategy goes down the drain.</p> </div> </article> <p>This theorem tells that <strong>some protocols in parallel repetition is not zero-knowledge</strong> against malicious verifier.</p><p>And the following theorem tells us that <u>the parallel repetition of 3COL protocol is not zero-knowledge</u> if we run it in many and many times in parallel.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem [Holmgren-Lombardi-Rothblum’s21]:</strong></p> </div> <div class="message-body"> <p>Parallel Repetition of the (Goldreich-Micali-Wigderson) 3COL protocol is <strong>not</strong> zero-knowledge.</p> </div> </article> <p>Fortunately, we have zero-knowledge protocols <strong>in const rounds</strong> with <u>exponentially small soundness error</u>, rather in a million rounds.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem [Goldreich-Kahan’95]:</strong></p> </div> <div class="message-body"> <p>There is a <strong>constant-round</strong> ZK proof system for 3COL (will exponentially small soundness error), assuming discrete logarithms are hard.</p> </div> </article> <h1 id="Proofs-of-Knowledge"><a href="#Proofs-of-Knowledge" class="headerlink" title="Proofs of Knowledge"></a>Proofs of Knowledge</h1><p>So far, we focus on the <strong>decision problem</strong>: $y\in \mathcal{L}$  or $y\notin \mathcal{L}$.</p><p>(e.g. $y$ is quadratic residue $\mod N$ or it is not.)</p><p>Here is a <strong>different scenario</strong> that <u>Alice has the knowledge</u>, the discrete log of $y$ assuming $g$ is a generator.</p><p>And Alice wants to <u>convince Bob the discrete log of $y$ always exists.</u></p><img src="https://s1.ax1x.com/2022/08/24/vcbF3j.png" alt="Knowledge is the discrete log" style="zoom:33%;" /><p>In this scenario the <strong>prover wants to convince the verifier</strong> that <u>she knows a solution to a problem</u>, e.g. that she knows the discrete log of $y$.</p><p>It is difficult to formulate it as the decision problem.</p><p>It is <strong>Proof of Knowledge.</strong></p><p>Likewise, we can define the completeness, soundness and zero-knowledge.</p><img src="https://s1.ax1x.com/2022/08/24/vcbC4g.png" alt="Proof of Knowledge" style="zoom:33%;" /><ul><li><strong>Completeness</strong>: When Alice and Bob run the protocol where Alice has input $x$, Bob outputs accept.</li><li><strong>Soundness</strong>: <font color="red">How to define soundness that Alice dose not have the knowledge ?</font><br>It is difficult to <strong>formulate the leak of knowledge.</strong></li><li><strong>Zero-knowledge</strong>: There is a simulator that, given only $y$, outputs a view of Bob that is indistinguishable from his view in an interaction with Alice.</li></ul><h2 id="Extractor"><a href="#Extractor" class="headerlink" title="Extractor"></a>Extractor</h2><p>The <strong>main idea of Goldreich</strong> is that <u>if Alice knows $x$, there must be a way to “extract it from her”.</u></p><p>It’s not about putting diodes on her brain. [*diode 二极管]</p><p>It’s sort of talking to Alice.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Definition of Proof of Knowledge:</strong></p> </div> <div class="message-body"> <p>For any cheating$P^*$, if the prover can convince the verifier that the discrete log of $y$  always exist such that  $\operatorname{Pr}[\langle P^*,V\rangle(y)=\textrm{accept}]\ge \varepsilon$, then there exists an <strong>extractor</strong> $E$ such that $\operatorname{Pr}[E^{P^*}(y)=x \text{ s.t. }y=g^x]\ge \varepsilon'\approx \varepsilon$.</p> </div> </article> <p>The extractor is indeed the <strong>expected ppt adversary.</strong></p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <p>The definition of PoK is proposed in <strong><a href="https://www.wisdom.weizmann.ac.il/~oded/PSX/pok.pdf">On Defining Proofs of Knowledge</a></strong> by Mihir Bellare and Oded Goldreich.</p> </div> </article> <p>We will not dig into the definition but give an example of PoK.</p><h2 id="ZK-Proof-of-Knowledge-of-Discrete-Log"><a href="#ZK-Proof-of-Knowledge-of-Discrete-Log" class="headerlink" title="ZK Proof of Knowledge of Discrete Log."></a>ZK Proof of Knowledge of Discrete Log.</h2><p>The protocol is as follows.</p><img src="https://s1.ax1x.com/2022/08/24/vcbiCQ.png" alt="ZK Proof of Knowledge of DLOG" style="zoom:33%;" /><p><font color=blue><u><b>ZK Proof of Knowledge of DLOG:</b></u></font> </p><ol><li>Prover: Pick a random $r$ and send $z=g^r$ to Verifier.</li><li>Verifier: Pick a random challenge $c$</li><li>Prover: Answer the challenge<ol><li>If $c=0$: send $s=r$ </li><li>If $c=1$: send $s=r+x$</li></ol></li><li>Verifier: Accept iff $g^s=z\cdot y^c$.</li></ol><p>The above protocol is completeness, soundness and zero-knowledge.</p><p>The completeness and zero-knowledge is well proven.</p><p><font color=blue><u><b><i>Completeness: </i></b></u></font> </p><ul><li>If the prover has the discrete log of $y$, the verifier accepts with probability 1.</li><li>$g^s=g^{r+cx}=g^r\cdot (g^{x})^c=z\cdot y^c$</li></ul><p><font color=blue><u><b><i>Zero-knowledge: </i></b></u></font> </p><ul><li>The real view of $V^*$ is $\texttt{view}_{V^*}=(z,c,s)$ </li><li>The simulator works as follows<ol><li>Generate $z=g^s/y^c$ for a random $s$ and a random $c$.</li><li>Feed $z$ to verifier and get the challenge $c^*=V^*(z)$. </li><li>If $c^*=c$, output as the simulated transcript.</li><li>If $c^*\ne c$, back to step 1 and repeat.</li></ol></li><li>The simulated view is <strong>identical</strong> to the view in real execution.</li></ul><p><font color=blue><u><b><i>Soundness: </i></b></u></font> </p><ul><li><p>The <strong>key</strong> is to <u>construct an extractor by the contradiction.</u></p></li><li><p>If the protocol is of <strong>soundness</strong>, the cheating prover $P^*$ can convince the verifier with probability $1/2$.</p></li><li><p>Assume for the <strong>contradiction</strong> that $P^*$ <u>convinces the verifier with probability $\ge 1/2+1/poly$.</u></p></li><li><p>Then the prover $P^*$ <u>should prepare for both challenges.</u></p></li><li><p>It’s easy to <strong>extract</strong> the discrete log of $y$ form $P^*$.</p>  <img src="https://s1.ax1x.com/2022/08/24/vcb9US.png" alt="Extractor" style="zoom:33%;" /><ol><li>Runs $P^*$ with $c=0$ and gets $s_0$.</li><li><u>Rewind</u> $P^*$ to the first message.</li><li>Runs $P^*$ with $c=1$ and gets $s_1$.</li></ol><ul><li>By <strong>contradiction</strong>, $g^{s_0}=z$ and $g^{s_1}=zy$ with probability $1/poly$.</li><li>That is $g^{s_1-s_0}=y$ w.p. $1/poly$.</li><li>So $s_1-s_0$ is the discrete log of $y$ w.p. $1/poly$.</li></ul></li></ul><p>It’s known as Schnorr proof, or Schnorr Signature.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <a href="https://mit6875.github.io/PAPERS/Schnorr-POK-DLOG.pdf">Efficient Signature Generation by Smart Cards</a>  </div> </article> <h1 id="Non-Interactive-ZK"><a href="#Non-Interactive-ZK" class="headerlink" title="Non-Interactive ZK"></a>Non-Interactive ZK</h1><p>Let’s proceed to the next topic.</p><p>Can we make proofs <strong>non-interactive</strong> again ?</p><p>The <strong>advantages</strong> of <strong>Non-Interactive ZK (NIZK)</strong>:</p><ol><li>$V$ <u>dose not need to be online</u> during the proof process.</li><li>Proofs are <u>not ephemeral</u>, can stay into the future.[*ephemeral 短暂的]</li></ol><h2 id="NIZK-is-Impossible"><a href="#NIZK-is-Impossible" class="headerlink" title="NIZK is Impossible"></a>NIZK is Impossible</h2><p>Firstly, we claim that <strong>NIZK is impossible.</strong></p><p>Suppose there were an <u>NIZK proof system for 3COL.</u></p><p>The NIZK proof is of completeness and zero-knowledge, but <strong>NOT sound.</strong></p><p><font color=blue><u><b>Proof:</b></u></font> </p><ol><li><p><strong>Completeness</strong>: When $G$ is in 3COL, $V$ accepts the proof $\pi$.</p> <img src="https://s1.ax1x.com/2022/08/24/vcbkgs.png" alt="completeness" style="zoom:33%;" /></li><li><p><strong>Zero Knowledge</strong>: PPT <strong>simulator</strong> $S$, <u>given only G​ in 3COL</u>, produces an indistinguishable proof $\tilde{\pi}$.<br>In particular, $V$ accepts $\tilde{\pi}$.</p> <img src="https://s1.ax1x.com/2022/08/24/vcbZD0.png" alt="zk" style="zoom:33%;" /></li><li><p>Imagine running the <strong>Simulator</strong> $S$ on a $\underline{G\notin 3COL}$.<br>It produces a proof $\tilde{\pi}$ which the verifier still accepts!</p> <img src="https://s1.ax1x.com/2022/08/24/vcqNyn.png" alt="simulator S for G notin 3COL" style="zoom:33%;" /><p> Because $S$ and $V$ are PPT.<br> They together <strong>cannot tell if the input graph is 3COL</strong> or not without the witness.</p></li><li><p>Therefore, $S$ is indeed <strong>a cheating prover!</strong><br>It <u>can produce a proof for a $G\notin 3COL$ that the verifier nevertheless accepts.</u></p> <img src="https://s1.ax1x.com/2022/08/24/vcbVuq.png" alt="S is indeed a cheating prover" style="zoom:33%;" /></li><li><p>Ergo, the proof system is <strong>NOT SOUND.</strong></p></li></ol><h2 id="Two-Roads-to-NIZK"><a href="#Two-Roads-to-NIZK" class="headerlink" title="Two Roads to NIZK"></a>Two Roads to NIZK</h2><p>But we can <u>achieve NIZK under some models.</u></p><p>There are two roads to NIZK.</p><ol><li>Random Oracle Model &amp; Fiat-Shamir Transform</li><li>Common Random String Model</li></ol><h2 id="NIZK-Random-Oracle-Model"><a href="#NIZK-Random-Oracle-Model" class="headerlink" title="NIZK: Random Oracle Model"></a>NIZK: Random Oracle Model</h2><p>As discussed before <strong>randomness of verifier</strong> for ZK proofs is necessary.</p><p>Otherwise, it is not interactive.</p><p>More discussion about randomness.<br>Give an example in <strong>ZK proof of knowledge for discrete log.</strong><br>The protocol is <strong>not sound</strong> if $P^*$ <u>knows the random challenge $c$ beforehand.</u></p><img src="https://s1.ax1x.com/2022/08/24/vcbnET.png" alt="PoK for DLOG" style="zoom:25%;" /><ul><li><p>If $P^*$ knows $c=0$ beforehand, it’s useless.</p></li><li><p>If $P^*$ knows $c=1$ beforehand,</p><ol><li>Send $z=g^s/y$ for random $s$.</li><li>Send $s$.</li></ol></li></ul><h3 id="NIZK-for-3COL"><a href="#NIZK-for-3COL" class="headerlink" title="NIZK for 3COL"></a>NIZK for 3COL</h3><p>Consider NIZK Proof for 3COL.</p><p>Start with the <strong>parallel repetition of 3COL protocol.</strong></p><p>It is complete, has exponentially small soundness error, and is hones-verifier ZK.</p><img src="https://s1.ax1x.com/2022/08/24/vcbebV.png" alt="ZK Proof for 3COL" style="zoom:33%;" /><p>Similarly, the randomness is necessary. </p><p>Otherwise, the cheating prover can make up message $a$, $z$ beforehand.</p><p>However, the protocol can be <strong>non-interactive in the random oracle model.</strong></p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>Recap Random Oracle Model<a href="/2022/08/04/mit6875-lec12/" title="[Lecture 12]">[Lecture 12]</a><p>In random oracle model, the only way to compute $H$ is by calling the oracle.<br>We can consider it as a very complicated <u>public function</u>, e.g. SHA-3.<br>Moreover, we can consider the public function as a proxy to a random function. </p> </div> </article> <p>But in the <strong>Random Oracle Heuristic world</strong>, the only way to compute $H$, virtually a black box, is by  <u>calling the oracle.</u></p><img src="https://s1.ax1x.com/2022/08/24/vcbK5F.png" alt="NIZK in Random Oracle Model" style="zoom:33%;" /><p><font color=blue><u><b> Fiat and Shamir (1986): </b></u></font>  </p><p>Let $c=H(a)$. Now the prover <strong>can compute the challenge herself!</strong></p><p>It is potentially harmful for soundness.</p><p>But in random oracle model for $H$, it can <strong>prove soundness.</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sequential vs Parallel Repetition: reduce soundness error&lt;/li&gt;
&lt;li&gt;Proof of Knowledge&lt;ul&gt;
&lt;li&gt;PoK of DLOG&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-Interactive ZK(NIZK)&lt;ul&gt;
&lt;li&gt;NIZK in The Random Oracle Model&lt;ul&gt;
&lt;li&gt;NIZK for 3COL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NIZK in The Common Random String Model (&lt;a href=&quot;/2022/08/23/mit6875-lec16/&quot; title=&quot;Lecture 16&quot;&gt;Lecture 16&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="ZK" scheme="https://f7ed.com/tags/ZK/"/>
    
      <category term="PoK" scheme="https://f7ed.com/tags/PoK/"/>
    
      <category term="NIZK" scheme="https://f7ed.com/tags/NIZK/"/>
    
      <category term="3COL" scheme="https://f7ed.com/tags/3COL/"/>
    
      <category term="Random Oracle Model" scheme="https://f7ed.com/tags/Random-Oracle-Model/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 14</title>
    <link href="https://f7ed.com/2022/08/11/mit6875-lec14/"/>
    <id>https://f7ed.com/2022/08/11/mit6875-lec14/</id>
    <published>2022-08-10T16:00:00.000Z</published>
    <updated>2022-08-15T06:56:48.462Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Perfect ZK Proof for QR Language</li><li>Perfect ZK Proof for Graph Isomorphism</li><li>Comp. ZK Proof for 3Coloring</li><li>All NP Languages have Comp. ZK Proofs</li><li>Commitment Schemes</li></ul><span id="more"></span> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i>Note:  <p>The first two sections are also posted at the end of last <a href="/2022/08/09/mit6875-lec13/" title="Lecture">Lecture</a>, just for the continuous and complete description.</p> </div> </article> <h1 id="ZK-Definition"><a href="#ZK-Definition" class="headerlink" title="ZK Definition"></a>ZK Definition</h1><p>In the previous Lecture is introduced the <strong>definition of honest-verifier Zero-knowledge.</strong></p><p>When the theorem is true, the view gives the <strong>honest verifier</strong> $V$ <strong>nothing</strong> that $V$ couldn’t have it on his own.</p><p>In <strong>real world,</strong> the verifier could be <strong>malicious</strong> that he can do anything he wants.</p><p>Refine the ZK definitions <strong>for any verifier</strong> $V^*$.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> Recap:<p>A <strong>language</strong> $\mathcal{L}$  is actually a set of strings which represent true statements.<br>The <strong>view</strong> of $V^*$ is the transcripts and the coins, which contains all the messages going back and forth.</p> </div> </article>  <article class="message is-info"> <div class="message-header"> <p><strong>Perfect Zero-knowledge Definition:</strong></p> </div> <div class="message-body"> <p>An interactive Protocol $(P,V)$ is <strong>perfect zero-knowledge</strong> for a language $\mathcal{L}$ if <font color="red"><strong>for every PPT $V^*$,</strong>  </font>there exists a  (expected) poly time simulator $S$ s.t. for every $x\in \mathcal{L}$, the following two distributions are <strong>identical</strong>:</p><ul><li>$\texttt{view}_{V^*}(P,V^*) $</li><li>$\texttt{sim}_S(x,1^\lambda)$ </li></ul> </div> </article>  <article class="message is-info"> <div class="message-header"> <p><strong>Statistical Zero-knowledge Definition:</strong></p> </div> <div class="message-body"> <p>An interactive Protocol $(P,V)$ is <strong>statistical zero-knowledge</strong> for a language $\mathcal{L}$  if <font color="red"><strong>for every PPT $V^*$,</strong>  </font>there exists a  (expected) poly time simulator $S$ s.t. for every $x\in \mathcal{L}$, the following two distributions are <strong>statistical indistinguishable</strong>:</p><ul><li>$\texttt{view}_{V^*}(P,V^*) $</li><li>$\texttt{sim}_S(x,1^\lambda) $</li></ul> </div> </article>  <article class="message is-info"> <div class="message-header"> <p><strong>Computational Zero-knowledge Definition:</strong></p> </div> <div class="message-body"> <p>An interactive Protocol $(P,V)$ is <strong>computational zero-knowledge</strong> for a language $\mathcal{L}$ if<font color="red"> <strong>for every PPT $V^*$,</strong> </font>there exists a  (expected) poly time simulator $S$ s.t. for every $x\in \mathcal{L}$, the following two distributions are <strong>computationally indistinguishable</strong>:</p><ul><li>$\texttt{view}_{V^*}(P,V^*)$ </li><li>$\texttt{sim}_S(x,1^\lambda) $</li></ul> </div> </article> <h1 id="Perfect-ZK-Proof-for-QR-Language"><a href="#Perfect-ZK-Proof-for-QR-Language" class="headerlink" title="Perfect ZK Proof for QR Language"></a>Perfect ZK Proof for QR Language</h1><p>Recap the zero-knowledge proof for QR Language.</p>$\mathcal{L}=\{(N,y):y \textrm{ is a quadratic residue }\mod N\}$. <img src="https://s1.ax1x.com/2022/08/12/vJUBmF.png" alt="ZK Proof for QR" style="zoom:33%;" /><p>We proved in last Lecture that the QR protocol is honest-verifier zero knowledge.</p><p>Now we only consider the <strong>malicious-verifier zero knowledge.</strong></p><p>The QR protocol is <strong>malicious-verifier zero knowledge</strong>.</p><ul><li>The view of a malicious verifier $V^*$ is $\texttt{view}_{V^*}(P,V^*)=(s,b,z)$.<ul><li>When $V^*$ obtains the $s$, the <strong>only power</strong> that $V^*$ has is to <u>choose the $b$ in a bizarre fashion rather than random.</u></li><li>So the distribution of $b$ is <strong>not random.</strong></li><li>Let $b=V^*(s)$ denote the bizarre thing generated by $V^*$ after receiving $s$.</li></ul></li><li>The simulator $S$ only gets an instance of $(N,y)$ and wants to generate $\texttt{sim}_S((N,y),1^\lambda)=(s,b,y)$.</li></ul><p>In order to produce the same distribution of $b$, the <strong>simulator</strong> $S$ <u>needs to interact with the malicious verifier $V^*$.</u></p><p>It’s sort of a prover which also needs to interact with the verifier.</p><p>The only <strong>distinction</strong> is that the <strong>prover</strong> $P$ has to <u>be online to answer the challenge</u> from the verifier  $V^*$ and the <strong>simulator</strong> $S$ can <u>be offline with the goal of generating the view.</u></p><p><font color=blue><u><b>Claim:</b></u></font> </p><p>The QR protocol is (malicious-verifier) <strong>perfect zero knowledge.</strong></p><p><font color=blue><u><b>Simulator S works as follows:</b></u></font> </p><ol><li>First set $s=z^2/y^b$ for a random $z$ and a random $b$ and “feed” $s$ to $V^*$</li><li>Let $b'=V^*(s)$. (generated by $V^*$ in any bizarre fashion rather than random)</li><li>If $b=b’$, output $(s,b,z)$ and stop.</li><li>Otherwise, go back to step 1 and repeat. (also called “<strong>rewinding</strong>”)</li></ol><p><font color=blue><u><b>Lemma: </b></u></font> </p><ol><li>$S$ runs in expected polynomial-time.</li><li>When $S$ outputs a view, it is <strong>identical</strong> to the view of $V^*$ in a real execution.</li></ol><p>Lemma 1 is well proven.</p><p>The probability of terminating in one iteration is $1/2$ since $b$ is random.</p><p>So the expected iterations is $2$.</p><p><font color=blue><u><b>Proof of Lemma 2:</b></u></font> </p><ul><li>We need to <strong>prove</strong> that the <strong>distribution of every component is identical.</strong></li><li>Real transcript: $\texttt{view}_{V^*}(P,V^*)=(s,b,z)$ <ul><li>$s$ is square as same as random</li><li>$b$ is <u>generated in any bizarre way,</u> i.e. $b=V^*(s)$.</li><li>$z=\sqrt{sy^b}$ is random</li></ul></li><li>Simulated transcript: $\texttt{sim}_S((N,y),1^\lambda)=(s,b,y)$<ul><li>$s=z^2/y^b$ is square as same as random ($(N,y)\in \mathcal{L}$ so $y$ is square)<br>Besides, the distribution of $s$ hides $b$ perfectly.</li><li>$b$ has <strong>the same distribution</strong> with $b’=V^*(s)$.</li><li>$z$ is random</li></ul></li><li>QED</li></ul><p>So far we have proven the QR protocol is (malicious verifier) zero-knowledge.</p><h1 id="ZK-Proof-for-Graph-Isomorphism"><a href="#ZK-Proof-for-Graph-Isomorphism" class="headerlink" title="ZK Proof for Graph Isomorphism"></a>ZK Proof for Graph Isomorphism</h1> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <b>Recap the Graph Isomorphism Problem:</b>Two graphs $G_0$ and $G_1$ are **isomorphic graphs** if they have the same number of vertices, edges and also the same edge connectivity.<p>The graph isomorphism problem is the <strong>computational problem</strong> of determining whether two finite graphs are isomorphic.</p> </div> </article> <p>Now we describe the <strong>zero-knowledge proof for the claim</strong> that $G$ and $H$ are isomorphism graphs.</p><img src="https://s1.ax1x.com/2022/08/12/vJal1x.png" alt="Graph-iso Language" style="zoom:33%;" /><p>The <strong>interaction protocol</strong> is as shown below:</p><img src="https://s1.ax1x.com/2022/08/12/vJaQ91.png" alt="Perfect ZK Proof for Graph-iso" style="zoom:43%;" /><p>Prover has the <strong>knowledge</strong> that she knows a map $\pi$  such that $H=\pi(G)$.</p><p><font color=blue><u><b>ZK Proof for Graph-iso:</b></u></font> </p><ol><li><strong>Prover</strong>:<ol><li>choose <u>a random permutation</u> $\rho$</li><li><u>generate a new random graph</u> $K$ such that $K=\rho(G)$.</li><li><u>send the graph</u> $K$ to verifier</li></ol></li><li><strong>Verifier</strong>: generate a random challenge bit $b$</li><li><strong>Prover</strong>: has to answer the challenge bit<ol><li>If $b=0$: Prover <u>sends the map</u> $\pi_0=\rho$ such that $K=\pi_0(G)$<br>(prove that she can map $G$ → $K$)</li><li>If $b=1$: Prover <u>sends the map</u> $\pi_1=\pi\circ \rho^{-1}$ such that $H=\pi_1(K)$<br>(prove that she can map $K$ → $H$)</li></ol></li></ol><p><font color=blue><u><b>Completeness:</b></u></font> </p><p><strong>Completeness</strong> is a property of the protocol when $P$ and $V$ are both honest.</p><p>We prove that the verifier will pass the equations and accept it.</p><ul><li>If $b=0$: check $K=\pi_0(G)=\rho(G)=K$.</li><li>If $b=1$: check $H=\pi_1(K)=\pi\rho^{-1}(K)=\pi(G)=H$</li></ul><p><font color=blue><u><b>Soundness:</b></u></font> </p><p><strong>Soundness</strong> is a property of the protocol when $P$ is malicious.</p><p>The <strong>verifier has to be sound</strong> because it has to work against an arbitrary $P$.</p><ul><li><strong>Suppose</strong> $G$ and $H$ are <strong>non-isomorphic</strong>, and <u>the prover could answer both the verifier challenges.</u></li><li>Then the prover both prepares the $\pi_0$ and $\pi_1$ such that $K=\pi_0(G)$ and $H=\pi_1(K)$.<br>(Otherwise, she’ll be caught in half chance.)</li><li>In other words, the prover can get $H=\pi_0\circ\pi_1(G)$, a <strong>contradiction</strong>.</li><li>QED.</li></ul><p><font color=blue><u><b>Perfect Zero Knowledge:</b></u></font> </p><article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> The following proof is my <strong>own deduction</strong> since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article><ul><li>The view of an arbitrary verifier $V^*$ is $\texttt{view}_{V^*}(P,V^*)=(K,b,\pi_b)$. <ul><li>When $V^*$ receives the graph $K$, the <strong>only power</strong> that $V^*$ has is to <u>choose the $b$ in a bizarre fashion rather than random.</u></li><li>So the distribution of $b$ is <strong>not random.</strong></li><li>Let $b=V^*(K)$ denote the bizarre thing generated by $V^*$ after receiving $K$.</li></ul></li><li>The simulator $S$ only gets an instance of $(G,H)$ and wants to generate $\texttt{sim}_S((G,H),1^\lambda)=(K,b,\pi_b)$.</li></ul><p><font color=blue><u><b>Simulator S works as follows:</b></u></font> </p><ol><li>Pick a random permutation $\rho$ and a random $b$.</li><li>Generate a graph $K$ such that<ol><li>If $b=0$: $K=\rho(G)$.<br>Let $\pi_0=\rho$.</li><li>If $b=1$: $K=\rho(H)$, i.e. $H=\rho^{-1}(K)$.<br>Let $\pi_1=\rho^{-1}$.</li></ol></li><li>Feed the graph $K$ to verifier $V^*$</li><li>Let $b’=V^*(K)$</li><li>If $b=b’$, output $(K, b, \pi_b)$ and stop.</li><li>Otherwise, go back to step 1 and repeat.</li></ol> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> Intuition of my proof:<p>It sort of split the $K$ into two intermediate points, $K_0$ and $K_1$.<br>The <strong>prover</strong> has the knowledge that could map $G$ to $K$ and map $K$ to $H$.<br>The point is that the <strong>simulator</strong> can <strong>simulate the knowledge</strong> that could map $G$ to $K_0$ and map $K_1$ to $H$.</p> </div> </article> <p><font color=blue><u><b>Lemma: </b></u></font> </p><ol><li>$S$ runs in expected polynomial-time.</li><li>When $S$ outputs a view, it is <strong>identical</strong> to the view of $V^*$ in a real execution.</li></ol><p>Lemma 1 is well proven.</p><p>The probability of terminating in one iteration is $1/2$ since $b$ is random.</p><p>So the expected iterations is $2$.</p><p><font color=blue><u><b>Proof of Lemma 2: </b></u></font> </p><ul><li>We need to <strong>prove</strong> that the <strong>distribution of every component is identical.</strong></li><li>Real transcript: $\texttt{view}_{V^*}(P,V^*)=(K,b,\pi_b)$ <ul><li>$K$ is a <strong>random</strong> graph since the random $\rho$.</li><li>$b$ is generated <strong>in any bizarre way</strong>, i.e. $b=V^*(s)$.</li><li>$\pi_b$ is a <strong>random</strong> map<ul><li>If $b=0$: $\pi_0=\rho$. → random map</li><li>If $b=1$: $\pi_1=\pi\circ \rho^{-1}$. → random map.</li></ul></li></ul></li><li>Simulated transcript: $\texttt{sim}_S((G,H),1^\lambda)=(K,b,\pi_b)$<ul><li>$\pi_b$ is a <strong>random map</strong> since $\rho$ is random.<ul><li>If $b=0$: $\pi_0=\rho$.</li><li>If $b=1$: $\pi_1=\rho^{-1}$.</li></ul></li><li>$K$ is a <strong>random graph.</strong><ul><li>If $b=0$, $K=\pi_0(G)=\rho(G)$</li><li>If $b=1$, $K=\pi_1(H)=\rho^{-1}(H)$</li></ul></li><li>$b$ has <strong>the same distribution</strong> with $b’=V^*(s)$.</li></ul></li><li>QED</li></ul><h1 id="Efficient-Prover-given-a-Witness"><a href="#Efficient-Prover-given-a-Witness" class="headerlink" title="Efficient Prover (given a Witness)"></a>Efficient Prover (given a Witness)</h1><p>So far we keep saying that the <strong>prover is unbounded</strong> and the verifier is ppt.</p><p>But there are no unbounded people.</p><p>In both these protocols above, <font color="red"><strong>the (honest) prover is actually polynomial-time</strong> given the NP witness </font> （the square root of $y$ in the case of QR, and the isomorphism in the case of graph-iso).</p><p>Therefore, the <strong>prover and the verifier can both be polynomial-time.</strong></p><p>The only <strong>difference</strong> between the (honest) prover and the verifier is that the <strong>prover</strong> <u>knows some privileged knowledge</u>, a witness or a solution to a problem, that the <strong>verifier dose not know</strong>.</p><p>That’s the common way to reduce the zero-knowledge proofs.</p><p>The thing to point is that <strong>soundness</strong> is nevertheless <u>against any, even computationally unbounded, prover $P^*$.</u></p><h1 id="All-NP-languages-have-Comp-ZK-Proofs"><a href="#All-NP-languages-have-Comp-ZK-Proofs" class="headerlink" title="All NP languages have Comp. ZK Proofs"></a>All NP languages have Comp. ZK Proofs</h1><p>We shows two languages with <strong>perfect ZK proofs</strong>, the QR protocol and the Graph-iso protocol.</p><ul><li><strong>Do all NP languages have perfect ZK proofs ?</strong><ul><li>The theorem[Fortnow’89, Aiello-Hastad’s 87] answered <strong>NO</strong>, unless bizarre stuff happens in complexity theory.</li><li>Technically, the polynomial hierarchy collapses. That is NP = P.</li></ul></li></ul><p>Nevertheless, we can relax the question.</p><ul><li><p><strong>Do all NP languages have ZK proofs ?</strong></p><ul><li><p><strong>Theorem:</strong>  [Goldreich-Micali-Wigderson’87]</p><p>  Assuming <strong>one-way permutations</strong> exist, all of NP has <strong>computational zero-knowledge proofs.</strong></p></li><li><p>It means that every language of NP problem has computational zero-knowledge proof given a witness.</p></li></ul></li></ul><p>Moreover, the assumption can <strong>be relaxed to one-way functions.</strong></p><p>This <strong>theorem</strong> is amazing and it tells us that <u>everything can be proved (in the sense of Euclid) can be proven in zero knowledge!</u></p><hr><p>How to prove the theorem ? We cannot prove every NP problem one by one.</p><p>Luckily, we can prove <strong>NP-Complete Problem</strong> to <u>which every other problem in NP can be reduced.</u></p><p>It turns out that there are a whole of complete problems. </p><p>There is a list of 20 odd problems that came up with in the 70s already and this list keeps increasing. So NP-complete problems is sort of a wealth.</p><p>We are going to pick the <strong>Graph Coloring Problem</strong> and prove it has Comp. ZK Proofs.</p><h1 id="ZK-Proof-for-3Coloring"><a href="#ZK-Proof-for-3Coloring" class="headerlink" title="ZK Proof for 3Coloring"></a>ZK Proof for 3Coloring</h1><p>Here is the <strong>Graph 3Coloring Problem.</strong></p><p>Given a graph and three colors, red blue and green, you’re <u>supposed to assign colors to every vertex</u> such that <strong>no two adjacent vertexes have the same color.</strong> </p><p>(or every two adjacent vertexes have different colors)</p><img src="https://s1.ax1x.com/2022/08/12/vJanAJ.png" alt="3COL" style="zoom:33%;" /><p>Before proceeding to the zero-knowledge proof of the three-coloring, let’s introduce the <strong>lead-box model.</strong></p><h2 id="The-Lead-box-Model"><a href="#The-Lead-box-Model" class="headerlink" title="The Lead-box Model"></a>The Lead-box Model</h2><p>The lead-box model is as shown below.</p><img src="https://s1.ax1x.com/2022/08/12/vJaKhR.png" alt="The Lead-box" style="zoom:33%;" /><p><font color=blue><u><b>Lead-box Model: </b></u></font> </p><ul><li>The sender Alice has a bit $b$.<ol><li><strong>Commit</strong> to $b$: put $b$ in a lead-box and locks it, and send the box to receiver.</li><li><strong>Open $b$</strong>: send $b$ together with the key.</li></ol></li><li>Then the receiver Bob can check the thing in box is what Alice claims.</li></ul><p>The lead-box above should be <strong>hiding and binding</strong> $b$.</p><p><font color=blue><u><b>Properties: </b></u></font> </p><ul><li><strong>Hiding</strong> means that <u>the lead-box should completely hide $b$.</u></li><li><strong>Blinding</strong> means that the <u>sender shouldn’t be able to open to $1-b$.</u></li></ul><p>Once the Alice sends the box to Bob, she <u>should not be able to change her mind</u> about what’s inside the box. </p><p>That’ blinding. That’s a <strong>commitment</strong>.</p><p>It can be used for <u>computational zero-knowledge.</u></p><p>It can also be used to <u>ensure fairness.</u></p><p>We will later show <strong>how to implement such a lead-box</strong> (as <strong>a commitment protocol</strong>) <u>using one-way permutations.</u></p><h2 id="ZK-Proof-with-Lead-box-Part-I"><a href="#ZK-Proof-with-Lead-box-Part-I" class="headerlink" title="ZK Proof with Lead-box: Part I"></a>ZK Proof with Lead-box: Part I</h2><p>The language $\mathcal{L}$ is that the graph $G$ is 3-colorable.</p><p><u>Given a 3-colorable witness (solution),</u> it <strong>can be proven in computational zero-knowledge.</strong></p><p>The prover is given the graph $G$ and the <strong>3-colorable witness.</strong></p><p>The verifier is given the graph $G$.</p><img src="https://s1.ax1x.com/2022/08/12/vJauN9.png" alt="have Comp. ZK with a witness" style="zoom:33%;" /><p>The interaction of ZK proof is as shown below.</p><img src="https://s1.ax1x.com/2022/08/12/vJa3jK.png" alt="Comp. ZK Proof Protocol with lead-box for 3COL" style="zoom:33%;" /><p><font color=blue><u><b>Interactive Protocol for 3COL: </b></u></font> </p><ol><li>Prover: come up with <strong>a random permutation of the colors</strong>, $\rho:V\rightarrow \{R,G,B\}$. <br>The color of every vertex is <strong>masked</strong> by the random permutation.</li><li>Prover: <strong>commit</strong> to (the color of) every vertex.</li><li>Verifier: pick a random edge $(i,j)$</li><li>Prover: <strong>open</strong> $\rho(i)$ and $\rho(j)$</li><li>Verifier: <strong>check</strong><ol><li>Check the openings that $\rho(i)$ and $\rho(j)$ are what Alice claims.</li><li>Chek the $\rho(i),\rho(j)\in \{R,G,B\}$ </li><li>Check: $\rho(i)\ne \rho(j)$</li></ol></li></ol><p>The <strong>completeness</strong> is well proven.</p><p><font color=blue><u><b>Soundness: </b></u></font> </p><p><strong>Soundness</strong> is the property of the protocol against <strong>dishonest</strong> prover $P$.</p><ul><li>If the graph is <strong>not 3COL</strong>, <u>in every 3-coloring (that $P$ commits to)</u>, <strong>there is some edge</strong> whose end-points <strong>have the same color.</strong></li><li>$V$ will <strong>catch this edge and reject</strong> with probability $\ge 1/|E|$.</li><li>In one time:<ul><li>the verifier accepts with probability $\le1-1/|E|$.</li></ul></li><li>Repeat $|E|\cdot \lambda$ times:<ul><li>he verifier accepts with probability $\le (1-1/|E|)^{|E|\cdot \lambda}\le 2^{-\lambda}$.</li><li>which is <strong>negligible</strong>.</li><li>QED</li></ul></li></ul><p>Moreover, the proof is <strong>Computational Zero-knowledge.</strong></p><p>The <strong>key reason</strong> of zero-knowledge is the <u>prover commits to all colors</u> (of the vertices) <strong>but only open two colors (of the vertexes).</strong></p><p>It <strong>leaks nothing to the verifier</strong> since the <u>colors have been randomly permuted.</u></p><p>So the prover gives zero-knowledge to the verifier.</p><p>We will elaborate the Comp. Zero-knowledge in the following Part II.</p><hr><p>More <strong>analysis into the first message</strong>(the message in the lead-box).</p><p>If the first message <strong>dose not exis</strong>t, the proof is <strong>not sound.</strong></p><p>The malicious prover can always answer “red” and “blue” because the verifier cannot check what Alice claims without the commitment.</p><p>If the first message are <strong>not in the lead-box</strong>, the proof is <strong>not zero-knowledge.</strong></p><h2 id="Commitment-Schemes"><a href="#Commitment-Schemes" class="headerlink" title="Commitment Schemes"></a>Commitment Schemes</h2><p>The lead-box is indeed a <strong>commitment protocol.</strong></p><p>The Commitment Protocol $(S,R)$ works as follows.</p><img src="https://s1.ax1x.com/2022/08/12/vJa1c6.png" alt="Commitment Protocol" style="zoom:33%;" /><p><font color=blue><u><b>Commitment Protocol $(S,R)$: </b></u></font> </p><ul><li>There are two parties, sender $S$ and receiver $R$.</li><li>Sender $S$ <strong>commits to a bit</strong> $b$, so the protocol is instanced to $(S(b,1^\lambda),R(1^\lambda))$.<ul><li>Let $\texttt{dec}$ be the <strong>sender’s output</strong>, decommitment.</li><li>Let $\texttt{com}$ be the <strong>receiver’s output</strong>, commitment.</li></ul></li><li>Sender $S$ <strong>opens</strong> $b$<ul><li>$S$ sends $b$ together with  $\texttt{dec}$.</li></ul></li><li>Receiver $R$ <strong>checks</strong> $b$ using $\texttt{dec}$.</li></ul><p><font color=blue><u><b>Properties of Commitment Protocol:</b></u></font> </p><ol><li><strong>Completeness</strong>: $R$ always accepts in an honest execution.</li><li><strong>Computational Hiding</strong>: <ul><li>For <strong>every possibly malicious</strong> (PPT) $R^*$, $\texttt{view}_{R^*}(S(0),R^*)\approx_c\texttt{view}_{R^*}(S(1),R^*)$     (the view of $R^*$   is $(\texttt{com},b,\texttt{dec})$) </li></ul></li><li><strong>Perfect Binding</strong>: <ul><li>For <strong>every possibly malicious</strong>  $S^*$, let $\texttt{com}$ be the receiver’s output in an execution of $(S^*, R)$. </li><li>There is no pair of decommitments $(\texttt{dec}_0,\texttt{dec}_1)$ s.t. $R$ accepts both $(\texttt{com},0,\texttt{dec}_0)$ and $(\texttt{com},1,\texttt{dec}_1)$.</li></ul></li></ol><p>Completeness is the property of the commitment protocol when $S$ and $R$ are honest.</p><p>Computational Hiding  is the property of commitment protocol against malicious $R^*$.</p><p>Perfect Binding is the property of commitment protocol against malicious $S^*$.</p><h3 id="A-Commitment-Scheme-from-any-OWP"><a href="#A-Commitment-Scheme-from-any-OWP" class="headerlink" title="A Commitment Scheme from any OWP"></a>A Commitment Scheme from any OWP</h3><p>There is a commitment scheme starting from any OWP as follows.</p><img src="https://s1.ax1x.com/2022/08/12/vJaJBD.png" alt="Commitment Scheme from any OWP" style="zoom:33%;" /><p><font color=blue><u><b>Commitment Protocol $(S,R)$ from OWP: </b></u></font> </p><ul><li><p>Sender $S$ <strong>commits</strong> to bit $b$</p><ol><li>Pick a random $r$ as the <strong>decommitment</strong>, $\texttt{dec}=r$.</li><li>Compute the <strong>commitment</strong> $\texttt{com}=(f(r),HCB(r)\oplus b)$.</li><li>Send the commitment to $R$.</li></ol></li><li><p>Sender $S$ <strong>opens</strong> $b$</p><ol><li>Send $(b,r)$ to $R$, $r$ as the $\texttt{dec}$.</li></ol></li><li><p>Receiver $R$ <strong>checks</strong> $b$ using $\texttt{dec}$.</p><ul><li>Let $\texttt{com}=(x,y)$</li></ul><ol><li>Check $f(r)=x$.</li><li>Check $HCB(r)\oplus b=y$.</li></ol></li></ul><p>This commitment scheme has completeness, comp. hiding and perfect binding.</p><p><font color=blue><u><b>Properties of Commitment Scheme: </b></u></font></p><ul><li><strong>Completeness</strong> is well proven.</li><li><strong>Computational Hiding</strong> can <u>be proven by the hardcore bit property.</u><ul><li>As for any arbitrary receiver $R^*$, it is hard to compute $HCB(r)$ given $f(r)$ since $f$ is OWP.</li><li>We say that the hardcore bit of $f(r)$ computational hides the bit $b$.</li></ul></li><li>The point in <strong>Perfect Binding</strong> is that <u>$f$ is a permutation.</u><ul><li>If $x=f(r)$ is fixed, then $r$ is fixed. There is no other $r’$ such that $f(r’)=f(r)$.</li><li>Then $HCB(r)$ is fixed, and the bit $b$ is fixed since $y$ is fixed.</li><li>That’s perfect binding.</li></ul></li></ul><h2 id="ZK-Proof-with-Commitment-Part-II"><a href="#ZK-Proof-with-Commitment-Part-II" class="headerlink" title="ZK Proof with Commitment: Part II"></a>ZK Proof with Commitment: Part II</h2><p>Replace the lead-box with the commitment protocol.</p><ul><li><strong>Commitment</strong> to $\rho(k)$ is $\texttt{com}(\rho(k);r_k)$ where $r_k$ is the random.</li><li><strong>Decommitment</strong> to $\rho(k)$ is $r_k$.</li></ul><img src="https://s1.ax1x.com/2022/08/12/vJaGnO.png" alt="Comp. ZK Proof for 3COL with Commitment Scheme" style="zoom:33%;" /><p>Why is this protocol zero-knowledge?</p><p><font color=blue><u><b>Comp.  Zero-knowledge:</b></u></font> </p><p>We dive into a malicious-verifier zero-knowledge.</p><ul><li>What can an arbitrary verifier $V^*$ do ?<ul><li>He can see all these commitments to</li><li>He can pick an arbitrary edge in bizarre fashion.</li></ul></li><li><strong>Real transcript</strong> of malicious $V^*$: $\texttt{view}_{V^*}(P,V^*)=\left(\{\texttt{com}_k(\rho(k);r_k)\}_{k=1}^{n},(i,j),(\texttt{dec}_i,\texttt{dec}_j)\right)$. <ul><li>The <strong>commitment to every color</strong> (of the vertex): $\texttt{com}_k(\rho(k);r_k)$</li><li>The <strong>edge</strong> chosen in bizarre fashion: $(i,j)=V^*(\{\texttt{com}_k\})$ </li><li>The <strong>decommitment</strong>: $(\texttt{dec}_i,\texttt{dec}_j)$.</li></ul></li></ul><p><font color=blue><u><b>Simulator S works as follows:</b></u></font></p><ol><li>First pick a random edge $(i^*,j^*)$.<br><u>Color this edge with random, different colors.</u><br><u>Color all other edges red.</u></li><li>Feed the commitments of the colors to $V^*$ and get edge $(i,j)=V^*(\{\texttt{com}_k\})$.</li><li>If $(i,j)=(i^*,j^*)$, output the commitments and the openings $r_i$ and $r_j$ as the <strong>simulated transcript</strong>.</li><li>If $(i,j)\ne(i^*,j^*)$, go back to step 1 and repeat. </li></ol><p>The <strong>key reason</strong> why it works is that the <u>prover commits to all colors (of the vertices) but only open two colors (of the vertexes).</u><br>It <strong>leaks nothing to the verifier</strong> since the <u>colors have been randomly permuted.</u></p><p>So the prover gives zero-knowledge to the verifier.</p><p><font color=blue><u><b>Lemma: </b></u></font> </p><ol><li>Assuming the commitments is hiding, $S$ runs in expected polynomial-time.</li><li>When $S$ outputs a view, it is <strong>computationally indistinguishable</strong> to the view of $V^*$ in a real execution.</li></ol><p><font color=blue><u><b>Proof of Lemma 2: </b></u></font> </p><p>Analysis the distribution of real transcript and simulated transcript.</p><ul><li>Real transcript: $\texttt{view}_{V^*}(P,V^*)=\left(\{\texttt{com}_k\}_{k=1}^{n},(i,j),(\texttt{dec}_i,\texttt{dec}_j)\right)$ <ul><li>The commitments are <strong>computationally random</strong> since the computational hiding property.</li><li>The distribution of $(i,j)$ is in bizarre fashion.</li><li>The decommitments are random.</li></ul></li><li>Simulated transcript: $\texttt{sim}_{S}(G)=\left(\{\texttt{com}_k\}_{k=1}^{n},(i^*,j^*),(\texttt{dec}_i,\texttt{dec}_j)\right)$ <ul><li>The commitments are <strong>computationally random</strong> since the computational hiding property.</li><li>The distribution  of $(i^*,j^*)$ is same as $(i,j)=V^*(\{\texttt{com}_k\})$. </li><li>The decommitments are random.</li></ul></li></ul><h2 id="Examples-of-NP-Assertions"><a href="#Examples-of-NP-Assertions" class="headerlink" title="Examples of NP Assertions"></a>Examples of NP Assertions</h2><ul><li><strong>My public key is well-formed.</strong><br>e.g. in RSA, prove the public key is $N$, a product of two primes together with an $e$ that is relatively to $\varphi(N)$.</li><li><strong>Encrypted bitcoin (or Zcash): “I have enough money to pay you.”</strong><br>e.g. I will publish an encryption of my bank account and prove to you that my balance is $\ge \$X$. </li><li><strong>Running programs on encrypted inputs</strong>: Given $Enc(x)$ and $y$, prove that $y=\textrm{PROG}(x)$.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Perfect ZK Proof for QR Language&lt;/li&gt;
&lt;li&gt;Perfect ZK Proof for Graph Isomorphism&lt;/li&gt;
&lt;li&gt;Comp. ZK Proof for 3Coloring&lt;/li&gt;
&lt;li&gt;All NP Languages have Comp. ZK Proofs&lt;/li&gt;
&lt;li&gt;Commitment Schemes&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="ZK Proof" scheme="https://f7ed.com/tags/ZK-Proof/"/>
    
      <category term="Commitment" scheme="https://f7ed.com/tags/Commitment/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 13</title>
    <link href="https://f7ed.com/2022/08/09/mit6875-lec13/"/>
    <id>https://f7ed.com/2022/08/09/mit6875-lec13/</id>
    <published>2022-08-08T16:00:00.000Z</published>
    <updated>2022-08-12T08:17:41.659Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Zero knowledge, definitions and example</li><li>ZK Proof for QR Language.<ul><li>honest-verifier ZK</li><li>malicious-verifier ZK</li></ul></li></ul><span id="more"></span><p>In the following lectures, we will introduce <strong>much more than communicating securely.</strong></p><ul><li>Complex <strong>Interactions</strong>: proofs, computations, games.</li><li>Complex <strong>Adversaries</strong>: Alice or Bob, adaptively chosen.</li><li>Complex <strong>Properties</strong>: Correctness, Privacy, Fairness.</li><li>Many <strong>Parties</strong>: this class, MIT, the Internet.</li></ul><p>Today’s gist is the proof.</p><p>It’s very different from the <strong>classic proofs.</strong></p><h1 id="NP-Proofs"><a href="#NP-Proofs" class="headerlink" title="NP Proofs"></a>NP Proofs</h1><p>In classic proofs, <strong>prover</strong> <u>writes down a string(proof) and verifier checks.</u> </p><img src="https://s1.ax1x.com/2022/08/12/vJUwOU.png" alt="classic proofs" style="zoom:33%;" /><p>Little formally, it can be formalized as below.</p><img src="https://s1.ax1x.com/2022/08/12/vJUeJI.png" alt="classic proof" style="zoom:33%;" /><ul><li>There is a <strong>claim</strong>(or theorem), e.g. 15627 is a prime.</li><li>There are <strong>two parties</strong>, a prover and a verifier.<ul><li>The <strong>prover</strong> <u>provide a proof to the verifier.</u></li><li>The <strong>verifier</strong> can <u>accept or reject the proof.</u></li></ul></li></ul><h2 id="NP-Language-Definition"><a href="#NP-Language-Definition" class="headerlink" title="NP Language Definition"></a>NP Language Definition</h2><p>The proofs of $\mathcal{NP}$ are efficiently verifiable proofs. </p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <b>Nondeterministic Polynomial-time (NP)</b><p>In computational complexity theory, $\mathcal{NP}$ (nondeterministic polynomial time) is a complexity class used to classify decision problems.<br>$\mathcal{NP}$ is the set of decision problems for which the problem instances, <strong>where the answer is “yes”</strong>,  <strong>have proofs verifiable in polynomial time by a deterministic Turing machine</strong>, or alternatively the set of problems that can be solved in polynomial time by a nondeterministic Turing machine.</p> </div> </article> <p>The <strong>prover</strong> has <u>no computational bounce</u> and needs to work hard to produce a proof. </p><p>But the <strong>verifier</strong> can efficiently <u>verify it in polynomial time.</u></p><img src="https://s1.ax1x.com/2022/08/12/vJUKQf.png" alt="NP proof" style="zoom:33%;" /><p>The $\mathcal{NP}$ proof of a theorem is actually a set of strings which can be written down.</p><p><font color=blue><u><b>Definition of Language Procedure: </b></u></font> </p><p>A language/decision procedure $\mathcal{L}$ is simply <strong>a set of strings</strong>. So $\mathcal{L}\subseteq \{0,1\}^*$. </p><p>The <strong>language</strong> is actually <u>a set of strings which represent the true statements.</u></p> <article class="message is-info"> <div class="message-header"> <p><strong>Definition of $\mathcal{NP}$-language:</strong>  </p> </div> <div class="message-body"> <p>$\mathcal{L}$ is an $\mathcal{NP}$-language if there is a <strong>poly-time verifier</strong> $V$ where </p><ul><li><strong>Completeness</strong>: True theorems have (short) proofs.<br>For all $x\in \mathcal{L}$, there is a $\texttt{poly}(|x|)\texttt{-long}$ witness (proof) $w\in\{0,1\}^*$ s.t. $V(x,w)=1$. </li><li><strong>Soundness</strong>: False theorems have no short proofs.<br>For all $x\notin \mathcal{L}$, there is no witness. That is, for all polynomially long $w\in\{0,1\}^*$ s.t. $V(x,w)=0$. </li></ul> </div> </article> <p>Look at some examples.</p><h2 id="e-g-1-N-PQ-Language"><a href="#e-g-1-N-PQ-Language" class="headerlink" title="e.g.1 N=PQ Language"></a>e.g.1 N=PQ Language</h2><p>Theorem: $N$ is a produce of two prime numbers.</p><img src="https://s1.ax1x.com/2022/08/12/vJUuSP.png" alt="N=PQ Proof" style="zoom:33%;" /><ul><li><p>Prover: Give the <strong>two prime factors as proof</strong>. $=(P,Q)$.</p></li><li><p>Verifier: Accept if and only if $N=PQ$ and $P,Q$ are primes.</p></li><li><p><strong>After interaction</strong>, Bob, the <strong>Verifier knows</strong></p><ol><li>$N$ is a produce of two primes.</li><li><strong>Also</strong>, the <strong>two factors</strong> of $N$.</li></ol></li></ul><h2 id="e-g-2-QR-Language"><a href="#e-g-2-QR-Language" class="headerlink" title="e.g.2 QR Language"></a>e.g.2 QR Language</h2><p>Theorem: $y$ is a quadratic residue $\mod N$ where $N=PQ$.</p><img src="https://s1.ax1x.com/2022/08/12/vJUmWt.png" alt="QR Proof" style="zoom:33%;" /><ul><li><p>Prover: Give <strong>the square root as proof</strong>. $=x$</p></li><li><p>Verifier: Accept if and only if $y=x^2\mod N$. </p></li><li><p><strong>After interaction</strong>, Bob, the Verifier knows</p><ol><li>$y$ is a quadratic residue $\mod N$. </li><li>Also, <strong>the square root</strong> of $y$.</li></ol></li></ul><h2 id="e-g-3-Graph-Isomorphism-Language"><a href="#e-g-3-Graph-Isomorphism-Language" class="headerlink" title="e.g.3 Graph Isomorphism Language"></a>e.g.3 Graph Isomorphism Language</h2> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <b>Graph Isomorphism Problem</b><p>Two graphs $G_0$ and $G_1$ are <strong>isomorphic graphs</strong> if they <u>have the same number of vertices, edges and also the same edge connectivity.</u></p><p>The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic.<br>The problem is <strong>not known to be solvable in polynomial time</strong> nor to be NP-complete, and therefore may be in the computational complexity class NP-intermediate.</p> </div> </article> <p>Theorem: Graphs $G_0$ and $G_1$ are isomorphic.</p><img src="https://s1.ax1x.com/2022/08/12/vJUMy8.png" alt="Graph-iso Proof" style="zoom:33%;" /><ul><li><p>Prover: Give <strong>the map of nodes as proof</strong>. $=\pi$</p></li><li><p>Verifier: Accept iff the edge connectivity is retained after mapping.</p></li><li><p><strong>After interaction</strong>, Bob, the <strong>Verifier knows</strong></p><ol><li>$G_0$ and $G_1$ are isomorphic.</li><li>Also, <strong>the isomorphism.</strong></li></ol></li></ul><h2 id="e-g-4-Hamiltonian-Cycle-Language"><a href="#e-g-4-Hamiltonian-Cycle-Language" class="headerlink" title="e.g.4 Hamiltonian Cycle Language"></a>e.g.4 Hamiltonian Cycle Language</h2> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i><b>Hamiltonian Path Problem</b> and <b>Hamiltonian Cycle Problem</b> <p><strong>A Hamiltonian path</strong> is a path that visits each vertex <strong>exactly once.</strong><br><strong>A Hamiltonian cycle</strong> is a closed loop on a graph where every vertex is visited exactly once.</p><p>In the mathematical field of graph theory the Hamiltonian path problem and the Hamiltonian cycle problem are problems of determining whether a Hamiltonian path or a Hamiltonian cycle exists in a given graph. </p><p>Both problems are <strong>NP-complete.</strong></p> </div> </article> <p>Theorem: Graph $G$ has a Hamiltonian cycle.</p><img src="https://s1.ax1x.com/2022/08/12/vJU8oj.png" alt="Hamiltonian Cycle Proof" style="zoom:33%;" /><ul><li><p>Prover: Give <strong>the Hamiltonian circle as proof.</strong></p></li><li><p>Verifier: Accept iff every edge exists.</p></li><li><p><strong>After interaction</strong>, Bob, the <strong>Verifier knows</strong></p><ol><li>$G$ has a Hamiltonian cycle.</li><li>Also, <strong>the Hamiltonian cycle</strong> itself.</li></ol></li></ul><hr><p>Every one of the  above can be <u>reduced to $\mathcal{NP}$-Complete Problem.</u> </p><p>Besides, every proof above <strong>leaks some valuable information</strong> more than the proof itself.</p><p>Is there any other way that the  prover only tells the verifier whether $y$ is quadratic residue <strong>without telling the square root</strong>?(Example 2)</p><p>The point is that we want to reveal the minimal information.</p><h1 id="Interactive-Proofs"><a href="#Interactive-Proofs" class="headerlink" title="Interactive Proofs"></a>Interactive Proofs</h1><p>Before proceeding to Zero Knowledge Proof, let’s start with Interactive Proofs.</p><p>Interactive Proof is not a monologue but a <strong>dialogue</strong>.</p><img src="https://s1.ax1x.com/2022/08/12/vJUQOS.png" alt="Interactive Proof" style="zoom:33%;" /><p>There are two necessary new ingredients in Interactive Proofs.</p><p><font color=blue><u><b>Two (Necessary) New Ingredients: </b></u></font> </p><ol><li><strong>Interaction</strong>: Rather than passively reading the proof, the <strong>verifier</strong> <u>engages in a conversation with the prover.</u></li><li><strong>Randomness</strong>: The <strong>verifier</strong> <u>is randomized</u> and can make a mistake with a (exponentially small) probability.</li></ol> <article class="message message-immersive is-danger"> <div class="message-body"> <i class="fas fa-bug mr-2"></i> <b>Note: The randomness of the verifier is necessary.</b><p>If the <strong>verifier</strong> is <u>completely deterministic as a function</u>, the <strong>prover</strong>, since the first message she sends, she <u>knows what the verifier is going to send back.</u> </p><p>So she can prepare her second message beforehand, then she can prepare her third message…</p><p>It’s <strong>not interactive.</strong></p><p>The thing to point is that for the <strong>prover</strong>, she can do no more <u>powerful thing than sending a single message.</u></p> </div> </article> <h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><p>Let’s start with <strong>solving the Rubik’s Cube.</strong></p><img src="https://s1.ax1x.com/2022/08/12/vJU1eg.png" alt="Rubik's Cube" style="zoom:20%;" /><p><strong>Theorem</strong>: There is an $\le k$ move solution to this cube. </p><p>The <strong>idea</strong> is <u>split the process of resolving in half.</u></p><img src="https://s1.ax1x.com/2022/08/12/vJUJFs.png" alt="Split in half" style="zoom:33%;" /><p><u><b>Interactive Proof: </b></u> </p><ol><li><p>Prover: sends a “random” cube.</p> <img src="https://s1.ax1x.com/2022/08/12/vJU3wQ.png" alt="random cube" style="zoom:33%;" /></li><li><p>Verify: sends a Challenge (0 or 1)</p></li><li><p>Prover</p><ol><li><p>If Challenge 0: show $k/2$ moves</p> <img src="https://s1.ax1x.com/2022/08/12/vJUtWq.png" alt="Challenge 0" style="zoom:33%;" /></li><li><p>If Challenge 1: show $k/2$ moves</p> <img src="https://s1.ax1x.com/2022/08/12/vJUYYn.png" alt="Challenge 1" style="zoom:33%;" /></li></ol></li></ol><p>The <strong>point</strong> is that if the <u>prover can do both challenges consistently</u> (or many and many times), then there exists $k$ moves.</p><p>We do not take it seriously since there are many flaws in the above protocol.</p><p>We just get some inspiration from it.</p><h2 id="Definition-of-IP"><a href="#Definition-of-IP" class="headerlink" title="Definition of IP"></a>Definition of IP</h2><p>The <strong>Interactive Proofs</strong> for a Language $\mathcal{L}$ is as follows. </p><img src="https://s1.ax1x.com/2022/08/12/vJUdyT.png" alt="Interactive Proof Definition" style="zoom:33%;" /><ul><li>There is a claim or a theorem.</li><li>Two parties are interacting for a language $\mathcal{L}$. <ul><li>Prover: <strong>Computational Unbounded</strong></li><li>Verifier: <strong>Probabilistic Polynomial-time</strong></li></ul></li></ul> <article class="message is-info"> <div class="message-header"> <p>Definition of $\mathcal{IP}$-Language: </p> </div> <div class="message-body"> <p>$\mathcal{L}$ is an $\mathcal{IP}$-language if there is a <strong>probabilistic poly-time verifier</strong> $V$ where<br>(There are three similar definitions)</p><ul><li>Definition in words.<ul><li><strong>Completeness</strong>: If $x\in \mathcal{L}$, $V$ always accepts.</li><li><strong>Soundness</strong>: If $x\notin \mathcal{L}$, <font color="red">regardless of the cheating prover strategy</font>, $V$ accepts with negligible probability.</li></ul></li><li>Rewrite with probability.<ul><li><strong>Completeness</strong>: If $x\in \mathcal{L}$, $\operatorname{Pr}[(P,V)(x)=\texttt{accept}]=1. $ </li><li><strong>Soundness</strong>: If $x\notin \mathcal{L}$, there is a negligible function $negl$ s.t.  <font color="red">for every $P^*$, </font> $\operatorname{Pr}[(P^*,V)(x)=\texttt{accept}]=negl(\lambda).$  </li></ul></li><li>Relax the probability: Equivalent as long as $c-s\ge 1/poly(\lambda)$<ul><li><strong>Completeness</strong>: If $x\in \mathcal{L}$, $\operatorname{Pr}[(P,V)(x)=\texttt{accept}]\color{blue}{\ge c}.$ </li><li><strong>Soundness</strong>: If $x\notin \mathcal{L}$, there is a negligible function $negl$ s.t.<font color="red"> for every $P^*$,</font> $\operatorname{Pr}[(P^*,V)(x)=\texttt{accept}]\color{blue}{\le s}$.  </li></ul></li></ul> </div> </article> <p><strong>Completeness</strong> is a property of the protocol <u>when $P$ and $V$ are both honest.</u></p><p><strong>Soundness</strong> is a property of the protocol <u>when $P$ is dishonest.</u></p><p>So soundness is also a property of the verifier.<br>The <strong>verifier has to be sound</strong> because it has to work against an arbitrary $P$.</p><h2 id="IP-for-QR-Language"><a href="#IP-for-QR-Language" class="headerlink" title="IP for QR Language"></a>IP for QR Language</h2><p>We can give the interactive proof for QR.</p>$\mathcal{L}=\{(N,y): y \textrm{ is a quadratic residue}\mod N\}$  <p><font color=blue><u><b>Interactive Proof: </b></u></font> </p><img src="https://s1.ax1x.com/2022/08/12/vJUUS0.png" alt="IP for QR Language" style="zoom:33%;" /><ol><li>Prover: send a random square $s$</li><li>Verifier: flip a coin $b$ <strong>(randomness)</strong></li><li>Prover<ol><li>If $b=0$, send $z=r$</li><li>If $b=1$, send $z=rx$ where $y=x^2 \mod N$.</li></ol></li><li>Verifier: Accept iff $z^2=sy^b \mod N$.</li></ol><p><font color=blue><u><b>Completeness: </b></u></font> </p><p><font color=blue><u><i>Claim: </i></u></font> </p><p>If $(N,y)\in \mathcal{L}$, then the verifier accepts the proof with probability 1.</p><p><font color=blue><u><i>Proof: </i></u></font> </p><ul><li>$z^2=(rx^b)^2=r^2(x^2)^b=sy^b$.</li><li>So the verifier’s check passes and he accepts.</li></ul><p><font color=blue><u><b>Soundness: </b></u></font> </p><p><font color=blue><u><i>Claim: </i></u></font> </p><p>If $(N,y)\notin \mathcal{L}$, then for <strong>every cheating prover</strong> $P^*$, the verifier accepts <strong>with probability at most</strong> $1/2$.</p><p><font color=blue><u><i>Proof: </i></u></font> </p><ul><li><p>The <strong>challenge is random</strong> and the <strong>randomness is over the coin.</strong></p></li><li><p>If the probability <strong>equals</strong> $1/2$, it means that the cheating prover $P^*$ <u>can only solve one of the challenges, Challenge 0.</u></p></li><li><p><strong>Suppose</strong> the verifier accepts with probability $\ge1/2$. </p></li><li><p>Then, <u>there is</u> some $s\in \mathbb{Z}_N^*$, the <strong>prover</strong> is able to <u>pass both challenges.</u></p><p>  So the prover can produce both $z_0$ and $z_1$ beforehand.</p><ul><li>$z_0:z_0^2=s\mod N$</li><li>$z_1:z_1^2=sy\mod N$</li></ul></li><li><p>This means $(z_1/z_0)^2=y\mod N$, which tells us that $(N,y)\in \mathcal{L}$. (Contradiction)</p></li></ul><p>Moreover, we can make the probability of soundness <strong>negligible</strong>.</p><p>Just *<em>repeat the procedure sequentially $\lambda$ times. *</em> </p><p><font color=blue><u><i>Claim: </i></u></font> </p><p>If $(N,y)\notin \mathcal{L}$, then for every cheating prover $P^*$, the verifier accepts <strong>with probability at most</strong> $(\frac{1}{2})^\lambda$.</p><h1 id="Zero-Knowledge-Proof"><a href="#Zero-Knowledge-Proof" class="headerlink" title="Zero Knowledge Proof"></a>Zero Knowledge Proof</h1><p>Actually, the interactive proof for QR language is Zero-Knowledge.</p><img src="https://s1.ax1x.com/2022/08/12/vJUUS0.png" alt="ZK Proof for QR Language" style="zoom:33%;" /><p>But what dose zero-knowledge mean?</p><ul><li>After the interaction, $V$ <strong>knows:</strong><ul><li>The theorem is true; and</li><li><strong>A view</strong> of the interaction (=<strong>transcript + coins</strong> of $V$)<br>The transcript is all the messages going back and forth.</li></ul></li><li>$P$ gives <strong>zero knowledge</strong> to $V$:<br>When the theorem is true, the view <strong>gives $V$ nothing</strong> that he couldn’t have obtained on his own without interacting with $P$.<ul><li>That means the view is not going to <strong>give any new knowledge</strong> that $V$ couldn’t have it on his own.</li><li>We can consider the <strong>knowledge</strong> as <u>some stuff that we cannot generate in probabilistic poly-time on our own.</u><br>If we are given something we can generate by ourselves, it’s zero-knowledge.</li></ul></li></ul><p>$(P,V)$ is zero-knowledge if $V$ can <strong>generate his view</strong> of the interaction all by himself in probabilistic polynomial time.</p><p>$(P,V)$ is zero-knowledge if $V$ can <strong>“simulate” his view</strong> of the interaction all by himself in probabilistic polynomial time.</p><p>We formalize it by the <strong>Simulation Paradigm.</strong></p><h2 id="The-Simulation-Paradigm"><a href="#The-Simulation-Paradigm" class="headerlink" title="The Simulation Paradigm"></a>The Simulation Paradigm</h2><p>There are two <strong>indistinguishable distributions.</strong> </p><img src="https://s1.ax1x.com/2022/08/12/vJUalV.png" alt="Simulation Paradigm" style="zoom:33%;" /><ul><li>The real view of the interaction: $\texttt{view}_V(P,V)=(s,b,z$).<br>It consists of<ul><li>Transcript: $(s,b,z)$</li><li>Coins: $b$</li></ul></li><li>The simulated view: $\texttt{sim}_S(s,b,z)$  </li></ul><p>We can give the zero-knowledge definition by the simulation paradigm.</p><h2 id="Zero-knowledge-Definition-for-honest-V"><a href="#Zero-knowledge-Definition-for-honest-V" class="headerlink" title="Zero-knowledge Definition (for honest V)"></a>Zero-knowledge Definition (for honest V)</h2> <article class="message is-info"> <div class="message-header"> <p><strong>(Honest-verifier) Zero-knowledge Definition:</strong></p> </div> <div class="message-body"> <p>An <strong>Interactive Protocol</strong> $(P,V)$ is <strong>zero-knowledge</strong> for a language $\mathcal{L}$ if there exists a <strong>PPT</strong> algorithm $S$ (a simulator) such that <strong>for every $x\in \mathcal{L}$,</strong> the following two distributions are <strong>indistinguishable</strong>.</p><ul><li>$\texttt{view}_V(P,V)$ </li><li>$\texttt{sim}_S(x,1^\lambda) $ </li></ul> </div> </article> <p>$(P,V)$ is <strong>zero-knowledge interactive protocol</strong> if it is complete, sound and zero-knowledge.</p><ul><li><strong>Completeness</strong> is a property of the protocol <u>when $P$ and $V$ are both honest.</u></li><li><strong>Soundness</strong> is a property of the protocol <u>when $P$ is dishonest.</u><ul><li>So it’s also a property of the verifier.<br>The verifier has to be sound because it has to work against an arbitrary $P$.</li></ul></li><li><strong>Zero-knowledge</strong> is a property <u>against the verifier.</u></li></ul><p>Actually, we give the definition of zero-knowledge <strong>against a honest verifier.</strong></p><p>We’ll refine it for any arbitrary verifier in the next section.</p><p>There are some analogous definitions of <strong>honest-verifier zero-knowledge.</strong></p> <article class="message is-info"> <div class="message-header"> <p><strong>Perfect (Honest-verifier) Zero-knowledge:</strong> </p> </div> <div class="message-body"> <p>An Interactive Protocol $(P,V)$ is <strong>perfect (honest-verifier) zero-knowledge</strong> for a language $\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\in \mathcal{L}$, the following two distributions are <strong>identical</strong>.</p><ul><li>$\texttt{view}_V(P,V) $ </li><li>$\texttt{sim}_S(x,1^\lambda) $ </li></ul> </div> </article>  <article class="message is-info"> <div class="message-header"> <p><strong>Statistical (Honest-verifier) Zero-knowledge:</strong> </p> </div> <div class="message-body"> <p>An Interactive Protocol $(P,V)$ is <strong>statistical (honest-verifier) zero-knowledge</strong> for a language $\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\in \mathcal{L}$, the following two distributions are <strong>statistically indistinguishable.</strong></p><ul><li>$\texttt{view}_V(P,V)$  </li><li>$\texttt{sim}_S(x,1^\lambda) $ </li></ul> </div> </article>  <article class="message is-info"> <div class="message-header"> <p><strong>Computational (Honest-verifier) Zero-knowledge:</strong> </p> </div> <div class="message-body"> <p>An Interactive Protocol $(P,V)$ is <strong>computational (honest-verifier) zero-knowledge</strong> for a language $\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\in \mathcal{L}$, the following two distributions are <strong>computationally indistinguishable.</strong></p><ul><li>$\texttt{view}_V(P,V) $ </li><li>$\texttt{sim}_S(x,1^\lambda)$  </li></ul> </div> </article> <h2 id="QR-Protocol-is-honest-V-ZK"><a href="#QR-Protocol-is-honest-V-ZK" class="headerlink" title="QR Protocol is (honest-V) ZK"></a>QR Protocol is (honest-V) ZK</h2><p><font color=blue><u><b>Claim:</b></u></font> </p><p>The QR protocol is (honest-verifier) <strong>perfect zero knowledge.</strong></p><img src="https://s1.ax1x.com/2022/08/12/vJUalV.png" alt="QR Protocol is Perfect ZK" style="zoom:33%;" /><p><font color=blue><u><b>Simulator S works as follows: </b></u></font> </p><ol><li>First pick a random bit $b$.</li><li>Pick a random $z\in \mathbb{Z}_N^*$.</li><li>Compute $s=z^2/y^b$.</li><li>Output $(s,b,z)$.</li></ol><p>The <strong>simulator</strong> can sort of <u>permute things which is offline.</u></p><p>And the <strong>verifier</strong> is <u>online in the real world.</u></p><p><font color=blue><u><b>Lemma: </b></u></font> </p><p>The simulated transcript is <strong>identically</strong> distributed as the real transcript in the interaction $(P,V)$.</p><p><font color=blue><u><b>Proof of Lemma: </b></u></font> </p><ul><li>The thing we need to <strong>prove</strong> is that <u>the distribution of every component in the transcript is identical.</u></li><li>Real transcript: $\texttt{view}_{V}(P,V)=(s,b,z) $<ul><li>$s$ is square as same as random</li><li>$b$ is random coin(since the honest verifier)</li><li>$z=\sqrt{sy^b}$ is random</li></ul></li><li>Simulated transcript: $\texttt{sim}_S((N,y),1^\lambda)=(s,b,z) $<ul><li>$s=z^2/y^b$ is square as same as random ($(N,y)\in \mathcal{L}$ so $y$ is square)<br>Besides, the distribution of $s$ hides $b$ perfectly.</li><li>$b$ is random</li><li>$z$ is random</li></ul></li><li>QED</li></ul><p>Actually we only prove the QR protocol is <strong>honest-verifier Zero-knowledge.</strong> </p><p>When the theorem is true, the view gives the <strong>honest verifier</strong> $V$ nothing that $V$ couldn’t have it on his own.</p><p>What if $V$ is <strong>NOT honest</strong> ?</p> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i>Note:  <p>The following malicious part is actually lectured in the <a href="/2022/08/11/mit6875-lec14/" title="Lecture 14">Lecture 14</a>.<br>I put it here for the continuous and complete description.</p> </div> </article> <h2 id="Zero-knowledge-Definition-for-malicious-V"><a href="#Zero-knowledge-Definition-for-malicious-V" class="headerlink" title="Zero-knowledge Definition (for malicious V)"></a>Zero-knowledge Definition (for malicious V)</h2><p>In real world, the verifier could be <strong>malicious</strong> that he can do anything he wants.</p><p>We hope the view also gives zero-knowledge to the malicious verifier $V^*$.</p><p>The definition for honest verifier undermines the definition for malicious verifier.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> Recap:<p>A <strong>language</strong> $\mathcal{L}$  is actually a set of strings which represent true statements.<br>The <strong>view</strong> of $V^*$ is the transcripts and the coins, which contains all the messages going back and forth.</p> </div> </article> <p>Refine the definitions <strong>for malicious verifier</strong> $V^*$.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Perfect Zero-knowledge Definition:</strong></p> </div> <div class="message-body"> <p>An interactive Protocol $(P,V)$ is <strong>perfect zero-knowledge</strong> for a language $\mathcal{L}$ if <font color="red"><strong>for every PPT $V^*$,</strong>  </font>there exists a  (expected) poly time simulator $S$ s.t. for every $x\in \mathcal{L}$, the following two distributions are <strong>identical</strong>:</p><ul><li>$\texttt{view}_{V^*}(P,V^*) $</li><li>$\texttt{sim}_S(x,1^\lambda)$ </li></ul> </div> </article>  <article class="message is-info"> <div class="message-header"> <p><strong>Statistical Zero-knowledge Definition:</strong></p> </div> <div class="message-body"> <p>An interactive Protocol $(P,V)$ is <strong>statistical zero-knowledge</strong> for a language $\mathcal{L}$ if <font color="red"><strong>for every PPT $V^*$,</strong>  </font> there exists a  (expected) poly time simulator $S$ s.t. for every $x\in \mathcal{L}$, the following two distributions are <strong>statistical indistinguishable</strong>:</p><ul><li>$\texttt{view}_{V^*}(P,V^*) $ </li><li>$\texttt{sim}_S(x,1^\lambda) $</li></ul> </div> </article>  <article class="message is-info"> <div class="message-header"> <p><strong>Computational Zero-knowledge Definition:</strong></p> </div> <div class="message-body"> <p>An interactive Protocol $(P,V)$ is <strong>computational zero-knowledge</strong> for a language $\mathcal{L}$ if<font color="red"> <strong>for every PPT $V^*$,</strong> </font>there exists a  (expected) poly time simulator $S$ s.t. for every $x\in \mathcal{L}$, the following two distributions are <strong>computationally indistinguishable</strong>:</p><ul><li>$\texttt{view}_{V^*}(P,V^*)$ </li><li>$\texttt{sim}_S(x,1^\lambda) $</li></ul> </div> </article> <h2 id="QR-Protocol-is-malicious-V-ZK"><a href="#QR-Protocol-is-malicious-V-ZK" class="headerlink" title="QR Protocol is (malicious-V) ZK"></a>QR Protocol is (malicious-V) ZK</h2><p>The ZK proof for QR language we gave above is actually honest-verifier zero-knowledge.</p><p>In this section, we consider the <strong>malicious-verifier zero-knowledge</strong> for QR Protocol.</p><p>Recap the zero-knowledge proof for QR Language.</p>$\mathcal{L}=\{(N,y):y \textrm{ is a quadratic residue }\mod N\}$. <img src="https://s1.ax1x.com/2022/08/12/vJUBmF.png" alt="QR Protocol is (malicious-verifier) Perfect ZK" style="zoom:33%;" /><ul><li>The view of a malicious verifier $V^*$ is $\texttt{view}_{V^*}(P,V^*)=(s,b,z)$.<ul><li>When $V^*$ obtains the $s$, the <strong>only power</strong> that $V^*$ has is to <u>choose the $b$ in a bizarre fashion rather than random.</u></li><li>So the distribution of $b$ is <strong>not random.</strong></li><li>Let $b=V^*(s)$ denote the bizarre thing generated by $V^*$ after receiving $s$.</li></ul></li><li>The simulator $S$ only gets an instance of $(N,y)$ and wants to generate $\texttt{sim}_S((N,y),1^\lambda)=(s,b,z)$. </li></ul><p>In order to produce the same distribution of $b$, the <strong>simulator</strong> $S$ <u>needs to interact with the malicious verifier $V^*$.</u></p><p>It’s sort of a prover which also needs to interact with the verifier.</p><p>The only <strong>distinction</strong> is that the <strong>prover</strong> $P$ has to <u>be online to answer the challenge</u> from the verifier $V^*$ and the <strong>simulator</strong> $S$ <u>can be offline with the goal of generating the view.</u></p><p><font color=blue><u><b>Claim:</b></u></font> </p><p>The QR protocol is (malicious-verifier) <strong>perfect zero knowledge.</strong></p><p><font color=blue><u><b>Simulator S works as follows:</b></u></font> </p><ol><li>First set $s=z^2/y^b$ for a random $z$ and a random $b$ and “feed” $s$ to $V^*$</li><li>Let $b'=V^*(s)$. (generated by $V^*$ in any bizarre fashion rather than random)</li><li>If $b=b’$, output $(s,b,z)$ and stop.</li><li>Otherwise, go back to step 1 and repeat. (also called “<strong>rewinding</strong>”)</li></ol><p><font color=blue><u><b>Lemma: </b></u></font> </p><ol><li>$S$ runs in expected polynomial-time.</li><li>When $S$ outputs a view, it is <strong>identical</strong> to the view of $V^*$ in a real execution.</li></ol><p>Lemma 1 is well proven.</p><p>The probability of terminating in one iteration is $1/2$ since $b$ is random.</p><p>So the expected iterations is $2$.</p><p><font color=blue><u><b>Proof of Lemma 2:</b></u></font> </p><ul><li>We need to <strong>prove</strong> that <u>the distribution of every component is identical.</u></li><li>Real transcript: $\texttt{view}_{V^*}(P,V^*)=(s,b,z) $ <ul><li>$s$ is square as same as random</li><li>$b$ is <u>generated in any bizarre way</u>, i.e. $b=V^*(s)$.</li><li>$z=\sqrt{sy^b}$ is random</li></ul></li><li>Simulated transcript: $\texttt{sim}_S((N,y),1^\lambda)=(s,b,y) $<ul><li>$s=z^2/y^b$ is square as same as random ($(N,y)\in \mathcal{L}$ so $y$ is square)<br>Besides, the distribution of $s$ hides $b$ perfectly.</li><li>$b$ has <strong>the same distribution</strong> with $b’=V^*(s)$.</li><li>$z$ is random</li></ul></li><li>QED</li></ul><p>So far we have proven the QR protocol is (malicious-verifier) zero-knowledge.</p><h2 id="What-made-ZK-Proof-possible"><a href="#What-made-ZK-Proof-possible" class="headerlink" title="What made ZK Proof possible ?"></a>What made ZK Proof possible ?</h2><ol><li><strong>Each statement had multiple proofs</strong> of which <u>the prover chooses one at random.</u><br>(They are $(s,\sqrt{s})$ and $(s,\sqrt{sy})$ as for the QR protocol.)</li><li><strong>Each such proof is made of two parts</strong> (as shown above): <ol><li>seeing either one on its own gives the verifier no knowledge ; </li><li>seeing both imply 100% correctness. (That’s the <strong>completeness</strong>)</li></ol></li><li><strong>Verifier choose to see either part, at random.</strong><br>(Verifier can choose to see $\sqrt{s}$ or $\sqrt{sy}$ at random)<br>The prover’s ability to provide either part on demand convinces the verify. (That’s the <strong>soundness</strong>)<br>The <u>prover has to prepare both part correctly</u>, otherwise there is a half chance that he’ll get caught.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zero knowledge, definitions and example&lt;/li&gt;
&lt;li&gt;ZK Proof for QR Language.&lt;ul&gt;
&lt;li&gt;honest-verifier ZK&lt;/li&gt;
&lt;li&gt;malicious-verifier ZK&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="ZK Proof" scheme="https://f7ed.com/tags/ZK-Proof/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 12</title>
    <link href="https://f7ed.com/2022/08/04/mit6875-lec12/"/>
    <id>https://f7ed.com/2022/08/04/mit6875-lec12/</id>
    <published>2022-08-03T16:00:00.000Z</published>
    <updated>2022-08-24T12:14:13.394Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li><p>Construction of CRHF from Discrete Log</p></li><li><p>Digital Signatures only from OWF</p></li><li><p>Direct Constructions:Trapdoor Permutation and the Hash-and-Sign Paradigm.</p></li><li><p>Random Oracles.</p></li></ul><span id="more"></span><h1 id="Digital-Signature-from-CRHF"><a href="#Digital-Signature-from-CRHF" class="headerlink" title="Digital Signature from CRHF"></a>Digital Signature from CRHF</h1><p>We showed the theorem about digital signature in <a href="/2022/07/29/mit6875-lec10/" title="Lecture 10">Lecture 10</a>.</p><p><strong>Theorem:</strong></p><p>Assuming the existence of <strong>one-way functions</strong> and <strong>collision-resistant hash function families</strong>, there are digital signature schemes.</p><h2 id="CRHF-Definition"><a href="#CRHF-Definition" class="headerlink" title="CRHF Definition"></a>CRHF Definition</h2><p>Recall the definition of <strong>Collision-Resistant Hash Functions.</strong></p><p>A <strong>compressing</strong> family of functions  $\mathcal{H}=\{h:\{0,1\}^m\rightarrow \{0,1\}^n\}$ (where $m&gt;n$ ) for which it is <u>computationally hard to find collisions.</u></p> <article class="message is-info"> <div class="message-header"> <p><strong>Definition:</strong></p> </div> <div class="message-body"> <p>$\mathcal{H}$ is collision-resistant if for every PPT algorithm $A$, there is a negligible function $\mu$ s.t.</p>$$\operatorname{Pr}_\mathcal{h\gets H}[A(1^n,h)=(x,y):x\ne y,h(x)=h(y)]=\mu(n)$$ </div> </article> <p>The function $h$ is <u>given to the adversary</u>. And the advantage of finding a collision is negligible.</p><p>How can we construct the CRHF ?</p><h2 id="Construction-of-CRHF-from-Discrete-Log"><a href="#Construction-of-CRHF-from-Discrete-Log" class="headerlink" title="Construction of CRHF from Discrete Log"></a>Construction of CRHF from Discrete Log</h2><p><font color=blue><u><b>Construction:  </b></u></font> </p><ul><li>Let $p=2q+1$ be a “safe” prime.</li><li>Let  $\mathcal{H}=\{h:(\mathbb{Z}_q)^2\rightarrow QR_p\}$ <ul><li>$\mathcal{H}$ maps two element in $\mathbb{Z}_q$ to one element in $QR_p$, the <strong>subgroup of quadratic residues</strong>  in $\mathbb{Z}_p^*$ with order $q$. </li><li>Each function $h_{g_1,g_2}\in \mathcal{H}$ is <strong>parameterized</strong> by two generators $g_1$ and $g_2$ of $QR_p$.</li></ul></li><li>Define $h_{g_1,g_2}(x_1,x_2)=g_1^{x_1}g_2^{x_2} \mod p$.</li><li>This <strong>compresses</strong> $2\log q$ bits into $\log q\approx \log {q+1}$ bits.</li></ul><p>Prove $h_{g_1,g_2}$ is collision-resistant.</p><p><font color=blue><u><b>Proof:  </b></u></font> </p><ul><li>Suppose for <strong>contradiction</strong> that there is an adversary that <strong>finds a collision</strong> $(x_1,x_2)$ and $(y_1,y_2)$.</li><li>$g_1^{x_1}g_2^{x_2}= g_1^{y_1}g_2^{y_2}\mod p$</li><li>$g_1^{x_1-y_1}= g_2^{y_2-x_2}\mod p$</li><li>$g_1=g_2^{(y_2-x_2)(x_1-y_1)^{-1}}\mod p$ (assuming $x_1-y_1\ne 0$)</li><li>This turns to <strong>a discrete log problem</strong> of $DLOG_{g_2}(g_1)$.</li></ul><p>Turns out to another theorem of digital signature scheme.</p><p><font color=blue><u><b>Theorem:  </b></u></font> </p><p>Assuming the <strong>hardness of the discrete logarithm problem</strong>, there are digital signature schemes.</p><h2 id="Other-Constructions-of-CRHF"><a href="#Other-Constructions-of-CRHF" class="headerlink" title="Other Constructions of CRHF"></a>Other Constructions of CRHF</h2><p>Similarly, we can construct CRHF from the hardness of factoring, lattice problems etc.</p><p>It’s <strong>not known</strong> to follow from the <u>existence of one-way functions</u> or even <u>one-way permutations.</u> It’s still a big open problem.</p><blockquote><p>“Black-box separations”: Certain ways of constructing CRHF from OWF/OWP cannot work.<br>”Finding collisions on a one-way street”, Daniel Simon, Eurocrypt 1998.</p></blockquote><h1 id="Digital-Signature-from-OWF"><a href="#Digital-Signature-from-OWF" class="headerlink" title="Digital Signature from OWF"></a>Digital Signature from OWF</h1><p>But it turns out that <strong>collision-resistant hashing is not necessary</strong>; something weaker called <strong>universal one-way hashing (UOWHF) suffices.</strong></p><p>Furthermore, <strong>UOWHFs</strong> can be <u>constructed from one-way functions alone.</u></p><p>The challenge is different between CRHF and UOWHF.</p><ul><li><strong>CRHF</strong><ol><li>Give $\mathcal{A}$ the <strong>function</strong> $h$</li><li>It’s computationally hard for $\mathcal{A}$ to gives $(x,y)$ such that $h(x)=h(y)$ s.t. $x\ne y$.</li></ol></li><li><strong>UOWHF</strong><ol><li>$\mathcal{A}$ requests for the hash of $x$.</li><li>Give $\mathcal{A}$ the <strong>hash</strong> $h(x)$</li><li>It’s computationally hard for $\mathcal{A}$ to give $y$ such that $h(x)=h(y)$ s.t. $x\ne y$.</li></ol></li></ul><p>So we can construct Digital Signature only from OWF.</p><p><font color=blue><u><b>Theorem:  </b></u></font> </p><p>Digital Signature schemes exist <strong>if and only if</strong> <u>one-way functions exist.</u></p><hr><p>We can construct Digital Signatures from two routes.</p><ul><li>OWF → UOWHF → Digital Signatures</li><li>CRHF(+OWF) → Digital Signatures</li></ul><p>Now we catch the sight of words in crypto.</p><img src="https://s1.ax1x.com/2022/08/07/vuxO5n.png" alt="words in crypto" style="zoom:33%;" /><h1 id="Direct-Constructions"><a href="#Direct-Constructions" class="headerlink" title="Direct Constructions"></a>Direct Constructions</h1><p>We will show that “Hash-and-Sign” is secure <strong>in random oracle model.</strong></p><h2 id="“Vanilla”-RSA-Signatures"><a href="#“Vanilla”-RSA-Signatures" class="headerlink" title="“Vanilla” RSA Signatures"></a>“Vanilla” RSA Signatures</h2><p>We can construct Digital Signature scheme directly <u>from any trapdoor permutation</u>, e.g. RSA.</p><p><font color=blue><u><b>Vanilla RSA Signatures:  </b></u></font> </p><ul><li>$Gen(1^\lambda)$<ul><li>Pick primes $(P,Q)$ and let $N=PQ$.</li><li>Pick $e$ relatively prime to $\phi(N)$ and let $d=e^{-1} \pmod {\phi(N)}$.</li><li>$SK=(N,d)$ and $VK=(N,e)$</li></ul></li><li>$Sign(SK,m)$<ul><li>Output signature $\sigma=m^d \pmod N$</li></ul></li><li>$Verify(VK,m,\sigma)$<ul><li>Check if $\sigma^e=m\pmod N$</li></ul></li></ul><p>But it is existentially forgeable and malleable.</p><p><font color=blue><u><b><i>Problems: </i></b></u></font> </p><ul><li><strong>Existentially forgeable</strong><ul><li>Attack1: Pick a random $\sigma$ and output $(m=\sigma^e,\sigma)$ as the forgery.</li></ul></li><li><strong>Malleable</strong><ul><li>Attack2: Given a signature of $m$, you can produce a signature of $2m,3m,\dots$</li></ul></li></ul><p><font color=blue><u><b><i>Fundamental issues </i></b></u></font> under the problems:</p><ol><li>Can <u>“reverse-engineer” the message</u> starting from the signature. (Attack 1)</li><li><u>Algebraic structure</u> allows malleability. (Attack 2)</li></ol><hr><p>How to fix Vanilla RSA ?</p><p><font color=blue><u><b>Fixed Vanilla RSA Signature: </b></u></font> </p><ul><li>$Gen(1^\lambda)$<ul><li>Pick primes $(P,Q)$ and let $N=PQ$.</li><li>Pick $e$ relatively prime to $\phi(N)$ and let $d=e^{-1} \pmod {\phi(N)}$.</li><li>$SK=(N,d)$ and $VK=(N,e,\color{blue}{H})$</li></ul></li><li>$Sign(SK,m)$<ul><li>Output signature: $\sigma= \color{blue}{H(m)}^d \pmod N$</li></ul></li><li>$Verify(VK,m,\sigma)$<ul><li>Check if $\sigma^e=\color{blue}{H(m)}\pmod N$</li></ul></li></ul><p><strong>What is $H$   ?</strong></p><p>$H$ is some very complicated “hash” function.</p><p>$H$ should be <u>at least one-way<strong>.</strong></u> ( to prevent Attack 1)</p><p>$H$ should be <u>hard to “algebraically manipulate”</u> $H(m)$ into $H(\text{related } m’)$.(to prevent Attack 2)</p><p><strong>Collision-resistance</strong> dose <strong>not</strong> seem to be enough.</p><p>Given a CRHF $H(m)$, you may be able to produce $H(m’)$ for related $m’$.</p><h2 id="The-Random-Oracle-Heuristic"><a href="#The-Random-Oracle-Heuristic" class="headerlink" title="The Random Oracle Heuristic"></a>The Random Oracle Heuristic</h2><p>We want a <strong>public</strong> $H$ that is “<strong>non-malleable”.</strong></p><p>Given $H(m)$, it is <u>hard to produce $H(m’)$ for any non-trivially related $m’$.</u></p> <article class="message is-info"> <div class="message-header"> <p><strong>Random Oracle Definition:</strong> </p> </div> <div class="message-body"> <p>For every PPT adversary $A$ and <strong>“every non-trivial relation”</strong> $R$, </p><p>$$<br>\operatorname{Pr}[A\left(H(m)\right)=H(m’):R(m,m’)=1]=negl(\lambda)<br>$$</p> </div> </article> <p>The <strong>goal</strong> of adversary is to <strong>come up with the relation</strong> $R$ such that <u>you can somehow manipulate $H(m)$ into $H(m’)$.</u></p><p>How about the <strong>relation</strong> $R$ where $R(x,y)=1$ if and only if $y=H(x)$ ?</p><p>A <strong>public</strong> $H$ that <strong>“behaves like a random function”.</strong></p><p>We can consider it as a <strong>proxy</strong> to <u>a random function.</u></p><p>(A PRF also behaves like a random function, but $PRF_K$ is <strong>not publicly computable. )</strong></p><p>The adversary $\mathcal{A}$ can <u>get the public function</u> $H$ in <strong>reality</strong>.</p><p>But in the <strong>Random Oracle Heuristic world</strong>, the <u>only way to compute $H$,</u> virtually a black box, is <u>by  calling the oracle.</u></p><img src="https://s1.ax1x.com/2022/08/07/vuxqEj.png" alt="reality vs. random oracle" style="zoom:33%;" /><p><font color=blue><u><b>Claim:</b></u></font> </p><p>The hashed RSA is EUF-CMA secure <strong>in the random oracle model.</strong></p><p><font color=blue><u><b>Proof:</b></u></font> </p><ul><li><p><strong>Assume</strong> there is a PPT adversary $\mathcal{A}$ that breaks the EUF-CMA security of hashed RSA <strong>in the random oracle model.</strong></p>  <img src="https://s1.ax1x.com/2022/08/07/vux7Dg.png" alt="adversary of EUF-CMA security" style="zoom:33%;" /><ol><li>Given $\mathcal{A}$ the verification key.</li><li>$\mathcal{A}$ asks the <strong>Hash Query</strong> for poly. times.<br>(We can model it to split the hash queries and sign queries.)</li><li>$\mathcal{A}$ asks the <strong>Sign Query</strong> for poly. times.</li><li>$\mathcal{A}$ gives a forgery  $(m^*, \sigma^*)$.   </li></ol></li><li><p>Recall the <strong>RSA assumption:</strong><br>given $N,e$ and $y=x^e\mod N$, hard to compute $x$.</p></li><li><p>Then, there is an algorithm $\mathcal{B}$ that solves the RSA problem.</p>  <img src="https://s1.ax1x.com/2022/08/07/vuxLUs.png" alt="algorithm B for RSA" style="zoom:33%;" /><ul><li>The task of $\mathcal{B}$ is to compute $x$ given the $(N,e,y)$.</li><li>$\mathcal{B}$ needs to interact with the adversary $\mathcal{A}$</li></ul><ol><li><p>$\mathcal{B}$ gives the verification key $VK=(N,e)$ to $\mathcal{A}$.</p></li><li><p>$\mathcal{A}$ asks polynomially many <strong>Hash Queries.</strong></p><ol><li>For all hash queries, $\mathcal{B}$ <u>picks a random</u> $\tilde{m}$ as the <strong>trap</strong>.</li><li>For the <strong>trap</strong> $\tilde{m}$, $\mathcal{B}$ sets the hash $H(\tilde{m})=y$.</li><li>For other <strong>normal</strong> $m$, $\mathcal{B}$ picks a random $x$ and sets the hash $H(m)=x^e$.</li></ol></li><li><p>$\mathcal{A}$ asks polynomially many <strong>Sign Queries.</strong></p><p> For each Sign Query for $m$:</p><ol><li>If $m=\tilde{m}$, i.e. hits the trap, $\mathcal{B}$ aborts.<br>Because $\mathcal{B}$ <u>cannot produce the signature.</u></li><li>Otherwise, $\mathcal{B}$ is <u>able to produce the signature</u> $\sigma=x$.</li></ol></li><li><p>$\mathcal{A}$ promises to produce the <strong>forgery</strong>    $(m^*,\sigma^*)$.  </p><ul><li>The thing to <strong>notice</strong> is that the message $m^*$ is <u>new to all the messages in Sign Query</u>, not in Hash Query.</li><li>If   $m^*=\tilde{m}$,   <strong>hits the trap</strong>, then the signature   $\sigma^*=x$ is what she wants.   </li><li><strong>Claim:</strong><br>To produce <strong>a successful forgery</strong>, $\mathcal{A}$ must <u>have queried the hash oracle</u> on   $m^*$. With probability $1/q$, $m^*$ is the trap.  <br>(where $q$ is the number of hash queries)</li></ul></li></ol></li></ul><h2 id="Bottomline-Hashed-RSA-SHA-3"><a href="#Bottomline-Hashed-RSA-SHA-3" class="headerlink" title="Bottomline: Hashed RSA (SHA-3)"></a>Bottomline: Hashed RSA (SHA-3)</h2><p>In <strong>practice</strong>, we let $H$ be the <strong>SHA-3 hash function.</strong></p><img src="https://s1.ax1x.com/2022/08/07/vuxHbQ.png" alt="SHA-3" style="zoom:33%;" /><p>And we believe that SHA-3 <u>acts like a random function.</u></p><p>That’s the heuristic.</p><p>On the one hand, it doesn’t make any sense, but one the other hand, it has served us well so far.</p><p>There are <u>no attacks against RSA+SHA-3</u>, for example.</p>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Construction of CRHF from Discrete Log&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Digital Signatures only from OWF&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Direct Constructions:Trapdoor Permutation and the Hash-and-Sign Paradigm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Random Oracles.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="Digital Signatures" scheme="https://f7ed.com/tags/Digital-Signatures/"/>
    
      <category term="Random Oracles" scheme="https://f7ed.com/tags/Random-Oracles/"/>
    
      <category term="Hashed RSA" scheme="https://f7ed.com/tags/Hashed-RSA/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 11</title>
    <link href="https://f7ed.com/2022/08/02/mit6875-lec11/"/>
    <id>https://f7ed.com/2022/08/02/mit6875-lec11/</id>
    <published>2022-08-01T16:00:00.000Z</published>
    <updated>2022-10-18T12:28:26.876Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p>Today’s topic is Many-time Digital Signatures.</p><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Many-time, stateful, signature schemes.</li><li>Naor-Yung construction: stateless EUF-CMA-secure signature schemes.</li></ul><span id="more"></span><p>In last blog was introduced the <strong>definition of Digital Signatures</strong> and the <strong>EUF-CMA Security.</strong></p><p>Moreover, we introduced <strong>Lamport (One-time) Digital Signatures.</strong></p><p>We can use it to <u>sign polynomially many bits with a fixed verification key.</u></p><p>The <strong>main idea</strong> is <u>hashing</u> the message into $n$ bits and <u>signing</u> the hash.</p><p>So far, it’s <strong>one-time security</strong>. The adversary can forge signature on any message <strong>given the signatures on (some) two messages.</strong></p><p>How to achieve <strong>Many-time Signature Scheme ?</strong></p><p>It’s today’s gist.</p><p>We will achieve many-time signature scheme in four+ steps.</p><ol><li>Stateful, Growing Signatures.<br>Idea: Signature <strong>Chains</strong></li><li>How to Shrink the signatures.<br>Idea: Signature <strong>Trees</strong></li><li>How to Shrink Alice’s storage.<br>Idea: <strong>Pseudorandom Trees</strong></li><li>How to make Alice stateless.<br>Idea: <strong>Randomization</strong></li><li>(optional). How to make Alice stateless and deterministic.<br>Idea: <strong>PRFs</strong>.</li></ol><h1 id="S1-Stateful-Many-time-Signatures"><a href="#S1-Stateful-Many-time-Signatures" class="headerlink" title="S1: Stateful Many-time Signatures"></a>S1: Stateful Many-time Signatures</h1><p>The first step is to achieve stateful many-time signatures.</p><p>The main idea is <strong>Signature Chains.</strong></p><h2 id="sign-m1"><a href="#sign-m1" class="headerlink" title="sign m1"></a>sign m1</h2><ul><li>Alice starts with a secret signing Key $SK_0$.<br>Her public verification Key $VK_0$ is stored in the (public) “directory”.</li></ul><img src="https://s1.ax1x.com/2022/08/06/vu9p6K.png" alt="public verification key" style="zoom:33%;" /><ul><li><p>When <strong>signing</strong> a message $m_1$:</p><ol><li>Generate <strong>a new pair</strong> $(VK_1,SK_1)$.</li><li>Produce signature $\sigma_1\leftarrow Sign(SK_0,m_1||VK_1)$. </li><li>Output $VK_1||\sigma_1$</li><li><strong>Remember</strong> $VK_1||m_1||\sigma_1$  as well as $SK_1$.</li></ol><ul><li>Alice is going to sign not only the message, but <u>the message together with the next verification key.</u></li><li>She uses it to <strong>authenticate a new verification.</strong></li><li>It’s the concatenation of two strings and we can sign polynomially many bits.</li></ul></li><li><p>To <strong>verify</strong> a signature $VK_1||\sigma_1$ for message $m_1$:</p><ul><li><p>Run $Verify(VK_0,m_1||VK_1,\sigma_1)$.</p>  <img src="https://s1.ax1x.com/2022/08/06/vupolV.png" alt="verify the first message" style="zoom:25%;" /></li><li><p>We use $VK_0$ to verify the signature $\sigma_1$ that $m_1$is sent from Alice and $VK_1$ is authenticated from Alice.</p></li></ul></li></ul><h2 id="sign-m2"><a href="#sign-m2" class="headerlink" title="sign m2"></a>sign m2</h2><p>But how about the next message $m_2$ ?</p><p>Can we just output $VK_2||\sigma_2$ as follows ? (NO!)</p><ul><li>When <strong>signing</strong> the next message $m_2$:<ol><li>Generate <strong>a new pair</strong> $(VK_2,SK_2)$.</li><li>Produce signature $\sigma_2\leftarrow Sign(SK_1,m_2||VK_2)$. </li><li>Output $VK_2||\sigma_2$</li></ol></li></ul><p>The thing to point is that each signing is <strong>independent</strong> and everyone <u>only knows the verification key $VK_0$ in that “directory”.</u></p><p>The verifier <strong>dosen’t know</strong> the <u>verification key</u> $VK_1$ for the signature $\sigma_2$ nor the <u>authentication for $VK_1$</u> when he receives $VK_2||\sigma_2$.</p><hr><p>So Alice needs to <strong>send $VK_1$ as well as the authentication for $VK_1$.</strong></p><p>We should output $VK_1||m_1||\sigma_1||VK_2||\sigma_2$ as follows.</p><ul><li><p>When <strong>signing</strong> the next message $m_2$:</p><ol><li>Generate <strong>a new pair</strong> $(VK_2,SK_2)$.</li><li>Produce signature $\sigma_2\leftarrow Sign(SK_1,m_2||VK_2)$. </li><li><strong>Output</strong> $VK_1||m_1||\sigma_1||VK_2||\sigma_2$.</li><li>(additionally) <strong>Remember</strong> $VK_2||m_2||\sigma_2$  as well as $SK_2$.</li></ol><ul><li>The <strong>first part</strong> $VK_1||m_1||\sigma_1$ is to <u>authenticate the verification key $VK_1$.</u></li><li>The verify uses $VK_1$ to <u>verify the message $m_2$ together with the next verification $VK_2$.</u></li></ul></li><li><p>To <strong>verify</strong> a signature $VK_1||m_1||\sigma_1||VK_2||\sigma_2$ for message $m_2$:</p><ol><li><p>Run $Verify(VK_0,m_1||VK_1,\sigma_1)$ to authenticate $VK_1$.</p></li><li><p>Run $Verify(VK_1,m_2||VK_2,\sigma_2)$ to authenticate the message $m_2$ together with the next verification key $VK_2$.</p> <img src="https://s1.ax1x.com/2022/08/06/vupTyT.png" alt="Verify the sencond message." style="zoom:25%;" /></li></ol></li></ul><p>It’s <strong>growing signatures</strong> since Alice needs remember the $VK_i||m_i||\sigma_i$ as well as $SK_i$.</p><p>And the <strong>signature chains</strong> is as follows.</p><img src="https://s1.ax1x.com/2022/08/06/vup7OU.png" alt="signature chains" style="zoom:25%;" /><h2 id="An-optimization"><a href="#An-optimization" class="headerlink" title="An optimization"></a>An optimization</h2><p>In fact, Alice stores the $m_i$ just to use $\sigma_i$ to authenticate $VK_i$.</p><p>So there is an <strong>optimization</strong> that need to <u>remember only the past verification keys</u>, not the past messages.</p><p>Suppose we can split the verification into two halves.</p><p>We use part of $VK_i$ to sign $m_{i+1}$ and the rest to sign $VK_{i+1}$.</p><p>The signature chains is as follows.</p><p>The verifier only needs to <u>verify the past verification keys</u>, not the past messages.</p><img src="https://s1.ax1x.com/2022/08/06/vupbmF.png" alt="only verify the past verification keys" style="zoom:25%;" /><h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><p>There are still two <strong>major problems.</strong></p><ol><li>Alice is <strong>stateful</strong>.<br>Alice needs to remember a whole lot of things, $\mathcal{O}(T)$ information after $T$ steps. </li><li>The signatures <strong>grow</strong>.<br>Length of the signature of the $T$-th message is $\mathcal{O}(T)$. </li></ol><h1 id="S2-How-to-Shrink-the-signatures"><a href="#S2-How-to-Shrink-the-signatures" class="headerlink" title="S2: How to Shrink the signatures ?"></a>S2: How to Shrink the signatures ?</h1><p>The next step is to shrink the signature.</p><p>The main idea is <strong>Signature Trees.</strong></p><ul><li><p>Alice starts with a secret signing Key $SK_\epsilon$.<br>Her <u>public</u> verification Key $VK_\epsilon$ is stored in the (public) “directory”.</p></li><li><p>Alice <u>generates many random $(VK,SK)$ pairs</u> and arrange them <strong>in a tree</strong> of depth = security parameter $\lambda$.<br>There are $2^\lambda$ leaves and Alice <strong>only uses the leaf to sign the message.</strong></p>  <img src="https://s1.ax1x.com/2022/08/06/vupqw4.png" alt="signatures tree" style="zoom:33%;" /></li><li><p>When <strong>signing</strong> the first message $m_0$</p><ul><li><p>Alice only <u>uses the leaf to sign the message</u> while <u>use the parent node to sign <strong>both</strong> children nodes.</u></p><img src="https://s1.ax1x.com/2022/08/06/vupLTJ.png" alt="sign the first message m0" style="zoom:33%;" /></li><li><p>Use $VK_{000}$ to sign $m_0$.</p><ul><li>$\tau_0\gets Sign(SK_{000},m_0)$ </li></ul></li><li><p>“<strong>Authenticate</strong> ” $VK_{000}$ <u>using the “signature path”.</u><br>Alice produces the <strong>authentication path</strong> for $VK_{000}$ : $(\sigma_\epsilon,\sigma_0,\sigma_{00})$.</p><ol><li><p>Authenticate $VK_0$: use $VK_\epsilon$ to <u>sign both</u>  $VK_0$ and $VK_1$<br> $\sigma_\epsilon\gets Sign(SK_\epsilon,VK_{0},VK_{1})$ </p><ol start="2"><li><p>Authenticate $VK_{00}$: use $VK_{0}$ to <u>sign both</u>  $VK_{00}$ and $VK_{01}$<br>$\sigma_0\gets Sign(SK_0,VK_{00},VK_{01})$</p></li><li><p>Authenticate $VK_{000}$: use $VK_{00}$ to <u>sign both</u>  $VK_{000}$ and $VK_{001}$</p><p>$\sigma_{00}\gets Sign(SK_{00},VK_{000},VK_{001})$ </p></li></ol></li></ol></li><li><p><strong>Signatures</strong> of $m_0$: (Authentication path for $VK_{000}$, $\tau_0\gets Sign(SK_{000},m_0)$) </p></li></ul></li><li><p>When <strong>signing</strong> the next message $m_1$</p>  <img src="https://s1.ax1x.com/2022/08/06/vupXk9.png" alt="sign the next message m1" style="zoom:30%;" /><ul><li>Use $VK_{001}$ to sign $m_1$.<ul><li>$\tau_1\gets Sign(SK_{001},m_1) $ </li></ul></li><li>“<strong>Authenticate</strong> ” $VK_{001}$ using the “signature path”.<br>Alice produces the <strong>authentication path</strong> for $VK_{001}$ : $(\sigma_\epsilon,\sigma_0,\sigma_{00})$.<ol><li>Authenticate $VK_0$: $\sigma_\epsilon\gets Sign(SK_\epsilon,VK_{0},VK_{1})$ </li><li>Authenticate $VK_{00}$: $\sigma_0\gets Sign(SK_0,VK_{00},VK_{01})$ </li><li>Authenticate $VK_{000}$:  $\sigma_{00}\gets Sign(SK_{00},VK_{000},VK_{001})$ </li></ol></li><li><strong>Signatures</strong> of $m_1$: (Authentication path for $VK_{001}$, $\tau_1\gets Sign(SK_{001},m_1)$) </li></ul></li></ul><p>The <strong>good</strong> news is the <u>signatures consist of $\lambda$ one-time signatures</u> and <strong>do not grow with time.</strong></p><p>But the <strong>bad</strong> news is the <strong>signer</strong> <u>generates and keeps the entire ($\approx 2^\lambda$-size) signature tree</u> in <strong>memory</strong>. </p><p>Besides, the signer also needs to <strong>remember the state</strong> that <u>what is the last leaf used for signing.</u></p><h1 id="S3-How-to-Shrink-Alice’s-storage"><a href="#S3-How-to-Shrink-Alice’s-storage" class="headerlink" title="S3: How to Shrink Alice’s storage."></a>S3: How to Shrink Alice’s storage.</h1><p>The main idea is <strong>Pseudorandom Trees.</strong></p><p>Instead of truly random signature trees, Alice uses <strong>PRF</strong> to <u>build a pseudorandom signature trees.</u></p><img src="https://s1.ax1x.com/2022/08/06/vupvf1.png" alt="pseudorandom signature tree" style="zoom:33%;" /><ul><li>Alice <u>keeps a secret PRF key</u> $K$.</li><li>Alice <u>populates the nodes with</u> $r_x=PRF(K,x)$</li><li>Use $r_x$ to <u>derive the key pair</u> $(VK_x,SK_x)\gets Gen(1^\lambda;r_x)$.</li><li>The thing to notice is that Alice <u>only registers the verification key</u> $VK_\epsilon$.<br>So the verifier <strong>only knows</strong> $VK_\epsilon$, not the PRF key.</li></ul><p>We can use the pseudorandom signature tree to sign many-time signatures same as above.</p><img src="https://s1.ax1x.com/2022/08/06/vupjYR.png" alt="pseudorandom signature tree" style="zoom:33%;" /><p>As a matter of fact, the signer can <strong>do lazy evaluation</strong> instead of evaluating every node beforehand.</p><p>So the signer can achieve <strong>short signatures and small storage</strong> at the same time.</p><p>However, it’s still <strong>stateful</strong>.</p><p>The signer still needs to <strong>keep a counter</strong> indicating which leaf (which tells her which secret key) to use next.</p><p>It proceeds to the next step.</p><h1 id="S4-How-to-make-Alice-stateless"><a href="#S4-How-to-make-Alice-stateless" class="headerlink" title="S4: How to make Alice stateless."></a>S4: How to make Alice stateless.</h1><p>The main idea is <strong>randomization</strong>.</p><p>We can achieve <strong>stateless</strong> via randomization.</p><ul><li><p>When <strong>signing</strong> a message $m$</p>  <img src="https://s1.ax1x.com/2022/08/06/vupzSx.png" alt="sign the first message" style="zoom:33%;" /><ol><li>Pick a <strong>random</strong> leaf $r$.</li><li>Use $VK_r$ to sign $m$.<br>$\sigma_r\gets Sign(SK_r,m)$ </li><li>Output $(r,\sigma_r,\text{authentication path for }VK_r)$.</li></ol></li></ul><p>The good news is it’s <strong>stateless</strong>.</p><p>But we <strong>cannot pick the same leaf twice</strong> since we <u>are using the one-time signature scheme.</u></p><p>The key idea of security analysis is <strong>birthday attack.</strong></p><p>If the signer produces $q$ signatures, the <strong>probability</strong> she <u>picks the same leaf twice</u> is $\le q^2/2^\lambda$, which is negligible. </p><h1 id="S5-How-to-make-Alice-stateless-and-deterministic"><a href="#S5-How-to-make-Alice-stateless-and-deterministic" class="headerlink" title="S5: How to make Alice stateless and deterministic."></a>S5: How to make Alice stateless and deterministic.</h1><p>The key idea is generating $r$ <strong>pseudo-randomly.</strong></p><p>Have <strong>another PRF key</strong> $K’$ and let $r=PRF(K’,m)$.</p><ul><li><p>When <strong>signing</strong> a message $m$</p>  <img src="https://s1.ax1x.com/2022/08/06/vupzSx.png" alt="stateless and deterministic" style="zoom:33%;" /><ol><li>Pick a pseudorandom leaf $r=PRF(K’,m)$.</li><li>Use $VK_r$ to sign $m$.<br>$\sigma_r\gets Sign(SK_r,m) $</li><li>Output $(r,\sigma_r,\text{authentication path for }VK_r)$.</li></ol></li></ul><h1 id="Security-Analysis"><a href="#Security-Analysis" class="headerlink" title="Security Analysis"></a>Security Analysis</h1><p>For simplicity, we analyze the <strong>randomization stateless scheme</strong>. (S4)</p><p>The <strong>many-time digital signature</strong> scheme is <u>EUF-CMA secure</u> if the <strong>one-time digital signature</strong> is <u>one-time secure.</u></p><p>Assume for the <strong>contradiction</strong> that there is <u>an adversary breaking the EUF-CMA security,</u> then we <u>can construct a one-time forger</u>.</p><ul><li>We <strong>have the adversary</strong> $\mathcal{A}$ <u>for EUF-CMA security.</u><ol><li>Get the verification key $VK$.</li><li>Request for signatures of $q$ messages. ($q$-time)<ol><li>Request for message $m_i$</li><li>Obtain the signature $\sigma_i$ for $m_i$. </li></ol></li><li>Produce $(m^*,\sigma^*)$ that a signature against a new message $m^*\notin \{m_1,m_2,\dots m_q\}$ with non-negligible advantage.</li></ol></li><li>We <strong>want to construct a forger</strong> $\mathcal{B}$ <u>for one-time security. </u><ol><li>Get the one-time verification key $OVK$.</li><li>Request for the signatures $\sigma$ of a single message $m$. (only one-time)</li><li>Produce $(m’,\sigma’)$ that a signature against a new message $m’\ne m$.</li></ol></li><li>For simplicity, we <strong>condition</strong> on the event $E$ where all our random $r$’s are distinct.<ul><li>$\operatorname{Pr}[\mathcal{A}\text{ wins }\mid E]\ge q^2/2^\lambda$ </li><li>Suppose the probability above dosen’t change very much.</li><li>$\ge \operatorname{Pr}[\mathcal{A}\text{ wins }]-\operatorname{Pr}[E]=1/poly(\lambda)-negl(\lambda)$ </li><li>$\ge 1/poly(\lambda)$. </li><li>So the <strong>advantage</strong> of $\mathcal{A}$ is <u>non-negligible even on the condition. </u></li></ul></li></ul><p>So We need the forger $\mathcal{B}$ to <strong>interact with the adversary $\mathcal{A}$</strong> to win the game. </p><p><font color=blue><u><b><i>Construction of One-time Forger $\mathcal{B}$:</i></b></u></font></p><ul><li><p>Plop $OVK$ <strong>into a random leaf</strong> $r$.</p><ol><li><p>$\mathcal{B}$ get the one-time verification key $OVK$.</p></li><li><p>$\mathcal{B}$ <u>generates the pseudorandom signature trees</u> and <u>plop the $OVK$ into a random leaf $r_{OVK}$.</u></p></li><li><p>$\mathcal{B}$ sends the root verification key $VK_\epsilon$ to $\mathcal{A}$. </p></li><li><p>$\mathcal{A}$ requests for the signatures for $q$ times.<br>For each message $m_i$, there are <strong>two cases.</strong></p><ol><li><p>If $\mathcal{B}$ picks the random leaf $r\ne r_{OVK}$, $\mathcal{B}$ is <u>able to produce the signature. </u></p></li><li><p>If $\mathcal{B}$ picks the random leaf $r= r_{OVK}$, $\mathcal{B}$ <u>cannot produce the signature. </u><br>But $\mathcal{B}$ <strong>can request the signature for a single message.</strong></p><ol><li><p>$\mathcal{B}$ requests for the message $m_i$.</p></li><li><p>$\mathcal{B}$ obtains the signature $\sigma_i$ for the single message $m_i$. </p></li><li><p>$\mathcal{B}$ <u>passes the signature $\sigma_i$ to the $\mathcal{A}$ as</u> the response. </p><p>Note: $\mathcal{B}$ can only picks $r_{OVK}$ <strong>once</strong>. Besides, it’s <strong>necessary</strong> to pick it. </p></li></ol></li></ol></li><li><p>$\mathcal{A}$ promises to <strong>produce the signature for a new message</strong>  $m^*\notin \{m_1,m_2,\dots m_q\}$ </p><ul><li><p>The signature consists of  $(r^*,\sigma^*,\text{authentication path for }VK_{r^*})$ </p></li><li><p>If   $r^*= r_{OVK}$ and $\sigma ^*$  is <u>different from the previous</u> $\sigma_i$ generated by $\mathcal{B}$. </p></li><li><p>Then $\mathcal{B}$ <u>wins</u>. $\mathcal{B}$ just passes the  $(m^*,\sigma^*)$ as the <strong>forgery signature</strong>. </p></li><li><p>But it could happen only if $\mathcal{A}$  picks $r^*$ from one of the leaves $\{r_1,r_2,\dots r_q\}$ given by $\mathcal{B}$.  </p><p>  <font color=blue><u><b>Claim : </b></u></font><br>  If $\mathcal{A}$ picks one of the leaves $\{r_1,r_2,\dots r_q\}$ when forging, then $\mathcal{B}$ can produce the one-time forgery w.p. $1/q$.  </p></li></ul></li></ol></li></ul><p>But there is <strong>no guarantee</strong> that $\mathcal{A}$ could pick one of the leaves $\{r_1,r_2,\dots r_q\}$ given from $\mathcal{B}$.  </p><p>Instead, we plop $OVK$ into a <strong>node</strong>.</p><ul><li><p>Plop $OVK$ <em>into</em> the $VK_\epsilon$ location as follows.</p>  <img src="https://s1.ax1x.com/2022/08/06/vu9Sl6.png" alt="polp into the the root" style="zoom:33%;" /><ol><li><p>$\mathcal{B}$ get the one-time verification key $OVK$. </p></li><li><p>$\mathcal{B}$ generates the pseudorandom signature trees and plop the $OVK$ <strong>into the root node</strong>. So $\mathcal{B}$ <u>knows all secret key except $SK_\epsilon$. </u></p></li><li><p>$\mathcal{B}$ sends the root verification key $VK_\epsilon$ to $\mathcal{A}$. </p></li><li><p>$\mathcal{A}$ requests for the signatures for $q$ times.<br>For each message $m_i$, there are <strong>two cases.</strong></p><ol><li>Pick a <strong>random</strong> leaf $r$.</li><li>Use $VK_r$ to sign $m$.<br>$\tau_i\gets Sign(SK_r,m) $</li><li>Produce authentication path for $VK_r$.<br>Signatures for message $m_1$: $(r,\tau_1,(\sigma_\epsilon,\epsilon_0,\epsilon_{01}))$.</li></ol><ul><li>$\mathcal{B}$ <strong>cannot produce the signature</strong> of $\sigma_\epsilon$ since she dosen’t know the $SK_\epsilon$. </li><li>But  $\mathcal{B}$ can <u>request the signature $\sigma_\epsilon$ for a single message</u> $(VK_0||VK_1)$. </li></ul></li><li><p>$\mathcal{A}$ promises to <strong>produce the signature for a new message</strong>  $m^*\notin \{m_1,m_2,\dots m_q\}$ </p><ul><li>The signature consists of  $(r^*,\tau^*,\text{authentication path for }VK_{r^*})$ </li><li>Suppose $\mathcal{A}$ picks $r^*=1$ as above figure.<br>The authentication path for $VK_{001}$: $(\sigma_\epsilon’,\sigma_0’,\sigma_{00}’)$.</li><li>In fact, the output of forgery consists $VK_0’,VK_1’,VK_{00}’,VK_{01}’,VK_{000}’,VK_{001}’$, which could be different from the tree built by $\mathcal{B}$. </li><li>If $VK_0||VK_1 \ne VK_0’||VK_1’$, $\mathcal{A}$ wins. </li></ul></li></ol></li><li><p>Plop $OVK$ into the $VK_0$ location.<br>The analysis is the same.<br>If $VK_{00}||VK_{01} \ne VK_{00}’||VK_{01}’$, $\mathcal{A}$ wins. </p></li></ul><p>The main idea is as above.</p>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;Today’s topic is Many-time Digital Signatures.&lt;/p&gt;
&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Many-time, stateful, signature schemes.&lt;/li&gt;
&lt;li&gt;Naor-Yung construction: stateless EUF-CMA-secure signature schemes.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="Digital Signatures" scheme="https://f7ed.com/tags/Digital-Signatures/"/>
    
      <category term="EUF-CMA Security" scheme="https://f7ed.com/tags/EUF-CMA-Security/"/>
    
      <category term="Lamport Signature" scheme="https://f7ed.com/tags/Lamport-Signature/"/>
    
      <category term="Many-time Signature" scheme="https://f7ed.com/tags/Many-time-Signature/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 10</title>
    <link href="https://f7ed.com/2022/07/29/mit6875-lec10/"/>
    <id>https://f7ed.com/2022/07/29/mit6875-lec10/</id>
    <published>2022-07-28T16:00:00.000Z</published>
    <updated>2022-08-15T04:11:45.704Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p>Today’s topic is Digital Signatures.</p><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Motivation for Digital Signatures.</li><li>Definition: EUF-CMA Security</li><li>One-time signatures: Lamport’s Scheme.<ul><li>how to sign a single bit, once</li><li>how to sign $n$ bits, once</li></ul></li><li>Collision-resistant hashing<ul><li>how to sign polynomially many bits, once.</li></ul></li></ul><span id="more"></span><h1 id="Digital-Signatures"><a href="#Digital-Signatures" class="headerlink" title="Digital Signatures"></a>Digital Signatures</h1><h2 id="Digital-Signatures-vs-MACs"><a href="#Digital-Signatures-vs-MACs" class="headerlink" title="Digital Signatures vs. MACs"></a>Digital Signatures vs. MACs</h2><h3 id="MACs"><a href="#MACs" class="headerlink" title="MACs"></a>MACs</h3><p>In <a href="/2022/07/12/mit6875-lec5/" title="Lecture 5">Lecture 5</a>, we mentioned the applications of PRF and one of them is authentication.</p><p><strong>Message Authentication Codes</strong>(MAC) can <u>be evaluated by PRF, the message taken as input.</u></p><img src="https://s1.ax1x.com/2022/07/30/vinYXF.png" alt="MAC" style="zoom:33%;" /><p>MAC  gives the <strong>Authenticity</strong> that <u>Bob is able to ensure the messages came from Alice.</u></p><p>Yet it needs Alice and Bob to <strong>share a secret key beforehand.</strong></p><p>Then Alice uses the $sk$ to produce the MAC.</p><h3 id="Digital-Signatures-1"><a href="#Digital-Signatures-1" class="headerlink" title="Digital Signatures"></a>Digital Signatures</h3><p>Digital Signatures is the <strong>Public-key analog</strong> of MACs.</p><p>The goal:</p><ul><li><strong>Only Alice</strong> can produce signatures.</li><li>Bob (or <strong>anyone</strong> else) can verify them.</li></ul><p>There is a pair of keys, <strong>secret key</strong> $sk$ and the corresponding <strong>(public) verification key</strong> $vk$.</p><p>(Public) verification keys are stored in a “dictionary”.</p><img src="https://s1.ax1x.com/2022/07/30/vinU0J.png" alt="Digital Signatures" style="zoom:33%;" /><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><table><thead><tr><th></th><th>Signatures</th><th>MACs</th></tr></thead><tbody><tr><td>1</td><td>n uses require n key-pairs</td><td> $n$ user require $n^2$ keys</td></tr><tr><td>2</td><td>Publicly Verifiable</td><td>Privately Verifiable</td></tr><tr><td>3</td><td>Transferable</td><td>Not transferable</td></tr><tr><td>4</td><td>Provides Non-Repudiation</td><td>Dose not provide Non-Repudiation</td></tr></tbody></table><ol><li>Signatures have public verification keys and private secret keys, so $n$ uses require $n$ key-pairs.<br>But MAC have (private) shared keys beforehand, so $n$ users require $n^2$ keys.</li><li>Anyone can verify the signatures using the <strong>public</strong> $vk$.<br>But only Bob who has the <strong>secret</strong> key $sk$ can verify the MACs.</li><li>Signatures are <strong>Transferable</strong>. That is, you <u>can take signatures and show it to others.</u></li><li>Signatures provides <strong>Non-Repudiation</strong>. That is, you <u>cannot claim that you didn’t sign it</u>.[*不可抵赖]</li></ol><p>Transferability and Non-Repudiation whether they are good properties or bad properties depends on the scenario. It is double edged sword.</p><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><p>There are abundant applications of signatures.</p><ol><li><strong>Certificates</strong>, or a public-key directory in practice.<ul><li>Trusted Certificate Authority(CA), e.g. Verisign, Let’s Encrypt.</li></ul><ol><li>When <strong>Alice</strong> (=<a href="http://www.google.com">www.google.com</a>) wants to <em>register</em> her public (encryption and signing) keys $pk$ and $vk$, <strong>CA</strong> first checks that she is Alice.</li><li><strong>CA</strong> <em>issues</em> a “certificate”  $\sigma\leftarrow Sign(SK_{Verisign},Alice||pk||vk)$.<br>The certificate is essentially the signature.<br>The <strong>certificate</strong> indicates that $pk$ and $sk$ are indeed Alice’s.</li></ol> <strong>CA</strong> produces the signature using its sign key $SK_{Verisign}$.<ol start="3"><li><strong>Alice</strong> can later <em>stores</em> this certificate to prove she “owns” $pk$ and $sk$.</li><li><strong>Browsers</strong> <em>store</em> $VK_{Verisign}$ and check the certificate.<br>$VK_{Verisign}$ is the public verification key of CA.</li></ol></li><li>Bitcoin and other cryptocurrencies.<ul><li>I am <strong>identified</strong> by <u>my verification key $vk$.</u></li><li>When I pay you (your verification key = $vk’$), I sign “$x$ paid to $vk’$” with my $sk$.</li></ul></li></ol><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>The definition of Digital Signatures consists of <strong>a triple of PPT algorithms</strong> $(Gen,Sign,Verify)$.</p><ul><li>$Gen(1^n)\rightarrow (vk,sk)$ running by Alice(signer)<br><strong>PPT</strong> Key Generation algorithm <u>generates a public-private key pair.</u></li><li>$Sign(sk,m)\rightarrow \sigma$ running by Alice (signer)<br><strong>(possible probabilistic</strong>) Signing algorithm <u>uses the secret signing key to produce a signature</u> $\sigma$.<br>Whether deterministic or probabilistic Signing algorithm makes sense.</li><li>$Verify(vk,m,\sigma)\rightarrow Acc(1)/Rej(0)$ running by Bob (any verifier)<br>Verification algorithm <u>uses the public verification key to check</u> the signature $\sigma$ <strong>against</strong> a message $m$.</li></ul><p><font color=blue><u><b><i>Correctness: </i></b></u></font> </p><p>For all $vk,sk,m$: $Verify(vk,m,Sign(sk,m))=accept$.</p><h2 id="EUF-CMA-Security"><a href="#EUF-CMA-Security" class="headerlink" title="EUF-CMA Security"></a>EUF-CMA Security</h2><p>Define the adversary first.</p><p>The adversary <u>after seeing signatures of many messages</u>, should <strong>not be able</strong> to produce a signature of any <strong>new message</strong>.</p><p><font color=blue><u><b>Adversary: </b></u></font> </p><ul><li>Power of adversary: <strong>Chosen-message attack</strong><br>The adversary can request for, and obtain, signatures <u>of (poly. many) messages</u> $m_1,m_2,\dots$</li><li>Goal of adversary: <strong>Existential Forgery</strong><br>The adversary wins if she <u>produces a signature</u> of any <strong>new</strong> message  $m^*\notin \{m_1,m_2,\dots \}$.</li></ul><p>Then we give <strong>Existentially Unforgeable against a Chosen Message Attack (EUF-CMA) security</strong> by a game.</p><p><font color=blue><u><b>Game :</b></u></font> </p><ol><li>The <strong>Challenger</strong> generates a public-private key pair $(vk,sk)$ and sends the public verification key $vk$ to Eve.</li><li><strong>Eve</strong> can <u>request for poly. many messages</u>   $\{m_1,m_2,\dots\}$ and <u>obtain the corresponding signatures</u> $\{\sigma_1,\sigma_2,\dots\}$. </li><li><strong>Eve</strong> produce a signature $\sigma^*$ <u>against a new message</u> $m^*\notin \{m_1,m_2,\dots\}$.</li></ol><img src="https://s1.ax1x.com/2022/07/30/vinNm4.png" alt="Game in Digital Signature EUF-CMA Definition" style="zoom:30%;" /> <article class="message is-info"> <div class="message-header"> <p><strong>EUF-CMA Definition:</strong></p> </div> <div class="message-body"> <p>Eve wins if  $Verify(vk,m^*,\sigma^*)=1$ and $m^*\notin\{m_1,m_2,\dots\}$. </p><p>The signature scheme is <strong>EUF-CMA-secure</strong> if <strong>no PPT</strong> Eve can win with probability better than $negl(n)$.</p> </div> </article> <p>The challenger gives Eve a lot of signatures. </p><p>Now Eve wants to forge a signature of other message right.</p><p>We say that she wins if she <strong>produces a signature right</strong> of <u>even one message that was not signed already.</u></p><p>We call this an <strong>existential forgery</strong> because there <u>exists a $m^*$ not in the set</u> for which she produces a signature.</p><p>It’s a very strong definition.</p><p>But the definition <strong>dose not prevent</strong> the adversary from producing <strong>a new signature</strong> for the <strong>same message.</strong></p><p>Yet she dose not  win. </p><p>In other words, it’s consistent with the definition to <u>allow the adversary to produce a new signature for the same message.</u></p><p>We can make the job easier for adversary. She <strong>wins as well</strong> if she <u>produces a new signature for the same message.</u></p><p>Then we can get <strong>strong EUF-CMA definition.</strong></p><p>So the stronger adversary, the stronger security by definition.</p><h1 id="Lamport-One-time-Signatures"><a href="#Lamport-One-time-Signatures" class="headerlink" title="Lamport (One-time) Signatures"></a>Lamport (One-time) Signatures</h1><p>In this section, we introduce a beautiful signature, <strong>Lamport Signature.</strong></p><p>It’s <strong>One-time signature</strong> sort of like One-time Pads.</p><p>The <strong>one-time signatures</strong> means that the <u>adversary gets a signature of some message</u> <strong>once</strong>, a single message, and she <u>should not be able to produce a signature of any other different message.</u></p><h2 id="How-to-sign-a-bit"><a href="#How-to-sign-a-bit" class="headerlink" title="How to sign a bit"></a>How to sign a bit</h2><p>We only use the signing key to <strong>sign once</strong> and we are <u>really signing a bit.</u></p><ul><li>$Gen(1^n)\rightarrow (SK,VK)$<ul><li>Signing Key $SK:[x_0,x_1]$<br>where $x_0,x_1$ <strong>are both $n$-bit string.</strong></li><li>Verification Key $VK:[y_0=f(x_0),y_1=f(x_1)]$<br>where $f$ is a <strong>OWF</strong>.</li></ul></li><li>$Sign(SK,b)\rightarrow \sigma$ where $b$ is a <strong>bit</strong><ul><li>The signature is $\sigma=x_b$.</li></ul></li><li>$Verify(VK,b,\sigma)$<ul><li>Check if $f(\sigma)\overset ? = y_b$</li></ul></li></ul><p>The <strong>signing keys $SK$</strong> are <u>two random $n$-bit string</u> and the <strong>verification keys</strong> $VK$ are the <u>corresponding values of OWF</u>, i.e. the signing keys are <strong>images</strong> of verification keys.</p><p>The game in Lamport (One-time) Signatures <strong>differs</strong> from that <u>Eve only gets a signature of a single message.</u></p><p>If $b=0$, the challenger gives Eve $x_0$, the signature of $0$, and Eve wants to <strong>forge the signature of</strong> $1$.</p><p>So the task of adversary is producing a signature of $1$.</p><p>That’s $x_1$. She <strong>cannot</strong> do it <u>because $x_1$ is random.</u></p><p><font color=blue><u><b>Claim :</b></u></font> </p><p>Assuming $f$ is a <strong>OWF</strong>, no PPT adversary can produce a signature of $\bar{b}$ <strong>given</strong> a signature of $b$.</p><p>The intuition of proof is easy.</p><p>There’s no way Eve can produce $x_1$ unless she can invert the $y_1$ she have.</p><h2 id="How-to-sign-n-bits"><a href="#How-to-sign-n-bits" class="headerlink" title="How to sign n bits"></a>How to sign n bits</h2><p>We can use the signing keys to <strong>sign $n$ bits, once.</strong></p><p>Just repeat it.</p><ul><li>$Gen(1^n)\rightarrow (SK,VK)$<ul><li>Signing Key  $ SK:\left[\begin{array}{c}x_{1,0},x_{2,0},\dots, x_{n,0}\\ x_{1,1},x_{2,1} ,\dots, x_{n,1}\end{array} \right]$ <br>where $x_{\cdot,\cdot}$ is $n$-bit random string and <strong>each column</strong> is <u>a pair-key for one bit.</u></li><li>Verification Key  $VK:\left[\begin{array}{c}y_{1,0},y_{2,0},\dots, y_{n,0}\\ y_{1,1},y_{2,1} ,\dots, y_{n,1}\end{array} \right]$ <br>where $f$ is a OWF and $y_{i,c}=f(x_{i,c})$.</li></ul></li><li>$Sign(SK,\vec m)\rightarrow \vec\sigma$ where <u>$m$ is a $n$-bit message</u> $(m_1,\dots, m_n)$.<ul><li>The signature is $\vec \sigma=(x_{1,m_1},\dots, x_{n,m_n})$.</li></ul></li><li>$Verify(VK,\vec m,\vec\sigma)$<ul><li>Check if  $\forall i:f(\sigma_i)\overset ? = y_{i,m_i}$.</li></ul></li></ul><p>In the game, Eve can <strong>request for a signature once</strong> of any $n$-bit message $m$ and she <strong>wants to forge the signature</strong> of a different message $m’\ne m$.</p><p>There are two claims.</p><p><font color=blue><u><b>Claim 1:</b></u></font></p><p>Assuming $f$ is a <strong>OWF</strong>, no PPT adversary can produce a signature of $m’$ <strong>given a signature of a single</strong> message $m\ne m’$.</p><p><font color=blue><u><b>Claim 2:</b></u></font></p><p>The adversary can forge signature on any message <strong>given the signatures on (some) two messages.</strong></p><p>We only give the proof of Claim 1.</p><p>Claim 2 can be comprehended easily after proving Claim1.</p><p><font color=blue><u><b>Proof for Claim 1: </b></u></font> </p><p>Suppose for the <strong>contradiction</strong> that <u>there is a adversary forging a signature</u> of $m’$ <u>given a signature of a single message</u> $m$ s.t. $m’\ne m$.</p><p>We want to construct a <strong>OWF Inverter for $y$</strong> so we need to interact with the forger.</p><p><font color=blue><u><b><i>Interaction with the forger: </i></b></u></font> </p><ol><li><strong>Inverter</strong> gives the <em>verification keys</em> $VK$ to the forgery.</li><li>The <strong>forger</strong> request for a signature of a single message $m$.</li><li><strong>Inverter</strong> produces the signature $\sigma$ she wants.</li><li>Then the <strong>forger</strong> promises to give a signature $\sigma’$ against a new message $m’$</li></ol><p>The <strong>key idea</strong> is we can take $y$ into $VK$ and plop it into one of the two slots in one column.</p><p>The <strong>thing to notice</strong> is that <u>there is at least one different bit between $m$ and $m’$.</u></p><p><u>Note:</u></p><ul><li>The message $m$ is what the <strong>forger</strong> wants to <u>obtain its signature</u></li><li>The message $m’$ is what the <strong>forger</strong> wants to <u>forge its signature.</u></li></ul><p>For simplicity, we <strong>suppose</strong> there is <u>only one different bit</u> between $m$ and $m’$.</p><p><font color=blue><u><b><i>messages $m$ and $m’$ are known in advance: </i></b></u></font> </p><ul><li><p>Suppose the Inverter knows $m$ and $m’$in advance.<br>For simplicity, suppose $m=00\dots 0$ and $m’=10\dots 0$.</p></li><li><p>The Inverter wants to get the inverse of $y$ against the OWF $f$.</p></li><li><p>So the Inverter <strong>interacts with the forger.</strong></p><ol><li><p>The Inverter samples $SK$ and <u>generates the verification keys</u> (put $y$ into one slot as follows)</p>       $$        VK=\left[\begin{array}{c}f(x_{1,0}) &,f(x_{2,0}),\dots, f(x_{n,0})\\ y &,f(x_{2,1}) ,\dots, f(x_{n,1})\end{array} \right]       $$               <p> where $x_{\cdot,\cdot}$ is <strong>known to the Inverter.</strong></p></li><li><p>The <strong>forger</strong> requests for the signature of $m=00\dots 0$.</p></li><li><p>The <strong>Inverter</strong> produces the signature $\sigma=(x_{1,0},\dots,x_{n,0})$.</p></li><li><p>The <strong>forger</strong> promises to produce the signature on $m’=10\dots0$ from the contradiction <strong>which has the inverse</strong> of $y$.</p></li></ol></li><li><p>Done.</p></li></ul><p>However, the Inverter <strong>dosen’t know the two messages in advance.</strong><br>The Invert only knows there is one different bit between $m$ and $m’$.</p><p>So the Inverter could <strong>guess it.</strong></p><p><font color=blue><u><b><i>messages $m$ and $m’$ are unknown in advance: </i></b></u></font></p><ul><li><p>The Inverter <strong>guesses</strong> the <u>different bit locates in $i$-th bit</u>  right <strong>w.p.</strong> $1/n$.</p></li><li><p>Suppose the Inverter <strong>plants the trap</strong> $y$ into the slot $(i,0)$,  $i$-th column and $0$-th row.<br>Then the generated <strong>verification keys</strong> is as follows.</p>  $$    VK=\left[\begin{array}{c}f(x_{1,0}) ,\dots ,&y&,\dots, f(x_{n,0})\\ f(x_{1,1}) ,\dots ,&f(x_{i,1})& ,\dots, f(x_{n,1})\end{array} \right]  $$      <p>where $x_{\cdot,\cdot}$ is <strong>known to the Inverter.</strong></p></li><li><p>In the forger’s point of view, when she looks at the verification keys $VK$, she <strong>has no information about where the trap is.</strong></p></li><li><p>There are <strong>two events</strong> that could happen <u>even if $m$ differs from $m’$ in $i$-th bit.</u></p><ul><li>The message $m$ <u>hits the trap</u> while the $m’$  dose not hit the trap.</li><li>The message $m$ <u>dosen’t hit the trap</u> while the $m’$  hits the trap.</li></ul></li><li><p>The Inverter can <strong>only handle</strong> with the second event.<br>The message $m$ hits the opposite location to the trap while the $m’$  hits the trap.</p><ul><li>The <strong>Inverter</strong> is <u>able to give the signature of</u> $m$, i.e. $(\dots,x_{i,1},\dots)$</li><li>The <strong>forger</strong> promises to <u>produce the signature of $m’$from the contradiction</u>, which <strong>have the inverse of</strong> $y$.</li><li>Otherwise, the forger gives something that the Inverter already has known.</li></ul></li><li><p>So the <strong>probability of inverting right</strong> is $\varepsilon/2n$ if the advantage of forging is non-negligibly $\varepsilon$.<br>The $1/n$ is the probability of <u>guessing the location of different bit</u> and the $1/2$ is the probability of <u>guessing whether $m$ hits the trap.</u></p></li><li><p>Done.</p></li></ul><p>Besides, <strong>Claim 2</strong> says that <u>once I give you the two signatures</u>, I am actually <u>giving you all the inverses</u> and you can produce signatures of any message you want.</p><p>This violates one-wayness.</p><p>So far, the length of message is limited by the size of verification keys.</p><p>Before proceeding to the next question that how to sign poly. many bits, we <strong>take a detour</strong> in <u>collision-resistant hash function.</u></p><h2 id="Detour-Collision-Resistant-Hash-Functions"><a href="#Detour-Collision-Resistant-Hash-Functions" class="headerlink" title="Detour: Collision-Resistant Hash Functions"></a>Detour: Collision-Resistant Hash Functions</h2><p>A <strong>compressing function</strong>  $h:\{0,1\}^m\rightarrow \{0,1\}^n$ (where $m&gt;n$ ) for which it is <u>computationally hard to find collisions.</u></p><p>It compresses the bits sort of the opposite of PRG.</p><p><font color=blue><u><b><i>Definition: </i></b></u></font> </p><p>$h$ is <strong>collision-resistant</strong> if for every PPT algorithm $A$, there is a negligible function $\mu$ s.t.</p>$$\operatorname{Pr}[A(1^n)=(x,y):x\ne y, h(x,y)]=\mu(n)$$<p><strong>In theory</strong>, we like to talk about <u>families of functions</u> to handle non-uniform adversaries (who could have hardcoded collision for the fixed function $h$).</p><p>A <strong>compressing family of functions</strong>  $\mathcal{H}=\{\{0,1\}^m\rightarrow \{0,1\}^n\}$ (where $m&gt;n$) for which it is computationally hard to find collisions.</p><p><font color=blue><u><b><i>Collision-Resistant Definition: </i></b></u></font> </p><p>$\mathcal{H}$ is collision-resistant if for every PPT algorithm $A$, there is a negligible function $\mu$ s.t.</p>$$\operatorname{Pr}_{h\gets \mathcal{H}}[A(1^n)=(x,y):x\ne y, h(x,y)]=\mu(n)$$<h2 id="How-to-sign-poly-many-bits"><a href="#How-to-sign-poly-many-bits" class="headerlink" title="How to sign poly. many bits"></a>How to sign poly. many bits</h2><p>Back to the question that how to <strong>sign poly. many bits</strong> with <u>a fixed verification key.</u></p><p>The <strong>key idea</strong> is hashing the message into $n$ bits and <u>sign the hash.</u></p><ul><li>$Gen(1^n)\rightarrow (SK,VK)$<ul><li>Signing Key  $SK:\left[\begin{array}{c}x_{1,0},x_{2,0},\dots, x_{n,0}\\ x_{1,1},x_{2,1} ,\dots, x_{n,1}\end{array} \right]$ <br>where $x_{\cdot,\cdot}$ is $n$-bit random string and each column is a pair-key for one bit.</li><li>Verification Key  $VK:\left[\begin{array}{c}y_{1,0},y_{2,0},\dots, y_{n,0}\\ y_{1,1},y_{2,1} ,\dots, y_{n,1}\end{array} \right]$ <br>where $f$ is a OWF and $y_{i,c}=f(x_{i,c})$.</li><li><strong>Sample</strong> $h\gets \mathcal{H}$.</li></ul></li><li>$Sign(SK,\vec m)\rightarrow \vec\sigma$ where $m$ is a $n$-bit message $(m_1,\dots, m_n)$.<ul><li><strong>Compute the hash</strong> $z=h(m)$.</li><li>The signature is $\vec \sigma=(x_{1,z1},\dots, x_{n,z_n})$.</li></ul></li><li>$Verify(VK,\vec m,\vec\sigma)$<ul><li><strong>Recompute the hash</strong> $z=h(m)$</li><li>Check if  $\forall i:f(\sigma_i)\overset ? = y_{i,z_i}$.</li></ul></li></ul><p><font color=blue><u><b>Claim :</b></u></font> </p><p>Assuming $f$ is a <strong>OWF</strong> and $\mathcal{H}$ s a <strong>collision-resistant family</strong>, no PPT adversary can produce a signature of $m’$ given a signature of a single $m\ne m’$.</p><p>We only give the idea of proof.</p><p><font color=blue><u><b><i>Intuition of Proof: </i></b></u></font> </p><p>Suppose for the contradiction.</p><p>There are two possibilities.</p><ul><li>Either the adversary picked $m’$ s.t. $h(m’)=h(m)$, in which case she <strong>violated collision-resistance</strong> of $\mathcal{H}$.</li><li>Or she produced a Lamport signature on a “message” $z’\ne z$, in which case she <strong>violated one-time security</strong> of Lamport, and therefore the one-wayness of $f$.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;Today’s topic is Digital Signatures.&lt;/p&gt;
&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Motivation for Digital Signatures.&lt;/li&gt;
&lt;li&gt;Definition: EUF-CMA Security&lt;/li&gt;
&lt;li&gt;One-time signatures: Lamport’s Scheme.&lt;ul&gt;
&lt;li&gt;how to sign a single bit, once&lt;/li&gt;
&lt;li&gt;how to sign $n$ bits, once&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Collision-resistant hashing&lt;ul&gt;
&lt;li&gt;how to sign polynomially many bits, once.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="Digital Signatures" scheme="https://f7ed.com/tags/Digital-Signatures/"/>
    
      <category term="EUF-CMA Security" scheme="https://f7ed.com/tags/EUF-CMA-Security/"/>
    
      <category term="Lamport Signature" scheme="https://f7ed.com/tags/Lamport-Signature/"/>
    
      <category term="One-time Signature" scheme="https://f7ed.com/tags/One-time-Signature/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 9</title>
    <link href="https://f7ed.com/2022/07/27/mit6875-lec9/"/>
    <id>https://f7ed.com/2022/07/27/mit6875-lec9/</id>
    <published>2022-07-26T16:00:00.000Z</published>
    <updated>2022-10-03T14:08:11.178Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Quadratic Residue and Quadratic Residuosity Assumption</li><li>Goldwasser-Micali Encryption and Homomorphism</li><li>Diffie-Hellman Key Exchange</li><li>El Gamal Encryption</li></ul><span id="more"></span><p>In last blog is showed one of the constructions of public-key encryption using Trapdoor Permutations, RSA encryption.</p><p>The gist in this blog is the remaining three constructions of public-key encryption.</p><p><u><b>Constructions of Public-Key Encryption: </b></u> </p><ol><li>Trapdoor Permutations (RSA)</li><li><em>Quadratic Residuosity/Goldwasser-Micali</em></li><li><em>Diffie-Hellman/El Gamal</em></li><li><em>Learning with Errors/Regev</em> </li></ol><h1 id="Quadratic-Residue"><a href="#Quadratic-Residue" class="headerlink" title="Quadratic Residue"></a>Quadratic Residue</h1><h2 id="QR-mod-P"><a href="#QR-mod-P" class="headerlink" title="QR mod P"></a>QR mod P</h2><p>Let $P$  be prime.</p><p>We saw (in <a href="/2022/07/27/mit6875-lec9/" title="Lecture 6">Lecture 6</a>) that <u>exactly half</u> of  $\mathbb{Z}_P^*$ are squares.</p><p>Define <strong>Legendre symbol</strong> $\left(\frac{x}{P}\right)=1$ is $x$ is <em>square</em>, $-1$ if $x$ is <em>not a square</em>, and $0$ if $x=0\mod P$.</p><p>We can <strong>tell efficiently</strong> whether $x$ is a <em>quadratic residue</em> mod P using <strong>Legendre symbol.</strong></p>$$\left(\frac{x}{P}\right)=x^{(P-1) / 2}=\begin{cases} 1 &,\text{ if }x \text{ is square} \\ -1 &,\text{ if }x \text{ is non-square} \\ 0 &,\text{ if }x \text{ is 0}\end{cases}$$<p>Then we  split $\mathbb{Z}_P^*$ in half, one is <strong>square</strong> and the other is <strong>non-square.</strong></p><img src="https://s1.ax1x.com/2022/07/29/vPB30s.png" alt="Split Zp* in half" style="zoom:33%;" /><p>It is <strong>easy</strong> to compute square roots mod $P$.</p><p>Besides, it’s an <strong>explicit</strong> formula for the case where $P=3\pmod 4$.</p><p><font color=blue><u><b>Claim:</b></u></font> </p><p>The square roots of $x \mod P$ are  $\pm x^{(P+1) / 4}$.</p><p><font color=blue><u><b>Proof:</b></u></font> </p> $\left(\pm x^{(P+1)/4}\right)^2=x^{(P+1)/2}=x\cdot x^{(P-1)/2}=x\mod P$.<h2 id="QR-mod-N"><a href="#QR-mod-N" class="headerlink" title="QR mod N"></a>QR mod N</h2><p>Now, let $N=PQ$ be a <strong>product of two primes</strong> and look at  $\mathbb{Z}_N^*$.</p><p>Define <strong>Jacobi symbol</strong>  $\left(\frac{x}{N}\right)=\left(\frac{x}{P}\right) \left(\frac{x}{Q}\right)$ to be $+1$ if $x$ is <u>a square mod both</u> $P$ and $Q$ <u>or a non-square mod both</u> $P$ and $Q$.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <b>Jacobi Symbol</b>  — — from Wiki<p>The <strong>Jacobi symbol</strong> is a <em>generalization of the Legendre symbol.</em><br>For any integer $a$ and any positive odd integer $n$, the Jacobi symbol $(\frac{a}{n})$ is <strong>defined as the product of the Legendre symbols</strong> <em>corresponding to the prime factors</em> of $n$:</p>$$\left(\frac{a}{n}\right)=\left(\frac{a}{p_1}\right)^{\alpha_1} \left(\frac{a}{p_2}\right)^{\alpha_2}\dots \left(\frac{a}{p_k}\right)^{\alpha_k}$$<p>where $n=p_1^{\alpha_1}p_2^{\alpha_2}\dots p_k^{\alpha_k}$ is the <strong>prime factorization</strong> of $n$.<br>Some properties:</p><ul><li><p>Fix the <strong>bottom</strong> argument: $\left(\frac{ab}{n}\right)=\left(\frac{a}{n}\right)\left(\frac{b}{n}\right)$</p></li><li><p>Fix the the <strong>top</strong> argument: $\left(\frac{a}{mn}\right)=\left(\frac{a}{m}\right)\left(\frac{a}{n}\right)$</p></li></ul> </div> </article> <p>So we can also split $\mathbb{Z}_N^*$ in half, and the <strong>Jacobi symbol</strong> for <u>one half is $+1$ and the other is $-1$.</u></p><img src="https://s1.ax1x.com/2022/07/29/vPB87n.png" alt="Split ZN* in half" style="zoom:33%;" /><p>A <strong>surprising fact</strong> is Jacobi symbol $\left(\frac{x}{N}\right)=\left(\frac{x}{P}\right) \left(\frac{x}{Q}\right)$ is <u>computable in poly. time without knowing $P$ and $Q$.</u></p><p>The <strong>key</strong> is <u>Law of Quadratic Reciprocity.</u></p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <b>Law of Quadratic Reciprocity:</b><p>For all $x, N$:</p><ul><li><p>$\left(\frac{x}{N}\right)=\left(\frac{N}{x}\right) \cdot (-1)^{(N-1)\cdot (x-1)/4}$ </p></li><li><p>$\left(\frac{x}{N}\right)=\left(\frac{x\mod N}{N}\right)$</p></li><li><p>$\left(\frac{2}{N}\right)=\left(-1\right)^{\frac{\left(n-1\right)^2}{8}}$</p></li><li><p>$\left(\frac{-1}{N}\right)=(-1)^{\frac{(n-1)}{2}}$</p></li></ul> </div> </article> <p>The one thing to <strong>notice</strong> is that the <strong>Legendre symbol</strong>$\left(\frac{x}{P}\right)$ tells you <strong>exactly</strong> whether $x$ is the square mod $P$ or not.</p><p>But the <strong>Jacobi symbol</strong> $\left(\frac{x}{N}\right)=1$ only gives you <strong>partial information</strong> that is square mod both $P$ and $Q$ <strong>or</strong> non-square mod both $P$ and $Q$.</p><p>But $x$ is <strong>square</strong> mod $N$ <strong>iff</strong> $x$ is square mod $P$ and it is a square mod $Q$.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Claim:</strong></p> </div> <div class="message-body"> <p>$x$ is <strong>square</strong> mod $N$ iff $x$ is <em>square</em> mod $P$ and it is a <em>square</em> mod $Q$.</p> </div> </article> <p>Hence, there are <u>two cases for $Jac_{+1}$.</u></p><ul><li>$QR_N$: the set of <strong>squares</strong> mod $N$.</li><li>$QNR_N$: the set of <strong>non-squares</strong> mod $N$. (but with Jacobi symbol $+1$)</li></ul><img src="https://s1.ax1x.com/2022/07/29/vPBJkq.png" alt="QR and QNR with Jac+1" style="zoom:33%;" /><p>The Jacobi symbol can be $+1$ even though $x$ is not square mod $N$, so it’s <strong>pseudo-square.</strong></p><p>The <strong>conjecture</strong> is that <u>distinguishing between $QR_N$ and $QNR_N$ is</u> a <strong>hard problem.</strong></p><p>We cannot distinguish from squares and pseudo-squares.</p><p>Can we use this hardness?</p><h2 id="Finding-Square-Roots-Mod-N"><a href="#Finding-Square-Roots-Mod-N" class="headerlink" title="Finding Square Roots Mod N"></a>Finding Square Roots Mod N</h2><p>The fact is <u>finding square roots</u> mod $N$ is <strong>as hard as</strong> <u>factoring</u> $N$.</p><h3 id="Suppose-we-know-P-and-Q"><a href="#Suppose-we-know-P-and-Q" class="headerlink" title="Suppose we know P and Q."></a>Suppose we know P and Q.</h3><p>Suppose we know $P$ and $Q$, and we want to find the square root of $x \mod N$.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> Before that, let’s introduce the <b>Chinese Reminder Theorem(CRT). </b><p>It’s a nice morphism.<br>Informally, we can get  $\mathbb{Z}_N^*\cong \mathbb{Z}_P^* \cdot \mathbb{Z}_Q^*$.</p><ul><li>In <strong>forward direction</strong>, we can map $\mathbb{Z}_N^*$ to $(\mathbb{Z}_P^* ,\mathbb{Z}_Q^*)$  <em>uniquely</em>.<ul><li>That is: $x \rightarrow(x\mod P, x\mod Q)$.</li></ul></li><li>In <strong>back direction</strong>, we can map  $(\mathbb{Z}_P^* ,\mathbb{Z}_Q^*)$ to $\mathbb{Z}_N^*$  <em>uniquely</em>.<ul><li>That is: $(x_1, x_2)\rightarrow c_Px_1+c_Qx_2\mod N$.<br>where $c_P$ and $c_Q$ are <em>CRT coefficients.</em></li></ul></li></ul> </div> </article> <p>We can find the square root of $x\mod N$ by following <strong>algorithm</strong>.</p><p><font color=blue><u><b>Algorithm:</b></u></font> </p><ol><li>Find the square roots of  $y\mod P$ and  $y\mod Q$.<ol><li>$x=y_P^2\mod P$ </li><li>$x=y_Q^2\mod Q$</li></ol></li><li>Let $y=c_Py_P+c_Qy_Q$ where the <em>CRT coefficients.</em><ol><li>$c_P=1\mod P \text{ and }0 \mod Q$</li><li>$c_Q=0\mod P \text{ and }1 \mod Q$</li></ol></li><li>Then $y$ is <strong>a square root</strong> of $x\mod N$.</li></ol><p><font color=blue><u><b>Proof:</b></u></font> </p><p>The proof is easy using CRT.</p><ul><li>$y^2 = (c_Py_P + c_Qy_Q)^2\mod N$</li><li>We can map $y^2$ to pair $(y^2\mod P, y^2\mod Q)$</li><li>Then we get pair $(y_P^2\mod P,y_Q^2 \mod Q)$</li><li>That is $x$.</li><li>Moreover, we can get  $x=y^2\mod N \longleftrightarrow \begin{array}{c}x=y^2\mod P \\x=y^2 \mod Q\end{array}$. </li></ul><p>Therefore, the <strong>takeaway</strong> is if $x$ is a <em>square</em>, it <u>has $4$ distinct square roots</u> $\mod N$.</p><p>Because it <em>respectively</em> has $2$ distinct square roots $\mod P$ and $\mod Q$.</p><h3 id="Suppose-we-know-square-root"><a href="#Suppose-we-know-square-root" class="headerlink" title="Suppose we know square root."></a>Suppose we know square root.</h3><p>Suppose we have a box that computes square roots $\mod N$.</p><p>The thing to notice is that the box <strong>only returns one square</strong> $y$ as it has $4$ distinct square roots.</p><p>So if we feed the box $x=z^2\mod N$ for <em>a random</em> $z$ of our choice, the <u>box can return other square root.</u></p><img src="https://s1.ax1x.com/2022/07/29/vPB1mj.png" alt="The Oracle" style="zoom:43%;" /><p>We can use the box to factor $N$.</p><p>Feed the box $x=z^2 \mod N$ for a random $z$.</p><p><font color=blue><u><b>Claim:</b></u></font> </p><p>With probability $1/2$,  $\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$.</p><p><font color=blue><u><b>Proof:</b></u></font> </p><ul><li>Notation:<ul><li>$z$ denotes the square root of $x$, which is known.</li><li>$x$ denotes the input of box s.t. $x=z^2 \mod N$.</li><li>$y$ denotes the output of box s.t. $x=y^2\mod N$, which <u>could be different with $z$ .</u></li></ul></li><li>We know $y^2=z^2\mod N$.<ul><li>$N\mid y^2-z^2$</li><li>$N\mid (y-z)(y+z)$</li></ul></li><li>There are <strong>three cases.</strong><ul><li>If  $N\mid (y-z)$, we can get $y=z\mod N$. (Useless)</li><li>If $N\mid (y+z)$, we can get $y=-z\mod N$. (Useless)</li><li>If $N\nmid (y-z)$ and $N\nmid (y+z)$, what can we get ?<ul><li>$(y-z)$ and $(y+z)$ <u>respectively have a factor</u> of $N$ since $N=PQ$.</li><li>So $\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$.</li></ul></li></ul></li><li>Hence, $\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$ if $y\ne z$ and $y\ne-z$.<br>The probability is $1/2$.</li><li>QED.</li></ul><p>It’s a very clever trick to factor.</p><p>I know a solution to a problem that turns it into an Oracle.</p><p>The Oracle could actually help me in solving another problem.</p><p>It’s an example of the <strong>reduction</strong>.</p><h2 id="Quadratic-Residuosity-Assumption-QRA"><a href="#Quadratic-Residuosity-Assumption-QRA" class="headerlink" title="Quadratic Residuosity Assumption(QRA)"></a>Quadratic Residuosity Assumption(QRA)</h2><p>Finding square roots is as hard as factoring $N$.</p><p>Moreover, <strong>recognizing squares</strong> $\mod N$ also <strong>seems hard</strong> as we mentioned above.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Quadratic Residuosity Assumption (QRA):</strong></p> </div> <div class="message-body"> <p>Let $N=PQ$ be a product of two large primes.</p><p>No PPT algorithm can <strong>distinguish</strong> <u>between a random element of $QR_N$ from a random element of $QNR_N$</u> <strong>given only</strong> $N$.</p> </div> </article> <h1 id="Goldwasser-Micali-GM-Encryption"><a href="#Goldwasser-Micali-GM-Encryption" class="headerlink" title="Goldwasser-Micali (GM) Encryption"></a>Goldwasser-Micali (GM) Encryption</h1><p><strong>Goldwasser-Micali (GM) Encryption</strong> is under the <strong>Quadratic Residuosity Assumption.</strong></p><h2 id="GM-Scheme"><a href="#GM-Scheme" class="headerlink" title="GM Scheme"></a>GM Scheme</h2><ul><li><p>$Gen(1^n)$:</p><ul><li><p>Generate random $n$-bit prime $p$ and $q$ and let $N=pq$.</p><p>  Let $y\in QNR_N$ be some <strong>quadratic non-residue</strong> with <em>Jacobi symbol</em> $+1$. </p>   <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>How to <b>sample</b> a quadratic non-residue with Jacobi symbol $+1$ ? <ol><li><p>Sample a random number $y$.</p></li><li><p>Check  $\left(\frac{y}{P}\right)\overset?{=} -1$ and $\left(\frac{y}{Q}\right)\overset?{=} -1$</p></li><li><p>Then we can get a quadratic non-residue with $\left(\frac{y}{N}\right)=1$.</p> </div> </article> </li></ol></li><li><p>Let $pk=(N,y)$.</p></li><li><p>Let $sk=(p,q)$.</p></li></ul></li><li><p>$Enc(pk,b)$ where $b$ is <u>a bit:</u></p><ul><li>Generate random $r\in\mathbb{Z}_N^*$.(randomness)</li><li>Output $\begin{cases}r^2\mod N &, \text{if }b =0 \\r^2y\mod N &,\text{if }b=1 \end{cases}$.</li></ul></li><li><p>$Dec(sk,c)$:</p><ul><li>Check if $c\in\mathbb{Z}_N^*$ is a quadratic residue using $p$ and $q$.</li><li>If yes, output $0$ else $1$.</li></ul></li></ul><p>If $b=0$, the encryption is $r^2$, which is <strong>quadratic residue</strong> with <em>Jacobi symbol</em> $+1$.</p>$\left(\frac{r^2}{N}\right)=\left(\frac{r}{N}\right)^2=1$. ( $r\in \mathbb{Z}_N^*$ so $r\ne0$)<p>If $b=1$, the encryption is $r^2y$, which is <strong>quadratic non-residue</strong> with <em>Jacobi symbol</em> $+1$.</p>$\left(\frac{r^2y}{N}\right)=\left(\frac{r^2}{N}\right)\left(\frac{y}{N}\right)=1$.<p>Although you <u>know the Jacobi symbol</u> is $+1$, you <strong>have no idea</strong> that <u>the ciphertext is square or pseudo square.</u></p><p>Hence, <strong>IND-security</strong> follows directly <strong>from the quadratic residuosity assumption.</strong></p><h2 id="GM-is-a-Homomorphic-Encryption"><a href="#GM-is-a-Homomorphic-Encryption" class="headerlink" title="GM is a Homomorphic Encryption"></a>GM is a Homomorphic Encryption</h2><p>Given a GM-ciphertext of $b$ and a GM-ciphertext of $b’$, I can compute a GM-ciphertext of $b+b’\mod 2$ <strong>without knowing anything about $b$ or $b’$.</strong></p><blockquote><p>$Enc(pk,b)$ where $b$ is a bit:<br>Generate random $r\in\mathbb{Z}_N^*$ and output $r^2y^b\mod N$.</p></blockquote><p><font color=blue><u><b>Claim:</b></u></font></p><p>$Enc(pk,b)\cdot Enc(pk,b’)$ is an encryption of $b\oplus b’=b+b’\mod2$.</p><p><font color=blue><u><b>Proof:</b></u></font> </p><ul><li>Consider $c_1=r_1^2y^b$ and $c_2=r_2^2y^{b’}$. </li><li>$c_1\cdot c_2 = r_1^2r_2^2 y^{b+b’}$ . </li><li>$c_1\cdot c_2$  is QR if $b+b’=0\mod 2$. </li></ul><p>The takeaway here is $QR\cdot QR=QR$ and $QNR\cdot QNR=QR$.</p><h1 id="Diffie-Hellman-Key-Exchange"><a href="#Diffie-Hellman-Key-Exchange" class="headerlink" title="Diffie-Hellman Key Exchange"></a>Diffie-Hellman Key Exchange</h1><p>The <strong>main idea</strong> is the <u>commutativity in the exponent</u>, $(g^x)^y=(g^y)^x$, where $g$ is an element of some group.</p><p>So you can compute $g^{xy}$ given either $g^x$ and $y$, or $g^y$ and $x$.</p><p>We elaborated the <strong>Diffie-Hellman Assumption</strong> in <a href="/2022/07/27/mit6875-lec9/" title="Lecture 6">Lecture 6</a>.</p><p><strong>Diffie-Hellman Assumption(DHA):</strong></p><p>Hard to compute $g^{xy}$ given only $g,g^x$and $g^y$.</p><hr><p><strong>Diffie-Hellman Key Exchange:</strong></p><ol><li>Let $p=2q+1$ be a safe prime, and $g$ be the generator of $QR_p$. </li><li>Alice picks a random number $x\in \mathbb{Z}_q$ and sends $g^x\mod p$ to Bob.</li><li>Bob picks a random number $y\in\mathbb{Z}_q$ and sends $g^y\mod p$ to Alice.</li></ol><ul><li>Alice and Bob have the shared key $k=g^{xy}\mod p$<ul><li>Alice computes it by $g^y$ and $x$.</li><li>Bob computes it by $g^x$  and $y$.</li></ul></li></ul><h2 id="El-Gamal-Encryption"><a href="#El-Gamal-Encryption" class="headerlink" title="El Gamal Encryption"></a>El Gamal Encryption</h2><ul><li>$Gen(1^n)$:<ul><li>Generate an $n$-bit safe prime $p=2q+1$ and a generator $g$ of $\mathbb{Z}_p^*$. <br>Let $h=g^2\mod p$ be a <strong>generator</strong> of $QR_p$.<br>Choose a <strong>random</strong> number $x\in \mathbb{Z}_q$.</li><li>Let $pk=(p,h,h^x)$.</li><li>Let $sk=x$.<br>(Finding $sk$ from $pk$ is the <em>discrete logarithm problem</em>.)</li></ul></li><li>$Enc(pk,m)$ where $m\in QR_p$.<ul><li>Generate <strong>random</strong> $y\in\mathbb{Z}_q$. (randomness)</li><li>Output $(h^y,h^{xy}\cdot m)$.</li></ul></li><li>$Dec(sk=x,c)$<ul><li>Compute $h^{xy}$ using $h^y$ and $x$.</li><li>Divide the second component to retrieve $m$.</li></ul></li></ul><p><font color=blue><u><b>Decisional Diffie-Hellman Assumption (DDHA):</b></u></font><br>Hard to distinguish between $g^{xy}$ and a uniformly random group element, given $g,g^x$ and $g^y$.<br>That is the following two distributions are <em>computationally indistinguishable:</em><br>$(g,g^x,g^y,g^{xy})\approx (g,g^x,g^y,u)$ </p><p>From DDH assumption, we know</p><ul><li>It’s <em>hard to distinguish</em> between $(h,h^x,h^y,h^{xy})$ and $(h,h^x,h^y,u)$.</li><li>Moreover, it’s <em>hard to distinguish</em> between  $(h,h^x,h^y,h^{xy})$ and $(h,h^x,h^y,h^{xy}\cdot m)$ since $m$ is random, so is $m\cdot h^{xy}$.</li><li>So it’s <em>hard to distinguish</em> between $(h,h^x,h^y,h^{xy}\cdot m)$ and $(h,h^x,h^y,u)$.</li></ul><p>Hence, DH/El Gamal is <strong>IND-secure</strong> under the <strong>DDH assumption.</strong></p><p>The source of <strong>hardness</strong> is <u>different in RSA and GM</u>. There is no factoring and only DH problem.</p><h2 id="Which-Group-to-Use"><a href="#Which-Group-to-Use" class="headerlink" title="Which Group to Use"></a>Which Group to Use</h2><p><strong>Quadratic Residue Group:</strong></p><p>We used the $QR_P$ group for a safe prime $P=2Q+1$ where $Q$ is prime so far. The order of the group is $Q$. </p><p>But it is <strong>not used</strong> in practice.</p><p>Because <strong>discrete log</strong> can be <em>broken</em> in <strong>sub-exponential</strong> time $2^{\sqrt{\log P\log\log P}}$.</p><p>It’s better than $poly(P)$ but worse than $poly(\log P)$. </p><p>And the $poly(\log P)$ is what we’re referring to $poly(n)$. </p><hr><p><strong>Elliptic Curve Groups:</strong></p><p>In practice, we use <strong>Elliptic Curve Groups</strong> in DH/El Gamal.</p><p>It is the set of solutions $(x,y)$ to the equation $y^2=x^3+ax+b\pmod P$ together with a very cool group addition law.</p><p>The <strong>best known discrete log algorith</strong>m is $\mathcal{O}(\sqrt{P})$ time!</p><p>That says that we can use <strong>much smaller keys</strong>. We can use 160-bit $P$ to suffice “80-bit security”.</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>We have elaborated three constructions of public-key encryption.</p><p><u><b>Constructions of Public-Key Encryption: </b></u> </p><ol><li><em>Trapdoor Permutations (RSA)</em></li><li><em>Quadratic Residuosity/Goldwasser-Micali</em></li><li><em>Diffie-Hellman/El Gamal</em></li><li>Learning with Errors/Regev</li></ol><p>If <strong>factoring</strong> is easy, <u>RSA is broken</u> (and that’s the only known way to break RSA).</p><p>If <strong>factoring</strong> is easy, <u>Goldwasser-Micali is broken</u> conjecturing that finding square roots or recognizing the squares are both as hard as factoring.</p><p>Moreover, the <u>discrete problem</u> is similarly broken to <strong>factoring</strong> in <em>quantum computer.</em></p><p>Hence, the preceding three constructions are dead if the quantum computer comes out.</p><p>Yet <strong>learning with errors</strong> is <em>post-quantum secure</em> as far as we know.</p><p>We will see more when we do homomorphic encryption.</p><h2 id="Practical-Considerations"><a href="#Practical-Considerations" class="headerlink" title="Practical Considerations"></a>Practical Considerations</h2><p>There are some practical considerations.</p><ul><li><p>How do I know the public key?</p><ul><li><strong>Public-key Infrastructure</strong>: a directory of identities together with their public keys.</li><li>But it needs to be “<strong>authenticated</strong>”.<br>Otherwise Eve could replace Bob’s $pk$ with her own.</li></ul></li><li><p>Public-key encryption is <strong>orders of magnitude slower</strong> than secret-key encryption.</p><ol><li><p>We mostly showed how to <u>encrypt bit-by-bit</u>! Super-duper <strong>inefficient</strong>.</p></li><li><p><u>Exponentiation</u> takes $O(n^2)$ time as opposed to typically <u>linear time</u> for secret key encryption (AES).</p></li><li><p>The $n$ <u>itself is large</u> for PKE (RSA: $n\ge 2048$) compared to SKE (AES: $n=128$).<br>(For Elliptic Curve El-Gamal, it’s $320$ bits.</p><p>We can solve problem 1 and minimize problems 2&amp;3 using <strong>hybrid encryption.</strong></p></li></ol></li><li><p><strong>Hybrid Encryption</strong></p><ul><li>To encrypt a long message $m$ (think 1GB)<ol><li>Pick a random key $K$ (think 128 bits) for a secret-key encryption.</li><li>Encrypt $K$ with the $PKE$: $PKE.Enc(pk,K)$. </li><li>Encrypt $m$ with the $SKE$: $SKE.Enc(K,m)$.</li></ol></li><li>To decrypt:<ol><li>Recover $K$ using $sk$.</li><li>Then using $K$, recover $m$.</li></ol></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;


&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quadratic Residue and Quadratic Residuosity Assumption&lt;/li&gt;
&lt;li&gt;Goldwasser-Micali Encryption and Homomorphism&lt;/li&gt;
&lt;li&gt;Diffie-Hellman Key Exchange&lt;/li&gt;
&lt;li&gt;El Gamal Encryption&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="Quadratic Residue" scheme="https://f7ed.com/tags/Quadratic-Residue/"/>
    
      <category term="QRA" scheme="https://f7ed.com/tags/QRA/"/>
    
      <category term="GM Encryption" scheme="https://f7ed.com/tags/GM-Encryption/"/>
    
      <category term="DH" scheme="https://f7ed.com/tags/DH/"/>
    
      <category term="El Gamla" scheme="https://f7ed.com/tags/El-Gamla/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 8</title>
    <link href="https://f7ed.com/2022/07/23/mit6875-lec8/"/>
    <id>https://f7ed.com/2022/07/23/mit6875-lec8/</id>
    <published>2022-07-22T16:00:00.000Z</published>
    <updated>2022-08-12T04:33:27.686Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Public-key Encryption and Key exchange.</li><li>Definitions: IND-Security (IND-CPA)</li><li>Trapdoor permutations.</li></ul><span id="more"></span><h1 id="Key-Agreement-Problem"><a href="#Key-Agreement-Problem" class="headerlink" title="Key Agreement Problem"></a>Key Agreement Problem</h1><p>There is a remaining problem in secret-key encryption (or symmetric encryption), the <strong>Key Agreement Problem.</strong></p><p>How did Alice and Bob get the same $sk$ to begin with ?</p><img src="https://s1.ax1x.com/2022/07/23/jXf2Ax.png" alt="secret-key encryption" style="zoom:33%;" /><p>Can Alice and Bob, who never previously met, <u>exchange messages securely ?</u></p><h2 id="Merkle’s-1974"><a href="#Merkle’s-1974" class="headerlink" title="Merkle’s [1974]"></a>Merkle’s [1974]</h2><p>The Merkle’s idea is based on <u>one-way function.</u></p><p>Assume that $H:[n^2]\rightarrow [n^2]$ is an <strong>injective</strong> OWF. (or one-to-one).</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <b>Injective Function:</b><p>In mathematics, an <strong>injective function</strong> (also known as <strong>injection</strong>, or <strong>one-to-one function</strong>) is a function $f$ that maps distinct elements to distinct elements.</p><ul><li><p>That is, $f(x_1) = f(x_2)$ implies $x_1 = x_2$.</p></li><li><p>Equivalently, $x_1\ne x_2$ implies $f(x_1) \ne f(x_2)$ in the equivalent contrapositive statement.</p></li></ul> </div> </article> <ol><li>Alice picks $n$ random numbers $x_1,\dots,x_n$</li><li>Bob picks $n$ random numbers $y_1,\dots, y_n$</li></ol><img src="https://s1.ax1x.com/2022/07/23/jXfrjJ.png" alt="Merkle's" style="zoom:33%;" /><ol start="3"><li><p>There is a <u>common number</u> since <strong>birthday paradox.</strong><br>That says $x_i=y_j$ with high probability.</p><img src="https://s1.ax1x.com/2022/07/23/jXfyu9.png" alt="Merkle's" style="zoom:33%;" /></li><li><p>Alice and Bob can <u>detect it in time $\mathcal{O}(n)$,</u> and they set it as their shared key.</p></li></ol><hr><p>How long dose it take Eve, <strong>the adversary</strong> to <u>compute the shared key ?</u></p><p>She <u>knows $i$ and $j$</u> since she can see the ciphertexts in the channel, but <u>she needs to inver the OWF.</u></p><p>Assuming the OWF is very strong, that is $\Omega(n^2)$.</p><p>But the Merkle’s only <u>protects against quadratic-time Eves</u> although it’s still an excellent idea.</p><h1 id="Fascinating-History"><a href="#Fascinating-History" class="headerlink" title="Fascinating History"></a>Fascinating History</h1><p>The Public-key Encryption has a fascinating history.</p><ul><li><strong>Merkle (1974)</strong>:<br>“Secure Communications Over Insecure Channels.”<ul><li>Only protects against <u>quadratic-time adversary.</u></li></ul></li><li><strong>Diffie &amp; Hellman (1976</strong>):<br>“New Direction in Cryptography”<ul><li><strong>Turing Award 2015.</strong></li><li>Marked the birth of public-key cryptography.</li><li>Invented the <u>Diffie-Hellman key exchange.</u><br>(conjectured to be <u>secure against all poly-time attackers</u> unlike Merkle)</li><li>Used to this day (e.g., TLS 1.3) albeit with different groups than what DH had in mind.</li></ul></li><li><strong>Rivest, Shamir &amp; Adleman (1987)</strong>:<br>“A Method for Obtaining Digital Signatures and Public-Key Cryptosystems”<ul><li><strong>Turing Award 2002.</strong></li><li>Invented the <u>RSA trapdoor permutation</u>, <u>public-key encryption</u> and <u>digital signatures.</u></li><li>RSA Signatures used to this day (e.g., TLS 1.3) in essentially the original form it was invented.</li></ul></li><li><strong>Goldwasser &amp; Micali (1982):</strong><br>“Probabilistic Encryption”<ul><li>Turning Award 2012.</li><li>Defined what is now the gold-standard of security of public-key encryption<br>(two equivalent definitions: <u>indistinguishability and semantic security)</u></li><li>GM-encryption: based on the <u>difficulty of the quadratic residuosity problem</u>, the <u>first homomorphic encryption.</u></li></ul></li></ul><h1 id="Public-Key-Encryption"><a href="#Public-Key-Encryption" class="headerlink" title="Public-Key Encryption"></a>Public-Key Encryption</h1><p>It’s also called Asymmetric Encryption.</p><p>The goal is :</p><ul><li><strong>Anyone</strong> can encrypt to Bob</li><li>Bob, and <strong>only Bob</strong> can decrypt.</li></ul><h2 id="Public-key-Encryption-Scheme"><a href="#Public-key-Encryption-Scheme" class="headerlink" title="Public-key Encryption Scheme"></a>Public-key Encryption Scheme</h2><p>The <u>public encryption scheme</u> works as follows:</p><img src="https://s1.ax1x.com/2022/07/23/jXfB3F.png" alt="Public-key Encryption" style="zoom:33%;" /><ol><li>Bob generate a pair of keys, a public key $pk$, and a private (or secret) key $sk$.</li><li>Bob “<strong>publishes</strong>” $pk$ and keeps $sk$ to himself.</li><li>Alice encrypts $m$ to Bob using $pk$.</li><li>Bob decrypts using $sk$.</li></ol><p>Hence, there are a triple of <strong>PPT algorithms</strong> $(Gen, Enc, Dec)$ s.t.</p><ul><li>$Gen(1^n)\rightarrow (pk,sk)$: </li></ul><p><strong>PPT Key generation algorithm</strong> generates a public-private key pair.</p><ul><li>$Enc(pk,m)\rightarrow c$:</li></ul><p><strong>Encryption algorithm</strong> uses the public key to encrypt message $m$.</p><ul><li>$Dec(sk,c)\rightarrow m$:</li></ul><p><strong>Decryption algorithm</strong> uses the private key to decrypt ciphertext $c$.</p><p><strong>Correctness</strong>:<br>For all $pk,sk,m$: $Dec(sk,Enc(pk,m))=m$.</p><h2 id="Define-the-Adversary"><a href="#Define-the-Adversary" class="headerlink" title="Define the Adversary"></a>Define the Adversary</h2><p>But how to define security ?</p><img src="https://s1.ax1x.com/2022/07/23/jXfDc4.png" alt="The Adversary" style="zoom:33%;" /><p>What dose <strong>Eve know ?</strong></p><ul><li>Eve <u>knows Bob’s public key</u> $pk$</li><li>Eve <u>sees polynomially many ciphertext</u>s $c_1,c_2,\dots$ of messages $m_1,m_2,\dots$</li></ul><p>So the <strong>challenge</strong> is that Eve should <strong>not</strong> get <u>any partial information about the set of messages.</u></p><h2 id="IND-Security-IND-CPA"><a href="#IND-Security-IND-CPA" class="headerlink" title="IND-Security (IND-CPA)"></a>IND-Security (IND-CPA)</h2><p>We define the Indistinguishability Security, or Indistinguishability-CPA.</p><blockquote><p>CPA is the abbreviation of <strong>Chosen Plaintext Attack.</strong><br>That is, the adversary has the <strong>power</strong> of <u>obtaining the encryption of arbitrary message of his choice.</u></p></blockquote><p>We give the definition by a game, which is actually the same as the Turning Test.</p><h3 id="Many-Message-Security"><a href="#Many-Message-Security" class="headerlink" title="Many Message Security"></a>Many Message Security</h3><p><strong>Define the Game:</strong></p><ol><li>The <strong>Challenger</strong> generates a pair of key $(pk, sk)$ and publishes the $pk$.</li><li><strong>Eve</strong> sends two vectors of message, $\vec{m_0}$ and $\vec{m_1}$s.t. $|m_0^i|=|m_1^i|$ for all $i$.<br>(A <strong>important restrict</strong> is $|m_0^i|=|m_1^i|$ for all $i$, or there is a length attack.)</li><li>The challenger samples $b$ from ${0,1}$ and <u>encrypts all the messages  in vector</u> $\vec{m_b}$ using $pk$.<br>And send the sequence of the ciphertext to Eve.</li><li>Eve guesses <u>which vector of message is encrypted</u> and output $b’$.</li><li>Eve wins if $b’=b$.</li></ol><img src="https://s1.ax1x.com/2022/07/23/jXf09U.png" alt="IND-Security(many message)" style="zoom:40%;" /> <article class="message is-info"> <div class="message-header"> <p><strong>IND-security Definition (Many-message) (unachievable):</strong></p> </div> <div class="message-body"> <p>The encryption scheme is <strong>IND-secure</strong> if no PPT EVE can win in this game with probability better than $1/2 + \text{negl}(n)$.</p><hr><p>Or written <u>in more traditional (and cumbersome) notation</u> as below.<br>For all PPT pair of algorithms $(M,A)$, there is a negligible function $\mu$ s.t.:[cumbersome:笨重的；不方便的]<br>$$<br>\operatorname{Pr}\left[\begin{array}{c} (pk,sk)\gets Gen(1^n); \ (\vec {m_o},\vec{m_1},state)\gets M(pk) s.t. |m_0^i|=|m_1^i|; \ \vec{c}\gets Enc(pk,\vec{m_b}) :A(state,\vec{c})=b\end{array}\right]  \le  \frac{1}{2} +\mu(n)<br>$$</p> </div> </article> <p>Yet the definition is <strong>unachievable</strong>. There is a simple way to win the game.</p><p>You can construct two vector of message in which one is composed of the same message and the other is composed of the different message.</p><p>Hence, it has to <strong>be randomized.</strong></p><hr><p>In <a href="/2022/06/30/mit6875-lec1/" title="[Lecture 1]">[Lecture 1]</a>, we gave two security definitions in <u>Symmetric-key Encryption</u>, <strong>Shannon’s Perfect Secrecy</strong> and <strong>Perfect Indistinguishability.</strong></p><p>Similarly, <u>in Asymmetric-key Encryption</u>, there is an alternative definition, <strong>Semantic Security</strong>, corresponding to the <strong>Indistinguishability Security.</strong></p><p>The <strong>Semantic Security</strong> is <u>the computational analog</u> of <strong>Shannon’s Shannon’s Perfect Secrecy,</strong> and it turns to be <strong>equivalent</strong> to Indistinguishability Security.</p><p>i.e. Semantic Security = IND-Security. But the proof is more complex.</p><p>We will stick to IND-security as it’s easy to work with.</p><h3 id="One-Message-Security"><a href="#One-Message-Security" class="headerlink" title="One Message Security"></a>One Message Security</h3><p>It’s cumbersome to define with many messages.</p><p>Actually, the definition <u>can be simplified to One Message Security.</u></p><p>The only difference is that Eve <strong>can only see one ciphertext</strong> rather than a sequence of ciphertexts in the game.</p><p><strong>Define the Game:</strong></p><ol><li>The <strong>Challenger</strong> generates a pair of key $(pk, sk)$ and publishes the $pk$.</li><li><strong>Eve</strong> send two <strong>single messages</strong>, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$.</li><li>The challenger sample $b$ from ${0,1}$ and encrypt the message $m_b$ using $pk$.<br>And send the ciphertext to Eve.</li><li>Eve guesses <u>which message is encrypted</u> and output $b’$.</li><li>Eve wins if $b’=b$.</li></ol><img src="https://s1.ax1x.com/2022/07/23/jXf6BR.png" alt="IND-Security (one messagle)" style="zoom:40%;" /><p><strong>One-message IND-security Definition (unachievable):</strong></p><p>The encryption scheme is <strong>single-message-IND-secure</strong> if no PPT EVE can win in this game with probability better than $1/2 + \text{negl}(n)$.</p><p>Similarly, it’s <strong>unachievable</strong>. </p><p>In public-key encryption, Eve knows the $pk$ as well. So she has the <strong>power</strong> of <u>generating any ciphertext for arbitrary message of her choice</u>, which is the meaning of <strong>CPA</strong>.</p><p>When she gets the ciphertext from the Challenger, she can easily distinguish it.</p><p>Because she can generate the ciphertexts of $m_0$ and $m_1$ using $pk$.</p><p>Hence, <strong>only in public-key encryption</strong>, the <u>many message security implies one message security.</u> </p><p>It <strong>dose not work</strong> with secret-key encryption.</p><p><strong>Theorem:</strong></p><p>A <strong>public-key encryption</strong> scheme is IND-secure <strong>iff</strong> it is single-message IND-secure.</p><p>The proof is the simple use of Hybrid Argument.</p><hr><p>We’ll show four constructions in following blogs.</p><ol><li><strong>Trapdoor Permutations (RSA)</strong></li><li>Quadratic Residuosity/Goldwasser-Micali</li><li>Diffie-Hellman/El Gamal</li><li>Learning with Errors/Regev</li></ol><p>In this blog, we introduce the Trapdoor Permutations.</p><h1 id="Trapdoor-Functions"><a href="#Trapdoor-Functions" class="headerlink" title="Trapdoor Functions"></a>Trapdoor Functions</h1><p>We know one-way function (family) is easy to compute but hard to invert.</p><p>But <strong>Trapdoor One-way Function</strong> (family) is <u>easy to invert given a trapdoor.</u></p><p>Besides, it’s <strong>Trapdoor One-way Permutation</strong> when <u>domain = range.</u></p><img src="https://s1.ax1x.com/2022/07/23/jXfcH1.png" alt="Trapdoor OWF" style="zoom:33%;" /><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p><strong>Definition:</strong></p><p>A function (family) $\mathcal{F=\{F_n\}}_{n\in\mathbb{N}}$ where each $\mathcal{F_n}$ is itself a collection of functions $\mathcal{F_n}=\{F_i:\{0,1\}^n\rightarrow \{0,1\}^{m(n)}\}_{i\in I_n}$ is a trapdoor one-way function family if</p><ul><li><p>Easy to <strong>sample</strong> function index with a trapdoor.<br>There is a PPT algorithm $Gen(1^n)$ that outputs <u>a function index</u> $i\in I_n$ together <u>with a trapdoor</u> $t_i$.</p></li><li><p>Easy to <strong>compute</strong> $F_i(x)$ given $i$ and $x$.</p></li><li><p>Easy to <strong>compute an inverse</strong> of $F_i(x)$ given $t_i$.</p></li><li><p>It is <strong>one-way.</strong><br>That is, for every p.p.t. $A$, there is a negligible function $\mu$ s.t.</p>  $$    \operatorname{Pr}\left[\begin{array}{c}(i, t) \leftarrow \operatorname{Gen}\left(1^{n}\right) ; x \leftarrow\{0,1\}^{n} ; y=F_{i}(x) ; \\ A\left(1^{n}, i, y\right)=x^{\prime}: y=F_{i}\left(x^{\prime}\right)\end{array}\right] \leq \mu(n)  $$    </li></ul><h2 id="Public-key-Encryption-Construction"><a href="#Public-key-Encryption-Construction" class="headerlink" title="Public-key Encryption Construction"></a>Public-key Encryption Construction</h2><p>We can construct <strong>IND-Secure Public-key Encryption</strong> from <u>Trapdoor Permutations.</u></p><p>There are three p.p.t. algorithms.</p><p><strong>Not IND-Secure (without randomization):</strong></p><ul><li>$Gen(1^n)$: Sample function index $i$ with a trapdoor $t_i$.<br>The <strong>public key</strong> is $i$ and the <strong>private key</strong> is $t_i$.</li><li>$Enc(pk=i,m)$: Output $c=F_i(m)$ as the ciphertext.</li><li>$Dec(sk=t_i,c)$: Output $F_i^{-1}(c)$ computed using the private key $t_i$.</li></ul><p>However, it is <strong>NOT</strong> IND-secure  since it <u>could reveal partial information</u> about $m$.</p><p>IND-Security is <strong>unachievable</strong> <u>without randomization</u> as we mentioned before in the IND-security definition. </p><p>In last lecture <a href="/2022/07/20/mit6875-lec7/" title="[(Lecture 7)]">[(Lecture 7)]</a>, we introduced <strong>GL Theorem</strong> that <u>every one-way function has a hardcore bit</u> . Besides, we showed <u>one-way permutation implies PRG</u>.</p><p>(In fact, one-way function implies PRG as well.)</p><p>Hence, we can construct a PRG from the trapdoor permutation, which is pseudorandom.</p><p><strong>IND-Secure (with PRG from trapdoor permutations):</strong></p><ul><li>$Gen(1^n)$: Sample function index $i$ with a trapdoor $t_i$.<br>The <strong>public key</strong> is $i$ and the <strong>private key</strong> is $t_i$.</li><li>$Enc(pk=i,m)$ where $m$ is a <u>bit</u>.<ul><li>Pick a <strong>random</strong> $r$.</li><li>Output $c=(F_i(r),HCB(r)\oplus m)$ as the ciphertext.</li></ul></li><li>$Dec(sk=t_i,c)$:<ul><li>Recover $r$ using the private key $t_i$.</li><li>Decrypt $m$ using $HCB(r)$.</li></ul></li></ul><p><u>Notation</u>: If the message is $k$-bit, it has to run the encryption $k$ times, <u>each of which is encrypting one bit.</u></p><p>This public-key encryption is <strong>IND-secure.</strong></p><p>It looks familiar with the stateless secret-key encryption with PRF.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> “Stateless Secret-key Encryption with PRF:” in Lecture 4<ul><li><p>$Gen(1^n)$: Generate a random $n$-bit key $k$ that defines  $f_k:\{0,1\}^l\rightarrow \{0,1\}^m$. <br>The domain size of $f_k$, $2^l$, had better be super-polynomially large in $n$.</p></li><li><p>$Enc(k,m)$: Pick a random $x$ and let the ciphertext $c$ be the pair $(x,y=f_k(x)\oplus m)$.<br>It’s a polynomial time to evaluate $f_k(x)$ since $f_k$ are random accessible.</p></li><li><p>$Dec(k,c=(x,y))$: Output $f_k(x)\oplus y$.</p></li></ul> </div> </article> <p>The proof is by hybrid argument. </p><p>(It is also familiar with the proof of secret-key encryption using PRF in Lecture 4.)</p><p><strong>Proof:</strong></p><article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> The following proof is my <strong>own deduction</strong> since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article><ul><li>From the <u>(One-message) IND-Security definition,</u><br>we <strong>want to prove</strong> the following two ciphertexts are <u>indistinguishable</u>.<ul><li>ciphertext of $m_0$: $c=(F_i(r),HCB(r)\oplus m_0)$</li><li>ciphertext of $m_1$: $c=(F_i(r),HCB(r)\oplus m_1)$</li></ul></li><li>We <strong>define</strong> a <u>sequence of hybrid distributions</u> by changing the ciphertext a little bit.<br>Consider the ciphertext as the distribution.<br>Direction: Hybrid 0 → Hybrid 3 ← Hybrid6<ul><li><strong>Hybrid 0</strong>: $D$ gets the ciphertext of $m_0$<br>$c=(F_i(r),HCB(r)\oplus m_0)$</li><li>Hybrid 1: replace with <u>the hardcore bit</u><br>$c=(F_i(r),HCB(r))$</li><li>Hybrid 2: replace with <u>a random bit</u> $r_b$<br>$c=(F_i(r),r_b)$</li><li>Hybrid 3: replace with <u>a random</u> $r_x$ s.t. $|r_x|=|F_i(r)|$<br>$c=(r_x,r_b)$</li><li>Hybrid 4: replace with a random bit $r_b$ (like Hybrid 2)<br>$c=(F_i(r),r_b)$</li><li>Hybrid 5: replace with the hardcore bit (like Hybrid 1)<br>$c=(F_i(r),HCB(r))$</li><li><strong>Hybrid 6</strong>:  $D$ gets the ciphertext of $m_1$<br>$c=(F_i(r),HCB(r)\oplus m_1)$</li></ul></li><li>The thing we <strong>want to prove</strong> is that <u>Hybrid 0 and Hybrid 6 are indistinguishable.</u></li><li>Prove <strong>Hybrid 0 = Hybrid 1</strong> (and Hybrid 6 = Hybrid 5) by <u>birthday paradox.</u><br>The probability of $D$ distinguishing from Hybrid 0 and Hybrid 1 is up to the collision probability of $r$.</li><li>Prove <strong>Hybrid 1 = Hybrid 2</strong> (and Hybrid 5 = Hybrid 4) <u>by HCB security.</u><br>The probability of $D$ distinguishing from Hybrid 1 and Hybrid 2 is determined by the <u>advantage of computing</u> the $HCB(r)$ from $F_i(r)$, which is negligible.</li><li>Prove <strong>Hybrid 2 = Hybrid 3</strong> ( and Hybrid 4 = Hybrid 3) by <u>birthday paradox.</u><br>The probability of $D$ distinguishing from Hybrid 0 and Hybrid 1 is up to the collision probability of $r$.</li><li>QED.</li></ul><h2 id="Trapdoor-Permutation-Candidates"><a href="#Trapdoor-Permutation-Candidates" class="headerlink" title="Trapdoor Permutation Candidates"></a>Trapdoor Permutation Candidates</h2><p>Trapdoor Permutations are <u>exceedingly rare.</u></p><p>There are two candidates. (both need factoring to be hard)</p><ul><li>The RSA (Rivest-Shamir-Adleman) Function.</li><li>The Rabin/Blum-Williams Function</li></ul><p>This blog only show the RSA Trapdoor Permutation.</p><h3 id="RSA-Trapdoor-Permutation"><a href="#RSA-Trapdoor-Permutation" class="headerlink" title="RSA Trapdoor Permutation"></a>RSA Trapdoor Permutation</h3><p>Let’s review some number theory from Lecture 6.</p><p>Let $N=pq$ be a product of two large primes.</p><p><strong>Facts:</strong>  $\mathbb{Z}_N^* =\{a\in \mathbb{Z}_N^*:\operatorname{gcd}(a,N)=1\}$ is a group. </p><ul><li>group operation is multiplication mod $N$.</li><li>inverses exist and are easy to compute.</li><li>the order of the group is $\phi(N)=(p-1)(q-1)$</li></ul><p>Let $e$ be an integer with $\operatorname{gcd}(e,\phi(N))=1$. </p><p>Then, the map $F_{N,e}(x)=x^e\mod N$ is a trapdoor permutation.</p><p>The <strong>key fact</strong> is given $d$ such that $ed=1 \mod \phi(N)$, then it is easy to compute $x$ given $x^e$.</p><hr><p>This gives us the <strong>RSA trapdoor permutation collection</strong>  $\{F_{N,e}:\operatorname{gcd}(e,N)=1\}$. </p><ul><li><u>Function index</u> is $(N,e)$</li><li><u>Trapdoor for inversion</u> is $d=e^{-1} \mod \phi(N)$</li></ul><p>The hardness of inversion without trapdoor = <strong>RSA assumption:</strong> </p><p>Given $N,e$ (as above) and $x^e\mod N$, hard to compute $x$.</p>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;



&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Public-key Encryption and Key exchange.&lt;/li&gt;
&lt;li&gt;Definitions: IND-Security (IND-CPA)&lt;/li&gt;
&lt;li&gt;Trapdoor permutations.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="Public-key Encryption" scheme="https://f7ed.com/tags/Public-key-Encryption/"/>
    
      <category term="IND-Secure" scheme="https://f7ed.com/tags/IND-Secure/"/>
    
      <category term="IND-CPA" scheme="https://f7ed.com/tags/IND-CPA/"/>
    
      <category term="Trapdoor Permutations" scheme="https://f7ed.com/tags/Trapdoor-Permutations/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 7</title>
    <link href="https://f7ed.com/2022/07/20/mit6875-lec7/"/>
    <id>https://f7ed.com/2022/07/20/mit6875-lec7/</id>
    <published>2022-07-19T16:00:00.000Z</published>
    <updated>2022-08-12T04:33:02.861Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article><p>It is indeed possible for $F(x)$ to leak a lot of information about $x$ even if $F$ is one-way.</p><p>The <strong>hardcore predicate</strong> $B(x)$ represent the specific piece of information about $x$ which is hard to compute given $F(x)$.</p><p><font color=blue><u><b>Topics Covered: </b></u></font> </p><ul><li>Definition of one-way functions (OWF)</li><li>Definition of hardcore bit/predicate (HCB)</li><li>One-way permutations → PRG.<br>(In fact, one-way functions → PRG, but that’s a much harder theorem.)</li><li>Goldreich-Levin Theorem: every OWF has a HCB.<br>(Proof for an important special case.)</li></ul><span id="more"></span><h1 id="One-way-Functions"><a href="#One-way-Functions" class="headerlink" title="One-way Functions"></a>One-way Functions</h1><p>Informally, one-way function is easy to compute and hard to invert.</p><img src="https://s1.ax1x.com/2022/07/23/jXsCHU.png" alt="OWF" style="zoom:43%;" /><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>In last blog, we introduced briefly the definition of One-way functions.</p><p><font color=blue><u><b>Take 1 (Not a useful definition): </b></u></font> </p><p>A function (family) $\{F_n\}_{n\in \mathbb{N}}$ where $F_n:\{0,1\}^n\rightarrow \{0,1\}^{m(n)}$  <strong>is one-way</strong> if for every <strong>p.p.t.</strong> adversary $A$, there is a <strong>negligible</strong> function $\mu$ s.t.</p>$$\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=F_n(x):A(1^n,y)=x]\le \mu(n)$$<p>Consider $F_n(x)=0$ for all $x$. </p><p>This is one-way according to the above definition. But it’s impossible to find <strong>the inverse</strong> even if $A$ <u>has unbounded time.</u></p><p>The probability of guessing the inverse of $0$ is negligible since it is essentially random. But $f(x)=0$ is <strong>not</strong> one-way function obviously.</p><p>Hence, it’s not a useful or meaningful definition.</p><p>The <strong>right definition</strong> should be that <u>it is impossible to find an inverse</u> in p.p.t. rather than the exactly chosen $x$.</p> <article class="message is-info"> <div class="message-header"> <p><strong>One-way Functions Definition:</strong></p> </div> <div class="message-body"> <p>A function (family) $\{F_n\}_{n\in \mathbb{N}}$ where $F_n:\{0,1\}^n\rightarrow \{0,1\}^{m(n)}$  <strong>is one-way</strong> if for every <strong>p.p.t.</strong> adversary $A$, there is a <strong>negligible</strong> function $\mu$ s.t.</p>$$\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=F_n(x):A(1^n,y)=\color{red}{x':y=F_n(x')}]\le \mu(n)$$ </div> </article> <p>I’m not going to ask the adversary to come up with $x$ itself which may be impossible. </p><p>Instead, I’m just going to ask the <strong>adversary</strong> to <u>come up with an $x’$ such that $y$ is equal to $F(x’)$.</u> It’s sort of the pre-image of $y$.</p><p>Hence, the adversary can always find an inverse <u>with unbounded time.</u></p><p>But it should <strong>be hard</strong> <u>with probabilistic polynomial time.</u></p><hr><p>Moreover, <strong>One-way Permutations</strong> are <u>one-to-one</u> one-way functions with $m(n)=n$.</p><h1 id="Hardcore-Bits"><a href="#Hardcore-Bits" class="headerlink" title="Hardcore Bits"></a>Hardcore Bits</h1><p>If $F$ is a one-way function, it’s <strong>hard to compute a pre-image</strong> of $F(x)$ for a randomly chosen $x$.</p><p>How about computing <strong>partial information</strong> about an inverse ?</p><p>We saw in last blog that we can tell efficiently if $h$ is quadratic residue.</p><p>So for the discrete log function $x=\operatorname{dlog}_g(h)$, <u>computing the least significant bit(LSB)</u> of $x$ is <strong>easy</strong>. </p><p>But <u>MSB</u> turns out to be <strong>hard</strong>.</p><p>Moreover, there are <strong>one-way functions</strong> for which it is <u>easy to compute the first half of the bits</u> of the inverse.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> <strong>One-way function family easy to compute the first half of the bits:</strong><p>There is an obvious fact that if $f(x)$ is one-way, then $f’(r||x)=r||f(x)$ is one-way. ($|r|=|f(x)|$)<br>It’s hard to compute the pre-image of $f’(r||x)$ because if you can break $f’$ then you can break $f$.<br>But it’s easy to compute the first half of the bits since they are written in the output.</p> </div> </article> <p>Nevertheless, there has to <strong>be a hardcore set of hard</strong> to invert inputs.</p><p>Concretely, dose there necessarily <strong>exist some bit</strong> of $x$ that is hard to compute ?</p><p>Particularly, “<u>hard to compute</u>” means “<u>hard to guess with probability non-negligibly better than 1/2</u>” since any bit can be guessed correctly with probability 1/2.</p><p>So dose there necessarily <strong>exist some bit</strong> of $x$ that is hard to guess with probability non-negligibly better than 1/2 ?</p><h2 id="Hardcore-Bit-Def"><a href="#Hardcore-Bit-Def" class="headerlink" title="Hardcore Bit Def"></a>Hardcore Bit Def</h2><p><font color=blue><u><b>Hardcore Bit Definition (Take 1):</b></u></font> </p><p>For any function (family) $F:\{0,1\}^n\rightarrow \{0,1\}^m$ , a bit $i=i(n)$ is <strong>hardcore</strong> if for every <strong>p.p.t.</strong> adversary $A$, there is a <strong>negligible</strong> function $\mu$ s.t.</p>$$\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=F(x):A(y)=x_i]\le 1/2 +\mu(n)$$<p>The definition says that it is <u>hard to guess the $i$-th bit</u> of the inverse given the $y$.</p><p>I mentioned above that there are <strong>one-way functions</strong> for which it is easy to compute the first half of the bits of the inverse.</p><p>Moreover, there are <strong>functions that are one-way</strong>, yet <u>every bit is somewhat easy to predict</u> with probability $\frac{1}{2}+1/n$.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> This is actually an (hard) exercise in the lecture.<p>Sadly, I haven’t figured out the construction that every bit is easy to compute w.p.  $\frac{1}{2}+1/n$.<br>Hope to change the ideas with you.</p> </div> </article> <p>Although the entire inverse of $f(x)$ is hard to compute, it is indeed possible for $f(x)$ to <u>leak a lot of information</u> about $x$ even if $f$ is <u>one-way.</u></p><h2 id="Hardcore-Predicate-Def"><a href="#Hardcore-Predicate-Def" class="headerlink" title="Hardcore Predicate Def"></a>Hardcore Predicate Def</h2><p>So, we <strong>generalize</strong> the notion of <u>a hardcore “bit”.</u></p><p>We define <strong>hardcore predicate</strong> to <u>identify a specific piece of information about</u> $x$  that is “hidden” by $f(x)$.</p><p>Informally, a <strong>hardcore predicate</strong> of a one-way function $f(x)$ is <u>hard to compute given $f(x)$.</u></p> <article class="message is-info"> <div class="message-header"> <p><strong>Hardcore Predicate Definition:</strong></p> </div> <div class="message-body"> <p>For any function (family) $F:\{0,1\}^n\rightarrow \{0,1\}^m$ , a function $B:\{0,1\}^n\rightarrow \{0,1\}$  is a <strong>hardcore predicate</strong> if for every <strong>p.p.t.</strong> adversary $A$, there is a <strong>negligible</strong> function $\mu$ s.t.</p>$$\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=F(x):A(y)=B(x)]\le 1/2 +\mu(n)$$ </div> </article> <p>The definition says that <strong>a hardcore predicate</strong> $B(x)$ <u>of a one-way function</u> $f(x)$ is <strong>hard to compute</strong> with probability non-negligibly better than $1/2$ <strong>given</strong> $f(x)$ since the predicate $B(x)$ is a boolean function that can be computed with probability $1/2$.</p><p>Besides, this is perfectly <strong>consistent with the fact</strong> that the <u>entire inverse is actually hard to compute.</u> Otherwise, you can compute $B(x)$.</p><hr><p>Henceforth, for us, <strong>a hardcore bit will mean a hardcore predicate.</strong></p><p>We can represent the definition by the following picture.</p><img src="https://s1.ax1x.com/2022/07/23/jXsiEF.png" alt="hard to comput HCB from F" style="zoom:43%;" /><p>It’s <strong>easy</strong> to compute the one-way function $F(x)$ <strong>given</strong> $x$.</p><p>It’s <strong>easy</strong> to compute the hardcore predicate $B(x)$ of $F(x)$  <strong>given</strong> $x$.</p><p>But it’s <strong>hard</strong> to compute $B(x)$ <strong>given</strong> $F(x)$.</p><p>We know that it is <strong>indeed possible</strong> for $F(x)$ to <u>leak a lot of information about</u> $x$ even if $F$ is one-way.</p><p>Hence, the hardcore predicate $B(x)$ <u>represent the specific piece of information about</u> $x$ which is <strong>hard to compute given</strong> $F(x)$.</p><h1 id="One-way-Permutations-→-PRG"><a href="#One-way-Permutations-→-PRG" class="headerlink" title="One-way Permutations → PRG"></a>One-way Permutations → PRG</h1><p>In this section, we show that <u>One-way Permutations imply PRG.</u></p><p>The construction is as follows:</p><p><font color=blue><u><b>PRG Construction from OWP:</b></u></font> </p><p>Let $F$ be a one-way permutation, and $B$ an associated hardcore predicate for $F$.</p><p>Then, define $G(x)=F(x)||B(x)$.</p><p><u>Note</u>: $G$ stretches by one bit. We can turn this into a $G’$ that stretches to any poly. number of bits from <strong>PRG Length Extension.</strong></p><p><font color=blue><u><b>Theorem: </b></u></font> </p><p>$G$ is a PRG assuming $F$ is a one-way permutation.</p><p><font color=blue><u><b>Proof (using NBU): </b></u></font> </p><p>The thing <strong>we want to prove</strong> is that if $F$ is a <u>one-way permutation</u> and $B$ is <u>the hardcore predicate for</u> $F$, then $G(x)=F(x)||B(x)$ is a PRG.</p><p>We prove it using <strong>next-bit unpredictability.</strong></p><ul><li><p>Assume for <strong>contradiction</strong> that $G$ is <u>not a PRG.</u></p><p>  Therefore, there is <strong>a next-bit predictor</strong> $D$, and index $i$, and a polynomial function $p$ such that</p>   $\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=G(x):D(y_1\dots y_{i-1})=y_i]\ge \frac{1}{2}+1/p(n)$ </li><li><p>If we <u>want to get the contradiction to</u> $B(x)$, the <strong>index</strong> $i$ has to be $n+1$.<br>(Because the $G(x)=F(x)||B(x)$ where $F(x)$  is $n$-bit and $B(x)$ is $1$-bit.)</p> $\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=G(x):D(y_1\dots y_{n})=y_{n+1}]\ge \frac{1}{2}+1/p(n)$ </li><li><p>Then we can construct <u>a hardcore bit</u> (predicate) <strong>predictor</strong> from $D$.<br>In fact, $D$ is <strong>a hardcore predictor.</strong></p> $\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=G(x):D(F(x))=B(x)]\ge \frac{1}{2}+1/p(n)$ </li><li><p>QED.</p></li></ul><p><font color=blue><u><b>Proof (using Indistinguishability):</b></u></font> </p><p>We can also prove it <u>using indistinguishability.</u></p><ul><li><p>Assume for <strong>contradiction</strong> that $G$ is not a PRG.<br>Therefore, there is a <strong>p.p.t. distinguisher</strong> $D$, and a <strong>polynomial</strong> function $p$ such that</p>  $$  \begin{aligned}\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=G(x):D(y)=1]  &-  \\ \operatorname{Pr}[y\leftarrow \{0,1\}^{n+1}:D(y)] &\ge  1/p(n)\end{aligned}  $$  </li><li><p>We <strong>construct a hardcore predictor</strong> $A$ and show:</p>  $$    \begin{aligned}\operatorname{Pr}[x\leftarrow \{0,1\}^n:A(F(x))=B(x)]\ge \frac{1}{2}+1/p'(n)\end{aligned}  $$    </li><li><p>What <strong>information</strong> can we <u>learn from the distinguisher</u> $D$ ?</p><ul><li><p>Rewrite $y$ in the first term by <u>definition</u>.</p>      $$        \begin{aligned}\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=\color{blue}{F(x)||B(x)}:D(y)=1] &- \\\operatorname{Pr}[y\leftarrow \{0,1\}^{n+1}:D(y)=1]&\ge 1/p(n)\end{aligned}      $$        </li><li><p>Rewrite the second term <u>by a syntactic change.</u> </p>      $$        \begin{aligned}\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=F(x)||B(x):D(y)=1]&-\\ \operatorname{Pr}[{\color{blue}{y_0\leftarrow \{0,1\}^{n},y_1\leftarrow \{0,1\}},y=y_0||y_1}:D(y)=1] &\ge 1/p(n)\end{aligned}      $$        </li><li><p>Rewire the second term since $F$ is <u>one-way permutation.</u></p>      $$    \begin{aligned}\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=F(x)||B(x):D(y)=1]&-\\ \operatorname{Pr}[{\color{blue}{x\leftarrow \{0,1\}^{n},y_1\leftarrow \{0,1\},y=F(x)||y_1}}:D(y)=1] &\ge 1/p(n)\end{aligned}    $$    ​</li><li><p>Rewrite the second term since $\operatorname{Pr}[y_1=0]=\operatorname{Pr}[y_1=0]=1/2$.</p>      $$        \begin{aligned}\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=F(x)||B(x):D(y)=1] &-\\ \frac{\operatorname{Pr}[{x\leftarrow \{0,1\}^{n},\color{blue}{y=F(x)||0}}:D(y)=1]+\operatorname{Pr}[{x\leftarrow \{0,1\}^{n},\color{blue}{y=F(x)||1}}:D(y)=1]}{2} &\ge 1/p(n)\end{aligned}      $$        </li><li><p>Rewrite the second term using $B(x)$.</p>      $$        \begin{aligned}\operatorname{Pr}[x\leftarrow \{0,1\}^n;y=F(x)||B(x):D(y)=1] &-\\ \frac{\operatorname{Pr}[{x\leftarrow \{0,1\}^{n},\color{blue}{y=F(x)||B(x)}}:D(y)=1]+\operatorname{Pr}[{x\leftarrow \{0,1\}^{n},\color{blue}{y=F(x)||\overline{B(x)}}}:D(y)=1]}{2} &\ge 1/p(n)\end{aligned}      $$        </li><li><p>Putting thins together.</p>          $$        \begin{aligned}\frac{1}{2}(\operatorname{Pr}[{x\leftarrow \{0,1\}^{n},\color{blue}{y=F(x)||B(x)}}:D(y)=1] &- \\ \operatorname{Pr}[{x\leftarrow \{0,1\}^{n},\color{blue}{y=F(x)||\overline{B(x)}}}:D(y)=1]) &\ge 1/p(n)\end{aligned}        $$        </li></ul></li></ul> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> Although the mathematical transformations seem complex, the main idea is to <u>turn what we know to what we want.</u> <p>We want to know some information about $B(x)$. </p> </div> </article> <ul><li>The <strong>takeaway</strong> is that $D$ says 1 more often <u>when fed with the “right bit”</u> than the “wrong bit”.</li></ul> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> It’s a familiar statement.<p>In <a href="/2022/07/06/mit6875-lec3/" title="[Lecture 3]">[Lecture 3]</a> , we construct a predictor from a distinguisher to prove the next-bit unpredictability → the indistinguishability.</p><p>In that proof, we got the takeaway from <strong>hybrid argument</strong>, that the distinguisher $D$ says 1 more often when fed with the “right bit” than the “wrong bit”.</p> </div> </article> <ul><li><p>Construct <strong>a hardcore bit predictor</strong> $A$ from the distinguisher $D$ using the takeaway.</p><ul><li><p>The <strong>task</strong> of $A$ is get as input $z=F(x)$ and try to guess the hardcore predicate.</p></li><li><p>The predictor works as follows:</p>          $$        \begin{aligned}A(F(x))=\begin{cases} b,& \text{if }D(F(x)||b)=1\\ \overline{b},& \text{otherwise}\end{cases},b\leftarrow\{0,1\} \end{aligned}        $$        <ol><li>Pick a random bit $b$.</li><li>Feed $D$ with input $z||b$</li><li>If $D$ says “1”, output $b$ as the prediction for the hardcore bit and if $D$ says “0”, output $\overline{b}$.</li></ol></li></ul></li><li><p><strong>Analysis</strong> of the predictor $A$.</p><ul><li>$\operatorname{Pr}[x\leftarrow {0,1}^n:A(F(x))=B(x)]$</li><li>$A$ output $b=B(x)$ or $\overline{b}=B(x)$. $\begin{aligned} =\operatorname{Pr}[x\leftarrow\{0,1\}^n:D(F(x)||b)=1 \color{blue}{\wedge b=B(x)}] + \\ \operatorname{Pr}[x\leftarrow\{0,1\}^n:D(F(x)||b)=0 \color{blue}{\wedge b\ne B(x)}]& \\\end{aligned}$       $\begin{aligned}=\operatorname{Pr}[x\leftarrow\{0,1\}^n:D(F(x)||b)=1 \mid b=B(x)]\operatorname{Pr}[b=B(x)]& + \\ \operatorname{Pr}[x\leftarrow\{0,1\}^n:D(F(x)||b)=0 \mid b\ne B(x)]\operatorname{Pr}[b\ne B(x)]& \end{aligned}$ </li><li>Since $b$ is random.<br>$\begin{aligned}=\color{blue}{\frac{1}{2}}(\operatorname{Pr}[x\leftarrow{0,1}^n:D(F(x)||b)=1 \mid b=B(x)]&amp; + \ \operatorname{Pr}[x\leftarrow{0,1}^n:D(F(x)||b)=0 \mid b\ne B(x)]&amp;) \end{aligned}$</li><li>Put the prior condition in it.$\begin{aligned}=\frac{1}{2}(\operatorname{Pr}[x\leftarrow\{0,1\}^n:D(F(x)||\color{blue}{B(x)})=1]& + \\ \operatorname{Pr}[x\leftarrow\{0,1\}^n:D(F(x)||\color{blue}{\overline{B(x)}})=0 ) \end{aligned}$      $\begin{aligned}=\frac{1}{2}(\operatorname{Pr}[x\leftarrow\{0,1\}^n:D(F(x)||\color{blue}{B(x)})=1]& + \\ 1-\operatorname{Pr}[x\leftarrow\{0,1\}^n:D(F(x)||\color{blue}{\overline{B(x)}})=1 ) \end{aligned}$ </li><li>From the takeaway, we can get<br>$=\frac{1}{2}(1+(*))\ge \frac{1}{2} + 1/p(n)$</li><li>So the $A$ <u>can predict the hardcore bit</u> (predicate) with probability non-negligible better than $1/2$.</li></ul></li></ul><h1 id="Goldreich-Levin-Theorem-every-OWF-has-a-HCB"><a href="#Goldreich-Levin-Theorem-every-OWF-has-a-HCB" class="headerlink" title="Goldreich-Levin Theorem: every OWF has a HCB."></a>Goldreich-Levin Theorem: every OWF has a HCB.</h1><h2 id="A-Universal-Hardcore-Predicate-for-all-OWF"><a href="#A-Universal-Hardcore-Predicate-for-all-OWF" class="headerlink" title="A Universal Hardcore Predicate for all OWF"></a>A Universal Hardcore Predicate for all OWF</h2><p>We have defined the hardcore predicate $B(x)$ for the particular one-way function $F(x)$.</p><p>Let’s shoot for a <strong>universal</strong> hardcore predicate <u>for every one-way function</u>, i.e., a <strong>single</strong> predicate $B$ where it is hard to guess $B(x)$ given $F(x)$.</p><p>Is this possible ? </p><p>Turns out the answer is <strong>“no”.</strong></p><p>Pick a favorite amazing $B$. We can <u>construct a one-way function</u> $F$ which $B$ is <strong>not</strong> hardcore.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>There is a <strong>contradiction</strong> example. <p><u>Claim 1</u>: If $f(x)$ is a one-way function, then $f’(x)=f(x)||B(x)$ is one-way as well.<br><u>Claim 2</u>:<br>But $B(x)$ is <strong>NOT</strong> a hardcore predicate for $f’(x)$.</p><p>For claim 1, if you can compute the inverse of $f’(x)$, then you can compute the inverse of $f(x)$.<br>For claim 2, $B(x)$ is actually written in $f’(x)$.<br>So $f’(x)$ is one-way and dose not leak much information.</p> </div> </article> <h2 id="Goldreich-Levin-GL-Theorem"><a href="#Goldreich-Levin-GL-Theorem" class="headerlink" title="Goldreich-Levin(GL) Theorem"></a>Goldreich-Levin(GL) Theorem</h2><p>But Goldreich-Levin(GL) Theorem changes the statement to <strong>a random hardcore</strong> and tells us <strong>every one-way function has a hardcore predicate/bit.</strong></p> <article class="message is-info"> <div class="message-header"> <p><strong>Goldreich-Levin(GL) Theorem:</strong> </p> </div> <div class="message-body"> <p>Let $\{B_r:\{0,1\}^n\rightarrow \{0,1\}\}$ where</p>$$B_r(x)=\langle r, x \rangle =\sum_{i=1}^n r_ix_i \mod 2$$<p>be a <strong>collection</strong> of predicates (one for each $r$). </p><p>Then a <font color="red">random</font> $B_r$ is hardcore for <font color="red">every</font> one-way function $F$.</p><p>That is, for every one-way function $F$, every p.p.t. $A$, there is a negligible function $\mu$ s.t.</p>$$\operatorname{Pr}[x\leftarrow \{0,1\}^n;r\leftarrow \{0,1\}^n:A(F(x),r)=B_r(x)]\le\frac{1}{2} +\mu(n)$$ </div> </article> <p>It defines <u>a collection of predicates</u> ${B_r:{0,1}^n\rightarrow {0,1}}$ <strong>indexed</strong> by $r$, a $n$-bit vector. So the evaluation of $B_r(x)$ is essentially an <u>inner product</u> of $r$ and $x$ (mod 2).</p><p>The definition says that a <strong>random</strong> $B_r$ is hardcore <strong>for every one-way funciton</strong> $F$.</p><p>So it picks a random $x$ and a <strong>random</strong> $r$.</p><p>For every p.p.t. adversary $A$, it is <strong>hard to compute</strong> $B_r(x)$ given $F(x)$ and $r$.</p><h2 id="Alternative-Interpretations-to-GL-Theorem"><a href="#Alternative-Interpretations-to-GL-Theorem" class="headerlink" title="Alternative Interpretations to GL Theorem"></a>Alternative Interpretations to GL Theorem</h2><p>There are some alternative interpretations to GL Theorem.</p><p><font color=blue><u><b>Alternative Interpretation 1: </b></u></font> </p><p>For every one-way function $F$, there is a <strong>related</strong> one-way function $F’(x,r)=(F(x),r)$ which has a <strong>deterministic</strong> hardcore predicate.</p><hr><p>For every one-way function $F$, you can change it to <u>a related one-way function</u> $F’(x,r)=(F(x),r)$. Although $F’$ leak the second half of bits, $F’$  is one-way.</p><p>Then $F’$has a <strong>deterministic</strong> hardcore predicate $B(x,r)=\langle x,r \rangle\pmod 2$.</p><p>$B(x,r)$ is a <u>fixed function</u>, which performs an inner-product mod 2 between the first half and the second half of the inputs.</p><p>Hence, in this interpretation, GL Theorem says that $B$ is not a hardcore bit for every OWF, but it’s <u>a hardcore bit for the related function</u>, which we created it from the OWF.</p><p>Moreover, if $F(x)$ is one-way permutation, then $F’(x,r)=(F(x),r)$ is one-way permutation.</p><p>And we can get a PRG from $F’(x,r)$. Then $G(x,r)=F(x)||r||\langle x,r\rangle$  where the last bit is the hardcore bit.</p><p><font color=blue><u><b>Alternative Interpretation 2: </b></u></font> </p><p>For every one-way function $F$, there exists (non-uniformly) a (possibly different) hardcore predicate $\langle r_F,x\rangle$.</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> To be honest, I do not comprehend the advanced statement.<p>The professor left a cool open problem: remove the non-uniformity.</p> </div> </article> <h2 id="Proof-of-GL-Theorem"><a href="#Proof-of-GL-Theorem" class="headerlink" title="Proof of GL Theorem"></a>Proof of GL Theorem</h2><p>Assume for <strong>contradiction</strong> there is a <u>predictor</u> $P$ and a polynomial $p$ s.t.</p><p>$\operatorname{Pr}[x\leftarrow {0,1}^n;r\leftarrow {0,1}^n:P(F(x),r)=\langle r,x\rangle]\ge\frac{1}{2} +p(n)$</p><p>We need to show an <u>inverter</u> $A$ for $F$ and a polynomial $p’$  such that</p><p>$\operatorname{Pr}[x\leftarrow {0,1}^n:A(F(x))=x’:F(x’)=F(x)]\ge 1/p’(n)$</p><p>But it is too hard to prove this contradiction.</p><p>Let’s make our lives easier.</p><h3 id="A-Perfect-Predicator"><a href="#A-Perfect-Predicator" class="headerlink" title="A Perfect Predicator"></a>A Perfect Predicator</h3><p>For simplicity, we assume a perfect predicator.</p><p><font color=blue><u><b>Assume a perfect predictor: </b></u></font> </p><ul><li><p>Assume a <strong>perfect predictor</strong> $P$, which <u>can completely predicte the HCB correctly</u>, s.t.</p>      $\operatorname{Pr}[x\leftarrow \{0,1\}^n;r\leftarrow \{0,1\}^n:P(F(x),r)=\langle r,x\rangle]=1$ </li><li><p>Now we can <strong>construct an inverter</strong> $A$ for $F$.</p><p>  The inverter $A$ works as follows:</p><p>  On input $y=F(x)$, $A$ runs the predictor $P$ $n$ times on input $(y, e_1),(y,e_2),\dots,$and $(y,e_n)$ where $e_1=100..0,e_2=010..0,\dots$ are the <u>unit vectors.</u></p></li><li><p>Since $A$ is perfect, it returns $\langle e_i,x\rangle=x_i$, <u>the $i$-th bit</u> of $x$ on the $i$-th invocation.</p></li></ul><h3 id="A-Pretty-Good-Predictor"><a href="#A-Pretty-Good-Predictor" class="headerlink" title="A Pretty Good Predictor"></a>A Pretty Good Predictor</h3><p>Then we assume less.</p><p><font color=blue><u><b>Assume a pretty good predictor:</b></u></font> </p><ul><li>Assume a <strong>pretty good predictor</strong> $P$, s.t. $\operatorname{Pr}[x\leftarrow \{0,1\}^n;r\leftarrow \{0,1\}^n:P(F(x),r)=\langle r,x\rangle]\ge \frac{3}{4} + 1/p(n)$ </li><li>What can we learn from the predictor ?</li></ul><p><font color=blue><u><b><i>Claim: </i></b></u></font><br>For <strong>at least a $1/2p(n)$ fraction</strong> of the $x$,<br>$\operatorname{Pr}[r\leftarrow {0,1}^n:P(F(x),r)=\langle r,x\rangle]\ge \frac{3}{4} + 1/2p(n)$</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i><strong>How to derive the claim by averaging argument ?</strong>  <p>When I wrote this blog, I contemplated it for a long long time.<br>I figured it out with the help of <strong>Wengjie Li</strong>. Thanks!<br>I read the details about the proof of GL Theorem in some textbooks.<br>Some proved <u>the lower bound of fraction</u> $1/2p(n)$, but <u>how about the lower bound of probability</u>, $\frac{3}{4}+1/2p(n)$?<br>In fact, they are related.</p> </div> </article> <p>Before that, let’s introduce <u>averaging argument.</u></p><blockquote><p>In computational complexity theory and cryptography, <strong>averaging argument</strong> is a <u>standard argument for proving theorems.</u> It usually allows us to <u>convert probabilistic polynomial-time algorithms into non-uniform polynomial-size circuits.</u><br>——Wiki</p></blockquote> <article class="message is-info"> <div class="message-header"> <p><strong>Averaging Argument:</strong></p> </div> <div class="message-body"> <p>Let $f$ be some function. The averaging argument is the following claim:</p><p>If we have a circuit $C$ such that $C(x,y)=f(x)$ <u>with probability at least</u> $\rho$ where $x$ is <u>chosen at random</u> and $y$ is <u>chosen independently</u> from some distribution $Y$ over $\{0,1\}^m$ (which might not even be effciently sampleable),</p><p>then there <strong>exists a single string</strong>  $y_0\in \{0,1\}^m$  such that $\operatorname{Pr}_x[C(x,y_0)=f(x)]\ge \rho$. </p><hr><p>Indeed, for every $y$ define $p_y$ to be $\operatorname{Pr}_x[C(x,y)=f(x)]$, then</p>$$\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\mathbb{E}_y[p_y]$$<p>and then this reduces to the <strong>claim</strong> that for <u>every random variable</u> $Z$, if $\mathbb{E}[Z]\ge \rho$ then $\operatorname{Pr}[Z\ge \rho]&gt;0$ <em>(this holds since $\mathbb{E}[Z]$ is the weighted average of Z and clearly if the average of some values is at least $\rho$ then one of the values must be at least $\rho$.)</em> </p> </div> </article> <p>The <strong>claim</strong> tells us if $\operatorname{Pr}_{x,y}[C(x,y)=f(x)]\ge \rho$, then <u>there exist some</u> $y_0$ <strong>such that</strong> $\operatorname{Pr}_x[C(x,y_0)=f(x)]\ge \rho$ as well. </p><p>So we can <strong>single out</strong> these $y_0$ <u>using averaging argument.</u></p><p>Actually, I think the equation, $\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\mathbb{E}_y[p_y]$, is the <u>manifestation of averaging.</u></p><article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> The following analysis is mostly my <strong>own deduction and comprehension</strong> since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article><p><font color=blue><u><b><i>Some analysis to $\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\mathbb{E}_y[p_y]$: </i></b></u></font> </p><ul><li>By definition of $p_y$<ul><li>$y$ is a variable and $p_y$ is a variable.</li><li>By definition, $p_y$ <strong>is up to</strong> $y$.</li><li>So the <u>distribution of $p_y$ is equal to the distribution of  $y$.</u></li></ul></li><li>Expand the left term.<br>$\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\sum _i \operatorname{Pr}_x[C(x,y_i)=f(x)]\operatorname{Pr}[y=y_i]$</li><li>The right term is the <strong>expected value</strong> of variable $p_y$. <ul><li> $\mathbb{E}_y[p_y] = \sum_i p_{y_i} \cdot \operatorname{Pr}[p_y=p_{y_i}]$ </li><li>Because the distribution of $p_y$ is equal to the distribution of  $y$.<br>$\operatorname{Pr}[p_y=p_{y_0}]=\operatorname{Pr}[y={y_0}]$</li><li>So, $\mathbb{E}_y[p_y] = \sum_i p_{y_i} \cdot \operatorname{Pr}[y=y_i]$ </li><li>= the left term.</li></ul></li></ul><hr><p>Now back to the claim.</p><article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> The following is mostly my <strong>own deduction and comprehension</strong> since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article><p><font color=blue><u><b><i>Proof of the Claim : </i></b></u></font> </p><p><font color=blue><u><b><i>What can we learn from the claim? </i></b></u></font> </p><ul><li>What do we <strong>know from the predictor</strong> ?<ul><li>We know $\operatorname{Pr}_{x,r}[P(F(x),r)=\langle r,x\rangle]\ge \frac{3}{4} + 1/p(n)$.</li><li><u>For every</u> $x$, define $p_x=\operatorname{Pr}_r[P(F(x),r)=\langle r,x\rangle]$.</li><li>By <strong>averaging argument,</strong><ol><li>We know <u>there exists some</u> $x$ such that $p_x\ge \frac{3}{4} + 1/p(n)$.</li><li>We know $\operatorname{Pr}_{x,r}[P(F(x),r)=\langle r,x\rangle]=\mathbb{E}[p_x]$ by averaging argument.</li><li>So we know <u>the lower bound of</u>  $\mathbb{E}[p_x]$ (<strong>expected</strong> value of $p_x$) is $\frac{3}{4} + 1/p(n)$.</li></ol></li></ul></li><li>What do we <strong>really want</strong> ?<ul><li>We want to <u>single out these</u> $x$ such that $p_x$ is <u>non-negligibly better than</u> $\frac{3}{4}$ <strong>in (probabilistic) polynomial time.</strong></li><li>So we <strong>want a set of $x$</strong>, which <u>the fraction is polynomial</u> , for every $x$ in the set, the $p_x$ is <u>non-negligibly better than</u> $\frac{3}{4}$.</li></ul></li><li>We show <strong>the fraction and the (lower bound of ) probability are related.</strong></li><li><u>Notation</u><ul><li>$\epsilon$ define <u>the lower bound</u> of $\mathbb{E}[p_x]$, i.e. $\epsilon =\frac{3}{4} + 1/p(n)$.</li><li>Define a <strong>good set of $x$</strong> as $S$ and $s$ <strong>denotes</strong> <u>the poly. fraction of the</u> $x$. ($|S|=s\cdot 2^n$). $S=\{x\mid p_x \ge \varepsilon' \}$  <br>where $\varepsilon’$ <strong>denotes</strong> the <u>lower bound of $p_x$ for every $x\in S$,</u>  non-negligibly better than $\frac{3}{4}$.</li></ul></li><li>Rewrite $\mathbb{E}[p_x]$ using $s$ and $\varepsilon$.<ul><li>$p_x=    \begin{cases}     \varepsilon' \le p_x\le 1,& x\in S \\     0\le  p_x \le \varepsilon',& x\notin S    \end{cases}$ </li><li>Similarly, $p_x$ <strong>is up to</strong> $x$.<br>Hence, <u>the distribution of $p_x$ is equal to the distribution of $x$,</u> i.e.$\operatorname{Pr}[p_x]=\operatorname{Pr}[x]$</li><li>$\mathbb{E}[p_x] = \sum p_x \cdot \operatorname{Pr}[p_x]=\sum p_x \cdot \operatorname{Pr}[x]$<ul><li>$=\sum p_x \cdot \operatorname{Pr}[x\in S] + \sum p_x \cdot \operatorname{Pr}[x\notin S]$</li><li>$=\sum p_x \cdot s + \sum p_x \cdot (1-s)$</li></ul></li><li>We do not know the actual $p_x$ but we <strong>know the boundary</strong> of $p_x$.</li></ul></li><li>Calculate the boundary of $\mathbb{E}[p_x]$ using $s$ and $\varepsilon$.<ul><li>$\mathbb{E}[p_x] =\sum p_x \cdot s + \sum p_x \cdot (1-s)$</li><li>The lower bound is $\varepsilon’ \cdot s + 0\cdot (1-s)=\varepsilon’ \cdot s$ (useless)</li><li>The <u>upper bound</u> is $1\cdot s + \varepsilon’ \cdot (1-s)=s+\varepsilon’ (1-s)$  (<strong>useful</strong>)</li></ul>  <strong>This upper bound should be greater than the known lower bound $\epsilon$.</strong></li><li>We get the relation of $s$ and $\varepsilon’$.<ul><li>$\epsilon \le s+\varepsilon’ (1-s)$</li><li>$\frac{\epsilon-s}{1-s}\le \varepsilon’$  (since $s&lt;1$)</li></ul></li><li>Take $\epsilon$   with $\frac{3}{4} + 1/p(n)$.<ul><li>We want the fraction $s$ to <u>be polynomial.</u></li><li>Suppose the fraction $s=1/2p(n)$.<br>We can get $\varepsilon’ \ge \epsilon -s = \frac{3}{4} +1/2p(n)$. (since $1-s$ is very small).</li><li>We can also suppose the fraction $s=1/3p(n)$ and so on.</li><li>The <strong>point I want to elucidate</strong> here is the <strong>fraction</strong> $s$ and the <strong>lower bound</strong> of $p_x$ for every $x\in S$ <u>can both be polynomial</u> if <u>the advantage of predicting is non-negligible</u>.</li></ul></li><li>Hence, <font color="red"> the <strong>takeaway</strong> is if <u>the probability of predicting  is non-negligibly</u> better than $3/4$, then we <strong>can single out these “good $x$ ”</strong>in  (probabilistic) polynomial time.</font> </li><li>Similarly, <font color="red">if the probability of predicting  is <u>non-negligibly better than</u> $1/2$ (the predictor in general case), we <strong>can also single out these “good</strong> $x$ ” in (probabilistic) polynomial time.</font> </li></ul><hr><p>We get the <strong>takeaway</strong> that if the probability of predicting  is <u>non-negligibly</u> better than $3/4$, then we <u>can single out</u> these “good $x$ ” in (probabilistic) polynomial time.</p><p><font color=blue><u><b><i>Invert the i-th bit: </i></b></u></font> </p><ul><li><p>Now we <strong>can compute the $i$-th bit</strong> of $x$ with the probability better than $1/2$.<br>The key idea is <strong>linearity</strong>.</p><ol><li>Pick a random $r$</li><li>Ask $P$ to tell us $\langle r,x\rangle$ and $\langle r+e_i,x \rangle$.</li><li>Subtract the two answers to get $\langle e_i,x\rangle =x_i$</li></ol></li><li><p><strong>Proof of invert $i$-th bit:</strong></p><ul><li><p>$\operatorname{Pr}[\text{we compute }x_i \text{ correctly}]$</p></li><li><p>$\ge \operatorname{Pr}[P \text{ predicts }\langle r,x \rangle \text{ and }\langle r+e_i,x \rangle \text{ correctly}]$</p></li><li><p>$=1-\operatorname{Pr}[P \text{ predicts }\langle r,x \rangle \text{ or }\langle r+e_i,x \rangle \text{ wrong}]$</p></li><li><p>$\ge1-\operatorname{Pr}[P \text{ predicts }\langle r,x \rangle  \text{ wrong}]\text{ + } \operatorname{Pr}[P \text{ predicts }\langle r+e_i,x \rangle \text{ wrong}]$<br>(by <strong>union bound</strong>)</p></li><li><p>$\ge 1 - 2\cdot (\frac{1}{4} -\frac{1}{2p(n)})=\frac{1}{2}+1/p(n)$</p> <article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i> I think it takes the fraction as $1/2p(n)$ just to make the advantage equal to $1/p(n)$. It’s beautiful. </div> </article> </li></ul></li></ul><p><font color=blue><u><b><i>Invert the entire x: </i></b></u></font> </p><ul><li>Construct the Inverter $A$<ul><li>Repeat for each bit $i\in{1,2,\dots, n}$:<ul><li>Repeat $p(n) \cdot p(n)$ times:<br>(one $p(n)$ is for <u>singling out</u>, and another is <u>for computing correctly</u>)<ol><li>Pick a random $r$</li><li>Ask $P$ to tell us $\langle r,x\rangle$ and $\langle r+e_i,x \rangle$.</li><li>Subtract the two answers to get $\langle e_i,x\rangle =x_i$</li></ol></li><li>Compute the <strong>majority</strong> of all such guesses and set the bit as $x_i$.</li></ul></li><li>Output the concatenation of all $x_i$ as $x$.</li></ul></li><li>Analysis of $A$ is ommited.<br>Hint: Chernoff + Union Bound.</li></ul><h3 id="A-General-Predictor"><a href="#A-General-Predictor" class="headerlink" title="A General Predictor"></a>A General Predictor</h3><p>Let’s proceed to a general predictor, which is in the foremost contradiction.</p><p><font color=blue><u><b>Assume a general good predictor:</b></u></font> </p><p>Assume there is a predictor $P$ and a polynomial $p$ s.t.</p>$\operatorname{Pr}[x\leftarrow \{0,1\}^n;r\leftarrow \{0,1\}^n:P(F(x),r)=\langle r,x\rangle]\ge\frac{1}{2} +p(n)$<p>Likewise, there is a similar claim.</p><p><font color=blue><u><b><i>Claim: </i></b></u></font> </p><p>For <strong>at least a $1/2p(n)$ fraction</strong> of the $x$,</p> $\operatorname{Pr}[r\leftarrow \{0,1\}^n:P(F(x),r)=\langle r,x\rangle]\ge \frac{1}{2} + 1/2p(n)$ <p>The <strong>takeaway</strong> is that <u>probability of predicting  is non-negligibly better than</u> $1/2$ , we <u>can single out these “good</u> $x$ ” in (probabilistic) polynomial time.</p><p>However, we <strong>cannot</strong> invert the $i$-th bit of $x$ with the probability better than $1/2$ using the above method.</p><p><font color=blue><u><b><i>Analysis of Inverting $x_i$: </i></b></u>(error doubling)</font> </p><p>(For at least $1/2p(n)$ fraction of the $x$)</p><ul><li><p>If the probability of predicting  $\langle r,x\rangle$  correctly is non-negligibly <strong>better than</strong> $3/4 + 1/2p(n)$, i.e.</p> $\operatorname{Pr}[r\leftarrow \{0,1\}^n:P(F(x),r)=\langle r,x\rangle]\ge \frac{3}{4} + 1/2p(n)$ <ul><li><p>$\operatorname{Pr}[\text{we compute }x_i \text{ correctly}]$</p></li><li><p>$\ge \operatorname{Pr}[P \text{ predicts }\langle r,x \rangle \text{ and }\langle r+e_i,x \rangle \text{ correctly}]$</p></li><li><p>$=1-\operatorname{Pr}[P \text{ predicts }\langle r,x \rangle \text{ or }\langle r+e_i,x \rangle \text{ wrong}]$</p></li><li><p>$\ge1-\operatorname{Pr}[P \text{ predicts }\langle r,x \rangle  \text{ wrong}]\text{ + } \operatorname{Pr}[P \text{ predicts }\langle r+e_i,x \rangle \text{ wrong}]$</p></li><li><p>$\ge 1 -  \color {blue}{2\cdot(\frac{1}{4} -\frac{1}{2p(n)})}=\frac{1}{2}+1/p(n)$</p><p>So it <strong>can compute</strong> $x_i$ with advantage $1/p(n)$.</p></li></ul></li><li><p>If the probability of predicting  $\langle r,x\rangle$  correctly is <strong>exactly</strong> $3/4$, i.e.</p>$\operatorname{Pr}[r\leftarrow \{0,1\}^n:P(F(x),r)=\langle r,x\rangle]\ge \frac{3}{4}$ <ul><li><p>$\operatorname{Pr}[\text{we compute }x_i \text{ correctly}]$</p></li><li><p>$\ge 1 - \color{blue} {2\cdot(\frac{1}{4})}=1/2$ </p><p>It is <strong>unlikely</strong> to compute $x_i$ since it’s the same with random. </p></li></ul></li><li><p>If the probability of predicting  $\langle r,x\rangle$  correctly is non-negligibly <strong>better than</strong> $1/2 + 1/2p(n)$, i.e.<br>$\operatorname{Pr}[r\leftarrow {0,1}^n:P(F(x),r)=\langle r,x\rangle]\ge \frac{1}{2} + 1/2p(n)$</p><ul><li><p>$\operatorname{Pr}[\text{we compute }x_i \text{ correctly}]$</p></li><li><p>$\ge 1 - \color{blue} {2\cdot(\frac{1}{2} -\frac{1}{2p(n)})}=1/p(n)$</p><p>It cannot <strong>compute</strong> $x_i$.</p></li></ul></li></ul><hr><p>The problem above is that it <strong>doubles</strong> <u>the original error probability of predicting</u>,  i.e. $1-2\cdot(*)$.</p><p>When the probability of error is <strong>significantly smaller than</strong> $1/4$, the “error-doubling” phenomenon raises <u>no problem.</u></p><p>However, <strong>in general case</strong> (and even in special case where the error probability is <strong>exactly</strong> $1/4$), the procedure <u>is unlikely to compute</u> $x_i$.</p><p>Hence, <strong>what is required</strong> is an alternative way of using the predictor $P$, which <strong>dose not</strong> double the original error probability of $P$.</p><p>The complete proof is referred in <a href="http://www.wisdom.weizmann.ac.il/~oded/PSBookFrag/part2N.ps">Goldreich Book Part 1, Section 2.5.2.</a></p><p>The <strong>key idea</strong> is <u>pairwise independence.</u></p><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>I read the reference. To be honest, I do not comprehend it thoroughly.<br>Happy to exchange the ideas.</div> </article><h2 id="The-Coding-Theoretic-View-of-GL"><a href="#The-Coding-Theoretic-View-of-GL" class="headerlink" title="The Coding-Theoretic View of GL"></a>The Coding-Theoretic View of GL</h2><article class="message message-immersive is-primary"> <div class="message-body"> <i class="fas fa-info-circle mr-2"></i>To be honest, I just write it down so I could figure it out in the near future.<br>Happy to exchange the ideas.</div> </article><ul><li><p>$x\rightarrow (\langle x,r\rangle )_{r\in{0,1}^n}$ can be viewed as a highly redundant, exponentially long encoding of $x$ = <strong>the Hadamard code.</strong></p></li><li><p>$P(F(x),r)$ can be thought of as providing access to a <strong>noisy</strong> codeword.</p></li><li><p>What we proved = <strong>unique decoding</strong> algorithm for Hadamard code with error rate $\frac{1}{4}-1/p(n)$.</p></li><li><p>The real proof = <strong>list-decoding</strong> algorithm for Hadamard code with error rate rate $\frac{1}{2}-1/p(n)$.</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

&lt;p&gt;It is indeed possible for $F(x)$ to leak a lot of information about $x$ even if $F$ is one-way.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;hardcore predicate&lt;/strong&gt; $B(x)$ represent the specific piece of information about $x$ which is hard to compute given $F(x)$.&lt;/p&gt;
&lt;p&gt;&lt;font color=blue&gt;&lt;u&gt;&lt;b&gt;Topics Covered: &lt;/b&gt;&lt;/u&gt;&lt;/font&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Definition of one-way functions (OWF)&lt;/li&gt;
&lt;li&gt;Definition of hardcore bit/predicate (HCB)&lt;/li&gt;
&lt;li&gt;One-way permutations → PRG.&lt;br&gt;(In fact, one-way functions → PRG, but that’s a much harder theorem.)&lt;/li&gt;
&lt;li&gt;Goldreich-Levin Theorem: every OWF has a HCB.&lt;br&gt;(Proof for an important special case.)&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="OWF" scheme="https://f7ed.com/tags/OWF/"/>
    
      <category term="OWP" scheme="https://f7ed.com/tags/OWP/"/>
    
      <category term="HCB" scheme="https://f7ed.com/tags/HCB/"/>
    
      <category term="GL Theorem" scheme="https://f7ed.com/tags/GL-Theorem/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-MIT6875」: Lecture 6 - Number Theory</title>
    <link href="https://f7ed.com/2022/07/14/mit6875-lec6/"/>
    <id>https://f7ed.com/2022/07/14/mit6875-lec6/</id>
    <published>2022-07-13T16:00:00.000Z</published>
    <updated>2022-08-12T04:31:58.262Z</updated>
    
    <content type="html"><![CDATA[<article class="message message-immersive is-info"><div class="message-body"><i class="fas fa-info-circle mr-2"></i>In this <a href="/categories/Cryptography-MIT6875">series</a>, I will learn MIT 6.875, <strong>Foundations of Cryptography</strong>, lectured by <strong>Vinod Vaikuntanathan</strong>.<br>Any corrections and advice are welcome. ^ - ^</div></article> <article class="message message-immersive is-warning"> <div class="message-body"> <i class="fas fa-exclamation-triangle mr-2"></i> The motif of this blog is <strong>Number Theory</strong>, including the second half of Lecture 5 and the Lecture 6. <p>It’s an excellent opportunity to learn Number Theory in manner of English. </p><p>An important point in this blog is that we <strong>focus more on the statements</strong>, which is useful in later lectures, rather than the proof.</p><p>About 70% of the content in this blog is originally and literally from the lecture notes. I just organize and refine it according to the logic of the professor’s narration since the <a href="/files/numbertheory.pdf">lecture note</a> is awesome.</p><p>The rest is my own understanding and derivation of some theorems. And I will be learning Number Theory and completing the omitted proof.</p><p>So this blog will be updated continuously.</p> </div> </article> <p><strong>Topics covered:</strong></p><ul><li>Groups,  Order of a group and the Order of an element, Cyclic Groups.</li><li>The Multiplicative Group $\mathbb{Z}_N^*$ and $\mathbb{Z}_P^*$ for a prime $P$.</li><li>Generators of $\mathbb{Z}_P^*$.</li><li>Primes, Primality Testing.</li><li>The Discrete Logarithm (DLOG) problem and a candidate OWF.</li><li>Diffie-Hellman assumptions: DDH and CDH.</li></ul><span id="more"></span><h1 id="Groups"><a href="#Groups" class="headerlink" title="Groups"></a>Groups</h1><p>An <strong>Abelian group</strong> $\mathbb{G}=(S,\star)$ is a set $S$ together with a operation $\star :S\times S\rightarrow S$ which satisfies</p><ul><li><strong>Identity</strong>: There is an element $\mathcal{I}\in S$ such that for $a\in S$, $a\star \mathcal{I}=\mathcal{I}\star a=a$.</li><li><strong>Inverse</strong>: For every $a\in S$, there is an element $b\in S$ such that $a\star b =b\star a=\mathcal{I}$.</li><li><strong>Associativity</strong>: For every $a,b,c\in S$, $a\star (b\star c)=(a\star b)\star c$.</li><li><strong>Commutativity</strong>: For every $a,b\in S$, $a\star b=b\star a$.</li></ul><h2 id="Order-of-a-group-and-the-order-of-an-element"><a href="#Order-of-a-group-and-the-order-of-an-element" class="headerlink" title="Order of a group and the order of an element"></a>Order of a group and the order of an element</h2><ul><li><strong>The order of a group</strong> is <u>the number of elements in it</u>, namely $|S|$.</li><li><strong>The order of an element</strong> $g\in S$ is <u>the minimal number of times</u> one has to <u>perform the group operation on $g$ to get to the identity element</u> $\mathcal{I}$.<br>That is,  $\mathrm{ord}(g)=\min_{i>0}\{g^i=\mathcal{I}\}$. </li></ul> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem 1 (Lagrange’s Theorem):</strong></p> </div> <div class="message-body"> <p>The order of any element divides the order of the group.</p> </div> </article> <h2 id="Generator-of-a-Group"><a href="#Generator-of-a-Group" class="headerlink" title="Generator of a Group"></a>Generator of a Group</h2><p>A <strong>generator</strong> of a group $\mathbb{G}$ is <u>an element of order</u> $|\mathbb{G}|$. In other words,</p>$$\mathbb{G}=\{g,g^2,\dots,g^{|\mathbb{G}|}=\mathcal{I}\}$$<h2 id="Cyclic-group"><a href="#Cyclic-group" class="headerlink" title="Cyclic group"></a>Cyclic group</h2><p>A group $\mathbb{G}$ is called <strong>cyclic</strong> <u>if it has a generator.</u></p> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem 2:</strong></p> </div> <div class="message-body"> <p>Every group whose order is a <u>prime number</u> is <strong>cyclic</strong>.<br>Moreover, <u>every element other than the identity</u> is a <strong>generator</strong>.</p> </div> </article> <p><strong>Proof</strong>[2]</p><h2 id="Discrete-Logarithms"><a href="#Discrete-Logarithms" class="headerlink" title="Discrete Logarithms"></a>Discrete Logarithms</h2><p>Let $\mathbb{G}$ be a cyclic group. We know that $\mathbb{G}$ has a generator $g$, and that every $h\in \mathbb{G}$ can be written as $h=g^x$ for a <strong>unique</strong>  $x\in\{1,2,\dots,|\mathbb{G}|\}$. </p><p>We write </p><p>$$<br>x=\operatorname{dlog}_g(h)<br>$$</p><p>to denote the fact that $x$ is the <strong>discrete logarithm</strong> of $h$ to the base $g$.</p><hr><p>We will <strong>look for groups</strong> where <u>computing the group operation is easy</u> (namely, polynomial time) but <u>computing discrete logarithms is hard</u> (namely, exponential or sub-exponential time).</p><p>Our source for such groups will come from number theory.</p><p>Discrete logarithms in $\mathbb{Z}_N$ are, for better or worse, easy.</p><h1 id="Baby-Computational-Number-Theory"><a href="#Baby-Computational-Number-Theory" class="headerlink" title="Baby (Computational) Number Theory"></a>Baby (Computational) Number Theory</h1><p>The complexity of basic operations with numbers. $n$ denotes the input length for each of these operations.</p><img src="https://s1.ax1x.com/2022/07/15/jhldBt.png" alt="Complexity of Basic operations" style="zoom:37%;" /><p><strong>Greatest Common Divisors:</strong></p><p>The <strong>greatest common divisor</strong> (gcd) of positive integers $a$ and $b$ is <u>the largest positive integer $d$ that divides both $a$ and $b$.</u></p><p>$a$ and $b$ are <strong>relatively prime</strong> if their gcd is 1.</p><h1 id="The-Multiplicative-Group-mathbb-Z-N"><a href="#The-Multiplicative-Group-mathbb-Z-N" class="headerlink" title="The Multiplicative Group $\mathbb{Z}_N^*$"></a>The Multiplicative Group $\mathbb{Z}_N^*$</h1><p>The <strong>multiplicative group</strong> of numbers mod $N$, denoted $\mathbb{Z}_N^*$, consists of the set</p>$$S=\{1\le a< N:\operatorname{gcd}(a,N)=1\}$$<p><u>with multiplication mod $N$</u> being the <strong>operation</strong>.</p><hr><p><strong>Some further facts about</strong>  $\mathbb{Z}_N^*$</p><ul><li><p>The <strong>order</strong> of $\mathbb{Z}_N^*$, the number of <u>positive integers smaller than</u> $N$ that are <u>relatively prime to it</u>, is called the <strong>Euler totient function</strong> of $N$ denoted $\varphi(N)$.</p></li><li><p>$\varphi(p)=p-1$ if $p$ is prime.</p></li><li><p>$\varphi(p^k)=p^k-p^{k-1}$ if $p$ is prime.</p></li><li><p>$\varphi(pq)=\varphi(p)\varphi(q)$ if $\operatorname{gcd}(p,q)=1$.</p></li><li><p>If $N=\prod _i p_i^{\alpha_i}$ is the prime factorization of $N$, then  $\varphi(N)=\prod_i p_i^{\alpha_i-1}(pi-1)$.  </p></li></ul><h1 id="The-Multiplicative-Group-mathbb-Z-p"><a href="#The-Multiplicative-Group-mathbb-Z-p" class="headerlink" title="The Multiplicative Group $\mathbb{Z}_p^*$"></a>The Multiplicative Group $\mathbb{Z}_p^*$</h1> $\mathbb{Z}_p^*$  is Cyclic. <p>The following theorem is a very important property of $\mathbb{Z}_p^*$ when $P$ is prime.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem 4 :</strong></p> </div> <div class="message-body"> <p>If $P$ is prime, then $\mathbb{Z}_p^*$ is a cyclic group.</p> </div> </article> <p><strong>Proof</strong>[2]</p> <article class="message message-immersive is-danger"> <div class="message-body"> <i class="fas fa-bug mr-2"></i> <u>Note:</u> <p>It is very tempting to prove this theorem by appealing the <strong>Theorem 2</strong> which <u>says that every group with prime order is cyclic.</u>  Be careful, and note that the <strong>order</strong> of $\mathbb{Z}_p^*$ is $P-1$, which is <u>decidedly not prime.</u></p> </div> </article> <p>There are several followup questions.</p><ul><li><u>How many</u> generators are there for $\mathbb{Z}_p^*$ ?</li><li><u>How to tell (efficiency)</u> if a given element $g$ is a generator ?</li><li><u>How to sample</u> a random generator for $\mathbb{Z}_p^*$ ?</li></ul><h2 id=""><a href="#" class="headerlink" title=""></a>  $\mathbb{Z}_p^{*}$ and  $\mathbb{Z}_{P-1}$ </h2><p>Before proceeding further, let us note the following structural fact about $\mathbb{Z}_P^*$.</p><p>There are two groups are <strong>isomorphic</strong> with an isomorphism $\phi$ that maps $x\in\mathbb{Z}_{P-1}$ to $g^x\in \mathbb{Z}_P^*$. [isomorphic: 同构的]</p><p>In particular, consider $\phi(x)=g^x \pmod P$.</p><p>We have $\phi(x+y)=\phi(x)\cdot \phi(y)$.</p><p>The isomorphism is <strong>efficiently computable</strong> <u>in the forward direction</u> (exponentiation, using the repeated squaring algorithm) but <strong>not known to be efficiently computable</strong> <u>in the reverse direction.</u></p><p>For example, consider $\mathbb{Z}_7^*$ and $\mathbb{Z}_6$:</p><ul><li>$\mathbb{Z}_7^* = \{1,2,3,4,5,6\}$ and there is a  generator $g=5$. You can get $\{g,g^2,g^3,g^4,g^5,g^6\} =\{5, 4,6,2,3,1\}$.<br>When you <u>perform the multiplication</u> on $g$, you will <strong>wrap around</strong> the group.<br>That’s an intuitive reason why discrete logarithm is hard in $\mathbb{Z}_p^*$.</li><li>$\mathbb{Z}_6=\{1,2,3,4,5,6\}$ and the generator $g=1$. The group operation in $\mathbb{Z}_6$ is addition.<br>You can get $\{g,g^2,g^3,g^4,g^5,g^6\}=\{1,2,3,4,5,6\}$.<br>When you perform the addition on $g$, you <strong>just walk along</strong> the group.</li><li>We can know that there is a one-to-one mapping $\phi$  from $\mathbb{Z}_6$ to $\mathbb{Z}_7^*$, that is $\mathbb{Z}_6\cong \mathbb{Z}_7^*$ . </li></ul><hr><p>Here is another quick application of this isomorphism:</p><p><font color=blue><u><b>Lemma :</b></u></font> </p><p>Let $P$ be an odd prime.</p><p>If $g$ is a generator of $\mathbb{Z}_P^*$, then so is $g^x$ as long as $x$ and $P-1$ are <u>relatively prime.</u></p><p><font color=blue><u><b>Proof:</b></u></font> </p><article class="message message-immersive is-warning">  <div class="message-body">  <i class="fas fa-exclamation-triangle mr-2"></i> The following proof is my <strong>own deduction</strong> since the proof is omitted in the lecture.  <br> <strong>Corrections and advice are welcome.</strong>  </div>  </article><p>There is a generator $g’$ in  $\mathbb{Z}_{P-1}$corresponding to $g$ in $\mathbb{Z}_P^*$ from the isomorphism $\mathbb{Z}_P^*\cong \mathbb{Z}_{P-1}$. </p><p>$x$ and $P-1$ are relatively prime, and there are two statements.</p><p>In group $\mathbb{Z}_P^*$, if $g$ is a generator, then $g^x$ is a generator.</p><p>In group $\mathbb{Z}_{P-1}$, if $g’$ is a generator, then $xg’$ is a generator.</p><p>From the <strong>isomorphism</strong>, we  can turn the question in $\mathbb{Z}_P^*$ to a relative question in $\mathbb{Z}_{P-1}$.  </p><p>It’s easy to <strong>prove</strong> that if $x$ is a generator   in $\mathbb{Z}_{P-1}$ (which can iterate all elements), then $ax$ is also a generator in $\mathbb{Z}_{P-1}$ (which can also iterate all elements) where $\operatorname{gcd}(a, P-1)=1$. </p><ul><li>The main idea is to suppose for a <strong>contradiction</strong> that there are two different $x_1$ and $x_2$,i.e. $x_1\ne x_2 \pmod{P-1}$, satisfying $ax_1=ax_2 \pmod{P-1}$.</li><li>Then we know $P-1\mid a(x_1-x_2)$.</li><li>From $\operatorname{gcd}(a, P-1)=1$, we know $P-1\mid x_1-x_2$.</li><li>So $x_1= x_2 \pmod{P-1}$. (contradiction)</li><li>QED.</li></ul><hr><p>As a <strong>corollary</strong>, we immediately derive the fact <u>that $\phi(P-1)$ elements</u>  of $\mathbb{Z}_p^*$ are  <strong>generators</strong>.</p><h2 id="mathbb-Z-p-has-lots-of-generators"><a href="#mathbb-Z-p-has-lots-of-generators" class="headerlink" title="$\mathbb{Z}_p^*$  has lots of generators"></a>$\mathbb{Z}_p^*$  has lots of generators</h2><p>For the first question, $\mathbb{Z}_p^*$ has lots of generators.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem 5:</strong></p> </div> <div class="message-body"> <p>The number of generators in $\mathbb{Z}_p^*$ is $\varphi(P-1)$.</p> </div> </article> <p>But <u>how large</u> is $\varphi(P-1)$ asymptotically? This is answered by the following classical theorem.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Theorem 6:</strong></p> </div> <div class="message-body"> <p>For every integer $N$, $\varphi(N)=\Omega(N/\log \log N)$.</p> </div> </article> <p>In other words, if you <u>pick a random element</u> of  $\mathbb{Z}_p^*$,  you will see a generator <strong>with probability</strong></p><p>$$<br>\varphi(P-1)/(P-1)=\Omega(1/\log\log P)<br>$$</p><p><u>which is polynomial</u> in $1/\log P$. So, reasonably <strong>often</strong>.[asymptotical: 渐进的]</p><blockquote><p><strong>Difference between Big $\mathcal{O}$ and Big $\Omega$:</strong><br>Big $\mathcal{O}$ and Big $\Omega$ function are used in computer science to <u>describe the performance or complexity of an algorithm.</u></p><ul><li>Big $\mathcal{O}$  is used to describe <u>the worst case</u> running time for an algorithm.</li><li>But, Big $\Omega$ notation, on the other hand, is used to describe <u>the best case</u> running time for a given algorithm.</li></ul></blockquote><p>If you want to get a generator in  $\mathbb{Z}_p^*$,  you will <strong>hit a generator very quickly</strong> just <u>keeping picking random</u> in  $\mathbb{Z}_p^*$. </p><p>Then it comes to the second question.</p><h2 id="How-to-tell-a-generator-of-mathbb-Z-p"><a href="#How-to-tell-a-generator-of-mathbb-Z-p" class="headerlink" title="How to tell a generator of $\mathbb{Z}_p^*$ ?"></a>How to tell a generator of $\mathbb{Z}_p^*$ ?</h2><p>The answer is that you <strong>can tell in poly. time</strong> whether $g\in \mathbb{Z}_p^*$ is a generator <u>given the factorization of $P-1$.</u></p><h3 id="Given-the-factorization-of-P-1​"><a href="#Given-the-factorization-of-P-1​" class="headerlink" title="Given the factorization of P-1​"></a>Given the factorization of P-1​</h3><p>At first, we know that a generator is an element of order $P-1$, so it’s easy to check whether $g^{P-1}=1 \pmod P$.</p><p>Then we need to <strong>check</strong> if there is <u>some smaller power</u> of $g$ that equals $1$ since the order is the minimal power.</p><p>However, there are <u>a large number of divisors</u> of $P-1$, roughly $P^{1/\log \log P}$ which is <strong>not polynomial</strong> (in $\log P$).</p><p>It turns out, you <strong>do not</strong> need to <u>check all divisors,</u> <strong>but rather</strong> <u>only the terminal divisors</u> (or, the maximal factors less than $P-1$).</p><p>If $P-1=\prod_i q_i^{\alpha_i}$, the t<strong>erminal divisors</strong> are $(P-1)/q_i$ for each $i$.</p><p><strong>For example:</strong></p><ul><li>If $P-1=p_1^2p_2^3p_3^2$.</li><li>Suppose $g=p_1^2p_2^3$,  then $g^{p_1^2p_2^3}=1$.<br>You can get some powers of $g^{p_1^2p_2^3}$ that equals $1$.<br>And the power to $g^{p_1^2p_2^3}$ is <u>the multiplication to the exponent</u> $p_1^2p_2^3$.<ul><li>You can get $g^{p_1^2p_2^3p_3}=1$ which is <u>smaller power</u> than $P-1$.</li><li>You can also get $g^{p_1^2p_2^3p_3^2}=1$ which is useless since $P-1=p_1^2p_2^3p_3^2$.</li><li>So you only <u>hit one terminal divisor</u>, that is $(P-1)/p_3$.</li></ul></li><li>Suppose $g=p_1p_2^2p_3$, then $g^{p_1p_2^2p_3}=1$.<br>Similarly, you can get some powers of $g^{p_1p_2^2p_3}$ that equals $1$.<ul><li>You can get $g^{p_1p_2^3p_3^2}=1$ by power $p_2p_3$.<br>You hit one terminal divisor $(P-1)/p_1$.</li><li>You can get $g^{p_1^2p_2^2p_3^2}=1$ by power $p_1p_3$.<br>You hit one terminal divisor $(P-1)/p_2$.</li><li>You can get $g^{p_1^2p_2^3p_3}=1$ by power $p_1p_2$.<br>You hit one terminal divisor $(P-1)/p_3$.</li><li>So you <u>can hit 3 terminal divisors.</u></li></ul></li><li>The <strong>conclusion</strong> is that <u>if there is a smaller power</u> of $g$ that equals 1, then you <strong>can definitely</strong> <u>hit some powers of terminal divisor</u> that equals 1, which are maximal factors of $P-1$. So you can <strong>only check all the terminal divisiors</strong>.</li></ul><p><font color=blue><u><b><i>Algorithm: </i></b></u></font> </p><p>The following <strong>algorithm</strong> works on $g$ and <u>the prime factorization</u> of $P-1$.</p><ul><li>For each $i$, check if $g^{(P-1)/q_i}\overset{?}= 1\pmod P$.</li><li>If yes for <strong>any</strong> $i$, say “not a generator”.</li><li>Otherwise, say “generator”.</li></ul><h3 id="Given-only-g​-and-​P​"><a href="#Given-only-g​-and-​P​" class="headerlink" title="Given only g​ and ​P​"></a>Given only g​ and ​P​</h3><p>That’s nice.</p><p>But can one tell if $g$ is a generator <u>given only $g$ and $P$ ?</u><br>(as <u>opposed to the prime factorization</u> of $P-1$ which is in general hard to compute)</p><p>We don’t know, so there are some ways around it.</p><p><font color=blue><u><b><i>Solution 1: </i></b></u></font> </p><p>Pick a random $P$ <u>together with its prime factorization</u> of $P-1$.</p><p>This, it turns out, can be done due to a clever algorithm of Kalai [6].</p><p><font color=blue><u><b><i>Solution 2: </i></b></u></font> </p><p>Pick $P=2Q+1$ where $Q$ is prime. </p><p>Such primes like $P$ are called <strong>safe primes</strong>, and $Q$ is called a <strong>Sophie-Germain prime</strong> after the famous mathematicians. </p><p>While there are infinitely many primes, it has only been <strong>conjectured</strong> that <u>there are infinitely many Sophie-Germain primes</u>. This remains <strong>unproven</strong>.</p><p>Moreover, the <strong>Sophie-Germain primes</strong> are considered <u>as the hardest class of primes for discrete logarithms.</u></p><hr><p>Solution 2 is what people typically use in practice. </p><p><strong>In practice</strong>, you <u>just pick a random prime</u> as $Q$ and <strong>check</strong> whether $P=2Q+1$ is a prime.</p><p>It’s efficient <strong>as long as</strong> the <u>Sophie-Germain primes are in dense distribution.</u></p><p>The above solution 2 brings about new questions.</p><p>How to sample a random prime and how to test primality ?</p><h2 id="Primes"><a href="#Primes" class="headerlink" title="Primes"></a>Primes</h2><p>There are some questions about primes.</p><ul><li><u>How many</u> primes of $n$-bit length ?</li><li><u>How to test</u> primality ?</li><li><u>How to sample</u> a $n$-bit random prime ?</li></ul><h3 id="How-many-primes"><a href="#How-many-primes" class="headerlink" title="How many primes"></a>How many primes</h3><p>The <strong>prime number theorem</strong> tells us that <u>there are sufficiently many prime numbers.</u></p><p>In particular, letting $\pi(N)$ denote <u>the number of prime numbers less than</u> $N$, we know that $\pi(N)=\Omega(N/\log N)$.</p><p>Thus, if you <u>pick a random number smaller than</u> $N$, <strong>with probability</strong> $1/\log N$ (which is $1$/<strong>polynomial</strong> in bit-length $n$ in question) you have a prime number at hand.</p><h3 id="Primality-Testing"><a href="#Primality-Testing" class="headerlink" title="Primality Testing"></a>Primality Testing</h3><p>How to recognize that a given number is prime ?</p><p>This has been the subject of extensive research in computational number theory with many polynomial-time algorithms, culminating with the <strong>deterministic polynomial-time primality testing algorithm</strong> of Agrawal, Kayal and  Saxena[1] (a.k.a. AKS) in 2002.</p><h3 id="Sample-a-prime"><a href="#Sample-a-prime" class="headerlink" title="Sample a prime"></a>Sample a prime</h3><p>How to sample a $n$-bit random prime ?</p><p>The above two facts put together tell us how to <u>generate a random $n$-bit prime number.</u></p><ol><li>Just <u>pick a random number less than</u> $2^n$.</li><li>And <u>test</u> if it is prime.</li></ol><p>In expected $n$ iterations of this procedure, you will find a $n$-bit prime number, even a random prime number.</p><h1 id="One-way-Functions"><a href="#One-way-Functions" class="headerlink" title="One-way Functions"></a>One-way Functions</h1><p>One-way function is the atom of cryptography.</p><p>A one-function is a function $f$ that is <u>easy to compute but hard to invert</u> on average.</p><p>How to define a one-way function ?</p><p><font color=blue><u><b>Take 1 (Wrong): </b></u></font> </p><p>For every p.p.t. algorithm $A$ and <u>every chosen</u> $x$,there is a negligible function $\mu$, s.t.<br>$$<br>\operatorname{Pr}[A(f(x))=x]&lt;\mu(n)<br>$$<br>It is hard for $A$ to <u>guess the inverse</u> of $f(x)$ <u>for every chosen</u> $x$.</p><p>It seems right.</p><p>How about the function $f(x)=0$ which maps any $x$ to $0$ ?</p><p>The probability of <u>guessing the inverse of $0$ is negligible</u> if the domain size of $x$ is sufficiently large. Because it is essentially <u>random</u>.</p><p>But $f(x)=0$ is <strong>not</strong> one-way function obviously.</p><p><font color=blue><u><b>Take 2: </b></u></font> </p><p>For every p.p.t. algorithm $A$ and every chosen $x$, there is a negligible function $\mu$,s.t.<br>$$<br>\operatorname{Pr}[A(f(x))\in \tilde{f}(f(x))]&lt;\mu(n)<br>$$<br>where $\tilde{f}$ is <u>the inverse function.</u></p><p>The <strong>difference</strong> of the new definition is that $A$ is trying to <u>guess some possible inverses</u> of $f(x)$ rather than the exactly chosen $x$.</p><p>In this definition, $f(x)=0$ is not one-way function.</p><h2 id="Candidate-DLOG"><a href="#Candidate-DLOG" class="headerlink" title="Candidate: DLOG"></a>Candidate: DLOG</h2><p>Let us present an informal one-way function <strong>candidate</strong>.</p><p>$$<br>f(P,g,x)=(P,g,g^x \pmod P)<br>$$</p><p>Computing this function <u>can be done in time polynomial</u> in the input length.</p><p>However, <strong>inverting</strong> is the <u>discrete logarithm problem</u> which is conjectured to be hard.</p><p>Defined formally below.</p> <article class="message is-info"> <div class="message-header"> <p><strong>Discrete Log Assumption (DLOG):</strong></p> </div> <div class="message-body"> <p>For a random $n$-bit prime $P$ and random generator $g$ of $\mathbb{Z}_P^*$, and a random $x\in \mathbb{Z}_{P-1}$, there is <strong>no polynomial</strong> (in $n$) time algorithm that computes $x$ given $P,g,g^x \pmod P$.</p><p><u>Shorthand:</u></p><ul><li>Easy: $P,g,x\rightarrow g^x$</li><li>Hard: $P,g,g^x\rightarrow x$</li></ul> </div> </article> <p>In fact, this is not only a one-way function, but can also <u>be made into a family of one-way permutations.</u></p><p>More on that later, we will see that <strong>one-way permutations</strong> can be used to build <strong>pseudorandom generators</strong>; and as we saw already, <strong>pseudorandom generators</strong> can be sued to build <strong>pseudorandom functions</strong> and stateless secret-key encryption and authentication. </p><p>We can do all the crypto we saw so far <u>based on the hardness of the discrete logarithm problem.</u></p><p>Route: OWF→PRG→PRF.</p><p>However,  going via this route may <u>not be the most efficient.</u></p><p>So, we will look at related problems and try to <strong>build more efficient PRGs and PRFs.</strong></p><h2 id="The-Diffie-Hellman-Assumptions"><a href="#The-Diffie-Hellman-Assumptions" class="headerlink" title="The Diffie-Hellman Assumptions"></a>The Diffie-Hellman Assumptions</h2><p>There is another route to <u>build PRGs and PRFs efficiently</u>, that is <strong>Diffie-Hellman Assumptions.</strong></p><p>Route: DH→PRG.</p><p>Route: DH→PRF.</p><p>Given $g^x$ and $g^y\mod P$, you can <strong>easily</strong> compute $g^{x+y}=g^x\cdot g^y \pmod P$. But it is <strong>hard</strong> to compute $g^{xy}\pmod P$.</p><p>If you can compute discrete logarithms, then you can compute $x$ from $g^x$, and raise $g^y$ to $x$ to get $(g^y)^x=g^{xy} \pmod P$.</p><p>But discrete log is hard, so this isn’t an efficient way to solve the problem.</p><h3 id="CDH-Assumption"><a href="#CDH-Assumption" class="headerlink" title="CDH Assumption"></a>CDH Assumption</h3><p>The problem, called the <strong>computational Diffie-Hellman (CDH)</strong> problem, appears to be <strong>computationally hard</strong>, in fact <u>as hard as computing discrete logarithms!</u></p> <article class="message is-info"> <div class="message-header"> <p><strong>Computational Diffie-Hellman Assumption (CDH):</strong></p> </div> <div class="message-body"> <p>For a random $n$-bit prime $P$ and random generator $g$ of $\mathbb{Z}_P^*$, and random $x,y\in \mathbb{Z}_{P-1}$, there is <strong>no polynomial</strong> (in $n$) time algorithm that computes $g^{xy}\pmod P$ given $P,g,g^x\pmod P,g^y\pmod P$.</p><p><u>Shorthand:</u></p><ul><li>Easy: $P,g,g^x,g^y\rightarrow g^{x+y}$</li><li>Hard: $P,g,g^x,g^y\rightarrow g^{xy}$</li></ul> </div> </article> <p>Moreover, it appears <strong>hard</strong> to even <u>tell if you are given the right answer or not!</u></p><h3 id="DDH-Assumption"><a href="#DDH-Assumption" class="headerlink" title="DDH Assumption"></a>DDH Assumption</h3><p><font color="red">But this requires some care to formalize.</font> </p><p>At first, one <strong>may</strong> think that given $P,g,g^x,g^y$, it is <strong>hard to distinguish</strong> between the right answer $g^{xy} \pmod P$ versus a random number $u \mod P$.</p><p>Let us call the assumption that this <strong>decisional problem</strong> is hard the <strong>decisional Diffie-Hellman (DDH) assumption.</strong></p><p><font color=blue><u><b>Decisional Diffie-Hellman Assumption (first take):</b></u></font> </p><p>For a random $n$-bit prime $P$ and random generator $g$ of $\mathbb{Z}_P^*$, and random $x,y\in \mathbb{Z}_{P-1}$ and a random number $u\in \mathbb{Z}_P^*$, there is <strong>no polynomial</strong> (in $n$) time algorithm that <u>distinguishes</u> between $(P,g,g^x\pmod P, g^y \pmod P, g^{xy} \pmod P)$ and $(P,g,g^x\pmod P,g^y \pmod P, u\pmod P)$.</p><p>However, the assumption turns out to be <font color="red">false.</font> </p><h3 id="DDH-is-False-in-mathbb-Z-P"><a href="#DDH-is-False-in-mathbb-Z-P" class="headerlink" title="DDH is False in $\mathbb{Z}_P^*$"></a>DDH is False in $\mathbb{Z}_P^*$</h3><p>However, the DDH assumption is false in $\mathbb{Z}_P^*$.</p><p>It seems awfully strong on first look.</p><p>It says not only it is hard to compute $g^{xy}$ from $g^x$ and $g^y$, but also that <u>not even a single bit of</u> $g^{xy}$ can be computed (with any polynomial advantage beyond trivial guessing).</p><p>However, there are <u>some information about $g^{xy}$ indeed dose leak from $g^x$ and $g^y$.</u></p><hr><p>Before that, let us take a quick detour in <strong>Quadratic Residues.</strong></p><p>$$<br>h=g^2 \pmod P<br>$$</p><p>Some facts about Quadratic Residues: </p><ul><li>$h$ is a QR if $h=g^2\pmod P$.</li><li>1/2 of $\mathbb{Z}_P^*$ are QR.</li><li>We <strong>can tell efficiently</strong> if $h$ is a QR by evaluating $h^{(P-1)/2}\overset{?}=1 \pmod P$.</li><li>$h=g^x$ is a QR <strong>if and only if</strong> $x$ is <u>even</u>.</li></ul><hr><p>Use the facts of quadratic residue in $\mathbb{Z}_P^*$, we know the following statements are equivalent.</p><ul><li>$g^{xy}$ is a QR.</li><li>iff $xy$ is even.</li><li>iff $x$ is even or $y$ is even.</li><li>iff $g^x$ is a QR or $g^y$ is a QR.</li></ul><p>Now, we can <strong>distinguish</strong> between $(P,g,g^x,g^y,g^{xy})$ and $(P,g,g^x,g^y,u)$.</p><p> $g^{xy}$ is a quadratic residue <u>with probability</u> $3/4$  while $u$ is a quadratic residue <u>with probability</u> $1/2$.</p><p>So the advantage for distinguishing is $1/4$.</p><p>Thus, we <font color="red">need to refine our assumption.</font> </p><p>Looking at the <strong>core reason</strong> behind the above attack, we see that <u>there is a 1/2 chance</u> that $g^x$ <u>falls into a subgroup</u> (the subgroup of <strong>quadratic residues</strong>, to be precise) and once that happens<u>, $g^{xy}$ is also in the subgroup</u> no matter what $y$ is.</p><p>These properties are furthermore detectable in polynomial-time which led us to attack.</p><p>A <strong>solution</strong> is to work with <strong>subgroup of $\mathbb{Z}_P^*$ of prime order.</strong></p><p>In particular, we will take $P=2Q+1$ to <u>be a safe prime</u> and <u>work with</u> $\mathcal{QR}_P$, <strong>the subgroup of quadratic residues</strong> in $\mathbb{Z}_P^*$.</p><p>$\mathcal{QR}_P$ has <strong>order</strong> $(P-1)/2=Q$ which <u>is indeed prime!</u></p><p>By virtue of this, <u>every non-identity element</u> of $\mathcal{QR}_P$ is its <strong>generator</strong> w.r.t. theorem 2.</p><p>With this change, we can state the <strong>following DDH assumption</strong> which <u>is widely believed to be true.</u></p> <article class="message is-info"> <div class="message-header"> <p><strong>Decisional Diffie-Hellman Assumption (final):</strong></p> </div> <div class="message-body"> <p>Let $P=2Q+1$ be a random $n$-bit <strong>safe prime</strong> and let $\mathcal{QR}_P$ denote the <strong>subgroup of quadratic residues</strong> in $\mathbb{Z}_P^*$.</p><p>For a random generator $g$ of $\mathcal{QR}_P$, and random $x,y\in \mathbb{Z}_Q$ and a random number $u\in \mathcal{QR}_P$, there is no polynomial (in $n$) that distinguishes between $(P,g,g^x\pmod P,g^y \pmod P, g^{xy}\pmod P)$ and $(P,g,g^x\pmod P, g^y \pmod P,u\pmod P)$.</p><p><u>Shorthand:</u></p><ul><li>Hard: distinguish between $(P,g,g^x,g^y,g^{xy})$ and $(P,g,g^x,g^y,u)$.</li></ul> </div> </article> <p>We know DLOG→CDH→DDH but no implications are known in the reverse directions.</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>[1] M. Agrawal, N. Kayal, and N. Saxena. Primes is in p. Annals of mathematics, pages 781–793, 2004.</p><p>[2] D. Angluin. Lecture notes on the complexity of some problems est number theory. 1982.</p><p>[6] A. Kalai. Generating random factored numbers, easily. Journal of Cryptology, 16(4):287–289, 2003.</p>]]></content>
    
    <summary type="html">
    
      &lt;article class=&quot;message message-immersive is-info&quot;&gt;
&lt;div class=&quot;message-body&quot;&gt;
&lt;i class=&quot;fas fa-info-circle mr-2&quot;&gt;&lt;/i&gt;
In this &lt;a href=&quot;/categories/Cryptography-MIT6875&quot;&gt;series&lt;/a&gt;, I will learn MIT 6.875, &lt;strong&gt;Foundations of Cryptography&lt;/strong&gt;, lectured by &lt;strong&gt;Vinod Vaikuntanathan&lt;/strong&gt;.
&lt;br&gt;Any corrections and advice are welcome. ^ - ^
&lt;/div&gt;
&lt;/article&gt;

 &lt;article class=&quot;message message-immersive is-warning&quot;&gt; &lt;div class=&quot;message-body&quot;&gt; &lt;i class=&quot;fas fa-exclamation-triangle mr-2&quot;&gt;&lt;/i&gt; The motif of this blog is &lt;strong&gt;Number Theory&lt;/strong&gt;, including the second half of Lecture 5 and the Lecture 6. 

&lt;p&gt;It’s an excellent opportunity to learn Number Theory in manner of English. &lt;/p&gt;
&lt;p&gt;An important point in this blog is that we &lt;strong&gt;focus more on the statements&lt;/strong&gt;, which is useful in later lectures, rather than the proof.&lt;/p&gt;
&lt;p&gt;About 70% of the content in this blog is originally and literally from the lecture notes. I just organize and refine it according to the logic of the professor’s narration since the &lt;a href=&quot;/files/numbertheory.pdf&quot;&gt;lecture note&lt;/a&gt; is awesome.&lt;/p&gt;
&lt;p&gt;The rest is my own understanding and derivation of some theorems. And I will be learning Number Theory and completing the omitted proof.&lt;/p&gt;
&lt;p&gt;So this blog will be updated continuously.&lt;/p&gt;
 &lt;/div&gt; &lt;/article&gt; 

&lt;p&gt;&lt;strong&gt;Topics covered:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Groups,  Order of a group and the Order of an element, Cyclic Groups.&lt;/li&gt;
&lt;li&gt;The Multiplicative Group $&#92;mathbb{Z}_N^*$ and $&#92;mathbb{Z}_P^*$ for a prime $P$.&lt;/li&gt;
&lt;li&gt;Generators of $&#92;mathbb{Z}_P^*$.&lt;/li&gt;
&lt;li&gt;Primes, Primality Testing.&lt;/li&gt;
&lt;li&gt;The Discrete Logarithm (DLOG) problem and a candidate OWF.&lt;/li&gt;
&lt;li&gt;Diffie-Hellman assumptions: DDH and CDH.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cryptography-MIT6875" scheme="https://f7ed.com/categories/Cryptography-MIT6875/"/>
    
    
      <category term="Prime" scheme="https://f7ed.com/tags/Prime/"/>
    
      <category term="Number Theory" scheme="https://f7ed.com/tags/Number-Theory/"/>
    
      <category term="Multiplicative Group" scheme="https://f7ed.com/tags/Multiplicative-Group/"/>
    
      <category term="Generators" scheme="https://f7ed.com/tags/Generators/"/>
    
      <category term="Diffie-Hellman Assumptions" scheme="https://f7ed.com/tags/Diffie-Hellman-Assumptions/"/>
    
  </entry>
  
</feed>
