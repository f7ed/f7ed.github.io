<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>「机器学习-李宏毅」：HW2-Binary Income Predicting - fred&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="fred&#039;s blog"><meta name="msapplication-TileImage" content="/img/heart.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="fred&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。包括对数据集的处理，训练模型，可视化，预测等。有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub"><meta property="og:type" content="blog"><meta property="og:title" content="「机器学习-李宏毅」：HW2-Binary Income Predicting"><meta property="og:url" content="https://f7ed.com/2020/04/15/ml-lee-hw2/"><meta property="og:site_name" content="fred&#039;s blog"><meta property="og:description" content="这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。包括对数据集的处理，训练模型，可视化，预测等。有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://f7ed.com/gallery/thumbnails/80504355_p0_master1200.jpg"><meta property="article:published_time" content="2020-04-14T16:00:00.000Z"><meta property="article:modified_time" content="2020-07-03T08:42:48.419Z"><meta property="article:author" content="f7ed"><meta property="article:tag" content="Machine-Learning"><meta property="article:tag" content="Classification"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/thumbnails/80504355_p0_master1200.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://f7ed.com/2020/04/15/ml-lee-hw2/"},"headline":"「机器学习-李宏毅」：HW2-Binary Income Predicting","image":["https://f7ed.com/gallery/thumbnails/80504355_p0_master1200.jpg"],"datePublished":"2020-04-14T16:00:00.000Z","dateModified":"2020-07-03T08:42:48.419Z","author":{"@type":"Person","name":"f7ed"},"publisher":{"@type":"Organization","name":"fred's blog","logo":{"@type":"ImageObject","url":"https://f7ed.com/img/f1ed_logo.png"}},"description":"这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。包括对数据集的处理，训练模型，可视化，预测等。有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub"}</script><link rel="canonical" href="https://f7ed.com/2020/04/15/ml-lee-hw2/"><link rel="icon" href="/img/heart.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-M5KG3CQTSF" async></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-M5KG3CQTSF');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="fred's blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/f1ed_logo.png" alt="fred&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/liu">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/f7ed"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-bars"></i>「机器学习-李宏毅」：HW2-Binary Income Predicting</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2020-04-14T16:00:00.000Z" title="2020-04-14T16:00:00.000Z">2020-04-15</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2020-07-03T08:42:48.419Z" title="2020-07-03T08:42:48.419Z">2020-07-03</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/">机器学习-李宏毅</a></span><span class="level-item"><i class="far fa-clock"></i> 34 minutes read (About 5066 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><p>这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。<br>包括对数据集的处理，训练模型，可视化，预测等。<br>有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的<a target="_blank" rel="noopener" href="https://github.com/f1ed/ML-HW2">GitHub</a></p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="more"></span>
<h1 id="Task-introduction-and-Dataset"><a href="#Task-introduction-and-Dataset" class="headerlink" title="Task introduction and Dataset"></a>Task introduction and Dataset</h1><p> Kaggle competition: <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/ml2020spring-hw2">link</a> </p>
<p><strong>Task: Binary Classification</strong></p>
<p>Predict whether the income of an individual exceeds $50000 or not ?</p>
<p>*<em>Dataset: *</em> Census-Income (KDD) Dataset</p>
<p>(Remove unnecessary attributes and balance the ratio between positively and negatively labeled data)</p>
<h1 id="Feature-Format"><a href="#Feature-Format" class="headerlink" title="Feature Format"></a>Feature Format</h1><ul>
<li><p>train.csv, test_no_label.csv【都是没有处理过的数据，可作为数据参考和优化参考】</p>
<ul>
<li><p>text-based raw data</p>
</li>
<li><p>unnecessary attributes removed, positive/negative ratio balanced.</p>
</li>
</ul>
</li>
<li><p>X_train, Y_train, X_test【已经处理过的数据，可以直接使用】</p>
<ul>
<li><p>discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial state…)</p>
</li>
<li><p>continuous features in train.csv =&gt; remain the same in X_train (age, capital losses…).</p>
</li>
<li><p>X_train, X_test : each row contains one 510-dim feature represents a sample.</p>
</li>
<li><p>Y_train: label = 0 means “&lt;= 50K” 、 label = 1 means “ &gt;50K ”</p>
</li>
</ul>
</li>
</ul>
<p>注：数据集超大，用notepad查看比较舒服；调试时，也可以先调试小一点的数据集。</p>
<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><p>Logistic Regression 原理部分见<a href="/2020/04/01/Classification2/" title="这篇博客">这篇博客</a>。</p>
<h2 id="Prepare-data"><a href="#Prepare-data" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>本文直接使用X_train Y_train X_test 已经处理好的数据集。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">&#x27;./data/X_train&#x27;</span></span><br><span class="line">Y_train_fpath = <span class="string">&#x27;./data/Y_Train&#x27;</span></span><br><span class="line">X_test_fpath = <span class="string">&#x27;./data/X_test&#x27;</span></span><br><span class="line">output_fpath = <span class="string">&#x27;./logistic_output/output_logistic.csv&#x27;</span></span><br><span class="line">fpath = <span class="string">&#x27;./logistic_output/logistic&#x27;</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>统计一下数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;In logistic model:\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;Size of Training set: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(train_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Size of development set: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(dev_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Size of test set: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(test_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Dimension of data: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(data_dim))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">In</span> logistic model:</span><br><span class="line">Size of Training <span class="built_in">set</span>: <span class="number">48830</span></span><br><span class="line">Size of development <span class="built_in">set</span>: <span class="number">5426</span></span><br><span class="line">Size of test <span class="built_in">set</span>: <span class="number">27622</span></span><br><span class="line">Dimension of <span class="keyword">data</span>: <span class="number">510</span></span><br></pre></td></tr></table></figure>

<h3 id="normalize"><a href="#normalize" class="headerlink" title="normalize"></a>normalize</h3><p>normalize data.</p>
<p>对于train data，计算出每个feature的mean和std，保存下来用来normalize test data。</p>
<p>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_normalization</span>(<span class="params">X, train=<span class="literal">True</span>, X_mean=<span class="literal">None</span>, X_std=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Development-set-split"><a href="#Development-set-split" class="headerlink" title="Development set split"></a>Development set split</h3><p>在logistic regression中使用的gradient，没有closed-form解，所以在train set中划出一部分作为development set 优化参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_train_dev_split</span>(<span class="params">X, Y, dev_ratio=<span class="number">0.25</span></span>):</span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = <span class="built_in">int</span>(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br></pre></td></tr></table></figure>



<h2 id="Useful-function"><a href="#Useful-function" class="headerlink" title="Useful function"></a>Useful function</h2><h3 id="shuffle-X-Y"><a href="#shuffle-X-Y" class="headerlink" title="_shuffle(X, Y)"></a>_shuffle(X, Y)</h3><p>本文使用mini-batch gradient。</p>
<p>所以在每次epoch时，以相同顺序同时打乱X_train,Y_train数组，再mini-batch。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_shuffle</span>(<span class="params">X, Y</span>):</span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(<span class="built_in">len</span>(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="sigmod-z"><a href="#sigmod-z" class="headerlink" title="_sigmod(z)"></a>_sigmod(z)</h3><p>计算 $\frac{1}{1+e^{-z}}$ ，注意：防止溢出，给函数返回值规定上界和下界。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_sigmod</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="f-X-w-b"><a href="#f-X-w-b" class="headerlink" title="_f(X, w, b)"></a>_f(X, w, b)</h3><p>是sigmod函数的输入，linear part。</p>
<ul>
<li>输入：<ul>
<li>X：shape = [size, data_dimension]</li>
<li>w：weight vector, shape = [data_dimension, 1]</li>
<li>b: bias, scalar</li>
</ul>
</li>
<li>输出：<ul>
<li>属于Class 1的概率（Label=0，即收入小于$50k的概率）</li>
</ul>
</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_f</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h3 id="predict-X-w-b"><a href="#predict-X-w-b" class="headerlink" title="_predict(X, w, b)"></a>_predict(X, w, b)</h3><p>预测Label=0？（0或者1，不是概率）</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_predict</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.<span class="built_in">int</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="accuracy-Y-pred-Y-label"><a href="#accuracy-Y-pred-Y-label" class="headerlink" title="_accuracy(Y_pred, Y_label)"></a>_accuracy(Y_pred, Y_label)</h3><p>计算预测出的结果（0或者1）和真实结果的正确率。</p>
<p>这里使用 $1-\overline{error}$ 来表示正确率。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_accuracy</span>(<span class="params">Y_pred, Y_label</span>):</span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.<span class="built_in">abs</span>(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="cross-entropy-loss-y-pred-Y-label"><a href="#cross-entropy-loss-y-pred-Y-label" class="headerlink" title="_cross_entropy_loss(y_pred, Y_label)"></a>_cross_entropy_loss(y_pred, Y_label)</h3><p>计算预测出的概率（是sigmod的函数输出）和真实结果的交叉熵。</p>
<p>计算公式为： $\sum_n {C(y_{pred},Y_{label})}=-\sum[Y_{label}\ln{y_{pred}}+(1-Y_{label})\ln(1-{y_{pred}})]$ </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_cross_entropy_loss</span>(<span class="params">y_pred, Y_label</span>):</span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="gradient-X-Y-label-w-b"><a href="#gradient-X-Y-label-w-b" class="headerlink" title="_gradient(X, Y_label, w, b)"></a>_gradient(X, Y_label, w, b)</h3><p>和Regression的最小二乘一样。（严谨的说，最多一个系数不同）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_gradient</span>(<span class="params">X, Y_label, w, b</span>):</span><br><span class="line">    <span class="comment"># This function calculates the gradient of cross entropy</span></span><br><span class="line">    <span class="comment"># X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = - np.dot(X.T, pred_error)</span><br><span class="line">    b_grad = - np.<span class="built_in">sum</span>(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, <span class="built_in">float</span>(b_grad)</span><br></pre></td></tr></table></figure>

<h2 id="Training-Adagrad"><a href="#Training-Adagrad" class="headerlink" title="Training (Adagrad)"></a>Training (Adagrad)</h2><p>初始化一些参数。</p>
<p><strong>这里特别注意</strong> :</p>
<p>由于adagrad的参数更新是 $w \longleftarrow w-\eta \frac{gradient}{ \sqrt{gradsum}}$ .</p>
<p><strong>防止除0</strong>，初始化gradsum的值为一个较小值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.<span class="built_in">float</span>(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.<span class="built_in">float</span>(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Aagrad具体原理见<a href="/2020/03/01/Gradient/" title="这篇博客">这篇博客</a>的1.2节。</p>
<p>迭代更新时，每次epoch计算一次loss和accuracy，以便可视化更新过程，调整参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[<span class="built_in">id</span>*batch_size: (<span class="built_in">id</span>+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[<span class="built_in">id</span>*batch_size: (<span class="built_in">id</span>+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Loss-amp-accuracy"><a href="#Loss-amp-accuracy" class="headerlink" title="Loss &amp; accuracy"></a>Loss &amp; accuracy</h3><p>输出最后一次迭代的loss和accuracy。</p>
<p>结果如下：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Training loss: <span class="number">0.2933570286596322</span></span><br><span class="line">Training accuracy: <span class="number">0.8839238173254147</span></span><br><span class="line">Development loss: <span class="number">0.31029505347634456</span></span><br><span class="line">Development accuracy: <span class="number">0.8336166253549906</span></span><br></pre></td></tr></table></figure>

<p>画出loss 和 accuracy的更新过程：</p>
<p>loss：</p>
<p><a target="_blank" rel="noopener" href="https://imgchr.com/i/JPCjx0"><img src="https://s1.ax1x.com/2020/04/15/JPCjx0.png" alt="JPCjx0.png"></a> </p>
<p>accuracy：</p>
<p><a target="_blank" rel="noopener" href="https://imgchr.com/i/JPCxMV"><img src="https://s1.ax1x.com/2020/04/15/JPCxMV.png" alt="JPCxMV.png"></a> </p>
<p>由于Feature数量较大，将权重影响最大的feature输出看看：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Other Rel &lt;<span class="number">18</span> spouse of subfamily <span class="built_in">RP</span>: [<span class="number">7.11323764</span>]</span><br><span class="line"> Grandchild &lt;<span class="number">18</span> ever marr not <span class="keyword">in</span> subfamily: [<span class="number">6.8321061</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> ever marr <span class="built_in">RP</span> of subfamily: [<span class="number">6.77322397</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> ever marr <span class="built_in">RP</span> of subfamily: [<span class="number">6.76688406</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> never married <span class="built_in">RP</span> of subfamily: [<span class="number">6.37488958</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> spouse of subfamily <span class="built_in">RP</span>: [<span class="number">5.97717831</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.53932651</span>]</span><br><span class="line"> Grandchild <span class="number">18</span>+ spouse of subfamily <span class="built_in">RP</span>: [<span class="number">5.42948497</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.41543809</span>]</span><br><span class="line"> Mexico: [<span class="number">4.79920763</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>完整数据集、代码等，欢迎光临小透明<a target="_blank" rel="noopener" href="https://github.com/f1ed/ML-HW2">GitHub</a> </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################</span></span><br><span class="line"><span class="comment"># Data:2020-04-05</span></span><br><span class="line"><span class="comment"># Author: Fred Lau</span></span><br><span class="line"><span class="comment"># ML-Lee: HW2 : Binary Classification</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################</span></span><br><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">&#x27;./data/X_train&#x27;</span></span><br><span class="line">Y_train_fpath = <span class="string">&#x27;./data/Y_Train&#x27;</span></span><br><span class="line">X_test_fpath = <span class="string">&#x27;./data/X_test&#x27;</span></span><br><span class="line">output_fpath = <span class="string">&#x27;./logistic_output/output_logistic.csv&#x27;</span></span><br><span class="line">fpath = <span class="string">&#x27;./logistic_output/logistic&#x27;</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_normalization</span>(<span class="params">X, train=<span class="literal">True</span>, X_mean=<span class="literal">None</span>, X_std=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_train_dev_split</span>(<span class="params">X, Y, dev_ratio=<span class="number">0.25</span></span>):</span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = <span class="built_in">int</span>(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;In logistic model:\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;Size of Training set: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(train_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Size of development set: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(dev_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Size of test set: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(test_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Dimension of data: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(data_dim))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################</span></span><br><span class="line"><span class="comment"># useful function</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_shuffle</span>(<span class="params">X, Y</span>):</span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(<span class="built_in">len</span>(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_sigmod</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="comment"># Sigmod function can be used to calculate probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - (<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_f</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># This is the logistic function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       predict probability of each row of X being positively labeled, shape = [batch_size, 1]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_predict</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># This fucntion returns a truth value prediction for each row of X by logistic regression</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_accuracy</span>(<span class="params">Y_pred, Y_label</span>):</span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.<span class="built_in">abs</span>(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_cross_entropy_loss</span>(<span class="params">y_pred, Y_label</span>):</span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_gradient</span>(<span class="params">X, Y_label, w, b</span>):</span><br><span class="line">    <span class="comment"># This function calculates the gradient of cross entropy</span></span><br><span class="line">    <span class="comment"># X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = - np.dot(X.T, pred_error)</span><br><span class="line">    b_grad = - np.<span class="built_in">sum</span>(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, <span class="built_in">float</span>(b_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######################################</span></span><br><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.<span class="built_in">float</span>(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.<span class="built_in">float</span>(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[<span class="built_in">id</span>*batch_size: (<span class="built_in">id</span>+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[<span class="built_in">id</span>*batch_size: (<span class="built_in">id</span>+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;Training loss: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(train_loss[-<span class="number">1</span>]))</span><br><span class="line">    f.write(<span class="string">&#x27;Training accuracy: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(train_acc[-<span class="number">1</span>]))</span><br><span class="line">    f.write(<span class="string">&#x27;Development loss: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(dev_loss[-<span class="number">1</span>]))</span><br><span class="line">    f.write(<span class="string">&#x27;Development accuracy: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(dev_acc[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###################</span></span><br><span class="line"><span class="comment"># Plotting Loss and accuracy curve</span></span><br><span class="line"><span class="comment"># Loss curve</span></span><br><span class="line">plt.plot(train_loss, label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">plt.plot(dev_loss, label=<span class="string">&#x27;dev&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&#x27;./logistic_output/loss.png&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(train_acc, label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">plt.plot(dev_acc, label=<span class="string">&#x27;dev&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&#x27;./logistic_output/acc.png&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(output_fpath, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;id, label\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">id</span>, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(predictions):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;, &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>, label[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># Output the weights and bias</span></span><br><span class="line">ind = (np.argsort(np.<span class="built_in">abs</span>(w), axis=<span class="number">0</span>)[::-<span class="number">1</span>]).reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>, <span class="number">0</span>: <span class="number">10</span>]:</span><br><span class="line">       f.write(<span class="string">&#x27;&#123;&#125;: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(content[i], w[i]))</span><br></pre></td></tr></table></figure>

<h1 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h1><p>Generative Model 原理部分见 <a href="/2020/03/20/Classification1/" title="这篇博客">这篇博客</a></p>
<h2 id="Prepare-data-1"><a href="#Prepare-data-1" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>这部分和Logistic regression一样。</p>
<p>只是，因为generative model有closed-form solution，不需要划分development set。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">&#x27;./data/X_train&#x27;</span></span><br><span class="line">Y_train_fpath = <span class="string">&#x27;./data/Y_train&#x27;</span></span><br><span class="line">X_test_fpath = <span class="string">&#x27;./data/X_test&#x27;</span></span><br><span class="line">output_fpath = <span class="string">&#x27;./generative_output/output_&#123;&#125;.csv&#x27;</span></span><br><span class="line">fpath = <span class="string">&#x27;./generative_output/generative&#x27;</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_normalization</span>(<span class="params">X, train=<span class="literal">True</span>, X_mean=<span class="literal">None</span>, X_std=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;In generative model:\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;Size of training data: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(train_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Size of test set: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(test_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Dimension of data: &#123;&#125;\n\n&#x27;</span>.<span class="built_in">format</span>(data_dim))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Useful-functions"><a href="#Useful-functions" class="headerlink" title="Useful functions"></a>Useful functions</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_sigmod</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_f</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_predict</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_accuracy</span>(<span class="params">Y_pred, Y_label</span>):</span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.<span class="built_in">abs</span>(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="公式再推导"><a href="#公式再推导" class="headerlink" title="公式再推导"></a>公式再推导</h3><p>计算公式： </p>

$$
\begin{equation}\begin{aligned}P\left(C_{1} | x\right)&=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}\\&=\frac{1}{1+\frac{P\left(x | C_{2}\right) P\left(C_{2}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)}}\\&=\frac{1}{1+\exp (-z)} =\sigma(z)\qquad(z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}\end{aligned}\end{equation}
$$


<p>计算z的过程：</p>
<ol>
<li>首先计算Prior Probability。</li>
<li>假设模型是Gaussian的，算出 $\mu_1,\mu_2 ,\Sigma$  的closed-form solution 。</li>
<li>根据 $\mu_1,\mu_2,\Sigma$ 计算出 $w,b$ 。</li>
</ol>
<hr>
<ol>
<li><p><strong>计算Prior Probability。</strong> </p>
<p>程序中用list comprehension处理较简单。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>计算 $\mu_1,\mu_2 ,\Sigma$ （Gaussian）</p>
<p>$\mu_0=\frac{1}{C0} \sum_{n=1}^{C0} x^{n} $  (Label=0)</p>
<p>$\mu_1=\frac{1}{C1} \sum_{n=1}^{C1} x^{n} $  (Label=0)</p>
<p>$\Sigma_0=\frac{1}{C0} \sum_{n=1}^{C0}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$  (<strong>注意</strong> ：这里的 $x^n,\mu$ 都是行向量，注意转置的位置）</p>
<p>$\Sigma_1=\frac{1}{C1} \sum_{n=1}^{C1}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$ </p>
<p>$\Sigma=(C0 \times\Sigma_0+C1\times\Sigma_1)/(C0+C1)$   (shared covariance) </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="3">
<li><p>计算 $w,b$ </p>
<p>在 <a href="/2020/03/20/Classification1/" title="这篇博客">这篇博客</a>中的第2小节中的公式推导中， $x^n,\mu$ 都是列向量，公式如下：</p>

   $$
   z=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}
   $$
   
  $w^T=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} \qquad b=-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$ 

<hr>
<p><strong>但是</strong> ，一般我们在处理的数据集，$x^n,\mu$ 都是行向量。推导过程相同，公式如下：</p>
<p><font color=#f00> <strong>（主要注意转置和矩阵乘积顺序）</strong> </font></p>

   $$
   z=x\cdot \Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  -\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}
   $$
   
  $w=\Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  \qquad b=-\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}$ 

</li>
</ol>
<hr>
<p><font color=#f00>但是，协方差矩阵的逆怎么求呢？ </font> </p>
<p>numpy中有直接求逆矩阵的方法(np.linalg.inv)，但当该矩阵是nearly singular，是奇异矩阵时，就会报错。</p>
<p>而我们的协方差矩阵（510*510）很大，很难保证他不是奇异矩阵。</p>
<p>于是，有一个 <del>牛逼</del> 强大的数学方法，叫SVD(singular value decomposition, 奇异值分解) 。</p>
<p>原理步骤我……还没有完全搞清楚QAQ（先挖个坑）[1]</p>
<p>利用SVD，可以将任何一个矩阵（即使是奇异矩阵），分界成 $A=u s v^T$ 的形式：其中u,v都是标准正交矩阵，s是对角矩阵。（numpy.linalg.svd方法实现了SVD）</p>
<p><font color=#f00>可以利用SVD求矩阵的伪逆 </font> </p>
<ul>
<li>$A=u s v^T$<ul>
<li>u,v是标准正交矩阵，其逆矩阵等于其转置矩阵</li>
<li>s是对角矩阵，其”逆矩阵“<strong>（注意s矩阵的对角也可能有0元素）</strong> 将非0元素取倒数即可。</li>
</ul>
</li>
<li>$A^{-1}=v s^{-1} u$</li>
</ul>
<p>计算 $w,b$ 的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (-<span class="number">0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(<span class="built_in">float</span>(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><p> accuracy结果：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">0.8756450899439694</span></span><br></pre></td></tr></table></figure>

<p>也将权重较大的feature输出看看：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">age: [-<span class="number">0.51867291</span>]</span><br><span class="line"> Masters degree(MA MS MEng MEd MSW MBA): [-<span class="number">0.49912643</span>]</span><br><span class="line"> Spouse of householder: [<span class="number">0.49786805</span>]</span><br><span class="line">weeks worked <span class="keyword">in</span> year: [-<span class="number">0.44710924</span>]</span><br><span class="line"> Spouse of householder: [-<span class="number">0.43305697</span>]</span><br><span class="line">capital gains: [-<span class="number">0.42608727</span>]</span><br><span class="line">dividends from stocks: [-<span class="number">0.41994666</span>]</span><br><span class="line"> Doctorate degree(PhD EdD): [-<span class="number">0.39310961</span>]</span><br><span class="line">num persons worked <span class="keyword">for</span> employer: [-<span class="number">0.37345994</span>]</span><br><span class="line"> Prof school degree (<span class="built_in">MD</span> DDS DVM LLB JD): [-<span class="number">0.35594107</span>]</span><br></pre></td></tr></table></figure>



<h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><p>具体数据集和代码，欢迎光临小透明<a target="_blank" rel="noopener" href="https://github.com/f1ed/ML-HW2">GitHub</a> </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">&#x27;./data/X_train&#x27;</span></span><br><span class="line">Y_train_fpath = <span class="string">&#x27;./data/Y_train&#x27;</span></span><br><span class="line">X_test_fpath = <span class="string">&#x27;./data/X_test&#x27;</span></span><br><span class="line">output_fpath = <span class="string">&#x27;./generative_output/output_&#123;&#125;.csv&#x27;</span></span><br><span class="line">fpath = <span class="string">&#x27;./generative_output/generative&#x27;</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_normalization</span>(<span class="params">X, train=<span class="literal">True</span>, X_mean=<span class="literal">None</span>, X_std=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;In generative model:\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;Size of training data: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(train_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Size of test set: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(test_size))</span><br><span class="line">    f.write(<span class="string">&#x27;Dimension of data: &#123;&#125;\n\n&#x27;</span>.<span class="built_in">format</span>(data_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment">########################</span></span><br><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_sigmod</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_f</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_predict</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_accuracy</span>(<span class="params">Y_pred, Y_label</span>):</span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.<span class="built_in">abs</span>(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line"><span class="comment"># Generative Model: closed-form solution, can be computed directly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (-<span class="number">0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(<span class="built_in">float</span>(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute accuracy on training set</span></span><br><span class="line">Y_train_pred = <span class="number">1</span> - _predict(X_train, w, b)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;\nTraining accuracy: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(_accuracy(Y_train_pred, Y_train)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = <span class="number">1</span> - _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(output_fpath.<span class="built_in">format</span>(<span class="string">&#x27;generative&#x27;</span>), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;id, label\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(predictions):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;, &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(i, label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output the most significant weight</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">ind = np.argsort(np.<span class="built_in">abs</span>(np.concatenate(w)))[::-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fpath, <span class="string">&#x27;a&#x27;</span>)<span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>:<span class="number">10</span>]:</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;: &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(content[i], w[i]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li>SVD原理，待补充</li>
</ol>
</div><div class="article-licensing box"><div class="licensing-title"><p>「机器学习-李宏毅」：HW2-Binary Income Predicting</p><p><a href="https://f7ed.com/2020/04/15/ml-lee-hw2/">https://f7ed.com/2020/04/15/ml-lee-hw2/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>f7ed</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-04-15</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2020-07-03</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a><a class="icons" rel="noopener" target="_blank" title="Share Alike" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons-sa"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Machine-Learning/">Machine-Learning, </a><a class="link-muted" rel="tag" href="/tags/Classification/">Classification </a></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/qrcode_wechat.jpg" alt="Wechat"></span></a><a class="button donate" href="https://www.buymeacoffee.com/f7ed" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/qrcode_alipay.jpg" alt="Alipay"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/04/18/DL-introdunction/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">「机器学习-李宏毅」：Deep Learning-Introduction</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/04/06/ml-lee-hw1/"><span class="level-item">「机器学习-李宏毅」:HW1-Predict PM2.5</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "f014ddaacda47a311edb6fbacdc729d7",
            repo: "f7ed.github.io",
            owner: "f7ed",
            clientID: "ec59f5258ac0ec443907",
            clientSecret: "f092b308c3e1b46327481c3547ee0dd7fc1bda10",
            admin: ["f7ed"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "en",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/profile.png" alt="f7ed"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">f7ed</p><p class="is-size-6 is-block">热爱可抵漫长岁月。</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">70</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">15</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">135</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="mailto:f7edliu@outlook.com" target="_blank" rel="noopener">Email me</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/f7ed"><i class="fab fa-github"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Task-introduction-and-Dataset"><span class="level-left"><span class="level-item">1</span><span class="level-item">Task introduction and Dataset</span></span></a></li><li><a class="level is-mobile" href="#Feature-Format"><span class="level-left"><span class="level-item">2</span><span class="level-item">Feature Format</span></span></a></li><li><a class="level is-mobile" href="#Logistic-Regression"><span class="level-left"><span class="level-item">3</span><span class="level-item">Logistic Regression</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Prepare-data"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Prepare data</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#normalize"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">normalize</span></span></a></li><li><a class="level is-mobile" href="#Development-set-split"><span class="level-left"><span class="level-item">3.1.2</span><span class="level-item">Development set split</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Useful-function"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Useful function</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#shuffle-X-Y"><span class="level-left"><span class="level-item">3.2.1</span><span class="level-item">_shuffle(X, Y)</span></span></a></li><li><a class="level is-mobile" href="#sigmod-z"><span class="level-left"><span class="level-item">3.2.2</span><span class="level-item">_sigmod(z)</span></span></a></li><li><a class="level is-mobile" href="#f-X-w-b"><span class="level-left"><span class="level-item">3.2.3</span><span class="level-item">_f(X, w, b)</span></span></a></li><li><a class="level is-mobile" href="#predict-X-w-b"><span class="level-left"><span class="level-item">3.2.4</span><span class="level-item">_predict(X, w, b)</span></span></a></li><li><a class="level is-mobile" href="#accuracy-Y-pred-Y-label"><span class="level-left"><span class="level-item">3.2.5</span><span class="level-item">_accuracy(Y_pred, Y_label)</span></span></a></li><li><a class="level is-mobile" href="#cross-entropy-loss-y-pred-Y-label"><span class="level-left"><span class="level-item">3.2.6</span><span class="level-item">_cross_entropy_loss(y_pred, Y_label)</span></span></a></li><li><a class="level is-mobile" href="#gradient-X-Y-label-w-b"><span class="level-left"><span class="level-item">3.2.7</span><span class="level-item">_gradient(X, Y_label, w, b)</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Training-Adagrad"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">Training (Adagrad)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Adagrad"><span class="level-left"><span class="level-item">3.3.1</span><span class="level-item">Adagrad</span></span></a></li><li><a class="level is-mobile" href="#Loss-amp-accuracy"><span class="level-left"><span class="level-item">3.3.2</span><span class="level-item">Loss &amp; accuracy</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Code"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">Code</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Generative-Model"><span class="level-left"><span class="level-item">4</span><span class="level-item">Generative Model</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Prepare-data-1"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Prepare data</span></span></a></li><li><a class="level is-mobile" href="#Useful-functions"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Useful functions</span></span></a></li><li><a class="level is-mobile" href="#Training"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">Training</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#公式再推导"><span class="level-left"><span class="level-item">4.3.1</span><span class="level-item">公式再推导</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Accuracy"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">Accuracy</span></span></a></li><li><a class="level is-mobile" href="#Code-1"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">Code</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">5</span><span class="level-item">Reference</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/f1ed_logo.png" alt="fred&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2025 f7ed</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent " target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="f7ed&#039;s GitHub" href="https://github.com/f7ed"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script>
      var sc_project=12961083;
      var sc_invisible=1;
      var sc_security="ad3fb575";
      var sc_https=1;
      var sc_remove_link=1;</script><script src="https://www.statcounter.com/counter/counter.js" async></script><noscript><div class="statcounter"><img class="statcounter" src="https://c.statcounter.com/12961083/0/ad3fb575/1/" alt="real time web analytics"></div></noscript><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>