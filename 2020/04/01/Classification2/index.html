<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>「机器学习-李宏毅」:Classification-Logistic Regression - fred&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="fred&#039;s blog"><meta name="msapplication-TileImage" content="/img/heart.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="fred&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="在上篇文章中，讲解了怎么用Generative Model做分类问题。这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。文章主要有两部分：第一部分讲解了Logistic Regression的三个步骤。第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。"><meta property="og:type" content="blog"><meta property="og:title" content="「机器学习-李宏毅」:Classification-Logistic Regression"><meta property="og:url" content="https://f7ed.com/2020/04/01/Classification2/"><meta property="og:site_name" content="fred&#039;s blog"><meta property="og:description" content="在上篇文章中，讲解了怎么用Generative Model做分类问题。这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。文章主要有两部分：第一部分讲解了Logistic Regression的三个步骤。第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://f7ed.com/gallery/thumbnails/0321.jpg"><meta property="article:published_time" content="2020-03-31T16:00:00.000Z"><meta property="article:modified_time" content="2020-11-24T13:10:31.379Z"><meta property="article:author" content="f7ed"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="公开课"><meta property="article:tag" content="Classification"><meta property="article:tag" content="Logistic Regression"><meta property="article:tag" content="Softmax"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/thumbnails/0321.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://f7ed.com/2020/04/01/Classification2/"},"headline":"「机器学习-李宏毅」:Classification-Logistic Regression","image":["https://f7ed.com/gallery/thumbnails/0321.jpg"],"datePublished":"2020-03-31T16:00:00.000Z","dateModified":"2020-11-24T13:10:31.379Z","author":{"@type":"Person","name":"f7ed"},"publisher":{"@type":"Organization","name":"fred's blog","logo":{"@type":"ImageObject","url":"https://f7ed.com/img/f1ed_logo.png"}},"description":"在上篇文章中，讲解了怎么用Generative Model做分类问题。这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。文章主要有两部分：第一部分讲解了Logistic Regression的三个步骤。第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。"}</script><link rel="canonical" href="https://f7ed.com/2020/04/01/Classification2/"><link rel="icon" href="/img/heart.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-M5KG3CQTSF" async></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-M5KG3CQTSF');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="fred's blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/f1ed_logo.png" alt="fred&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/liu">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/f7ed"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-bars"></i>「机器学习-李宏毅」:Classification-Logistic Regression</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2020-03-31T16:00:00.000Z" title="2020-03-31T16:00:00.000Z">2020-04-01</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2020-11-24T13:10:31.379Z" title="2020-11-24T13:10:31.379Z">2020-11-24</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/">机器学习-李宏毅</a></span><span class="level-item"><i class="far fa-clock"></i> 15 minutes read (About 2260 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><p>在上篇文章中，讲解了怎么用Generative Model做分类问题。<br>这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。<br>文章主要有两部分：<br>第一部分讲解了Logistic Regression的三个步骤。<br>第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="more"></span>


<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h2 id="Step1-Function-Set"><a href="#Step1-Function-Set" class="headerlink" title="Step1: Function Set"></a>Step1: Function Set</h2><p>在<a href="#">Post not found: Classification 上一篇</a>文章末尾，我们得出 $P_{w, b}\left(C_{1} | x\right)=\sigma(w\cdot x+b)$ 的形式，想跳过找 $\mu_1,\mu_2,\Sigma$ 的过程，直接找 $w,b$ 。</p>
<p>因此Function Set: $f_{w, b}(x)=P_{w, b}\left(C_{1} | x\right)$ 。值大于0.5，则属于C1类，否则属于C2类。</p>
<img src="https://s1.ax1x.com/2020/04/01/G8TlKf.md.png" alt="G8TlKf.md.png" style="zoom:67%;" />

<h2 id="Step2-Goodness-of-a-Function"><a href="#Step2-Goodness-of-a-Function" class="headerlink" title="Step2: Goodness of a Function"></a>Step2: Goodness of a Function</h2><p>使用极大似然的思想（在前一篇机率模型/生成模型中有讲）</p>
<p>估计函数是 ：$L(w, b)=f_{w, b}\left(x^{1}\right) f_{w, b}\left(x^{2}\right)\left(1-f_{w, b}\left(x^{3}\right)\right) \cdots f_{w, b}\left(x^{N}\right)$ </p>
<p>目标：  $ w^{*}, b^{*}=\arg \max _{w, b} L(w, b)$ </p>
<p>由于在之前的Regression中，我们都是找极小值点，为了方便处理，将估计函数转换为如下形式的<strong>损失函数：</strong> </p>

$$
\begin{equation}
\begin{aligned}
 -\ln L(w, b)&=-(\ln f_{w, b}\left(x^{1}\right)+\ln f_{w, b}\left(x^{2}\right)+\ln \left(1-f_{w, b}\left(x^{3}\right)\right) \cdots )
\\ Loss&=\sum_{n}-\left[\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)\right]
\end{aligned}
\end{equation}
$$

<p><strong>目标</strong> ： $w^{*}, b^{*}=\arg \min _{w, b} L(w, b)$ </p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy</a>（交叉熵）</p>
<p>关于熵、交叉熵、相对熵（KL散度）的理解：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ErfnhcEV1O8">强烈安利</a> </p>
<p>上式中的 $\left[\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)\right]$ 其实是两个Bernoulli distribution的交叉熵。</p>
<p>交叉熵是什么？ 简单来说，交叉熵是评估两个distribution 有多接近。所以当这两个Bernoulli 分布的交叉熵为0时，表明这两个分布一模一样。</p>
<p>对于 $\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)$ ：</p>
<p>Distribution p: p(x = 1) = $\hat{y}^n$ ; p( x = 0 ) = 1 - $\hat{y}^n$ </p>
<p>Distribution q: q(x = 1 ) =  $f(x^n)$ ; q(x = 0 ) = 1 - $f(x^n)$ </p>
<p>交叉熵  $H(p,q)=-\Sigma_xp(x)\ln(q(x))$ </p>
<p>p是真实的分布，q是预测的分布。</p>
</blockquote>
<p>因此，这个损失函数的表达式其实也是输出分布和target分布的交叉熵，即：</p>
<p>$L(f)=\sum_{n} C\left(f\left(x^{n}\right), \hat{y}^{n}\right)$ </p>
<p>（ $C\left(f\left(x^{n}\right), \hat{y}^{n}\right)=-\left[\hat{y}^{n} \ln f\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f\left(x^{n}\right)\right)\right]$  ）</p>
<hr>
<p>和Linear Regression不同，为什么Logistic Regression不用square error，而要使用cross entropy。</p>
<p>在1.4小节会给出解释。</p>
<h2 id="Step3-Find-the-best-function"><a href="#Step3-Find-the-best-function" class="headerlink" title="Step3: Find the best function"></a>Step3: Find the best function</h2><p>在第三步，同样使用Gradient来寻找最优函数。</p>
<p>推导过程：</p>
<ul>
<li><p>$\left.\frac{-\ln L(w, b)}{\partial w_{i}}=\sum_{n}-\left[\hat{y}^{n} \frac{\ln f_{w, b}\left(x^{n}\right)}{\partial w_{i}}+\left(1-\hat{y}^{n}\right) \frac{\ln \left(1-f_{w, b}\left(x^{n}\right)\right.}{\partial w_{i}}\right)\right]$ </p>
<ul>
<li><p>$\frac{\partial \ln f_{w, b}(x)}{\partial w_{i}}=\frac{\operatorname{\partial\ln} f_{w, b}(x)}{\partial z} \frac{\partial z}{\partial w_{i}}$ </p>
<ul>
<li>$\frac{\partial \ln \sigma(z)}{\partial z}=\frac{1}{\sigma(z)} \frac{\partial \sigma(z)}{\partial z}=\frac{1}{\sigma(z)} \sigma(z)(1-\sigma(z))$  </li>
<li>$\frac{\partial z}{\partial w_{i}}=x_{i}$ </li>
</ul>
</li>
<li><p>$\frac{\partial \ln \left(1-f_{w, b}(x)\right)}{\partial w_{i}}=\frac{\operatorname{\partial\ln}\left(1-f_{w, b}(x)\right)}{\partial z} \frac{\partial z}{\partial w_{i}}$ </p>
<ul>
<li>$\frac{\partial \ln (1-\sigma(z))}{\partial z}=-\frac{1}{1-\sigma(z)} \frac{\partial \sigma(z)}{\partial z}=-\frac{1}{1-\partial(z)} \sigma(z)(1-\sigma(z))$ </li>
<li>$\frac{\partial z}{\partial w_{i}}=x_{i}$ </li>
</ul>
</li>
<li><p>$\frac{\partial \sigma(z)}{\partial z}=\sigma(z)\cdot(1-\sigma(z))$ </p>
<img src="https://s1.ax1x.com/2020/04/01/G8TMxP.png" alt="G8TMxP.png" style="zoom:33%;" />      
</li>
</ul>
</li>
<li><p>注：$f_{w, b}(x)=\sigma(z)$   ; $z=w \cdot x+b=\sum_{i} w_{i} x_{i}+b$  </p>
</li>
<li> $$
  \begin{equation}
  \begin{aligned}
  \frac{-\ln L(w, b)}{\partial w_{i}}&=\sum_{n}-\left[\hat{y}^{n}\left(1-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}-\left(1-\hat{y}^{n}\right) f_{w, b}\left(x^{n}\right) x_{i}^{n}\right]
  \\&=\sum_{n}-\left(\hat{y}^{n}-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}
  \end{aligned}
  \end{equation}
  $$ 

</li>
</ul>
<p>因此Logistic Regression的损失函数的导数和Linear Regression的一样。</p>
<p>迭代更新： $w_{i} \leftarrow w_{i}-\eta \sum_{n}-\left(\hat{y}^{n}-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}$ </p>
<h2 id="与Linear-Regression-的对比"><a href="#与Linear-Regression-的对比" class="headerlink" title="与Linear Regression 的对比"></a>与Linear Regression 的对比</h2><p>如图所示。</p>
<img src="https://s1.ax1x.com/2020/04/01/G8Tu8I.md.png" alt="G8Tu8I.md.png" style="zoom:67%;" /> 

<h2 id="If-Logistic-Square-Error"><a href="#If-Logistic-Square-Error" class="headerlink" title="If : Logistic + Square Error"></a>If : Logistic + Square Error</h2><p>前面一小节我们提到，在Logistic Regression中使用cross entropy判别一个函数的好坏,那为什么不使用square error来judge the goodness？</p>
<p>如果使用 Square Error的方法，步骤如下：</p>
<img src="https://s1.ax1x.com/2020/04/01/G8TnPA.md.png" alt="G8TnPA.md.png" style="zoom:60%;" /> 

<p>来看Step 3: 损失函数的导数是 $2\left(f_{w, b}(x)-\hat{y}\right) f_{w, b}(x)\left(1-f_{w, b}(x)\right) x_{i}$ </p>
<p>考虑 $\hat{y}^n=1$ （即我们的target是1）：</p>
<ul>
<li>如果  $f_{w,b}(x^n)=1$ , 即预测值接近 target, 算出来的 $\partial{L}/\partial{w_i}=0$ 是期望的。</li>
<li>如果  $f_{w,b}(x^n)=0$ , 即预测值原理 target, 算出来的 $\partial{L}/\partial{w_i}=0$ 是不期望的。</li>
</ul>
<p>同理，当考虑 $\hat{y}^n=0$ 情况时，也是如此。</p>
<p>更直观的看：</p>
<img src="https://s1.ax1x.com/2020/04/01/G8Te5d.md.png" alt="G8Te5d.md.png" style="zoom:70%;" />

<p>上图中，画出了两种损失函数的平面，中心的最低点是我们的target。</p>
<p>但在Square Error中，远离target的蓝色点，也处在很平坦的位置，其导数小，参数的更新会很慢。</p>
<p>因此在Cross Entropy中，离target越远，其导数更大，更新更快。</p>
<p>所以Cross Entropy的效果比Square Error更快，效果更好。</p>
<h1 id="Discriminative-V-S-Generative"><a href="#Discriminative-V-S-Generative" class="headerlink" title="Discriminative V.S. Generative"></a>Discriminative V.S. Generative</h1><p>这篇文章中的Logistic Regression是Discriminative Model。</p>
<a href="/2020/03/20/Classification1/" title="上篇文章">上篇文章</a>中Classification是Generative Model。

<p>有什么区别呢？</p>
<img src="https://s1.ax1x.com/2020/04/01/G8TVVe.md.png" alt="G8TVVe.md.png" style="zoom:60%;" />

<p>上图中，Generative Model做了假设（脑补），假设它是 Gaussian Distribution，假设它是Bernoulli Distribution。然后去找这些分布的参数，在求出 $w,b$。</p>
<p>而在Discriminative Model中，没有做任何假设，直接找 $w,b$ 参数。</p>
<p>所以，这两种Model经过training找出来的参数一样吗？</p>
<p>答案是不一样的。</p>
<p>The same model(function set), but different function is selected by the same training data.</p>
<p>在上篇Pokemon的例子中，比较两种方法的结果差异。</p>
<img src="https://s1.ax1x.com/2020/04/01/G8TAbD.md.png" alt="G8TAbD.md.png" style="zoom:70%;" />

<p>可见，在Pokemon的例子总，Discriminative的效果比Generative的效果好一些。</p>
<hr>
<p>但是Generative Model就不好吗？</p>
<p><strong>Benefit of generative model</strong></p>
<ul>
<li><p>With the assumption of probability distribution, less training data is needed.</p>
<p>【训练生成模型所需数据更少】 </p>
</li>
<li><p>With the assumption of probability distribution, more robust to the noise.</p>
<p>【生成模型对noise data更兼容】</p>
</li>
<li><p>Priors and class-dependent probabilities can be estimated from different sources.</p>
<p>【生成模型中的 先验概率Priors 和 基于类别的分布概率不同】</p>
<p>比如，做语音辨识系统，整个系统是generative的。</p>
<p>因为Prior（某一句话的概率）并不需要从data中知道，可以直接在网络上爬虫统计。</p>
<p>而class-dependent probabilities（这段语音是这句话的概率）需要data进行训练才能得知。</p>
</li>
</ul>
<h1 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h1><h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><img src="https://s1.ax1x.com/2020/04/01/G8TkDO.md.png" alt="G8TkDO.md.png" style="zoom:67%;" />

<p>假设有三个类别：C1、C2、C3 。模型已经得到，参数分别是 w、b。</p>
<p>对于输入x, 判断x属于哪一个类别。</p>
<ol start="0">
<li>通过每个类别的 w、b求出 $z^i=w^i\cdot x+b_i$  </li>
</ol>
<p>Softmax的步骤：</p>
<ol>
<li>exponential：每个z值得到 $=e^z$ .</li>
<li>sum：将指数化后的值加起来$=\Sigma_{j=1}^3e^{z_j}$ </li>
<li>output: 每个类别的输出 $y_i=e^{z_1}/\Sigma_{j=1}^3e^{z_j}$  ，即x属于类别i的概率。</li>
</ol>
<p>求出的 $1&gt;y_i&gt;0$ 且 $\Sigma_iy_i=1$ 。</p>
<p>通过Softmax，得到 $y_i=P(C_i|x)$ 。</p>
<h2 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h2><p>（手写笔记，略倾斜，原来不切一切还不知道自己歪的这么厉害 泪）</p>
<h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1:"></a>Step 1:</h3><img src="https://s1.ax1x.com/2020/04/01/G8TFKK.md.png" alt="G8TFKK.md.png" style="zoom:77%;" /> 

<h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2:"></a>Step 2:</h3><img src="https://s1.ax1x.com/2020/04/01/G8TPv6.md.png" alt="G8TPv6.md.png" style="zoom:77%;" /> 

<h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3:"></a>Step 3:</h3><img src="https://s1.ax1x.com/2020/04/01/G8TCgx.md.png" alt="G8TCgx.md.png" style="zoom:77%;" /> 

<p>使用Stochastic Gradient（即每个样本更新一次）的话：</p>
<p>data: [x, $\hat{y}$ ] , $\hat{y}_i=1$ </p>
<p>更新 $w^j$ :</p>
<ol>
<li><p>$j=i$ :</p>
<p>$w^j \leftarrow w^j-\eta\cdot (y_i-1)\cdot x$ </p>
</li>
<li><p>$j\neq i$ :</p>
<p>$w^j \leftarrow w^j-\eta\cdot y_i\cdot x$  </p>
</li>
</ol>
<p>(下次一定，笔记写直一点！)</p>
<p>更为规范的推导见[1]</p>
<h1 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h1> <img src="https://s1.ax1x.com/2020/04/01/G8oLuT.md.png" alt="G8oLuT.md.png" style="zoom:67%;" /> 

<p>对于如上情况，Logistic Regression并不能进行分类，因为他的boundary 应该是线性的。</p>
<h2 id="Feature-Transforming"><a href="#Feature-Transforming" class="headerlink" title="Feature Transforming"></a>Feature Transforming</h2><p>如果对feature做转换后，就可以用Logistic Regression处理。</p>
<p>重定义feature， $x_1’$ :定义为到[0,0]的距离， $x_2’$ :定义为到[1,1]的距离。</p>
<p>于是图变成下图，即可用Logistic Regression进行分类。</p>
<img src="https://s1.ax1x.com/2020/04/01/G8oxUJ.md.png" alt="G8oxUJ.md.png" style="zoom:67%;" />

<p>但这样的做法，就不像人工智能了，因为Feature Transformation需要人来设计，而且较难设计。</p>
<h2 id="Cascading-logistic-regression-models"><a href="#Cascading-logistic-regression-models" class="headerlink" title="Cascading logistic regression models"></a>Cascading logistic regression models</h2><p>另一种做法是，将logistic regression连接起来。</p>
<img src="https://s1.ax1x.com/2020/04/01/G8TpCR.md.png" alt="G8TpCR.md.png" style="zoom:67%;" />

<p>上图中，左边部分的两个logistic regression就相当于在做Feature Transformation，右边部分相当于在做Classification。</p>
<p>而通过这种形式，将多个model连接起来，也就是大热的Neural Network。</p>
<img src="https://s1.ax1x.com/2020/04/01/G8T981.md.png" alt="G8T981.md.png" style="zoom:57%;" />

<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><p>Multi-class Classification推导：Bishop，P209-210</p>
</li>
<li><p>关于Entropy, Cross Entropy, KL-Divergence的理解：强烈安利：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ErfnhcEV1O8">https://www.youtube.com/watch?v=ErfnhcEV1O8</a> </p>
</li>
</ol>
</div><div class="article-licensing box"><div class="licensing-title"><p>「机器学习-李宏毅」:Classification-Logistic Regression</p><p><a href="https://f7ed.com/2020/04/01/Classification2/">https://f7ed.com/2020/04/01/Classification2/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>f7ed</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-04-01</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2020-11-24</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a><a class="icons" rel="noopener" target="_blank" title="Share Alike" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="icon fab fa-creative-commons-sa"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习, </a><a class="link-muted" rel="tag" href="/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/">公开课, </a><a class="link-muted" rel="tag" href="/tags/Classification/">Classification, </a><a class="link-muted" rel="tag" href="/tags/Logistic-Regression/">Logistic Regression, </a><a class="link-muted" rel="tag" href="/tags/Softmax/">Softmax </a></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/qrcode_wechat.jpg" alt="Wechat"></span></a><a class="button donate" href="https://www.buymeacoffee.com/f7ed" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/qrcode_alipay.jpg" alt="Alipay"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/04/06/ml-lee-hw1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">「机器学习-李宏毅」:HW1-Predict PM2.5</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/03/20/Classification1/"><span class="level-item">「机器学习-李宏毅」:Classification-Probabilistic Generative Model</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "ff5ba54815e491b8528496542791d70e",
            repo: "f7ed.github.io",
            owner: "f7ed",
            clientID: "ec59f5258ac0ec443907",
            clientSecret: "f092b308c3e1b46327481c3547ee0dd7fc1bda10",
            admin: ["f7ed"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "en",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/profile.png" alt="f7ed"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">f7ed</p><p class="is-size-6 is-block">热爱可抵漫长岁月。</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">70</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">15</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">135</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="mailto:f7edliu@outlook.com" target="_blank" rel="noopener">Email me</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/f7ed"><i class="fab fa-github"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Logistic-Regression"><span class="level-left"><span class="level-item">1</span><span class="level-item">Logistic Regression</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Step1-Function-Set"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Step1: Function Set</span></span></a></li><li><a class="level is-mobile" href="#Step2-Goodness-of-a-Function"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">Step2: Goodness of a Function</span></span></a></li><li><a class="level is-mobile" href="#Step3-Find-the-best-function"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Step3: Find the best function</span></span></a></li><li><a class="level is-mobile" href="#与Linear-Regression-的对比"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">与Linear Regression 的对比</span></span></a></li><li><a class="level is-mobile" href="#If-Logistic-Square-Error"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">If : Logistic + Square Error</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Discriminative-V-S-Generative"><span class="level-left"><span class="level-item">2</span><span class="level-item">Discriminative V.S. Generative</span></span></a></li><li><a class="level is-mobile" href="#Multi-class-classification"><span class="level-left"><span class="level-item">3</span><span class="level-item">Multi-class classification</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#softmax"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">softmax</span></span></a></li><li><a class="level is-mobile" href="#Steps"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Steps</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Step-1"><span class="level-left"><span class="level-item">3.2.1</span><span class="level-item">Step 1:</span></span></a></li><li><a class="level is-mobile" href="#Step-2"><span class="level-left"><span class="level-item">3.2.2</span><span class="level-item">Step 2:</span></span></a></li><li><a class="level is-mobile" href="#Step-3"><span class="level-left"><span class="level-item">3.2.3</span><span class="level-item">Step 3:</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Limitation-of-Logistic-Regression"><span class="level-left"><span class="level-item">4</span><span class="level-item">Limitation of Logistic Regression</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Feature-Transforming"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Feature Transforming</span></span></a></li><li><a class="level is-mobile" href="#Cascading-logistic-regression-models"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Cascading logistic regression models</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">5</span><span class="level-item">Reference</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/f1ed_logo.png" alt="fred&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2025 f7ed</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent " target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="f7ed&#039;s GitHub" href="https://github.com/f7ed"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script>
      var sc_project=12961083;
      var sc_invisible=1;
      var sc_security="ad3fb575";
      var sc_https=1;
      var sc_remove_link=1;</script><script src="https://www.statcounter.com/counter/counter.js" async></script><noscript><div class="statcounter"><img class="statcounter" src="https://c.statcounter.com/12961083/0/ad3fb575/1/" alt="real time web analytics"></div></noscript><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>