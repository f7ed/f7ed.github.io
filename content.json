{"pages":[{"title":"TRAPDOOR!","text":"Congratulations!! You find something wired. There is a hidden homepage on my website. The URL structure is https://f7ed/[key]/ and the SHA-256 of this key is: eb9e5c2a708dd6c4b57088c96553de8303cf431b5e42a0630625fa08d8dd6d6b Hint: You can use social engineering methods to reveal the key (´▽｀)","link":"/about/index.html"},{"title":"","text":"梅森素数 /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; max-height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 65%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } 梅森素数性质应用Modular ReductionMultiplicationReference性质本节研究 @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')an−1a^n-1an−1 素数。通过下表观察可得知：@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa 是奇数时，@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')an−1a^n-1an−1 是偶数。 所以@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa 只能为偶数。@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')an−1a^n-1an−1 总是被 @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')a−1a-1a−1 整除。证明：几何级数公式（Geometric Series）@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xn−1=(x−1)(xn−1+xn−2+⋯+x2+x+1)x^{n}-1=(x-1)\\left(x^{n-1}+x^{n-2}+\\cdots+x^{2}+x+1\\right)xn−1=(x−1)(xn−1+xn−2+⋯+x2+x+1) 展开右边乘法即可证明几何级数。所以@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa 只能为2。观察下表@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n−12^n-12n−1 的情况：不是所有@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n−12^n-12n−1 都是素数。如果@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn 被@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')mmm 整除，则@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n−12^n-12n−1 被@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2m−12^m-12m−1 整除。观察如果@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn 是偶数，@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n−12^n-12n−1 被@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')22−1=32^2-1=322−1=3 整除。如果@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn 被3整除，@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n−12^n-12n−1 被@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')23−1=72^3-1=723−1=7 整除。如果@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn 被5整除，@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n−12^n-12n−1 被@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')25−1=312^5-1=3125−1=31 整除。证明：同样利用几何级数公式，@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n=(2m)k2^n=(2^{m})^k2n=(2m)k @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n−1=(2m)k−1=(2m−1)((2m)k−1+(2m)k−2+⋯+(2m)2+(2m)+1)2^{n}-1=\\left(2^{m}\\right)^{k}-1=\\left(2^{m}-1\\right)\\left(\\left(2^{m}\\right)^{k-1}+\\left(2^{m}\\right)^{k-2}+\\cdots+\\left(2^{m}\\right)^{2}+\\left(2^{m}\\right)+1\\right)2n−1=(2m)k−1=(2m−1)((2m)k−1+(2m)k−2+⋯+(2m)2+(2m)+1) 所以@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn 一定是素数。命题1. 如果整数@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')a≥2a\\ge2a≥2 与@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')n≥2n\\ge2n≥2 ，如果@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')an−1a^n-1an−1 是素数，则@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa 一定等于2且@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn 一定是素数。Proposition 1. If @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')an−1a^n-1an−1 is prime for some numbers @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')a≥2a\\ge2a≥2 and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')n≥2n\\ge2n≥2 , then a must equal 2 and n must be a prime.不是所有的@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2p−12^p-12p−1 都是素数。应用在密码学中，有限域中的运算性能极大影响密码协议的实现。在这些运算中，逆运算是最复杂的，模运算、乘法次之。如果有限域选择梅森素数，得益于它的优良性质，可以极大提高运算效率，特别是有限域下的模运算、乘法操作。Modular Reduction根据性质：@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2p≡1(mod2p−1)2^p \\equiv 1 \\pmod{2^p-1}2p≡1(mod2p−1) 可得：@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')x=a⋅2p+b≡a+b(mod2p−1)x=a\\cdot 2^p +b\\equiv a+b \\pmod{2^p-1}x=a⋅2p+b≡a+b(mod2p−1) 如果将@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xxx 写作比特形式，模运算约减为将高位比特串和低位比特串相加。C++伪代码实现（以@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')p=261−1p=2^{61}-1p=261−1 为例）：代码#define MERSENNE_PRIME_EXP 61 const uint64_t PR = 2305843009213693951; //2^61-1 uint64_t mod(uint64_t x) { uint64_t i = (x &amp; PR) + (x &gt;&gt; MERSENNE_PRIME_EXP); return (i&gt;=p) ? i - p: i; }Multiplication以@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')p=261−1p=2^{61}-1p=261−1 为例，@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa 和@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bbb 相乘后，得到一个最多122-bit的数字，用同样的方法，将高位比特和低位比特相加。（如果结果更长，就按段一直叠加）在实现上可以使用64-bit乘法优化(可以用指令集加速)：代码/** * @brief 64-bit multiplication * * @param a * @param b * @param c [out]pointer to hign 64-bit * @return uint64_t [out]low 64-bit */ #if defined(__x86_64__) &amp;&amp; defined(__BMI2__) inline uint64_t mul64(uint64_t a, uint64_t b, uint64_t *c) { return _mulx_u64((unsigned long long)a, (unsigned long long)b, (unsigned long long*)c); } #else inline uint64_t mul64(uint64_t a, uint64_t b, uint64_t *c) { __uint128_t aa = a; __uint128_t bb = b; auto cc = aa*bb; // c is a pointer to high 64-bit *c = cc&gt;&gt;64; // return low 64-bit return (uint64_t)cc; } #endif inline uint64_t mult_mod(uint64_t a, uint64_t b) { uint64_t c = 0; uint64_t e = mul64(a, b, (uint64_t*)&amp;c); // c is hign 64-bit // e is low 64-bit uint64_t ret = (e &amp; PR) + ( (e&gt;&gt;MERSENNE_PRIME_EXP) ^ (c&lt;&lt; (64-MERSENNE_PRIME_EXP))); return (ret &gt;= PR) ? (ret-PR) : ret; }Reference数论概论（原书第4版本） A Friendly Introduction to Number Theory.代码参考于emp-zk.","link":"/notes/%5BMersenne-Prime%5D/index.html"},{"title":"","text":"[Naor-OT05]Computationally Secure Oblivious Transfer /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 90%; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 70%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } 📜[Naor-OT05]Computationally Secure Oblivious Transferkeywords: Cryptography, Privacy preserving computation, Secure function evaluation, Oblivious transferABSTRACT1 Introduction1.1 Oblivious Transfer1.2 Correctness and Security Definitions2 Protocols for 1-out-of-N Oblivious Transfer2.1 A Protocol for 1-out-of-N OTProtocolComplexityImprovement2.2 A Recursive Protocol for 1-out-of-N OTprotocolcomplexityABSTRACTcontributions: computationally secure protocols of 1-out-of-N OT: only log N executions of a 1-out-of-2 OT protocolk-out-of-N OT: more eff. than k repetitions of 1-out-of-N OToblivious transfer with adaptive queries.1 Introduction1.1 Oblivious TransferOblivious Transfer (OT)Note that this transformation takes DH-tuples to DH-tuples, and non-DH-tuples to non-DH-tuples. Furthermore, the new exponents are independent of the originals. This is easy to see if we start with a DH tuple.https://crypto.stanford.edu/pbc/notes/crypto/ot.html1.2 Correctness and Security DefinitionsThe Receiver's Security:receiver: 计算上不可区分获得的值和随机值The Sender's Security: sender: 对于sender，要求real implementation of the protocol 和 ideal model不可区分，即不可以获得比ideal model 下更多的信息 ideal implementation: a trusted partyformal def:2 Protocols for 1-out-of-N Oblivious Transferassumption: block ciphers or keyed one-way hash functions can be modeled as PRF.PRF @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')FK(x)F_K(x)FK​(x) : block cipher with key K and encrypting xkeying a hash function with K and applying it to x main idea of 1-out-of-N: use a small set of keys(logN) and mask each input with a combination of a different subset of the keys 用logN个不同的keys，通过不同的密钥组合来加密不同的输入p.s.: the keys are not applied directly(insecure) input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')XIX_IXI​ : the value @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')FK(I)F_K(I)FK​(I) is used for masking 不能直接把密钥应用到明文上加密（比如异或的形式）有两种实现1-out-of-N OT的协议： 一种是使用logN次 1-out-of-2 OT协议 另一种是使用递归式的方法：将1-out-of-N分为两个1-out-of-@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N\\sqrt{N}N​ OT两个协议中最主要的是transfer stage中调用1-out-of-2。 不过后一种协议再初始化阶段是O(N)的复杂度，另一个是O(NlogN)的复杂度。2.1 A Protocol for 1-out-of-N OTProtocolsender B input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')X1,X2,...,XNX_1,X_2,...,X_NX1​,X2​,...,XN​ , receiver want's to learn @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')XIX_IXI​ sender B prepares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')lll random pairs of keys B准备logN个密钥对，每个密钥对中一个对应比特0，一个对应比特1。对每一个@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')XIX_IXI​ 都做mask操作，具体来说就是根据@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')III 的比特位来选择密钥，用PRF的output依次mask。A and B engage in a 1-out-of-2 OT on @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')&lt;Kj0,Kj1&gt;&lt;K_j^0,K_j^1&gt;&lt;Kj0​,Kj1​&gt; A和B执行logN次 1-out-of-2 OT，得到A的选择@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')III 对应比特位的密钥。B sends to A the string Y B向A发送N个加密后的Y = E(X)。A reconstructs X A只有他所选择值的密钥，只能解密出其中一个。Complexitypreprocessing(step 1): @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Nlog⁡NN\\log{N}NlogN evaluations of the pseudo-random function FK 每一个X都要logN个密钥对其mask。transfer stage: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')log⁡N\\log{N}logN invocations of the 1-out-of-2 OT protocol invocation: a request for helpImprovement2.2 A Recursive Protocol for 1-out-of-N OTprotocolsender B input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')X1,X2,...,XNX_1,X_2,...,X_NX1​,X2​,...,XN​ , receiver want's to learn @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')XIX_IXI​ B prepares two sets of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N\\sqrt{N}N​ randomly chosen keys. B arranges the N inputs in a @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N×N\\sqrt{N}\\times\\sqrt{N}N​×N​ matrix. B准备两组sqrt(N)的随机密钥。再把这N个值组合为一个矩阵，这样就可以用行列来索引。B对每个值都用行列索引来加密。A and B engage in a 1-out-of-@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N\\sqrt{N}N​ OT protocol A 和B执行1-out-of-@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N\\sqrt{N}N​ OT，让A得到目标选择的行列密钥。B sends to A all the YA reconstructs X A只有目标选择对行列密钥，所以只能得到一个值。complexitytotal = 2N evaluations of FK for preprocessing + 2 invocations of the 1-out-of-sqrt(N) protocol for transfer2 1-out-of-sqrt(N) OTpreprocess: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2Nlog⁡N=Nlog⁡N2\\sqrt{N} \\log \\sqrt{N} = \\sqrt{N}\\log N2N​logN​=N​logN evaluations for FKtransfer stage: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2log⁡N=log⁡N2\\log\\sqrt{N} = \\log N2logN​=logN 1-out-of-2 OT 所以说两个协议调用1-out-of 2的次数是一样的totol invocations of PRF: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Nlog⁡N+2N\\sqrt{N}\\log N + 2NN​logN+2N total calls of 1-out-of-2 OT: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')log⁡N\\log NlogN 2.1协议被认为是 logn维密钥加密的，而2.2协议被认为是2维加密","link":"/paper/%5BNaor-OT05%5D/index.html"},{"title":"","text":"[Schneider13]GMW vs. Yao? Efficient Secure Two-Party Computation with Low Depth CItcuit /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 70%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } 📜 [Schneider13]GMW vs. Yao? Efficient Secure Two-Party Computation with Low Depth CItcuitKeywords: GMW protocol, optimizations, privacy-preserving face recognitionABSTRACT1 IntroductionContributions2 Preliminaries2.1 Oblivious Transfer2.2 Approaches for Secure Two-Party Computation2.2.1 Yao's Garbled Circuits Protocol2.2.2 GMW Protocol2.3 Evaluation Metrics and Notation3 Circuit Constructions with low Depth and Size3.1 Addition3.1.1 Ladner-Fscher Adder.3.1.2 Carry-Save Adder3.2 Squaring3.3 Comparison3.4 Hamming Weight4 Optimizations for Two Party GMWBenchmarking EnvironmentTable 1: Time improvements4.1 Multiplication Triples4.2 Using ASE instead of SHA as Pseudo-Random FunctionTable 2: Comparison of SHA-1 and AES128 implementation4.3 Load Balancing4.4 Implementation-Specific Optimizations4.5 Single Instruction Multiple Data (SIMD) OperationsABSTRACT提出了两种方案：Yao's garbled circuits 和GMW协议。 但因为Yao但方案有constant round complexity，所以大多数研究都表明其有更好的效率。这篇文章提出了一些GMW协议的优化。 总结了depth-optimized circuit constructions. 还考虑了隐私保护的人脸识别(privacy-preserving face recognition)1 IntroductionYao's garbled circuits, GMW: 都把函数表示为Boolean circuit，都提供security against semi-honest adversaries.semi-honest adversaries: honestly follow the protocol but try to learn additional information from observed messages.Yao's protocol:a constant number of roundsrequires OTs only for the input of one of the parties 只有一方的输入需要OTsecure evaluation of a gate: requires only symmetric cryptographic operations 计算门时只需要对称密码技术GMW protocol: requires an interactive OT for each AND gate 对每一个AND门都要求交互式的OT协议ContributionsGMW协议在two semi-honest parties中是可行的。 此外，GMW协议相对于Yao's的协议有一些额外的优势。three2 Preliminaries2.1 Oblivious Transfer在1-out-of n @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')OTlm\\mathrm{OT}_l^mOTlm​ 中： S：提供m个n元组 R：提供m个选择 最后R会根据每一个选择得到元组中的一个数字。Naor-Pinkas OT protocol[25]Naor, M., Pinkas, B.: Computationally secure oblivious transfer. Journal of Cryptology 18(1), 1–35 (2005)secure against semi-honest adversariesunder Decisional Diffie-Hellman(DDH) assumptionin the random-oracle modelrequires both parties to perform @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')O(m)\\mathcal{O(m)} O(m) modular exponentiations有两种加速OT的方法OT pre-computations[2]Beaver, D.: Precomputing oblivious transfer. In: Advances in Cryptology - CRYPTO’95. LNCS, vol. 963, pp. 97–109. Springer (1995)offline: pre-compute the OTs on random inputsonline: use pre-computed values as one-time pads to run the OT on the actual inputs.R: sends one message of m @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')log⁡2n\\log_2nlog2​n bits to S（隐藏选择）S: sends back a message of size mnl bits OT extensions[16,22] 2.2 Approaches for Secure Two-Party Computation2.2.1 Yao's Garbled Circuits Protocol[33]Yao, A.C.: How to generate and exchange secrets. In: Foundations of Computer Science (FOCS’86). pp. 162–167. IEEE (1986)Yao's protocol: 1. non-interactively 2. has a constant number of roundscreator（电路生成方）：encrypt the function to be computedFor each gate:map the plain values to random-looking symmetric keysgenerate an encryption table 通过该表，可以用给定的input keys计算gate's output keytransmit the encrypted circuit and (creator's) encrypted inputs to evaluatorevaluator（电路计算方）obtains his encrypted input via 1-out-of 2 OT (evaluator's input key)use the encrypted inputs to evaluate the encrypted function gate by gatecreatorprovides a mapping from the encrypted output to plain outputsome extension 2.2.2 GMW Protocol[11]Goldreich, O.: Foundations of Cryptography, vol. 2: Basic Applications. Cambridge University Press (2004) [12]Goldreich, O., Micali, S., Wigderson, A.: How to play any mental game or a completeness theorem for protocols with honest majority. In: Symposium on Theory of Computing (STOC’87). pp. 218–229. ACM (1987)GMW protocol: interactively compute a function using secret-shared values share each input and intermediate wire to 2 parties用2-out-of-2 secret sharing sheme把value v 分享给两方，每一方得到的一个random-looking share @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')viv_ivi​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=v1⊕v2v = v_1\\oplus v_2v=v1​⊕v2​ XOR gates: securely evaluated locally by XORing the sharesAND gates: parties run an interactively protocoloutput wire: 把和output wire相关的shares发送给需要得到output的参与方，计算output AND gate: 两种实现方法Oblivious TransferMultiplication Triples[1] Goldreich, O., Micali, S., Wigderson, A.: How to play any mental game or a com- pleteness theorem for protocols with honest majority. In: Symposium on Theory of Computing (STOC’87). pp. 218–229. ACM (1987) 2.3 Evaluation Metrics and Notation因为Yao's和GMW都提供free XORs，因此只比较AND gatessize S(C) : the number AND gates in circuit Cdepth D(C): the maximum number of AND gatesother notations:3 Circuit Constructions with low Depth and Size3.1 Additionlinear size and depth[18]Ripple-carry adder: Kolesnikov, V., Sadeghi, A.R., Schneider, T.: Improved garbled circuit building blocks and applications to auctions and computing minima. In: Cryptology And Network Security (CANS’09). LNCS, vol. 5888, pp. 1–20. Springer (2009)3.1.1 Ladner-Fscher Adder.[21] Ladner-Fischer adder Ladner,R.E.,Fischer,M.J.:Parallelprefixcomputation.JournaloftheACM27(4), 831–838 (1980)in logarthmic depth 3.1.2 Carry-Save Adder[8] carry-save adder: Earle, L.G.: Latched carry-save adder. IBM Technical Disclosure Bulletin 7(10), 909–910 (1965)has linear size and constant depth 3.2 Squaringa squaring circuit is smaller by a factor of about 2 @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xjxi+xixj=2xixjx_jx_i+x_ix_j = 2x_ix_jxj​xi​+xi​xj​=2xi​xj​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xixi=xix_ix_i=x_ixi​xi​=xi​ 3.3 Comparisonequal: greater than:3.4 Hamming Weight 4 Optimizations for Two Party GMWan implementation of GMW [5]Choi, S.G., Hwang, K.W., Katz, J., Malkin, T., Rubenstein, D.: Secure multi-party computation of Boolean circuits with applications to privacy in on-line market- places. In: Cryptographers’ Track at the RSA Conference (CT-RSA’12). LNCS, vol. 7178, pp. 416–432. Springer (2012)currently fastest garbled circuits implementation [15] Huang, Y., Evans, D., Katz, J., Malka, L.: Faster secure two-party computation using garbled circuits. In: Security Symposium. USENIX (2011)[5]能在n≥3时高效实现GMW协议，对于n=2的情况，[5]认为他们的速度会比当前最快的garbled circuits实现慢两倍。 Benchmarking Environmentevaluate the time for: using average time for 100 executionssetup phase: Naor-Pinkas OTs: group @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Zp∗\\Z_p^*Zp∗​ with @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')∣p∣=512|p|=512∣p∣=512 bitOT extension: security parameter t = @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')808080 online phase: sharing of inputscircuit evaluationcombining output sharesoverall time:circuit constructionsetup phaseonline phase circuit: an unoprmized 512-bit multiplication circuit CS(C) = 800,227D(C)=38PC:Table 1: Time improvementslist the modifications in the order. each modification include the improvement of all previous optimizations.4.1 Multiplication Triples[1] multiplication triples Beaver, D.: Efficient multiparty protocols using circuit randomization. In: Ad- vances in Cryptology – CRYPTO’91. LNCS, vol. 576, pp. 420–432. Springer (1991)Beaver's MT: (instead of pre-computed OTs)slightly slower in setup phasemore efficient in online phasetime:4.2 Using ASE instead of SHA as Pseudo-Random FunctionSHA-1 in original implementation：instantiant the random oraclegenerate pseudo-random values in the OT extensionASE (CTR mode) as PRF:decreased the number of expensive hash function calls per AND gate 降低了每个AND门调用的hash函数次数receiver R: from 3.5 to 1sender S: from 4.5 to 4numer of calls to instantiate the PRF 实例化PRF需要调用AES的次数R: 3.1 AES calls per AND gateS: 0.65 AES calls per AND gatethe sender and the receiver have different computational workloadTable 2: Comparison of SHA-1 and AES128 implementation 4.3 Load Balancingrun the OTs in either direction, each party has the same workload 因为MT在online phase 是对称形式的，所以可以双方各自并行运行OT协议2.5 SHA-1 invoations per AND gate1.8 AES invocations per AND gaterun the Naor-Pinkas OT protocol for the seed OTs twice, which however amortizes fairly quickly 虽然运行了两次，但非常快。four parallel threads during setup phaseone for pseudo-random functionthe other for random oracle time:4.4 Implementation-Specific Optimizationsarithmetic for modular arithmeticsuse GMPbitwise processing order during OT extension and online phaseperform operations bytewise 按字节异或，而不是按位SHA-1 for the random oracleuse an assembler implementation of OpenSSLtime:4.5 Single Instruction Multiple Data (SIMD) Operations","link":"/paper/%5BSchneider13%5D/index.html"},{"title":"","text":"[GMW87]How To Play Any Mental Game /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 50%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } 📜[GMW87]How To Play Any Mental Game1 Introduction2 Preliminary Definitions2.1 Notation and Conventions for Probabilistic Algorithms2.2 Game Networks and Distributed Algorithms2.3 Adversaries4 Hints on How to Play Hm-game With Passive Adversaries4.1 A New and General OT ProtocolOur Protocol4.2 Strengthening Yao's Combined Oblivious Transfer4.3 The Tm-game Solver for passive adversariesCase 1: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ⋅c\\sigma\\cdot cσ⋅c Case 2: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ−1⋅c\\sigma^{-1}\\cdot cσ−1⋅c Case 3: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ⋅τ\\sigma\\cdot \\tauσ⋅τ 1 Introduction 2 Preliminary Definitions2.1 Notation and Conventions for Probabilistic Algorithmsnotions 2.2 Game Networks and Distributed AlgorithmsProbabilistic Distributed Alg.2.3 AdversariesPassive adversariespassive adversarycompute more than required don't want ro corrupt, but try to compute more than their due share of knowledgemsgs it sends and outputs are inaccordance to original programMalicious Adv.malicious adversarydeviated from original progarm in any actionnot only disrupte the privacy constraintbut also make the outcome different than in an ideal world图灵游戏在any number of passive adv. or with n/2 malicious adv. 都是playable. 4 Hints on How to Play Hm-game With Passive Adversaries4.1 A New and General OT ProtocolA more general and useful notion of OT has been proposed by Even, Goldreich and Lempel: 1-out-of-2 OTproposed the first omplementation of a 1-out-of-2 OT using public key cryptosystems.requires a quite strong set of assumpeions even when the adversaries are only passive. 即使是在passive adv.面前也需要strong assumptions.Our Protocol为了简化处理，1-out-of-2 OT 中的msg只有1bit B不能预测另一个值，A不能预测B的选择。input:A: a pair of bits (b0, b1) and their encryption.B: private input bit @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')α\\alphaα desired properties:B can read @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bαb_\\alphabα​ , but can't predict @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bα‾b_{\\overline{\\alpha}}bα​ A cannot predict @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')α\\alphaα (A) randomly select a trap-door function keep @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')f−1f^{-1}f−1 secret and sends f to B(B) randomly selects @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')x0,x1x_0, x_1x0​,x1​ sends A the pair (u, v)(A)computes(c0, c1) use c to mask b and sends(d0, d1) to B(B)computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bαb_\\alphabα​ 4.2 Strengthening Yao's Combined Oblivious TransferCOTA, B: respectively owning private inputs a and bg: any chosen function gA computes g(a, b) while B don't know what A has computed如果把a, b当作secrets，B obliviously transfer a prescribed combination of his and A's secret to A. 四行中只有一对的异或是密钥D5，其他都是密钥D6，解密0（AND gate) E1,E2,E5,E6和0，1的对应关系是public labelled E3,E4和0,1的关系是secretly labelled(B)generates a COT AND-gate keep all decoding alg. and all strings in the rows(B)gives A the second input-wire: b b可能是E3或E4，因为A不知道对应关系(A)get either D1, D2 according to value a by 1-out-of-2 OT, B will not know which alg. A got(A)easily compute the value of output-wire only A know AND(a,b)4.3 The Tm-game Solver for passive adversariesuse CTO as a subroutingwant to use COT as subrouting to construct a Tm-solver 想用COT作为Tm-solver的subroutingif two parties i and j use COT so that i will compute g(xi, yi) 通过COT,party i 可以计算g(xi, yi)，而party j 不知道。[Ba]D. Barrington, Bounded-Width Branhing Program Recognize Exactly Those Languages in NC, Proc. 18th STOC, 1986 pp 1-5用了一个symmetric group on 5 elements in the program:0和1都被分别编码为2个5-permutations其中的变量都在S5中程序中的每个操作都包含两个5-permutaions值都乘法，值可能是常量、变量或者变量的逆等。each party:(input)takes private bits and encodes it by a 5-permutaion @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ\\sigmaσ (divide @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ\\sigmaσ )selects n-1 random 5-permutaion and gives@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(i,σi)(i,\\sigma_i)(i,σi​) to party i i是index，最后结果计算时的乘积顺序sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σn\\sigma_nσn​ and gives @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(n,σn)(n,\\sigma_n)(n,σn​) to party neach variable @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ\\sigmaσ : each party x possesses an index permutation pair @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(x,σx)(x, \\sigma_x)(x,σx​) @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')∏x=1nσx=σ\\prod_{x=1}^n\\sigma_x=\\sigma∏x=1n​σx​=σ 通过以下指令，each party 可以计算最终结果的pieceCase 1: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ⋅c\\sigma\\cdot cσ⋅c sigma is a variable and c is a constant(each party) has a piece: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(x,σx)(x, \\sigma_x)(x,σx​) (party n) sets his new piece: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(n,σn⋅c)(n, \\sigma_n\\cdot c)(n,σn​⋅c) (all each party) leaves his piece untouchedthe ordered product of the new pieces is @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ⋅c\\sigma\\cdot cσ⋅c pieces按照顺序的乘积 Case 2: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ−1⋅c\\sigma^{-1}\\cdot cσ−1⋅c 求逆是逆序求，所以index需要改变 求出@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ−1\\sigma^{-1}σ−1 ，再按照case 1的情况处理 Case 3: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ⋅τ\\sigma\\cdot \\tauσ⋅τ both @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ\\sigmaσ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τ\\tauτ are variables @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ⋅τ=σ1⋯σn⋅τ1⋯τn\\sigma\\cdot \\tau = \\sigma_1\\cdots\\sigma_n\\cdot \\tau_1\\cdots\\tau_nσ⋅τ=σ1​⋯σn​⋅τ1​⋯τn​ each party poseesses piece @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σi\\sigma_iσi​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τi\\tau_iτi​ S5 is not commutative party i在计算最终结果piece时，不能直接将两个piece相乘，因为S5是不满足交换律的。giving new piecesmove party 1's pieces closer by swaping 通过交换，让party 1的两个pieces靠近@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ1σ2τ1τ2\\sigma_1\\sigma_2\\tau_1\\tau_2σ1​σ2​τ1​τ2​ : swap @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ2\\sigma_2σ2​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τ1\\tau_1τ1​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ1τ1′σ2′τ2\\sigma_1\\tau_1'\\sigma_2'\\tau_2σ1​τ1′​σ2′​τ2​ : satisfy @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ2τ1=τ1′σ2′\\sigma_2\\tau_1=\\tau_1'\\sigma_2'σ2​τ1​=τ1′​σ2′​ 交换后，each party就可以将his pieces 相乘，得到最终结果的piece。计算结果时，按序相乘即可。@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ1⋯σn⋅τ1⋯τn\\sigma_1\\cdots\\sigma_n\\cdot \\tau_1\\cdots\\tau_nσ1​⋯σn​⋅τ1​⋯τn​ : swap @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σn\\sigma_nσn​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τ1\\tau_1τ1​ move party 1's pieces closerswap @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σn\\sigma_{n}σn​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τ1\\tau_1τ1​ : @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ1⋯σn−1τ1′⋅σn′⋯τn \\sigma_1\\cdots\\sigma_{n-1}\\tau_1'\\cdot \\sigma_n'\\cdots\\tau_nσ1​⋯σn−1​τ1′​⋅σn′​⋯τn​ satisfy @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σnτ1=τ1′σn′\\sigma_n\\tau_1=\\tau_1'\\sigma_n'σn​τ1​=τ1′​σn′​ swap @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σn−1\\sigma_{n-1}σn−1​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τ1′\\tau_1'τ1′​ swap party n-1's piece and party 1's piece...go on swapping(party 1 final)@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ1⋅τ1′⋯σn−1′⋅σn′τ2⋯τn\\sigma_1\\cdot \\tau_1'\\cdots\\sigma_{n-1}'\\cdot \\sigma_n'\\tau_2\\cdots\\tau_nσ1​⋅τ1′​⋯σn−1′​⋅σn′​τ2​⋯τn​ move party 2's pieces closerfinal: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ1⋅τ1′⋯⋯σn′τn\\sigma_1\\cdot \\tau_1'\\cdots\\cdots\\sigma_n'\\tau_nσ1​⋅τ1′​⋯⋯σn′​τn​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σnτ1=τ1′σn′\\sigma_n\\tau_1=\\tau_1'\\sigma_n'σn​τ1​=τ1′​σn′​ violate the privacy constraint: party 1 and party n tell each other @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σn\\sigma_n σn​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τ1\\tau_1τ1​ 其中一种实现new pieces的方法是party n和party 1互相告知对应的pieces，但这会破坏隐私性respect the privacy constraint use COT to achieve @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σnτ1=τ1′σn′\\sigma_n\\tau_1=\\tau_1'\\sigma_n'σn​τ1​=τ1′​σn′​ (party n)randomly selects a 5-permutation @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ρ\\rhoρ party 1 posesses: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ1,τ1\\sigma_1,\\tau_1σ1​,τ1​ party n posesses: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σn,τn,ρ\\sigma_n, \\tau_n, \\rhoσn​,τn​,ρ function g: for 5-permutations x, y and z @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(x,(y,z))=wwhere wz=yxg(x,(y,z))=w \\quad\\text{where }wz=yxg(x,(y,z))=wwhere wz=yx COTA(party 1): input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')a=τ1a=\\tau_1a=τ1​ B(party n): input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')b=(σn,ρ)b=(\\sigma_n,\\rho)b=(σn​,ρ) A(party 1)'s output: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(a,b)g(a,b)g(a,b) B(party n)'s output: noneset new pieces:party 1 : @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ1,τ1′=g(a,b)\\sigma_1, \\tau_1'=g(a,b)σ1​,τ1′​=g(a,b) party n: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σn′=ρ,τn\\sigma_n'=\\rho,\\tau_nσn′​=ρ,τn​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τ1′σn′=g(a,b)⋅ρ=σnτ1\\tau_1'\\sigma_n'=g(a,b)\\cdot\\rho=\\sigma_n\\tau_1τ1′​σn′​=g(a,b)⋅ρ=σn​τ1​ Analyze security informallyparty n has no info about party 1's old piece nor the now one party n's new piece is a random @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ρ\\rhoρ transfer g(a, b) is obliviousparty 1 has no info about party n'@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(τ1,(σn,ρ))g(\\tau_1,(\\sigma_n,\\rho))g(τ1​,(σn​,ρ)) : party 1 only knows @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')τ1\\tau_1τ1​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(x,(y,⋅)g(x,(y,\\cdot)g(x,(y,⋅) is injective on S5 and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ρ\\rhoρ is randomly selected by party n Complexityafter n 'swaps': party 1 can get the pieces for the variable @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')σ⋅τ\\sigma\\cdot \\tauσ⋅τ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')O(n2)\\mathcal{O}(n^2)O(n2) End","link":"/paper/%5BGMW87%5D/index.html"},{"title":"","text":"[ABY2.0]Improved Mixed -Protocol Secure Two-Party Computation /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 50%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } 📜[ABY2.0]Improved Mixed -Protocol Secure Two-Party ComputationABSTRACT1 Introduction1.1 Our ContributionsMixed Protocol ConversionsBuilding Blocks1.2 Related Work2 Preliminaries3 2PC in Arithmetic, Boolean and Yao's World3.1 2PC in Arithmetic World3.1.2 Sharing Semantics3.1.3 ProtocolsSharing ProtocolRECLinear OperationsMultiplication ProtocolFigure 2: Multiplication Protocol3.1.1 High-level Overview of Our 2PC over RingFigure 1: High Level overview of Beaver's and ABY2.03.1.4 Multi-Input Multiplications Gates3.2 2PC in Boolean World3.3 2PC in Yao World4 Mixed Protocol Conversions4.1 Standard ConversionsY2BB2YA2YY2AA2BBit2AB2A4.2 Special Conversions5 Building Blocks for Applications5.1 Scalar Product5.2 Matrix Multiplication5.3 Depth-Optimized Circuits5.4 Comparison5.5 TruncationABSTRACTContribution:an efficient mixed-protocol frameworkextend to multi-input multiplications gatesconstruct eff. protocols for several primitives scalar product, matrix multiplication, comparison, maxpool, and equality testing.1 IntroductionCategories:low-latency: using Yao's GC result in constant-round solutionshigh-throughput: using Secret-sharing(SS) conmmunication rounds linear in the multiplicative depth of the citcuitMixed-protocol blocks:blocks: Arithmetic; Boolean; Yaoinput:A performs ops on fieldsB and Y perform ops on bitsapproach:A and B using SS-based approachY using GC-based approach Practical runtimes: Beaver multiplication triples在input-independent setup phase 生成一些相关的随机组（Beaver multiplication triple），这些组会在input-dependent online phase使用，加快online的处理速度。Beaver multiplication triples: [9]D. Beaver. Efﬁcient multiparty protocols using circuit randomization. In CRYPTO, 1991.1.1 Our Contributionsan eff. mixed-protocol framework for secure 2PC over an l-bit ringsecure against a semi-honest adv.use an input-independent setupfocus on online efficiencyComparison of ABY2.0 and other 2PC protocols[41] ABY: [13]SPDZ:2-input multiplication: same complexity as SPDZ but using a different approach.N-input multiplication gate: constant cost of 2 ring elements and 1 round of itertationsMixed Protocol Conversionsreduces the number of online rounds from 2 to 1 use correlated OTs(cOT) Optimization: cOT[5]cOT Building Blocks 1.2 Related WorkEff. SS-based solutions for the dishonest majority over fields [40] [56] Extended to the ring [35][82] extended the multiplication from 2-input to N-input using Beaver triple extension 2 Preliminaries 3 2PC in Arithmetic, Boolean and Yao's World3.1 2PC in Arithmetic Worldhighlight: its effectiveness towards efficient realisations for multiple input multiplication gates and dot product operations. 3.1.2 Sharing SemanticsSharing Notions:&lt;·&gt;-sharing: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')([δv]i,Δv)\\left(\\left[\\delta_{v}\\right]_{i}, \\Delta_{v}\\right)([δv​]i​,Δv​) @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')δv\\delta_vδv​ is [·]-shared among P0, P1@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δv=v+δv\\Delta_{v}=v+\\delta_{v}Δv​=v+δv​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δv\\Delta_vΔv​ is known to both P0, P1 in clean3.1.3 ProtocolsSharing Protocolenable party @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPi​ to generate a &lt;·&gt;-sharing of its input value v@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPi​ generate a &lt;·&gt;-sharing of its input value v(setup)generate a [·]-shared: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δv][\\delta_v][δv​] @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPi​ samples random @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δv]i[\\delta_v]_i[δv​]i​ two parties together sample @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δv]1−i[\\delta_v]_{1-i}[δv​]1−i​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPi​ knows @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')δv=[δv]0+[δv]1\\delta_{v}=\\left[\\delta_{v}\\right]_{0}+\\left[\\delta_{v}\\right]_{1}δv​=[δv​]0​+[δv​]1​ (online) @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δv\\Delta_vΔv​ is konwn to both@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPi​ computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δv=v+δv\\Delta_{v}=v+\\delta_{v}Δv​=v+δv​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPi​ sends it to @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')P1−iP_{1-i}P1−i​ RECmutually exchange [·] share of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')δv\\delta_vδv​ Linear OperationsGiven &lt;a&gt;, &lt;b&gt; and public constants @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')c1,c2c_1,c_2c1​,c2​ .Parties can locally compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨y⟩=c1⋅⟨a⟩+c2⋅⟨b⟩\\langle\\mathbf{y}\\rangle=c_{1} \\cdot\\langle\\mathrm{a}\\rangle+c_{2} \\cdot\\langle\\mathrm{b}\\rangle⟨y⟩=c1​⋅⟨a⟩+c2​⋅⟨b⟩ Pi locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δy=c1⋅Δa+c2⋅Δb\\Delta_{\\mathrm{y}}=c_{1} \\cdot \\Delta_{\\mathrm{a}}+c_{2} \\cdot \\Delta_{\\mathrm{b}}Δy​=c1​⋅Δa​+c2​⋅Δb​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δa,Δb\\Delta_a,\\Delta_bΔa​,Δb​ is known constantsthen @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δy]i=c1⋅[δa]i+c2⋅[δb]i\\left[\\delta_{y}\\right]_{i}=c_{1} \\cdot\\left[\\delta_{a}\\right]_{i}+c_{2} \\cdot\\left[\\delta_{b}\\right]_{i}[δy​]i​=c1​⋅[δa​]i​+c2​⋅[δb​]i​ Multiplication Protocol@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δa,Δb\\Delta_a,\\Delta_bΔa​,Δb​ are consts.They can compute a []-sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δy\\Delta_yΔy​ if the parties obtain [·]-sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')δab=δaδb\\delta_{ab}=\\delta_a\\delta_bδab​=δa​δb​ The problem of multiplication reduces to generating @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δab][\\delta_{ab}][δab​] given @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δa][\\delta_a][δa​] and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δb][\\delta_b][δb​] Figure 2: Multiplication ProtocolOT based setup MULTCorrelated OTs(cOT) [5]Execute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')cOTll\\mathrm{cOT}_l^lcOTll​ to [·] sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δa]0[δb]1\\left[\\delta_{\\mathrm{a}}\\right]_{0}\\left[\\delta_{\\mathrm{b}}\\right]_{1}[δa​]0​[δb​]1​ For j : 0 ... @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')l−1l-1l−1 ( j-th instance of cOT)P0(sender):inputs correlation function: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')fj(x)=x+2j[δa]0f_{j}(x)=x+2^{j}\\left[\\delta_{a}\\right]_{0}fj​(x)=x+2j[δa​]0​ obtains: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(mj,0=rj,mj,1=rj+2j[δa]0)\\left(m_{j, 0}=r_{j}, m_{j, 1}=\\right.r_{j}+2^{j}\\left[\\delta_{a}\\right]_{0})(mj,0​=rj​,mj,1​=rj​+2j[δa​]0​) P1(receiver):inputs choice bit @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bjb_jbj​ : the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')jjj -th bit of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δb]1[\\delta_b]_1[δb​]1​ obtains: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')mj,bjm_{j,b_j}mj,bj​​ [·] shareP0: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[([δa]0[δb]1)]0=∑j=0ℓ−1(−rj)\\left[\\left(\\left[\\delta_{\\mathrm{a}}\\right]_{0}\\left[\\delta_{\\mathrm{b}}\\right]_{1}\\right)\\right]_{0}=\\sum_{j=0}^{\\ell-1}\\left(-r_{j}\\right)[([δa​]0​[δb​]1​)]0​=∑j=0ℓ−1​(−rj​) P1: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[([δa]0[δb]1)]1=∑j=0ℓ−1mj,bj\\left[\\left(\\left[\\delta_{\\mathrm{a}}\\right]_{0}\\left[\\delta_{\\mathrm{b}}\\right]_{1}\\right)\\right]_{1}=\\sum_{j=0}^{\\ell-1} m_{j, b_{j}}[([δa​]0​[δb​]1​)]1​=∑j=0ℓ−1​mj,bj​​ HE-based setupMULT:P0 : use pk0 encrypts its msg @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δa]0,[δb]0\\left[\\delta_{\\mathrm{a}}\\right]_{0},\\left[\\delta_{\\mathrm{b}}\\right]_{0}[δa​]0​,[δb​]0​ P1: use pk0 encrypts @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δa]1,[δb]1\\left[\\delta_{\\mathrm{a}}\\right]_{1},\\left[\\delta_{\\mathrm{b}}\\right]_{1}[δa​]1​,[δb​]1​ and random element @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rrr P0: sends ciphertext to P1P1: computes ciphertext correspnding to v: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=[δ2]0[δb]1+[δa]1[δb]0−rv=\\left[\\delta_{2}\\right]_{0}\\left[\\delta_{b}\\right]_{1}+\\left[\\delta_{a}\\right]_{1}\\left[\\delta_{b}\\right]_{0}-rv=[δ2​]0​[δb​]1​+[δa​]1​[δb​]0​−r (HE) and sends encryption of v to P0P0: use sk0 decrypt it[·] shareP0: vP1: r@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δa]0[δb]1+[δa]1[δb]0=v+r\\left[\\delta_{\\mathrm{a}}\\right]_{0}\\left[\\delta_{\\mathrm{b}}\\right]_{1}+\\left[\\delta_{\\mathrm{a}}\\right]_{1}\\left[\\delta_{\\mathrm{b}}\\right]_{0}=\\mathrm{v}+r[δa​]0​[δb​]1​+[δa​]1​[δb​]0​=v+r 3.1.1 High-level Overview of Our 2PC over RingBeaver's technique on gates inputsBeaver's multiplication triples: [9]Setup Phase:interactively generate the multiplication triple: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(δa,δb,δab) with δab=δaδb(\\left.\\delta_{a}, \\delta_{b}, \\delta_{a b}\\right) \\text { with } \\delta_{a b}=\\delta_{a} \\delta_{b}(δa​,δb​,δab​) with δab​=δa​δb​ Online Phase:mutually exchange the shares of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δa,Δb\\Delta_a,\\Delta_bΔa​,Δb​ compute an additive sharing @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δa,Δb\\Delta_a,\\Delta_bΔa​,Δb​ locally compute a sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')a⋅ba\\cdot ba⋅b Figure 1: High Level overview of Beaver's and ABY2.0requires communicating 4 elements per multiplication(2 elements per reconstruction) REC @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δa,Δb\\Delta_a,\\Delta_bΔa​,Δb​ Our technique on gate outputs:sharing semantics ensure the parties to have the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δ\\DeltaΔ value the reconstructions of input wires are no longer requiredBut inorder to proceed further, a sharing a y needs to be generated. So parties locally compute an additive sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δy\\Delta_yΔy​ and exchange the shares to reconstruct @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δy\\Delta_yΔy​ Main Idea:shifs the need of reconstruction from per input wire to the output wire.For a fan-in 2, it reduces the number of reconstructions from 2 to 1.3.1.4 Multi-Input Multiplications Gates3-Input Multiplication gate:need to generate the [·]-sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')δab,δbc,δac and δabc\\delta_{a b}, \\delta_{b c}, \\delta_{a c} \\text { and } \\delta_{a b c}δab​,δbc​,δac​ and δabc​ Multi-Input Multiplication gateextend to N-input without inflating the online communication.3.2 2PC in Boolean Worldin a Boolean ring @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Z21\\Z_{2^1}Z21​ replace additions with XORS and multiplications with ANDSNOT: Negation Protocol3.3 2PC in Yao WorldMain:Yao World from ABY [41]For a wire with @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v∈0,1v\\in0,1v∈0,1 P0 (garbler): zero-key on the wire@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v⟩0=Ku0\\langle v\\rangle_{0}=\\mathrm{K}_{\\mathrm{u}}^{0}⟨v⟩0​=Ku0​ P1 (evaluator): actual key @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v⟩1=Kuv\\langle v\\rangle_{1}=\\mathrm{K}_{\\mathrm{u}}^{v}⟨v⟩1​=Kuv​ GC Optimizationfree-XOR: [70] Point-and-permute: [11] 4 Mixed Protocol Conversions4.1 Standard ConversionsHighlight:invoke OT in the setup phase onlyY2BGoal: generate equivalent Boolean sharing given Yao sharingin Yao: LSB of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ku0 and Kuu\\mathrm{K}_{\\mathrm{u}}^{0} \\text { and } \\mathrm{K}_{\\mathrm{u}}^{\\mathrm{u}}Ku0​ and Kuu​ can reveal u free-XOR property: last bit of zero and one key are distinctY2B: convert: each party Boolean-shares the LSB of their @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨u⟩iY\\langle\\mathrm{u}\\rangle_{i}^{\\mathbf{Y}}⟨u⟩iY​ obtain u: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨u⟩B=⟨u⟩0B⊕⟨u⟩1B\\langle\\mathbf{u}\\rangle^{\\mathbf{B}}=\\left\\langle\\mathbf{u}\\right\\rangle_0^{\\mathbf{B}} \\oplus\\left\\langle\\mathbf{u}\\right\\rangle_1^{\\mathbf{B}}⟨u⟩B=⟨u⟩0B​⊕⟨u⟩1B​ optimization: P0可以在setup phase Boolean-shares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')LSB⁡(Ku0)\\operatorname{LSB}\\left(\\mathrm{K}_{\\mathrm{u}}^{0}\\right)LSB(Ku0​) ，因为这是一个随机选的label，而另一个指depends on the value.Q: 是否泄漏了中间值？总结来说，就是Y先转换为B中的表示，再根据成立的等式，用B中的运算得到相同的值。 其他也类似。B2Yin Boolean :Pi: locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ui=(1−i)⋅Δu⊕[δu]i\\mathrm{u}_{i}=(1-i) \\cdot \\Delta_{\\mathrm{u}} \\oplus\\left[\\delta_{\\mathrm{u}}\\right]_{i}ui​=(1−i)⋅Δu​⊕[δu​]i​ P0: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')u0=Δu⊕[δu]0u_0=\\Delta_u\\oplus [\\delta_u]_0u0​=Δu​⊕[δu​]0​ P1: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')u1=[δu]1u_1=[\\delta_u]_1u1​=[δu​]1​ satisfy: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')u=u0⊕u1u=u_0\\oplus u_1u=u0​⊕u1​ B2Y:convert: each party Yao-shares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')uiu_iui​ obtain u: compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨u⟩Y=⟨u0⟩Y⊕⟨u1⟩Y\\langle\\mathbf{u}\\rangle^{\\mathbf{Y}}=\\left\\langle\\mathbf{u}_{0}\\right\\rangle^{\\mathbf{Y}} \\oplus\\left\\langle\\mathbf{u}_{1}\\right\\rangle^{\\mathbf{Y}}⟨u⟩Y=⟨u0​⟩Y⊕⟨u1​⟩Y optimization: P1可以在setup phase Yao-shares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')u1=[δu]1u_1=[\\delta_u]_1u1​=[δu​]1​ ，因为这在setup phase可以确定。 A2Yin Arithmetic：Pi: locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')vi=(1−i)⋅Δv−[δv]i\\mathrm{v}_{i}=(1-i) \\cdot \\Delta_{\\mathrm{v}}-\\left[\\delta_{\\mathrm{v}}\\right]_{i}vi​=(1−i)⋅Δv​−[δv​]i​ satisfy: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=v0+v1\\mathrm{v=v_0+v_1}v=v0​+v1​ A2Y:convert: each party Yao-shares viobtain vY2Asetup phase: P0P0 sample a random value rP0 generate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨r⟩Y and ⟨r⟩A\\langle r\\rangle^{\\mathbf{Y}} \\text { and }\\langle r\\rangle^{\\mathbf{A}}⟨r⟩Y and ⟨r⟩A P0 garbles an Adder circuit and sends it to P1 (along with output decoding info)online phase: P1 evalute circuit:input: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v⟩Y and ⟨r⟩Y\\langle\\mathbf{v}\\rangle^{\\mathbf{Y}} \\text { and }\\langle r\\rangle^{\\mathbf{Y}}⟨v⟩Y and ⟨r⟩Y output: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v+r⟩Y\\langle\\mathrm{v}+r\\rangle^{\\mathbf{Y}}⟨v+r⟩Y decode: (v+r) in clearobtain:P1 Arithmetic-shares (v+r)parties compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v⟩A=⟨v+r⟩A−⟨r⟩A\\langle v\\rangle^{\\mathbf{A}}=\\langle v+r\\rangle^{\\mathbf{A}}-\\langle r\\rangle^{\\mathbf{A}}⟨v⟩A=⟨v+r⟩A−⟨r⟩A A2Bsimilar to A2Y, but it results in a non-constant round protocol (dependent on @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')lll )constant-round solution: use Y2B(A2Y(·))Bit2Arelation in Bit and A: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=v0⊕v1\\mathrm{v}=\\mathrm{v}_{0} \\oplus \\mathrm{v}_{1}v=v0​⊕v1​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')va=v0a+v1a−2v0av1a\\mathrm{v^{a}=v_{0}^{a}+v_{1}^{a}-2 v_{0}^{a} v_{1}^{a}}va=v0a​+v1a​−2v0a​v1a​ so: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')va=(Δv⊕δv)a=Δva+δva−2Δvaδva\\mathrm{v^{a}=\\left(\\Delta_{v} \\oplus \\delta_{v}\\right)^{a}=\\Delta_{v}^{a}+\\delta_{v}^{a}-2 \\Delta_{v}^{a} \\delta_{v}^{a} }va=(Δv​⊕δv​)a=Δva​+δva​−2Δva​δva​ setup phase: parties interactively generate share: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δva]\\mathrm{[\\delta_v^a]}[δva​] @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')δv=[δv]0⊕[δv]1\\mathrm{\\delta_{v}=\\left[\\delta_{v}\\right]_{0} \\oplus\\left[\\delta_{v}\\right]_{1}}δv​=[δv​]0​⊕[δv​]1​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')δva=[δva]0+[δva]1−2([δva]0[δva]1)\\mathrm{\\delta_{v}^{a}=\\left[\\delta_{v}^{a}\\right]_{0}+\\left[\\delta_{v}^{a}\\right]_{1}-2\\left(\\left[\\delta_{v}^{a}\\right]_{0}\\left[\\delta_{v}^{a}\\right]_{1}\\right)}δva​=[δva​]0​+[δva​]1​−2([δva​]0​[δva​]1​) execute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')cOTl1\\mathrm{cOT}_l^1cOTl1​ to share @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δva]0[δva]1\\left[\\delta_{v}^{a}\\right]_{0}\\left[\\delta_{v}^{a}\\right]_{1}[δva​]0​[δva​]1​ :P0 (sender): inputs correlation @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')fj(x)=x+[δv]0af_{j}(x)=x+\\left[\\delta_{v}\\right]_{0}^{a}fj​(x)=x+[δv​]0a​ and obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(s0=r,s1=r+[δv]0a)\\left(s_{0}=r, s_{1}=r+\\left[\\delta_{v}\\right]_{0}^{a}\\right)(s0​=r,s1​=r+[δv​]0a​) P1 (reciever): inputs the choice bit as @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δv]1\\left[\\delta_{v}\\right]_{1}[δv​]1​ and obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')s[δv]1=r+[δv]1⋅[δv]0as_{[\\delta_v]_1}=r+\\left[\\delta_{v}\\right]_{1} \\cdot\\left[\\delta_{v}\\right]_{0}^{a}s[δv​]1​​=r+[δv​]1​⋅[δv​]0a​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δv]1=0\\left[\\delta_{v}\\right]_{1}=0[δv​]1​=0 : obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')s0=rs_0=rs0​=r @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δv]1=1\\left[\\delta_{v}\\right]_{1}=1[δv​]1​=1 : obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')s0=r+[δv]0as_0=r+[\\delta_v]_0^as0​=r+[δv​]0a​ P0 locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[([δv]0a[δv]1a)]0=−r\\left[\\left(\\left[\\boldsymbol{\\delta}_{v}\\right]_{0}^{a}\\left[\\boldsymbol{\\delta}_{v}\\right]_{1}^{a}\\right)\\right]_{0}=-r[([δv​]0a​[δv​]1a​)]0​=−r P1 locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[([δv]0a[δv]1a)]0=s[δv]1\\left[\\left(\\left[\\boldsymbol{\\delta}_{\\mathrm{v}}\\right]_{0}^{\\mathrm{a}}\\left[\\boldsymbol{\\delta}_{\\mathrm{v}}\\right]_{1}^{\\mathrm{a}}\\right)\\right]_{0}=s_{\\left[\\delta_{\\mathrm{v}}\\right]_{1}}[([δv​]0a​[δv​]1a​)]0​=s[δv​]1​​ Pi locally sets [·]-share of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[δva]i=(1−i)⋅[δv]0a+i⋅[δv]1a−2[([δv]0a[δv]1a)]i[\\delta_v^a]_i=(1-i)\\cdot[\\delta_v]_0^a+i\\cdot \\left[\\delta_{v}\\right]_{1}^{a}-2\\left[\\left(\\left[\\delta_{v}\\right]_{0}^{a}\\left[\\delta_{v}\\right]_{1}^{a}\\right)\\right]_{i}[δva​]i​=(1−i)⋅[δv​]0a​+i⋅[δv​]1a​−2[([δv​]0a​[δv​]1a​)]i​ online phase:convert: Pi locally computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[va]i=i⋅Δva+(1−2Δva)⋅[δva]i\\left[\\mathrm{v}^{\\mathrm{a}}\\right]_{i}=i \\cdot \\Delta_{\\mathrm{v}}^{\\mathrm{a}}+\\left(1-2 \\Delta_{\\mathrm{v}}^{\\mathrm{a}}\\right) \\cdot\\left[\\delta_{\\mathrm{v}}^{\\mathrm{a}}\\right]_{i}[va]i​=i⋅Δva​+(1−2Δva​)⋅[δva​]i​ and Arithmetic-shares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[va]i\\left[\\mathrm{v}^{\\mathrm{a}}\\right]_{i}[va]i​ obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')va\\mathrm{v^a}va : @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨va⟩A=⟨[va]0⟩A+⟨[va]1⟩A\\left\\langle\\mathrm{v}^{\\mathrm{a}}\\right\\rangle^{\\mathbf{A}}=\\left\\langle\\left[\\mathrm{v}^{\\mathrm{a}}\\right]_{0}\\right\\rangle^{\\mathbf{A}}+\\left\\langle\\left[\\mathrm{v}^{\\mathrm{a}}\\right]_{1}\\right\\rangle^{\\mathbf{A}}⟨va⟩A=⟨[va]0​⟩A+⟨[va]1​⟩A B2Asetup phase: P0P0 sample a random value rP0 generate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨r⟩B and ⟨r⟩A\\langle r\\rangle^{\\mathbf{B}} \\text { and }\\langle r\\rangle^{\\mathbf{A}}⟨r⟩B and ⟨r⟩A P0 garbles a boolean circuit and sends it to P1 (along with output decoding info)online phase: P1 evalute circuit:input: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v⟩B and ⟨r⟩B\\langle\\mathbf{v}\\rangle^{\\mathbf{B}} \\text { and }\\langle r\\rangle^{\\mathbf{B}}⟨v⟩B and ⟨r⟩B output: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v−r⟩B\\langle\\mathrm{v}-r\\rangle^{\\mathbf{B}}⟨v−r⟩B decode: (v-r) in clearobtain:P1 Arithmetic-shares (v-r)parties compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v⟩A=⟨v−r⟩A+⟨r⟩A\\langle v\\rangle^{\\mathbf{A}}=\\langle v-r\\rangle^{\\mathbf{A}}+\\langle r\\rangle^{\\mathbf{A}}⟨v⟩A=⟨v−r⟩A+⟨r⟩A But it results in a non-constant round protocol in the online phase.A novel round makes uses of the Bit2A protocol.setup phase:fact: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=∑j=0ℓ−12j⋅v[j]\\mathbf{v}=\\sum_{j=0}^{\\ell-1} 2^{j} \\cdot \\mathbf{v}[j]v=∑j=0ℓ−1​2j⋅v[j] ( v[j] denotes the jth bit of v)parties possess @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v[j]⟩B for each j∈[0,ℓ)\\langle v[j]\\rangle^{\\mathbf{B}} \\text { for each } j \\in[0, \\ell)⟨v[j]⟩B for each j∈[0,ℓ) : execute l instances of Bit2A conversionsonline phase:for each bit v[j], parties locally compute the [·]-sharing: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[(v[j])a][(v[j])^a][(v[j])a] Bit2A: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v[j]a=(Δv[j]⊕δv[j])a=Δv[j]a+δv[j]a−2Δv[j]aδv[j]a\\mathrm{v[j]^{a}=\\left(\\Delta_{v[j]} \\oplus \\delta_{v[j]}\\right)^{a}=\\Delta_{v[j]}^{a}+\\delta_{v[j]}^{a}-2 \\Delta_{v[j]}^{a} \\delta_{v[j]}^{a} }v[j]a=(Δv[j]​⊕δv[j]​)a=Δv[j]a​+δv[j]a​−2Δv[j]a​δv[j]a​ each party locally computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[v]i=∑j=0ℓ−12j⋅[(v[j])a]i[\\mathrm{v}]_{i}=\\sum_{j=0}^{\\ell-1} 2^{j} \\cdot\\left[(\\mathrm{v}[j])^{a}\\right]_{i}[v]i​=∑j=0ℓ−1​2j⋅[(v[j])a]i​ each party geneate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨[v]i⟩A\\left\\langle[\\mathbf{v}]_{i}\\right\\rangle^{A}⟨[v]i​⟩A parties locally computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v⟩A=⟨[v0]⟩A+⟨[v1]⟩A\\langle\\mathbf{v}\\rangle^{\\mathbf{A}}=\\left\\langle\\left[\\mathbf{v}_{0}\\right]\\right\\rangle^{\\mathbf{A}}+\\left\\langle\\left[\\mathbf{v}_{1}\\right]\\right\\rangle^{\\mathbf{A}}⟨v⟩A=⟨[v0​]⟩A+⟨[v1​]⟩A 4.2 Special Conversions 5 Building Blocks for Applications5.1 Scalar Product有一种方法是使用n个3.1.3节的乘法，但这样online communication的开销会和vector size n的大小有关，即每次计算出两个向量对应元素的乘积的&lt;·&gt;-share，最后本地加起来。做出的优化，其实就是先本地求和，再share；make the online communication independent of the vector size5.2 Matrix MultiplicationNotions:MATMULT: 和乘法类似，唯一需要注意的是计算 @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[γAB][\\mathrm{\\gamma_{AB}}][γAB​] 5.3 Depth-Optimized CircuitsParallel-prefix Adders (PPA) [50] PPA电路可以被优化为提取最高位的电路(BitExt)5.4 Comparisonchecking x &lt; y = checking the sign of v = x - yparties locally compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')⟨v⟩=⟨x⟩−⟨y⟩\\langle v\\rangle=\\langle x\\rangle-\\langle y\\rangle⟨v⟩=⟨x⟩−⟨y⟩ P0, P1 boolean-share a and b respectively.v = a + b@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')a=−[δv]0a = -\\left[\\delta_{v}\\right]_{0}a=−[δv​]0​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')b=Δv−[δv]1\\mathrm{b}=\\Delta_{\\mathrm{v}}-\\left[\\delta_{\\mathrm{v}}\\right]_{1}b=Δv​−[δv​]1​ parties use Bit Extraction circuit to compute MSB(v)5.5 Truncation在online phase，计算@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[y]=[Δy]−[δy][\\mathrm{y}]=\\left[\\Delta_{y}\\right]-\\left[\\delta_{y}\\right][y]=[Δy​]−[δy​] 。 Each party 截断后再分享，最后再还原。","link":"/paper/%5BABY2.0%5D/index.html"},{"title":"","text":"[Beaver91]Efficient Multiplarty Protools Using Circuit Randomization /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 70%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } 📜[Beaver91]Efficient Multiplarty Protools Using Circuit Randomization1 IntroductionThe IdeaOne-time tables2 An Efficient Protocol for Circuit EvaluationUnifSecretAdditive GateMultiplicative Gate:Figure 2: the whole protocol2.1 An Optimizationdifference between theory and practice: efficiencyIn practice:express F as a circuit CFcall on each player to secretly share @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xix_ixi​ proceed to perform &quot; secret addition and multiplication&quot; on secretly shared valuescost: depth of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')CFC_FCF​ times cost of secret multiplication1 IntroductionSecrete Sharing:Advantage of using polynomialseasy to combine two secrets multiplication by a publicly known constant But when secrets are multiplied: it's hard fan-in: [Electronics]the number of inputs that can be connected to the circuit.Notions: Our Solution:dose evaluate the circuit level by level, but each level is simply a reconstruction of secretsThe Ideacompletely randomize every input and output to each gate in @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')CFC_FCF​ plus a very simple error-correction procedure error-correcting procedure One-time tablesAnalogous to one-time padOne-time table: a set of precomputed values that support direct secure computation without broadcast or private channels 2 An Efficient Protocol for Circuit Evaluation UnifSecretUnifSecret:generate uniformly random secrets by adding uniformly random secrets shared by each party.Calculate Circuit:Circuit：N wires carry inputsgates @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')gk∈{+,×}g_k\\in \\{+,\\times\\}gk​∈{+,×} : @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xk=gk(xik,xjk)x_k=g_k(x_{i_k},x_{j_k})xk​=gk​(xik​​,xjk​​) level(@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')gkg_kgk​ ): depth of the gateevaluate: 把电路的gates按照depth分层，一层一层的计算，每次计算出该层的输出（new secrets)，就是下一层的输入。 Share-Compute-Reveal Paradigm:evaluate gates at each levelthereby produce new secrets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xkx_kxk​ , until the final secret @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xNx_NxN​ is calculateduse REC reconstruct the result @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xNx_NxN​ Additive GateAn additive gate:create N uniformly random values @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rir_iri​ for every @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xix_ixi​ 用UnifySecret的方式，every party generate a random secret，其和就是分配给每个wire的random values。 （这里不考虑secret sharing，只考虑value calculation）for every gate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')gkg_kgk​ : compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sk=gk(rik,rjk)s_k=g_k(r_{i_k},r_{j_k})sk​=gk​(rik​​,rjk​​) every party可以直接对random value (pieces)计算，这里是加法门，所以@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sk=rik+rjks_k=r_{i_k}+r_{j_k}sk​=rik​​+rjk​​ 。不过each party得到的其实是PIECE(@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sks_ksk​ )。consider corrections @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δi=ri−xi\\Delta_i=r_i-x_iΔi​=ri​−xi​ to each wire 对于一层gates而言，input wire的corrections (pieces)可以locally calculate。而当要计算output wire的corrections时，就需要REC input wire的correction，所有parties的pieces 加起来，得到@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δi\\Delta_iΔi​ 。compute the correction @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δk\\Delta_kΔk​ : output wire of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')gk(+)g_k(+)gk​(+) praty know(pieces): random inputs @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rik,rjkr_{i_k},r_{j_k}rik​​,rjk​​ and their results @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sk=rik+rjks_k=r_{i_k}+r_{j_k}sk​=rik​​+rjk​​ random output @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rkr_krk​ input wire corrections @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δik,Δjk\\Delta_{i_k}, \\Delta_{j_k}Δik​​,Δjk​​ party calculate output wire correction: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δk=rk−sk−Δik−Δjk\\Delta_k = r_k-s_k-\\Delta_{i_k}-\\Delta_{j_k}Δk​=rk​−sk​−Δik​​−Δjk​​ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δk\\Delta_kΔk​ is a linear combination of &quot;known&quot; constant @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δik\\Delta_{i_k}Δik​​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δjk\\Delta_{j_k}Δjk​​ with the values @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rkr_krk​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sks_ksk​ 刚刚也说过，在计算这一层gates前，需要reconstruct input wire的correction，所以计算时， @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δik\\Delta_{i_k}Δik​​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δjk\\Delta_{j_k}Δjk​​ 是已经经过REC操作，every party都把他的pieces分享处理（但不会reveal info），得到 constant @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δik\\Delta_{i_k}Δik​​ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δjk\\Delta_{j_k}Δjk​​ 。Multiplicative Gate:Multiplicative gates:a linear combination: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Δk=rk−sk−rikΔjk−Δikrik−ΔikΔjk\\Delta_{k}=r_{k}-s_{k}-r_{i_{k}} \\Delta_{j_{k}}-\\Delta_{i_{k}} r_{i_{k}}-\\Delta_{i_{k}} \\Delta_{j_{k}}Δk​=rk​−sk​−rik​​Δjk​​−Δik​​rik​​−Δik​​Δjk​​ Security: Cost of REC: Figure 2: the whole protocol2.1 An OptimizationCompress additive gates:by computing the corrections to outputs at the same time as the corrections to input wires no &quot;randomization&quot; of additive gatesExample:把先做乘法，在做什么加法两层运算，压缩为一层运算。Add gate output correction和其input没有关系，包括@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')r5,r6,Δ5,Δ6r_5,r_6,\\Delta_5,\\Delta_6r5​,r6​,Δ5​,Δ6​ ，反而是和前一层的MUL gates的input有关系。Deduce：","link":"/paper/%5BBeaver91%5D/index.html"}],"posts":[{"title":"「Paper Reading」：未整理版论文笔记","text":"最近读了几篇论文 以下是未整理版论文笔记： [ABY2.0]Improved Mixed -Protocol Secure Two-Party Computation [Schneider13]GMW vs. Yao? Efficient Secure Two-Party Computation with Low Depth CItcuit [Beaver91]Efficient Multiplarty Protools Using Circuit Randomization [GMW87]How To Play Any Mental Game [Naor-OT05]Computationally Secure Oblivious Transfer","link":"/2021/11/08/2021-11-paper-reading/"},{"title":"Adagrag-demo","text":"实现这篇文章中前面两个tips。 实现了tip1 Adagrad + tip2 Stochastic Gradient Descent demo代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970################## 2020/03/06 ## Adagrad demo ##################import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model# datax_data = [[338.], [333.], [328.], [207.], [226.], [25.], [179.], [60.], [208.], [606.]]y_data = [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]# coordinatex = np.arange(-200, -100, 1)y = np.arange(-5, 5, 0.1)Z = np.zeros((len(y), len(x)))# cal the Loss of every point(function)for i in range(len(x)): for j in range(len(y)): b = x[i] w = y[j] for k in range(len(x_data)): Z[j][i] += (y_data[k] - b - w * x_data[k][0])**2# initialb = -120w = -4lr = 1 # learning rateiteration = 100000# record the iterationb_his = [b]w_his = [w]# Adagradb_grad_sum2 = 0.0w_grad_sum2 = 0.0for i in range(iteration): for k in range(len(x_data)): b_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-1) w_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-x_data[k][0]) b_grad_sum2 += b_grad**2 w_grad_sum2 += w_grad**2 b = b - lr / np.sqrt(b_grad_sum2) * b_grad w = w - lr / np.sqrt(w_grad_sum2) * w_grad b_his.append(b) w_his.append(w)# sklearn linear modelreg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print(reg.coef_[0])print(reg.intercept_)# display the figureplt.contourf(x, y, Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))plt.plot(reg.intercept_, reg.coef_, 'x', ms=13, lw=1.5, color='orange')plt.plot(b_his, w_his, 'o-', ms=3, lw=1.5, color='black')plt.xlim(-200, -100)plt.ylim(-5, 5)plt.xlabel('$b$', fontsize=16)plt.ylabel('$w$', fontsize=16)# plt.show()plt.savefig(&quot;Loss.png&quot;) Loss 迭代图画出的图片很直观","link":"/2020/03/09/Adagrad-demo/"},{"title":"「机器学习-李宏毅」：Backpropagation","text":"这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。 Gradient Descent在Neural Network中，参数的更新也是通过Gradient Descent。 但是当Neural Network层数很深，结构很复杂的时候，会有millions of parapmeters。 Backpropagation：To compute the gradient efficiently. Chain RuleBP中需要用到的数学知识：微积分中的链式法则。 Backpropagation 在NN中，定义损失函数 $L(\\theta)=\\sum_{n=1}^{N} C^{n}(\\theta)$ （$\\theta$ 代指NN中所有的weight 和bias，$C$ 为Cross-entropy） 对某一参数的gradient为 $\\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^{N} \\frac{\\partial C^{n}(\\theta)}{\\partial w}$ 在上图NN中，我们先只研究红框部分，即是以下结构： z：每个activation function的输入。 根据链式法则， $\\frac{\\partial C}{\\partial w}= \\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}$ . 要计算每个参数的 $\\frac{\\partial C}{\\partial w}$ ，分为两部分。 Forward pass: compute $\\frac{\\partial z}{\\partial w} $ for all parameters. Backward pass: compute $\\frac{\\partial C}{\\partial z} $ for all activation function inputs z. BP：Forward passCompute $\\frac{\\partial z}{\\partial w} $ for all parameters. 还是只看上图这一部分，可以轻易得出： $\\partial{z}/\\partial{w_1}=x_1\\qquad \\partial{z}/\\partial{w_2}=x_2$ 得到结论： $\\frac{\\partial z}{\\partial w} $ 等于 the value of the input connected by the weight. 【$\\frac{\\partial z}{\\partial w} $ 等于 连接w的输入的值】 那么，如何计算出NN中全部的 $\\frac{\\partial z}{\\partial w} $ ？ ：Forward pass. 用当前参数（w,b) 从hidden layer的第一层开始，计算出第一层的输出，即第二层的输入。 依次相前计算，计算出每一层的输出，即下一层的输入，即输入所连接权重的 $\\frac{\\partial z}{\\partial w}$ 。 BP：Backward passCompute $\\frac{\\partial C}{\\partial z} $ for all activation function inputs z. z：activation function的 input a：activation function的 output 这里的activation function 是 sigmod函数 $a=\\sigma(z)=\\frac{1}{1+e^{-z}}$ 要求 $\\frac{\\partial C}{\\partial z}$ ， 再根据链式法则： $\\frac{\\partial C}{\\partial z}=\\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}$ 求 $\\frac{\\partial{a}}{\\partial{z}}$ : $\\frac{\\partial{a}}{\\partial{z}}=\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$ （是其他activation function 也能轻易求出） 求 $\\frac{\\partial C}{\\partial a}$ ：根据链式法则： $\\frac{\\partial C}{\\partial a}=\\frac{\\partial z^{\\prime}}{\\partial a} \\frac{\\partial C}{\\partial z^{\\prime}}+\\frac{\\partial z^{\\prime \\prime}}{\\partial a} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}$ $\\frac{\\partial z^{\\prime}}{\\partial a} =w_3$ ， $\\frac{\\partial z^{\\prime\\prime}}{\\partial a} =w_4$ $\\frac{\\partial C}{\\partial z^{\\prime}}$ 和 $\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$ ？假设，已经通过某种方法算出这个值。 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ 这个式子，可以画成一个反向传播的NN，见下图。 $\\frac{\\partial C}{\\partial z^{\\prime}},\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$ 是这个neuron的输入， $w_3,w_4$ 仍然是 neuron的 weight（无bias）。 $\\sigma’(z)$ 是一个常数，因为在forward pass中每一个activation的输入已经被算出来了。 和forward pass中的NN的区别是，forward 中是一个activation function，输入z作用于这个函数； 而在 backward pass中，这更像一个放缩器，将他的输入变小，即乘上一个 $\\sigma’(z)$ 。 问题还是如何计算 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ ？ 分为两种情况讨论， $z’,z’’$ 是否为输出层的输入？ Output Layer： z’,z’’：activation function的输入。 y1,y2：actiavtion function（也是NN）的输出。 C：NN输出和target的cross entropy。 根据链式法则： $\\frac{\\partial C}{\\partial z^{\\prime}}=\\frac{\\partial y_{1}}{\\partial z^{\\prime}} \\frac{\\partial C}{\\partial y_{1}} \\quad \\frac{\\partial C}{\\partial z^{\\prime \\prime}}=\\frac{\\partial y_{2}}{\\partial z^{\\prime \\prime}} \\frac{\\partial C}{\\partial y_{2}}$ 所以，已知activation function（simod或者其他），可以轻易求出 $\\frac{\\partial y_{1}}{\\partial z^{\\prime}}(=\\sigma'(z'))$ 和 $\\frac{\\partial y_{2}}{\\partial z^{\\prime\\prime}}(=\\sigma''(z''))$ 。 所以，已知损失函数，也可以轻易求出 $\\frac{\\partial C}{\\partial y_1}$ 和 $\\frac{\\partial C}{\\partial y_2}$ 。（ $C\\left(y, \\hat{y}\\right)=-\\left[\\hat{y} \\ln y+\\left(1-\\hat{y}\\right) \\ln \\left(1-y\\right)\\right]$ ) 所以，可以直接求出 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ 。 Not Output Layer: 上图中，如果我们要计算 $\\frac{\\partial C}{\\partial z’}$ ，必须要已知下一层的 $\\frac{\\partial C}{\\partial z_a}$ ，然后一直递归下去，直到到达最后的输出层，也就是上面一种情况，可以直接计算出，再递归回来，计算当前层的 $\\frac{\\partial C}{\\partial z’}$ 。 但是，这样计算每个参数的 $\\frac{\\partial{C}}{\\partial{z}}$ 都要一直递归到输出层，效率显然太低了。 计算方法如上图： 当我们已知输出层的 $\\frac{\\partial{C}}{\\partial{z'}},\\frac{\\partial{C}}{\\partial{z''}}$ 时，再通过上面的步骤3（且的确算出了 $\\frac{\\partial{C}}{\\partial{z'}},\\frac{\\partial{C}}{\\partial{z''}}$ ），画成反向的NN，计算$\\frac{\\partial{C}}{\\partial{z}}$. 再依次反向传播计算出每一个neuron的输出z （也是正向传播neuron的输入）的 $\\frac{\\partial{C}}{\\partial{z}}$ . Backforward pass 的做法： 先计算出输出层的 $\\frac{\\partial{C}}{\\partial{z}}$ （也就是上图的 $\\frac{\\partial{C}}{\\partial{z_5}}$ 和 $\\frac{\\partial{C}}{\\partial{z_6}}$ ） 用反向传播的NN，向后依次计算出每一层每一个neuron的 $\\frac{\\partial{C}}{\\partial{z}}$ 。 Summary 公式： $\\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}=\\frac{\\partial C}{\\partial w}$ 在正向传播NN中，z是neuron的activation function的输入。 在反向传播NN中，z是neuron的放缩器的输出。 通过Forward Pass计算出正向传播NN的每一个neuron的 $\\frac{\\partial z}{\\partial w}$ ，等于该层neuron的输入。 通过Backward Pass计算出反向传播NN的每一个neuron的 $\\frac{\\partial C}{\\partial z}$ 。 然后，通过相乘，计算出每个参数的 $\\frac{\\partial C}{\\partial w}$。 Reference","link":"/2020/04/18/Backpropagation/"},{"title":"「机器学习-李宏毅」:Classification-Probabilistic Generative Model","text":"Classification 有Generative Model和Discriminative Model。这篇文章主要讲述了用生成模型来做分类的原理及过程。 What is Classification?分类是什么呢？分类可以应用到哪些场景呢？ Credit Scoring【贷款评估】 Input: income, savings, profession, age, past financial history …… Output: accept or refuse Medical Diagnosis【医疗诊断】 Input: current symptoms, age, gender, past medical history …… Output: which kind of diseases Handwritten character recognition【手写数字辨别】 Input： Output：金 Face recognition 【人脸识别】 Input: image of a face output: person Classification：Example Application【图】 如上图，Pokemon又来啦！ Pokemon有很多属性，比如皮卡丘是电属性，杰尼龟是水属性之类。 关于Pokemon的Classification：Predict the “type” of Pokemon based on the information Input：Information of Pokemon (数值化） Output：the type Training Data: ID在前400的Pokemon Testing Data: ID在400后的Pokemon Classification as Regression?1. 简化问题，只考虑二分类：Class 1 ， Class2。 如果把分类问题当作回归问题，把类别数值化。 在Training中： Class 1 means the target is 1; Class 2 means the target is -1. 在Testing中：如果Regression的函数值接近1，说明是class 1；如果函数值接近-1，说明是class 2. Regression：输入信息只考虑两个特征。 Model：$y=w_1x_1+w_2x_2+b$ 当Training data的分布如上图所示时，得到的（最优函数）分界线感觉很合理。 但当Training data在右下角也有分布时（如右图），训练中为了减少error，训练得到的分界线会变成紫色的那一条。 所以，如果用Regression来做Classification：Penalize to the examples that are “too correct” .[1] 训练中会因为惩罚一些“过于正确”（即和我们假定的target离太远）的example，得到的最优函数反而have bad performance. 2. 此外，如果用Regression来考虑多分类。 Multiple class: Class 1 means the target is 1; Class 2 means the target is 2; Class 3 means the target is 3…… 如果用上面这种假设，可以认为Class 3和Class 2 的关系更近，和Class 1的关系更远一些。但实际中，可能这些类别have no relation。 Classification: Ideal Alternatives在上面，我们假设二元分类每一个类别都有一个target，结果不尽人意。 如果将模型改为下图形式，也可以解决分类问题。（挖坑）[2] Generative Model(生成模型)Estimate the Probabilities用概率的知识来考虑分类这个问题，如下图所示，有两个两个类别，C1和C2。 在Testing中，如果任给一个x，属于C1的概率是（贝叶斯公式） $$ P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} $$ 所以在Training应该知道这些的概率： $P(C_1),P(x|C_1),P(C_2),P(x|C_2)$ 先验概率P(C1)和P(C2)很容易得知。 而似然P(x|C1)和P(x|C2)的概率应该如何得知呢？ 如果能假设：类别是C1中的变量x服从某种分布，如高斯分布等，即可以得到任意P(x|C1)的值。 所以Generative Model：是对examples假设一个分布模型，在training中调节分布模型的参数，使得examples出现的概率最大。（极大似然的思想） Prior Probabilities（先验概率）先只考虑Water和Normal两个类别。 先验概率：即通过过去资料分析得到的概率。 在Pokemon的例子中，Training Data是ID&lt;400的水属性和一般属性的Pokemon信息。 Training Data：79 Water，61 Normal。 得到的先验概率 P(C1)=79/(79+61)=0.56, P(C2)=61/(79+61)=0.44。 Probability from Class先只考虑Defense和SP Defense这两个feature。 如果不考虑生成分布模型，在testing中直接计算P(x|Water)的概率，如下图右下角的那只龟龟，在training data中没有出现过，那值为0吗？显然不对。 假设：上图中water type的examples是从Gaussian distribution（高斯分布）中取样出来的。 因此在training中通过training data得到最优的Gaussian distribution的参数，计算样本中没有出现过的P(x|Water)也就迎刃而解了。 Gaussian Distribution多维的高斯分布（高斯分布就是正态分布啦）的联合概率密度： $$ f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\} $$ D: 维数 $\\mu$ : mean $\\Sigma$ :covariance matrix(协方差矩阵) 协方差： $ cov(X,Y)=E[[X-E(X)][Y-E(Y)]]=E(XY)-E(X)E(Y)$ 具体协方差性质，查阅概率论课本吧。 x: vector,n维随机变量 高斯分布的性质只和 $\\mu$ 和 $\\Sigma$ 有关。 $\\Sigma$ 一定时，$\\mu$ 不同，如下图： $\\mu$ 一定， $\\Sigma$ 不同时，如下图： Maximum Likelihood（极大似然）样本分布如下图所示，假设这些样本是从Gaussian distribution中取样，那如何在训练中得到高斯分布的 $\\mu$ 和 $\\Sigma$ 呢？ 极大似然估计。 考虑Water，有79个样本，估计函数 $L(\\mu, \\Sigma)=f_{\\mu, \\Sigma}\\left(x^{1}\\right) f_{\\mu, \\Sigma}\\left(x^{2}\\right) f_{\\mu, \\Sigma}\\left(x^{3}\\right) \\ldots \\ldots f_{\\mu, \\Sigma}\\left(x^{79}\\right)$ 极大似然估计，即找到 $f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\}$ 中的 $\\mu$ 和$\\Sigma$ 使得估计函数最大（使得取出这些样本的概率最大化）。 $\\mu^{*}, \\Sigma^{*}=\\arg \\max _{\\mu, \\Sigma} L(\\mu, \\Sigma)$ 求导计算（过于复杂，但也不是不能做是吧） 背公式[3] $\\mu^{*}=\\frac{1}{79} \\sum_{n=1}^{79} x^{n} \\qquad \\Sigma^{*}=\\frac{1}{79} \\sum_{n=1}^{79}\\left(x^{n}-\\mu^{*}\\right)\\left(x^{n}-\\mu^{*}\\right)^{T}$ 得到Water和Normal的高斯分布，如下图: Do Classification: different $\\Sigma$TestingTesting： $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ P(x|C1)由训练得出的Water的高斯分布计算出，P(x|C2)由Normal的高斯分布计算出。（如下图，过于难打） 如果P(C1|x)&gt;0.5，说明x 属于Water(Class 1)。 Results如果只考虑两个feature（Defense和SP Defense），下图是testing data的样本图，蓝色属于Water，红色属于Normal。 用训练得出的模型，Testing Data: 47% accuracy。（结果如下图） 如果考虑全部features(7个)，重新训练出的模型，结果：Testing Data：54% accuracy。（结果如下图） 结果并不好。参数过多，模型过于复杂，有些过拟合了。 Modifying Model：same $\\Sigma$模型中的参数有两个的Gaussian Distribution中的 $\\mu^* $ 和 $\\Sigma^*$ ，其中协方差矩阵的大小等于feature的平方，所以让不同的class share 同一个 $\\Sigma$ ，以此来减少参数，简化模型。 极大似然估计的估计函数： $$ L\\left(\\mu^{1}, \\mu^{2}, \\Sigma\\right)=f_{\\mu^{1}, \\Sigma}\\left(x^{1}\\right) f_{\\mu^{1}, \\Sigma}\\left(x^{2}\\right) \\cdots f_{\\mu^{1}, \\Sigma}\\left(x^{79}\\right)\\times f_{\\mu^{2}, \\Sigma}\\left(x^{80}\\right) f_{\\mu^{2}, \\Sigma}\\left(x^{81}\\right) \\cdots f_{\\mu^{2}, \\Sigma}\\left(x^{140}\\right) $$ 公式推导:[3] $\\mu$ 的公式不变。 $\\Sigma=\\frac{79}{140} \\Sigma^{1}+\\frac{61}{140} \\Sigma^{2}$ ,即是原 $\\Sigma^1\\ \\Sigma^2$的加权平均。 Results当只考虑两个features，用同样的协方差参数，结果如下图： 可以发现，用了同样的协方差矩阵参数后，边界变成了线性的，所以这也是一个线性模型。 再考虑7个features，用同样的协方差矩阵参数，模型也是线性模型，但由于在高维空间，人无法直接画出其boundary，这也是机器学习的魅力所在，能解决一些人无法解决的问题。 结果：从之前的54% accuracy增加到 73% accurancy. 结果明显变好了。 SummaryThree Steps： Function Set（Model）： Goodness of a function: The mean µ and convariance $\\Sigma$ that maximizing the likelihood(the probability of generating data) Find the best function:easy(公式) Appendix为什么要选择Gaussian DistributionYou can always use the distribution you like. 可以选择你喜欢的任意分布，t分布，开方分布等。 （老师说：如果我选择其他分布，你也会问这个问题，h h h） Naive Bayes ClassifierIf you assume all the dimensions are independent, then you are using Naive Bayes Classifier. 如果假设features之间互相独立， $P\\left(x | C_{1}\\right)=P\\left(x_{1} | C_{1}\\right) P\\left(x_{2} | C_{1}\\right) \\quad \\ldots \\ldots \\quad P\\left(x_{k} | C_{1}\\right) $ 。 xi是x第i维度的feature。 对于每一个 P(xi|C1)，可以假设其服从一维高斯分布。如果是binary features（即feature取值只有两个），也可以假设它服从Bernoulli distribution(贝努利分布)。 Posterior Probability（后验概率）Posterior Probability后验概率，即使用贝叶斯公式，已知结果，寻找最优可能导致它发生的原因。 对 $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ 进行处理。 得到： $$ \\begin{equation} \\begin{aligned} P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\ &=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z) \\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} \\end{aligned} \\end{equation} $$ Worning of Math $z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}=\\ln \\frac{P\\left(x | C_{1}\\right)}{P\\left(x | C_{2}\\right)}+\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}$ $\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}=\\frac{\\frac{N_{1}}{N_{1}+N_{2}}}{\\frac{N_{2}}{N_{1}+N_{2}}}=\\frac{N_{1}}{N_{2}}$ $P\\left(x | C_{1}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma 1|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)\\right\\}$ $P\\left(x | C_{2}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{\\left|\\Sigma^{2}\\right| 1 / 2} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right\\}$ $\\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2}\\left[\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)-\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right]$ $\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)=x^{T}\\left(\\Sigma^{1}\\right)^{-1} x-2\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1}$ $\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)=x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-2\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}$ $\\begin{aligned} z=& \\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2} x^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1} \\\\ &+\\frac{1}{2} x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} \\end{aligned}$ 简化模型后， $\\Sigma^1=\\Sigma^2=\\Sigma$ : $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ 令 $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ 当简化模型后，z是线性的，这也是为什么在之前的结果中边界是线性的原因。 最后模型变成这样： $P\\left(C_{1} | x\\right)=\\sigma(w \\cdot x+b)$ . 在生成模型中，我们先估计出 $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ 的值，也就得到了 $w\\ b$ 的值。 那，我们能不能跳过 $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ ，直接估计 $w\\ b$ 呢？ 在下一篇博客[4]中会继续Classification。 Reference Classification as Regression: Bishop, P186. 挖坑：Classification：Perceptron，SVM. Maximum likelihood solution：Bishop chapter4.2.2","link":"/2020/03/20/Classification1/"},{"title":"「机器学习-李宏毅」:Classification-Logistic Regression","text":"在上篇文章中，讲解了怎么用Generative Model做分类问题。这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。文章主要有两部分：第一部分讲解了Logistic Regression的三个步骤。第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。 Logistic RegressionStep1: Function Set在Post not found: Classification 上一篇文章末尾，我们得出 $P_{w, b}\\left(C_{1} | x\\right)=\\sigma(w\\cdot x+b)$ 的形式，想跳过找 $\\mu_1,\\mu_2,\\Sigma$ 的过程，直接找 $w,b$ 。 因此Function Set: $f_{w, b}(x)=P_{w, b}\\left(C_{1} | x\\right)$ 。值大于0.5，则属于C1类，否则属于C2类。 Step2: Goodness of a Function使用极大似然的思想（在前一篇机率模型/生成模型中有讲） 估计函数是 ：$L(w, b)=f_{w, b}\\left(x^{1}\\right) f_{w, b}\\left(x^{2}\\right)\\left(1-f_{w, b}\\left(x^{3}\\right)\\right) \\cdots f_{w, b}\\left(x^{N}\\right)$ 目标： $ w^{*}, b^{*}=\\arg \\max _{w, b} L(w, b)$ 由于在之前的Regression中，我们都是找极小值点，为了方便处理，将估计函数转换为如下形式的损失函数： $$ \\begin{equation} \\begin{aligned} -\\ln L(w, b)&=-(\\ln f_{w, b}\\left(x^{1}\\right)+\\ln f_{w, b}\\left(x^{2}\\right)+\\ln \\left(1-f_{w, b}\\left(x^{3}\\right)\\right) \\cdots ) \\\\ Loss&=\\sum_{n}-\\left[\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)\\right] \\end{aligned} \\end{equation} $$ 目标 ： $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ Cross entropy（交叉熵） 关于熵、交叉熵、相对熵（KL散度）的理解：强烈安利 上式中的 $\\left[\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)\\right]$ 其实是两个Bernoulli distribution的交叉熵。 交叉熵是什么？ 简单来说，交叉熵是评估两个distribution 有多接近。所以当这两个Bernoulli 分布的交叉熵为0时，表明这两个分布一模一样。 对于 $\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)$ ： Distribution p: p(x = 1) = $\\hat{y}^n$ ; p( x = 0 ) = 1 - $\\hat{y}^n$ Distribution q: q(x = 1 ) = $f(x^n)$ ; q(x = 0 ) = 1 - $f(x^n)$ 交叉熵 $H(p,q)=-\\Sigma_xp(x)\\ln(q(x))$ p是真实的分布，q是预测的分布。 因此，这个损失函数的表达式其实也是输出分布和target分布的交叉熵，即： $L(f)=\\sum_{n} C\\left(f\\left(x^{n}\\right), \\hat{y}^{n}\\right)$ （ $C\\left(f\\left(x^{n}\\right), \\hat{y}^{n}\\right)=-\\left[\\hat{y}^{n} \\ln f\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f\\left(x^{n}\\right)\\right)\\right]$ ） 和Linear Regression不同，为什么Logistic Regression不用square error，而要使用cross entropy。 在1.4小节会给出解释。 Step3: Find the best function在第三步，同样使用Gradient来寻找最优函数。 推导过程： $\\left.\\frac{-\\ln L(w, b)}{\\partial w_{i}}=\\sum_{n}-\\left[\\hat{y}^{n} \\frac{\\ln f_{w, b}\\left(x^{n}\\right)}{\\partial w_{i}}+\\left(1-\\hat{y}^{n}\\right) \\frac{\\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right.}{\\partial w_{i}}\\right)\\right]$ $\\frac{\\partial \\ln f_{w, b}(x)}{\\partial w_{i}}=\\frac{\\operatorname{\\partial\\ln} f_{w, b}(x)}{\\partial z} \\frac{\\partial z}{\\partial w_{i}}$ $\\frac{\\partial \\ln \\sigma(z)}{\\partial z}=\\frac{1}{\\sigma(z)} \\frac{\\partial \\sigma(z)}{\\partial z}=\\frac{1}{\\sigma(z)} \\sigma(z)(1-\\sigma(z))$ $\\frac{\\partial z}{\\partial w_{i}}=x_{i}$ $\\frac{\\partial \\ln \\left(1-f_{w, b}(x)\\right)}{\\partial w_{i}}=\\frac{\\operatorname{\\partial\\ln}\\left(1-f_{w, b}(x)\\right)}{\\partial z} \\frac{\\partial z}{\\partial w_{i}}$ $\\frac{\\partial \\ln (1-\\sigma(z))}{\\partial z}=-\\frac{1}{1-\\sigma(z)} \\frac{\\partial \\sigma(z)}{\\partial z}=-\\frac{1}{1-\\partial(z)} \\sigma(z)(1-\\sigma(z))$ $\\frac{\\partial z}{\\partial w_{i}}=x_{i}$ $\\frac{\\partial \\sigma(z)}{\\partial z}=\\sigma(z)\\cdot(1-\\sigma(z))$ 注：$f_{w, b}(x)=\\sigma(z)$ ; $z=w \\cdot x+b=\\sum_{i} w_{i} x_{i}+b$ $$ \\begin{equation} \\begin{aligned} \\frac{-\\ln L(w, b)}{\\partial w_{i}}&=\\sum_{n}-\\left[\\hat{y}^{n}\\left(1-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n}-\\left(1-\\hat{y}^{n}\\right) f_{w, b}\\left(x^{n}\\right) x_{i}^{n}\\right] \\\\&=\\sum_{n}-\\left(\\hat{y}^{n}-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n} \\end{aligned} \\end{equation} $$ 因此Logistic Regression的损失函数的导数和Linear Regression的一样。 迭代更新： $w_{i} \\leftarrow w_{i}-\\eta \\sum_{n}-\\left(\\hat{y}^{n}-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n}$ 与Linear Regression 的对比如图所示。 If : Logistic + Square Error前面一小节我们提到，在Logistic Regression中使用cross entropy判别一个函数的好坏,那为什么不使用square error来judge the goodness？ 如果使用 Square Error的方法，步骤如下： 来看Step 3: 损失函数的导数是 $2\\left(f_{w, b}(x)-\\hat{y}\\right) f_{w, b}(x)\\left(1-f_{w, b}(x)\\right) x_{i}$ 考虑 $\\hat{y}^n=1$ （即我们的target是1）： 如果 $f_{w,b}(x^n)=1$ , 即预测值接近 target, 算出来的 $\\partial{L}/\\partial{w_i}=0$ 是期望的。 如果 $f_{w,b}(x^n)=0$ , 即预测值原理 target, 算出来的 $\\partial{L}/\\partial{w_i}=0$ 是不期望的。 同理，当考虑 $\\hat{y}^n=0$ 情况时，也是如此。 更直观的看： 上图中，画出了两种损失函数的平面，中心的最低点是我们的target。 但在Square Error中，远离target的蓝色点，也处在很平坦的位置，其导数小，参数的更新会很慢。 因此在Cross Entropy中，离target越远，其导数更大，更新更快。 所以Cross Entropy的效果比Square Error更快，效果更好。 Discriminative V.S. Generative这篇文章中的Logistic Regression是Discriminative Model。 上篇文章中Classification是Generative Model。 有什么区别呢？ 上图中，Generative Model做了假设（脑补），假设它是 Gaussian Distribution，假设它是Bernoulli Distribution。然后去找这些分布的参数，在求出 $w,b$。 而在Discriminative Model中，没有做任何假设，直接找 $w,b$ 参数。 所以，这两种Model经过training找出来的参数一样吗？ 答案是不一样的。 The same model(function set), but different function is selected by the same training data. 在上篇Pokemon的例子中，比较两种方法的结果差异。 可见，在Pokemon的例子总，Discriminative的效果比Generative的效果好一些。 但是Generative Model就不好吗？ Benefit of generative model With the assumption of probability distribution, less training data is needed. 【训练生成模型所需数据更少】 With the assumption of probability distribution, more robust to the noise. 【生成模型对noise data更兼容】 Priors and class-dependent probabilities can be estimated from different sources. 【生成模型中的 先验概率Priors 和 基于类别的分布概率不同】 比如，做语音辨识系统，整个系统是generative的。 因为Prior（某一句话的概率）并不需要从data中知道，可以直接在网络上爬虫统计。 而class-dependent probabilities（这段语音是这句话的概率）需要data进行训练才能得知。 Multi-class classificationsoftmax 假设有三个类别：C1、C2、C3 。模型已经得到，参数分别是 w、b。 对于输入x, 判断x属于哪一个类别。 通过每个类别的 w、b求出 $z^i=w^i\\cdot x+b_i$ Softmax的步骤： exponential：每个z值得到 $=e^z$ . sum：将指数化后的值加起来$=\\Sigma_{j=1}^3e^{z_j}$ output: 每个类别的输出 $y_i=e^{z_1}/\\Sigma_{j=1}^3e^{z_j}$ ，即x属于类别i的概率。 求出的 $1&gt;y_i&gt;0$ 且 $\\Sigma_iy_i=1$ 。 通过Softmax，得到 $y_i=P(C_i|x)$ 。 Steps（手写笔记，略倾斜，原来不切一切还不知道自己歪的这么厉害 泪） Step 1: Step 2: Step 3: 使用Stochastic Gradient（即每个样本更新一次）的话： data: [x, $\\hat{y}$ ] , $\\hat{y}_i=1$ 更新 $w^j$ : $j=i$ : $w^j \\leftarrow w^j-\\eta\\cdot (y_i-1)\\cdot x$ $j\\neq i$ : $w^j \\leftarrow w^j-\\eta\\cdot y_i\\cdot x$ (下次一定，笔记写直一点！) 更为规范的推导见[1] Limitation of Logistic Regression 对于如上情况，Logistic Regression并不能进行分类，因为他的boundary 应该是线性的。 Feature Transforming如果对feature做转换后，就可以用Logistic Regression处理。 重定义feature， $x_1’$ :定义为到[0,0]的距离， $x_2’$ :定义为到[1,1]的距离。 于是图变成下图，即可用Logistic Regression进行分类。 但这样的做法，就不像人工智能了，因为Feature Transformation需要人来设计，而且较难设计。 Cascading logistic regression models另一种做法是，将logistic regression连接起来。 上图中，左边部分的两个logistic regression就相当于在做Feature Transformation，右边部分相当于在做Classification。 而通过这种形式，将多个model连接起来，也就是大热的Neural Network。 Reference Multi-class Classification推导：Bishop，P209-210 关于Entropy, Cross Entropy, KL-Divergence的理解：强烈安利：https://www.youtube.com/watch?v=ErfnhcEV1O8","link":"/2020/04/01/Classification2/"},{"title":"「机器学习-李宏毅」：Deep Learning-Introduction","text":"这篇文章中，介绍了Deep Learning的一般步骤。 Up and downs of Deep Learning 1958: Perceptron (linear model) 1969: Perceptron has limitation 1980s: Multi-layer perceptron ​ Do not have significant difference from DNN today 1986: Backpropagation ​ Usually more than 3 hidden layers is not helpful 1989: 1 hidden layer is “good enough”, why deep? 2006: RBM initialization (breakthrough) 2009: GPU 2011: Start to be popular in speech recognition【语音辨识】 2012: win ILSVRC image competition 【图像识别】 Step 1: Neural Network在将Regression 和 Classification时，Step 1 是确定一个function set。 在Deep Learning中，也是相同的，只是这里的function set就是一个neural network的结构。 上图中，一个Neuron就是如上图所示的一个unit，neuron之间不同的连接方式构成不同的Neural Network。 Fully Connect Feedforward Network 这是一个Fully Connect Feedforward Network【全连接反馈网络】，其中每个neuron的activation function都是一个sigmod函数。 为什么说neural network其实就是一个function呢？上面两张图中，输入是一个vector，输出也是一个vector，可以用下面函数来表示。 $$ f\\left(\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]\\right)=\\left[\\begin{array}{c}0.62 \\\\ 0.83\\end{array}\\right] f\\left(\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]\\right)=\\left[\\begin{array}{l}0.51 \\\\ 0.85\\end{array}\\right] $$ 上图为全连接网络的一般形式，第一层是Input Layer，最后一层是Output Layer，中间的其他层称为Hidden Layer。 而Deep Learning中的Deep的含义就是Many hidden layers的意思。 Matrix Operation 上图的全连接网络中，第一个hidden layer的输出可以写成矩阵和向量的形式： $$ \\sigma\\left(\\left[\\begin{array}{cc}1 & -2 \\\\ -1 & 1\\end{array}\\right]\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]+\\left[\\begin{array}{c}1 \\\\ 0\\end{array}\\right]\\right)=\\left[\\begin{array}{c}0.98 \\\\ 0.12\\end{array}\\right] $$ 更为一般的公式，用W表示权重，b代表bias，a表示hidden layer的输出。输出vector y可以写成 $y = f(x)$ 的形式，即： $y= f(x)=$ 转换为矩阵运算的形式，就可以使用并行计算的硬件技术（GPU）来加速矩阵运算，这也是为什么用GPU来训练Neural Network 更快的原因。 Output Layer在 Logistic Regression中第4节讲到Logistic Regression有局限，消除局限的一种方法是Feature Transformation。 但是Feature Transformation需要人工设计，不太“机器学习”。 在下图全连接图中，把Output Layer换成一个Multi-class Classifier（SoftMax），而其中Hidden Layers的作用就是Feature extractor，从feature x提取出新的feature，也就是 output layer的输入。 这样就不需要人工设计Feature Transformation/Feature engineering，可以让机器自己学习：如何将原来的feature转换为更好分类的feature。 Handwriting Digit Recognition 在手写数字辨别中，输出是一个16*16的image（256维的vector），输出是一个10维的vector，每一维表示是该image是某个数字的概率。 在手写数字辨别中，需要设计neural network的结构来提取输入的256维feature。 Step 2: Goodness of function之前我们已经使用过的最小二乘法和交叉熵作为损失函数。 一般在Neural Network中，使用output vector 和target vector的交叉熵作为Loss。 Step 3: Pick the best function在NN中，也使用Gradient Descent。 但是，Deep Neural Network中，参数太多了，计算结构也很复杂。 Backpropagation：an efficient way to compute $\\partial{L}/\\partial{w}$ in neural network. Backpropagation本质也是Gradient Descent，只是一种更高效进行Gradient Descent的算法。 在很多 toolkit（TensorFlow，PyTorch ，Caffe等）中都实现了Backpropgation。 Backpropagation部分，见下一篇博客。 Reference","link":"/2020/04/18/DL-introdunction/"},{"title":"「Tools」：Docker","text":"本篇文章主要分四个部分，首先介绍了Docker是什么：为什么会有Docker技术的出现；虚拟化技术和容器虚拟化技术的区别；Docker的基本组成；Docker的运行为什么会比虚拟机快。 第二个部分主要介绍了Docker的常用命令，包括镜像命令和容器命令，文中还从底层的角度分析Docker镜像。 第三个部分介绍了Docker中的容器数据卷，和如何挂载数据卷。 最后一个部分，简单介绍了Dockerfile文件。 Docker简介Docker 是什么开发和运维之间的环境和配置问题：在我的机器上可以正常工作。 把代码/配置/系统/数据等全部打包成镜像，运维工程师带环境安装软件。 Docker基于Go语言实现的云开源项目，Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，做到一次封装，处处运行。 Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。 Docker解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体分布的容器虚拟化技术。 能干嘛？之前的虚拟化技术虚拟机是带环境安装的解决方案，可以在一种操作系统中运行另一种操作系统。 虚拟机用软件实现了硬件、内核、操作系统及应用程序，对底层来说，虚拟机就是一个普通文件。 虚拟机的缺点缺点： 资源占用多 冗余步骤 启动慢 容器虚拟化技术Linux容器（Linux Containers,LXC)，对进程隔离，将软件运行所需的资源打包到一个隔离的痛其中。 Linux容器不是模拟一个完整的操作系统，而是将软件工作所需的库资源和设置等资源打包到一个隔离的容器中，因此Linux容器变得高效且轻量，并且能保证部署在任何环境中的软件都能始终如一地运行。在 宿主机上，Linux容器就是一个运行的进程，所以Linux容器是对进程进行隔离。 再看Docker的图标，上面的集装箱就是一个一个容器，鲸鱼就是宿主机的硬件、内核。 比较： 传统虚拟机技术虚拟一套硬件，在其上运行一个完整的操作系统，再运行所需的应用进程。 容器内的应用直接运行于宿主的内核，容器内没有硬件虚拟，容器更轻便。 容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响。 所以，可以认为容器是一个轻量的Linux。 开发/运维（DevOps)DevOps, Develop and Operations, 可以利用Docker实现开发自运维。 更快速的应用交付和部署。 更便捷的升级和扩缩容器。 更简单的系统运维。 更高效的计算资源利用。 Docker的基本组成Docker的三要素： 镜像(image)：只读的模版，类比Java中的类。镜像可以用来创造Docker容器。 容器(container)：镜像的实例，独立运行的一个或一组实例。可以把容器看作一个简易版的Linux环境。 仓库(repository)：保存镜像的场所。 Docker本身是一个容器运行载体或管理引擎。 把应用程序和配置打包成为一个可交付的运行环境，打包好的运行环境就是一个image镜像文件，只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模版。Docker根据image文件生成容器的实例。 Docker运行原理Docker是一个C/S结构的系统。 Docker守护进程运行在宿主机上，客户通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。 为什么比虚拟机快 Docker有比虚拟机更少的抽象层，不需要实现硬件资源虚拟化，运行在docker容器中的程序直接使用的都是实际物理机的硬件资源。 Docker使用宿主机上的内核，新建容器时，不需要和虚拟机一样重新加载一个操作系统内核。因此新建一个dock er容器只需要几秒钟。 Docker镜像加速可以登陆阿里云获得专属镜像加速器链接，配置本机Docker拉取镜像仓库的链接，将拉取镜像的链接从DockerHub换成阿里云的仓库，下载更快捷。 具体按照系统自行Google。 Docker常用命令docker version docker info docker –help 帮助命令 镜像命令 列出本地images docker images repo 参数 -a :包括中间映像层 -q : 只显示镜像id –digests :显示摘要信息 –no-trunc :显示完整信息 从Docker Hub查询镜像名 docker search [OPTIONS] image_name –no-trunc -s n：收藏数不小于n的镜像 –automated 下载/拉取镜像 docker pull 镜像名[:TAG] 默认:latest 删除镜像 docker rmi 镜像唯一名字/镜像ID -f :强制删除运行中的镜像文件 删除单个： docker rmi -f 镜像ID 删除多个 docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除全部： docker rmi -f $(docker images -qa) 容器命令容器是一个建议的Linux。 启动容器： docker run [OPTIONS] IMAGE [COMMAND] [ARG...] --name 容器名 :为容器指定一个名字 -d ：后台运行容器，返回 -i : 以交互模式运行容器，通常与-t 一同使用 -t :为容器重新分配一个伪输入终端，通常与-i 一同使用。 -p :主机端口和容器端口 -p ip:hostPort:containerPort -p ip::containerPort -p hostPort:containerPort -p containerPort -P :随机分配端口 列出当前运行所有容器： docker ps -a : 列出当前所有正在运行的容器和历史上运行过的容器 -l :显示最近创建的容器 -n :显示最近创建的num个容器 docker ps -n 3 -q :静默模式，只显示容器编号 --no-trunc : 不截断输出 退出/停止容器 容器停止退出 exit 容器不停止退出 Ctrl + P + Q 启动容器 docker start 容器名/容器ID 重启容器 docker restart 容器名/容器ID 重启成功后返回容器名/容器ID 停止容器 docker stop 容器名/容器ID 强制停止容器 docker kill 容器名/容器ID 删除已停止的容器 docker rm 镜像ID 一次删除多个容器 docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm （管道传递参数） 启动守护式容器 docker run -d 镜像名/镜像ID docker run -d -p 主机端口:容器内端口 容器ID 如果使用 docker ps -a 查看，会发现容器已经退出 Docker容器后台运行，就必须要有一个前台进程与之交互 如果容器后台运行，如果不是一直挂起的命令，他就会自动退出。 所以最佳的解决方式是将运行的进程以前台进程运行。 查看容器日志 docker logs -f -t --tail 容器ID -t：显示加入时间戳 -f ：持续显示最新的日志 --tail ：显示最后多少条 显示容器内运行的进程 docker top 容器ID 查看容器内部的细节 docker inspect 容器ID 进入正在运行的容器并以命令行与之交互 直接进入容器启动命令的终端 docker attach 容器ID 在容器中打开新的终端，并且可以启动新的进程。 docker exec -it 容器ID bashShell docker exec -it 容器ID /bin/bash 和docker attach 容器ID 相同。 把容器内文件拷贝文件到主机上 docker cp 容器ID:容器内的路径 目录主机路径 docker cp 130b1f6708dd:/x.txt /Users Docker镜像image： 镜像是轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，包含运行某个软件所需的所有内容，包括代码、库、环境变量、配置文件等。 UnionFSUnionFS（联合文件系统）是一种分层、轻量高性能的文件系统，支持对文件系统的修改作为一次提交来一层层的叠加，同时将不同目录挂载到同一个虚拟文件系统下。 Union文件系统时Docker镜像的基础。 镜像通过分层来进行继承，基于基础镜像可以制作各种具体的应用镜像。 特点：一次加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，最终的文件系统包含所有底层的文件和目录。 Docker镜像的加载Docker镜像实际是由一层一层的文件系统组成。 bootfs(boot file system)包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。 Docker镜像的最底层就是bootfs，这一层和典型的Linux/Unix系统是一样的，包含bootloader和kernel。 当boot加载完成后，整个kernel就在内存中了，此时内存的使用权已由bootfs转交给kernel，此时系统也会卸载bootfs。 rootfs（root file system)，在bootfs之上，包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Linux，Centos等。 平常安装等虚拟机的CentOS都是几个G，为什么docker版的centos只有几百兆？ 对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库，因为底层直接使用宿主机的kernel，自己只需要提供rootfs就行了。 因此，对于不同的Linux发行版，bootfs基本一致，rootfs会有差别，因此不同的发行版可以共用bootfs。 分层的镜像在docker image下载、删除时，可以发现是一层一层的。 分层的镜像的一个最大的好处是共享资源。 如果有多个镜像都是从相同的base镜像build而来，那宿主机中只需在磁盘上保存一份base镜像，同时内存中也只需要加载一份base镜像，就可以为所有的容器服务了。 镜像commit操作Docker镜像都是只读的，但当镜像实例化，启动容器时，一个新的可写层被加载到镜像的顶部，这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 docker commit提交容器层副本使之成为一个新的镜像。 docker commit -m &quot;message&quot; -a &quot;author&quot; 容器ID 命名空间/新建镜像名[:TAGS] 容器数据卷Docker理念： 将代码和运行的环境打包形成容器，运行伴随着容器，但希望运行中的数据是持久化的，希望容器之间是共享数据的。 如果不通过docker commit生成新的镜像，使得数据作为镜像的一部分保存下来，那么容器删除后，数据也没有了，为了保存数据，使用容器数据卷。 如果不使用commit 生成新的镜像，Docker容器产生的数据将随着容器的删除而一起删除，为了保存数据，我们使用卷。 卷卷就是目录或者文件，存在于一个或多个容器中，由docker挂载到容器，但不属于UnionFS（联合文件系统），因此能绕过UnionFS，提供一些用于持续存储或共享数据的特性。 卷的设计目的就是为了数据持久化，完全独立于容器的生存周期，因此Docker不会在容器删除的时候删除其挂载的数据卷。 数据卷的特点： 数据卷可以在容器之间共享或重用数据。 卷中的更改直接在所有共享该卷容器中生效。 数据卷中的更改不会包含在镜像的更新中。 数据卷的生命周期一直持续到没有容器使用它为止。 数据卷挂载直接命令添加 数据挂载(-v value) docker run -it -v /宿主机目录:/容器内目录 镜像名 查看挂载是否成功 docker inspect 镜像名 宿主机和容器之间实现数据共享，在容器停止退出后，修改宿主机数据，数据完全同步。 带权限的数据挂载，加:ro (readonly) docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 此时容器中对数据卷只读。 当挂载主机目录事，Docker访问出现cannot open directory .: Permission denied 解决办法：在挂砸目录后加参数 --privileged=true DockerFile添加在DockerFile中可以使用VOLUME 指令给镜像添加一个或多个数据卷。 注意： Docker出于可移植性和分享的考虑，指令中只有容器内的地址，因为宿主主机目录依赖于特定的主机。 Dockerfile文件构建 1234FROM centosVOLUME [&quot;/dataVolumeContainer1&quot;, &quot;/dataVolumeContainer2&quot;, &quot;/dataVolumeContainer3&quot;]CMD echo &quot;finished,-----success&quot;CMD /bin/bash 以上docker文件类似于一下命令挂载 1docker run -it -v /host1:/dataVolumeContainer1 -v/host1:/dataVolumeContainer2 -v /host3:/dataVolumeContainer3 centos /bin/bash build构建镜像（-f file) docker build -f DockerFile文件路径 -t 命名空间/镜像名 镜像生成路径 docker build -f ./Dockerfile -t fred/centos . 数据卷容器数据容器卷： 命名的容器挂载数据卷，其他容器通过挂载这个父容器实现数据共享，挂载数据卷的容器称为数据卷容器。 容器之间可以传递配置信息，数据卷的生命周期一直持续到没有容器使用它为止。 挂载数据卷到父容器（命名为dc01 ）上：命令添加/Dockerfile添加 容器继承父容器的数据卷(--volumes-from ) docker run -it --name 子容器名 --volumes-from 父容器名 生成子容器的镜像名 e.g: docker run -it --name dc02 --volumes-from dc01 fred/centos dc01已经挂载数据卷，此时dc02继承它，那么dc01挂载的数据卷，dc02也实现了共享。 DockerfileDockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。 构建容器卷的步骤： 编写Dockerfile文件 docker build构建 docker run启动容器 Centos的Dockerfile文件 123456789101112131415FROM scratchADD centos-7.8.2003-x86_64-docker.tar.xz /LABEL \\ org.label-schema.schema-version=&quot;1.0&quot; \\ org.label-schema.name=&quot;CentOS Base Image&quot; \\ org.label-schema.vendor=&quot;CentOS&quot; \\ org.label-schema.license=&quot;GPLv2&quot; \\ org.label-schema.build-date=&quot;20200504&quot; \\ org.opencontainers.image.title=&quot;CentOS Base Image&quot; \\ org.opencontainers.image.vendor=&quot;CentOS&quot; \\ org.opencontainers.image.licenses=&quot;GPL-2.0-only&quot; \\ org.opencontainers.image.created=&quot;2020-05-04 00:00:00+01:00&quot;CMD [&quot;/bin/bash&quot;] Dockerfile构建过程基础规则： 保留字指令必须大写，且后面必须至少一个参数。 指令顺序执行。 注释符号：# 每条指令都会创建一个新的镜像层，并对该镜像进行提交。 执行流程： 从基础镜像运行一个容器 执行一条指令后并对容器进行修改 执行类似docker commit操作提交一个新的镜像层 docker再基于刚提交的镜像运行一个容器 直到文件所有指令执行完成 辨析Dockerfile，Docker镜像，Docker容器： Dockerfile、Docker镜像与Docker容器从软件应用的角度分别代表软件的三个不同阶段： Dockerfile是软件的原材料，是面向开发的。 Dockerfile定义了进程需要的一切东西。Dockerfile设计的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程等等。 Docker镜像是软件的交付品，是交付标准。 在用Dockerfile定义一个文件之后，docker build会产生一个Docker镜像，运行 Docker镜像时，才真正开始提供服务。 Docker容器则可以认为是软件的运行态，涉及部署和运维。 Docker容器是直接提供服务的。 Dockfile体系结构 FROM 基础镜像 MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 容器构建时需要运行的命令 EXPOSE 当前容器对外暴露的端口号 WORKDIR 指定在创建容器后，终端默认登陆进来的工作目录 ENV 构建容器中的设置环境变量 ADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包 COPY 拷贝文件和目录到镜像中 COPY src dest COPY [&quot;src&quot;, &quot;dest&quot;] VOLUME 容器数据卷 用于数据保存和持久化工作 CMD 指定一个容器启动时运行的命令 shell 格式：CMD &lt;命令&gt; exec格式：CMD[“可执行文件”, “arg1”, “arg2”,…] 参数列表格式：CMD [“arg1”, “arg2”,…] 在指定来ENTRYPOINT指令后，用⌘指定具体的参数。 只有最后一个CMD生效，CMD会被docker run之后的参数替换 ENTRYPOINT 指定一个容器启动时运行的命令 会在docker run后面追加参数 ONBUILD 当构建一个被继承的Dockerfile时，父镜像在被子镜像继承后父镜像的ONBUILD被触发","link":"/2020/07/21/Docker/"},{"title":"「Tools」:Git and GitHub","text":"这篇文章具体讲述了Git工具的基本本地库操作和与远程库交互的基本操作，包括使用GitHub进行团队外的协作开发。 GitGit简介Git历史：Linux的代码管理： 1991 Linus本人手动合并代码 2002 使用BitKeeper商业软件，授予Linux社区免费使用版本控制 2005 Linus自己用C语言开发了一个分布式版本控制系统：Git Linus: Talk is cheap, show me the code. 2008 Github上线 Git的优势： 大部分操作在本地完成，不需要联网。 完整性保证：每次提交进行哈希。 尽可能添加数据而不是删除/修改数据，版本都在。 分支操作快捷流畅，以快照的形式。 与Linux命令全面兼容。 Git的结构 Git和代码托管中心代码托管中心的任务：维护远程库 局域网环境：搭建GitLab作为代码托管中心 外网环境：可以用GitHub和码云作为代码托管中心 本地库和远程库的交互团队内： 团队外： fork：复制一份属于自己的远程库 开发新的内容后向库的拥有者 pull request拉取请求，原拥有者可以审核，审核通过后执行merge操作合并到自己的远程库的分支上。 Git命令行基本操作本地库初始化 初始化本地库 git init .git文件存放的是本地库相关的子目录和文件，不要删除和随意修改。 删除本地git库也就是删除本地的.git 文件 删除本地库：rm -rf .git 本地库设置签名 形式： 用户名： Email： 作用：区分不同开发人员的身份 注：这里设置的签名与远程代码托管中心没有关系。 命令： 项目级别：设置签名仅在本地库起效（如果既有项目级别和用户级别的签名，按照项目级别为准） 设置用户名命令：git config user.name *** 设置用户邮箱： git config user.email *****@outlook.com 该信息保存在.git/config文件中。 用户级别：设置签名在当前操作系统的用户范围 设置用户名命令：git config --global user.name *** 设置用户邮箱命令：git config --global user.email **** 该消息保存在系统文件~/.gitconfig文件 查看状态 查看工作区、暂存区状态。 git status 本地库版本信息查看HEAD: 指针，表示当前版本的位置。 显示版本： 完整的版本信息记录（包括完整版本哈希值、作者、提交时间） git log （空格向下翻页；b 向上翻页； q退出显示） 一行只显示一个版本，简洁版。 git log --pretty=oneline 一行也只显示一个版本，终极简洁版，哈希值也只显示前面的一部分（当作该版本的局部索引）。 git log --oneline git reflog ：跟踪本地库的历史更新，可以看到HEAD之后（“未来“）的版本。 HEAD@{i}：i表示HEAD指针移动到该版本需要后退的步数。 工作区操作：添加/修改/提交/删除对于这些命令，其实在使用git status 时，都会有相关提示。 提交到暂存区（增加、修改、删除）：git add &lt;filename&gt; 撤销工作区的修改（discard changes in working directory） git checkout -- &lt;file&gt; 暂存区操作：添加/修改/提交/删除对于这些命令，其实在使用git status 时，都会有相关提示。 添加/修改：将工作区的文件添加到暂存区（/或更新暂存区的文件）。 git add &lt;filename&gt; 删除：将文件从暂存区删除 git rm --cached &lt;filename&gt; 提交：将暂存区的文件提交到本地库。（输入提交信息） git commit -m &lt;message&gt; [filename] 修改后的提交：提交修改后的文件至本地库（已在暂存区有旧版本），同时更新暂存区和本地库。 git commit -m &lt;message&gt; [filename] 修改commit 注释git commit --amend 版本前进/后退本质是HEAD指针的移动。 基于索引操作：版本可以后退和前进。(索引就是reflog形式下的局部哈希值) git reset --hard [局部索引值] 使用^ ： 版本只能往后退 。（基于reflog形式下的步数） git reset --hard HEAD^^ (后退两步) 使用~n ：版本往后退n步。 git reset --hard HEAD~3 版本前进/后退reset命令的参数对比： --soft 仅仅在本地库移动HEAD指针。 如下图，显得暂存区和工作区版本比本地库前进了一步。 --mixed 在本地库移动HEAD指针 并重置暂存区，暂存区和本地库一致。 如下图，显得工作区版本比本地库和暂存区版本前进了一步。 --hard 在本地库移动HEAD指针 重置暂存区 重置工作区 删除文件后找回前提：删除前，文件存在的状态提交到了本地库。 操作： 删除的操作已经提交到本地库 删除操作： 123rm a.txtgit add a.txtgit commit -m &quot;delete a.txt&quot; a.txt git reset --hard [历史版本指针位置] 删除操作未提交到本地库 删除操作： 12345//工作区删除rm a.txt//缓存区也删除rm a.txtgit add a.txt git reset --hard HEAD 比较差异 工作区文件和暂存区文件比较 git diff [filename] 暂存区和本地库内容比较 git diff --cached 工作区文件和本地库文件比较，指针可以使用HEAD^ git diff [指针] [filename] 可以不加文件名，即比较全部文件。 任意版本之间的内容比较： 显示两次差异文件 git diff 版本1hash/指针 版本2hash/指针 --stat 具体显示差异文件 git diff hash1/指针 hash2/指针 filename 分支管理分支分支：版本控制过程中，使用多条线同时推进多个任务。 master: 主版本分支/部署到服务器运行的分支。 feature_ ：开发其他功能的分支。 hot_fix: 热修复，bug修复分支。 分支的好处： 并行：同时并行推进多个功能开发。 独立：各个分支在开发过程中，如果有个分支开发失败，不会影响其他分支。 分支操作：创建/查看/切换/合并 创建分支 git branch &lt;branch name&gt; 删除分支 git branch -d &lt;branch name&gt; 删除远程分支： 查看远程分支：git branch -r 删除本地的远程跟踪分支：git branch -r -d origin/branch-name 删除真正的远程分支：git push origin : branch-name 查看分支 git branch -v 切换分支 git checkout [branch name] 合并分支 切换到接受修改的分支（如master） git checkout [合并到的主分支] 执行merge合并操作 git merge [有修改的分支] 解决合并分支后产生的冲突冲突的表现，显示到有冲突的文件： 冲突解决： 删除文件中的特殊符号 协商再编辑文件 添加新文件 git add [filename] 提交（注意：此时的提交不能带文件名） git commit -m &quot;message&quot; rebase：整理分支 rebase 操作可以把本地未push的分叉提交历史 整理成直线。 rebase的目的：在查看历史提交的变化时更容易。 Git 基本原理哈希算法特点： 得到的加密密文长度相同。 算法确定，输入确定后，输出一定确定。 输入数据发生一点点变化，输出的变化会很大。 Git底层采用SHA-1算法。 哈希算法保证了Git的数据完整性。 Git保存版本的机制集中式版本控制工具（如SVN）：保存的信息是每个基本文件和每个文件随时间逐步累积的差异。 Git是分布式的版本控制工具。 Git把数据看作是文件系统的快照（可以理解为当前内存版本的文件的索引），每次提交更新时Git对当前内存的全部文件制作一个快照并保存这个快照的索引。如果文件没有修改，Git不会重新存储该文件，只是保留一个连接指向之前存储的文件。 Git的提交对象： 上图中，每个文件都有一个哈希值/索引，提交时新建一个树结点，其中包含指向每个文件的指针/索引，提交的对象包括该树结点的指针/哈希值。 Git版本对象链条： 所以： Git 分支的创建：等于新建一个指向版本的指针。 Git分支的切换：改变HEAD指针所指的指针。 Git分支版本的移动：分支指针的移动。 GitHub基本交互创建/查看远程库地址别名在GitHub创建远程库后 在本地添加远程库地址别名 git remote add [别名] [https/ssh 地址] git remote add orgin https://... 查看当前所有远程库地址别名 git remote -v 移除远程库 git remote remove [别名] 本地库内容推送到远程库前提：本地库已添加远程库地址别名。 在本地将本地库推送到远程分支 git push [别名] [分支名] git push origin master 将远程库克隆到本地库 git clone https/ssh_address 效果：完整把远程库下载到本地；添加origin作为远程库地址别名；初始化本地库（含有.git文件） 团队内协作团队成员邀请项目创建者在项目”Setting”-“Callaborators”里邀请成员。 拉取：同步本地库 在本地pull操作同步本地库与远程库相同。 fetch：查看远程库分支，可以切换至远程库分支，查看远程库分支的文件具体内容，决定是否合并。 git fetch [远程库地址别名] [远程分支名] 切换至远程库分支 git checkout orgin/master merge：（切换至本地库master分支），合并远程库分支。 git merge [远程库地址别名]/[远程分支名] pull = fetch + merge git pull [远程库地址别名] [远程分支名] 注：如果是简单的修改，可以直接pull拉取，如果不确定远程库修改内容，可以先fetch后再合并分支。 本地拉取与远程库冲突 冲突发生原因：不是基于GitHub远程库的最新版进行修改，就不能push，在修改之前必须pull。 pull拉取下来后如果进入冲突状态，就按照“分支冲突解决办法” 跨团队协作 fork操作：复制一份远程库。 团队外的人，在项目节目点fork，即可fork一份远程库，该远程库的来源是创建该库的开发者，而fork出的远程库的所有者是执行fork操作的人。 clone操作：下载到本地库。 push操作：本地修改，推送至远程库。 pull request 请求：在远程库（代码托管中心GitHub）执行pull request请求，请求合并该修改到原远程库。 （原远程库所有者）审核操作：确认是否合并。 SSH登陆 在当前用户的根目录，生产.ssh密钥目录 ssh-keygen -t rsa -C email@address 将.ssh/id_rsa.pub 文件的内容复制到GitHub新建ssh密钥的窗口下。 创建ssh远程地址别名 git remote add origin ssh_address Git仓库和SSH-key关联 ssh-add &quot;id_rsa address Git工作流待补充[1] Gitlab服务器搭建待补充[2] Reference Git工作流待补充 Gitlab服务器搭建","link":"/2020/07/18/Git-and-GitHub/"},{"title":"「机器学习-李宏毅」：Convolution Neural Network（CNN）","text":"这篇文章中首先介绍了为什么要用CNN做图像识别，或者说图像识别问题的特点是什么？文章中也详细介绍了CNN的具体架构，主要包括Convolution、Max Pooling、Flatten。文章最后简要介绍了CNN在诸多领域的应用。 Why CNN for Image?图片本质都是pixels。 在做图像识别时，本质是对图片中的某些特征像素（properities)识别。 So Why CNN for image? Some patterns are much smaller than the whole image. A neuron does not have to see the whole image to discover the pattern. Connecting to small region with less parameters. 【很多特征图案的大小远小于整张图片的大小，因此一个neuron不需要为了识别某个pattern而看完整张图片。并且，如果只识别某个小的region，会减少大量参数的数目。】 如下图，用一个neuron识别红框中的beak，即能大概率认为图片中有bird。 The same patterns appear in different regions. They can use the same set of parameters. 【同样的pattern可能出现在图片的不同位置。pattern几乎相同，因此可以用同一组参数。】 如下图，两个neuron识别两个不同位置的beak。被识别的beak几乎无差别，因此neuron的参数可以是相同的。 Subsampling the pixels will not change the object. 【一张图片是由许多pixel组成的，如下图，如果去掉图片的所有奇数行偶数列的pixel，图片内容几乎无差别。并且，Subsample pixels，即减少了输入的size，也可以减少NN的参数数量。】 The whole CNNCNN的架构如下图。 一张图片经过多次Convolution、Max Pooling得到新的image，再将新的image Flatten（拉直）得到一组提取好的features，将这组features放入前馈神经网络。 Convolution满足图片识别的： Property 1 : Some patterns are much smaller than the whole image. Property 2 : The same patterns appear in different regions. Max Pooling满足图片识别的： Property 3 : Subsamplingthe pixels will not change the object. CNN-Convolution一张简单的黑白图片如下图，0为白色，1为黑色。 如果图片是彩色的，即用RGB三原色来表示，用三个matrix分别表示R、G、B的值，如下图： 下文中，以黑白图举例。 Property 1设计Filer matrix满足Property 1，如下图： 上图中，filter的大小是3*3，可以检测到小区域的某个pattern。 每个filter的参数都是NN中的参数，需要learned。 如果是彩色图片，filter应该是3张3*3matrix组成的，分别代表R、G、B的filter。 Property 2为了满足Property 2，filter可以在图片中移动。设置stride，即每次filter移动的步长。 filter与覆盖图片的位置做内积，需要走完整张图片，最后得到一张feature map。 下图为stride=1的convolution结果： Convolution layer（卷积层）有几个filter，就会得到几张feature maps。 Convolution v.s. Fully ConnectedFully Connected: 如果用全连接的方式做图片识别，图片的每一个pixel都要和第一层的所有neurons连接，需要大量参数。 如下图： Convolution: 而在Convolution中，把feature map中的每一个值作为neuron的输出，因此图片中只有部分pixels会和第一层的第一个neuron连接，而不是全部pixels。 对于一个3*3的filter，一个neuron的连接如下： filter中的值是连接参数，则每一个neuron只需要与3*3个input连接，与全连接相比减少了大量参数。 shared weights filter在图中移动时，filter的参数不变，即第二个neuron的连接参数和第一个neuron的连接参数是相同的，连接图如下： 通过filter实现了shared weights（参数共享），更大幅度减少了参数数量。 CNN-Max PoolingMax Pooling：将convolution layer的neuron作为输入，neuron的activation function其实就是Maxout（Maxout介绍见 Post not found: tips-for-DL/#Maxout 这篇 的介绍）。 将convolution layer得到的feature map做Max pooling（池化），即取下图中每个框中的最大值。 如下图，6*6的image经过Convolution layer 和 Max Pooling layer后，得到了new but smaller image，新的image的由两层channel组成，每层channel都是2 * 2的image。 一个image每经过一次Convolution layer 和 Max Pooling layer，都会得到a new image。 This new image is smaller than the origin image. And the number of channel (of the new image) is the number of filters. 举个例子： Convolution layer有25个filters，再经过Max Pooling，得到的新的image有25 个channel。 再重复一次Convolution 和Max Pooling，新的Convolution layer也有25个filters，再经过Max Pooling，得到的新的image有多少个channel呢？ 答案是25个channel。 注意 ：在第二次Convolution中，image有depth，depth=25。因此在convolution中，filter其实是一个cubic，也有depth，depth=image-depth=25，再做内积。 因此，新的image的channel数是等于filter数的。 FlattenFlatten很好理解，将最后得到的新的image 拉直（Flatten）为一个vector。 拉直后的vector是一组提取好的features，作为 前馈神经网络的输入。 zero padding如何让卷积后的图像不变小？ 答案就是zero padding，在原图的padding填0，再做卷积。 zero-padding后如下图： 卷积后，图像大小不变： What dose CNN learn为什么CNN能够学习pattern，最终达到识别图像的目的？ Filter在下图CNN过程中，我们先分析能从Convolution layer的filter能够学到什么？ 每个filter本质上是一组shared weights 的neuron。 因此，定义这组filter的激活程度，即： Degree of the activation of the k-th filter: $a^k=\\sum_{i=1}^{11}\\sum_{j=1}^{11}a_{ij}^{k}$ . 目标是找到使k-th filter激活程度最大的输入image，即 $x^{*}=\\arg \\max _{x} a^{k}$ ，(method :gradient descent). 部分结果如下图： (每一张图都代表一个让filter激活程度最大的 $x$) 上图中，找到使filter激活程度最大的image，即上图中每个filter可以检测一定的条纹，只有当图像中有该条纹，filter（一组neuron）的激活程度（即输出）才能达到最大。 Neuron（Hidden layer）这里的neuron指前馈神经网络中的neuron，如下图的 $a_j$ : 目标：找到使neuron的输出最大的输入image，即： $x^{*}=\\arg \\max _{x} a^{j}$ . 部分结果如下： （每一张图代表一个neuron) 在上图中，感觉输入像一个什么东西吧emmmm。 但和filter学到的相比，neuron学到的不仅是图中的小小的pattern（比如条纹、鸟喙等），neuron学的是看整张图像什么。 Output（Output layer）再用同样的方法，看看输出层的neuron学到了什么，如下图的 $y_i$ ： 在手写数字辨识中 $y_i$ 是数字为 $i$ 的概率，因此目标是：找到一个使输出是数字 $i$ 概率最大的输入image，即： $x^{*}=\\arg \\max _{x} y^{i}$ . 结果如下图： 结果和我们期望相差甚远，根本不能辨别以上图片是某个数字。 这其实也是DNN的一个特点: Deep Neural Networks are Easily Fooled [1]，即NN学到的东西往往和人类学到的东西是不一样的。 CNN所以CNN到底学到了什么？ 上文中，output 学到的都是一团密密麻麻杂乱的像素点，根本不像数字。 但是，再考虑手写数字image的特点：图片中应该有少量模式，大片空白部分。 因此目标改进为： $x^{*}=\\arg \\max _{x}\\left(y^{i}+\\sum_{i, j}\\left|x_{i j}\\right|\\right)$ $\\sum_{i, j}\\left|x_{i j}\\right|$ 就像是regularization的限制。 结果如下： （注：图中白色为墨水，黑色为空白） ApplicationDeep DreamCNN exaggerates what it sees. CNN可以夸大图片中他所看到的东西。 比如： 可以把下图 变成下图（emmmm看着有点难受） 附上生成deep dream image的网站[2] . Deep Style[3]Given a photo, make its style like famous paintings. 上图中，用一个CNN学习图中的content，用另一个CNN学习风格图中的style。 再用一个CNN使得输入的图像content像原图，风格像另一张图。 Playing GoCNN 还可以用在下围棋中，如下图，输入是19 * 19的围棋局势（matrix/image），通过CNN，学出下一步应该走哪？ Why CNN playing Go?下围棋满足以下两个property： Some patterns are much smaller than the whole image. （围棋新手，博主只下赢过几次hhh) 如果白棋棋手，看到上图的pattern，上图的白子只有一口气了，被堵住就会被吃掉，那白棋棋手大概率会救那个白子，下在白棋的下方。 Alpha Go uese 5 * 5 for first layer. The same patterns appear in different regions. 但如何解释CNN的另一结构——Max Pooling？ 因为围棋的棋谱matrix不像image的pixel，subsample后，围棋的棋谱就和原棋谱完全不像了。 Alpha Go的论文中：Alpha Go并没有用Max Pooling。 所以，可以根据要训练的东西调整CNN模型。 Speech可以用CNN学习Spectrogram ，即识别出这一时段说的是什么话。 TextCNN还可以用在文本的情感分析中，对句子中每个word embedding后，通过CNN，学习sentence表达的是negative 还是positive还是neutral的情绪。 More（挖坑…生命很漫长，学无止境QAQ） The methods of visualization in these slides： https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html More about visualization： http://cs231n.github.io/understanding-cnn/ Very cool CNN visualization toolkit http://yosinski.com/deepvis http://scs.ryerson.ca/~aharley/vis/conv/ The 9 Deep Learning Papers You Need To Know About https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html How to let machine draw an image PixelRNN https://arxiv.org/abs/1601.06759 Variation Autoencoder (VAE) https://arxiv.org/abs/1312.6114 Generative Adversarial Network (GAN) http://arxiv.org/abs/1406.2661 Reference Deep Neural Networks are Easily Fooled： https://www.youtube.com/watch?v=M2IebCN9Ht4 deep dream generator: http://deepdreamgenerator.com/ A Neural Algorithm of Artistic Style: https://arxiv.org/abs/1508.06576","link":"/2020/04/25/CNN/"},{"title":"「机器学习-李宏毅」：Gradient","text":"总结「李宏毅老师-机器学习」的Gradient，主要从以下三个方面展开：调节learning rate；加快训练速度；对数据进行Feature Scaling。 Tip 1: Tuning your learning rates carefullyVisualize 损失函数随着参数变化的函数图 左图是Loss Function的函数图，红色是最好的Step，当Step过小（蓝色），会花费很多时间，当Step过大（绿色、黄色），会发现Loss越来越大，找不到最低点。 所以在Training中，尽可能的visualize loss值的变化。 但是当参数大于等于三个时， $loss function$的函数图就不能visualize了。 因此，在右图中，visualize Loss随着参数更新的变化，横轴即迭代次数，当图像呈现蓝色（small）时，就可以把learning rate 调大一些。 Adaptive Learning Rates(Adagrad)但是手动调节 $\\eta$是低效的，我们更希望能自动地调节。 直观上的原则是： $\\eta$ 的大小应该随着迭代次数的增加而变小。 最开始，初始点离minima很远，那step应该大一些，所以learning rate也应该大一些。 随着迭代次数的增加，离minima越来越近，就应该减小 learning rate。 E.g. 1/t decay： $\\eta^t=\\eta/ \\sqrt{t+1}$ 不同参数的 $\\eta$应该不同（cannot be one-size-fits-all)。 AdagradAdagrad 的主要思想是：Divide the learning rate of each parameter by the root mean squear of its previous derivatives.(通过除这个参数的 计算出的所有导数 的均方根) root mean squar : $ \\sqrt{\\frac{1}{n}(x_1^2+x_2^2+...+x_n^2)} $ Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $w^{t+1} \\leftarrow w^{t}-\\frac{\\eta^{t}}{\\sigma^{t}} g^{t} $ $\\eta^t$：第t次迭代的leaning rate $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}}$ $g^{t}=\\frac{\\partial L\\left(\\theta^{t}\\right)}{\\partial w} $ $\\sigma^t$：root mean squar of previous derivatives of w $\\tau^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}} $ 对比上面两种Adaptive Gradient，Adagrade的优势是learning rate 是和parameter dependent（参数相关的）。 Adagrad步骤简化步骤： 简化公式： $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ ( $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}} $ , $ \\sigma^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}$ ,约掉共同项即可) Adagrad Contradiction? ——Adagrad原理解释Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 在Vanilla Gradient descent中， $g^t$越大，也就是当前梯度大，也就有更大的step。 而在Adagrad中，当 $g^t$越大，有更大的step,而当 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 越大，反而有更小step。 Contradiction？ 「Intuitive Reason（直观上解释）」 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 是为了造成反差的效果。 类比一下，如果一个一直很凶的人，突然温柔了一些，你会觉得他特别温柔。所以同样是 $0.1$,第一行中，你会觉得特别大，第二行中，你会觉得特别小。 因此 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 这一项的存在就能体现 $g^t$的变化有多surprise。 「数学一些的解释」1. Larger Gradient,larger steps?在前面我们都深信不疑这一点，但这样的描述真的是正确的吗？ 在这张图中，只有一个参数，认为当该点的导数越大，离minima越远，这样看来，Larger Gradient,larger steps是正确的。 在上图中的 $x_0$点，该点迭代更新的best step 应该正比于 $|x_0+\\frac{b}{2a}|$ ，即 $\\frac{|2,a, x_0+b|}{2a}$。 而 $\\frac{|2,a, x_0+b|}{2a}$的分子也就是该点的一阶导数的绝对值。 上图中，有 $w_1,w_2$两个参数。 横着用蓝色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较a、b两点，a点导数大，离minima远。 竖着用绿色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较c、d两点，c点导数大，离minima远。 但是，如果比较a、c两点呢？ a点对 $w_1$ 的偏导数和c点对 $w_2$的偏导数比较？ 比较出来，c点点偏导数更大，离minima更远吗？ 再看左图的图像，横着的弧度更平滑，竖着的弧度更尖一些，直观上看应该c点离minima更近一些。 所以Larger Gradient,larger steps点比较方法不能（cross parameters)跨参数比较。 所以最好的step $\\propto$ 一阶导数（Do not cross parameters)。 2.** Second Derivative** 前面讨论best step $\\frac{|2,a, x_0+b|}{2a}$的分子是该点一阶导数，那么其分母呢？ 当对一阶导数再求导时，可以发现其二阶导数就是best step的分母。 得出结论：the best step $\\propto$ |First dertivative| / Second derivative。 因此，再来看两个参数的情况，比较a点和c点，a点的一阶导数更小，二阶导数也更小；c点点一阶导数更大，二阶导数也更大。 所以如果要比较a、c两点，谁离minima更远，应该比较其一阶导数的绝对值除以其二阶导数的大小。 回到 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 上一部分得出的结论是：the best step $\\propto$ |First dertivative| / Second derivative。 所以我们的learning rate 也应该和 |First dertivative| / Second derivative相关。 $g^t$也就是一阶导数，但为什么 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 能代表二阶导数呢？ 上图中，蓝色的函数图有更小的二阶导数，绿色的函数图有更大的二阶导数。 在复杂函数中，求二阶导数是一个很复杂的计算。 所以我们想用一阶导数来反映二阶导数的大小。 在一阶导数的函数图中，认为一阶导数值更小的，二阶导数也更小，但是取一个点显然是片面的，所以考虑取多个点。 也就是用 $ \\sqrt{\\text{(first derivative)}^2}$ 来代表best step中的二阶导数。 总结一下Adagrad的为了找寻最好的learning rate，从找寻best step下手，用简单的二次函数为例，得出 best step $\\propto$ |First dertivative| / Second derivative。 但是复杂函数的二阶导数是难计算的，因此考虑用多个点的一阶导数来反映其二阶导数。 得出 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 。 直观来解释公式中的一阶导数的root mean square，即来为该次迭代的一阶导数造成反差效果。 其他文献中的Adaptive Gradient理应都是为了调节learning rate使之有best step。(待补充的其他Gradient)[1] Tip 2:Stochastic Gradient DescentStochastic Gradient Descent在linear model中，我们这样计算Loss function： $L=\\sum_{n}\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ 每求一次Loss function，L都对所有training examples的 $\\text{error}^2$求和，因此每一次的loss function的计算，都是一重循环。 在Stochastic Gradient Descent中，每一次求loss function，只取一个example $x^n$，减少一重循环，无疑更快。 Stochastic Gradient Descent Pick an example $x^n$ $L=\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ 上图中，传统的Gradient Descent看完一次所有的examples，离minima还很远；而Stochastic Gradient Descent ，看完一次，已经离minima较近了。 Tip 3:Feature ScalingWhat is Feature Scaling 如上图所示，希望能让不同的feature能有相同的scale（定义域/规模） Why Feature Scaling假设model都是 $y = b+ w_1 x_1 +w_2 x_2$。 上图中，左边 $x_2$的规模更大，可以认为 $x_1$ 对loss 的影响更小， $ x_2$对loss的影响更大。 即当 $w_1,w_2$轻微扰动时，同时加上相同的 $\\Delta w$时，$x_2$ 使 $y$的取值更大，那么对loss 的影响也更大。 如图中下方的函数图 $w_1$方向的L更平滑， $w_2$ 方向更陡峭些，Gradient descent的步骤如图所示。 但当对 $x_2$进行feature scaling后，图像会更像正圆，Gradient descent使，参数更新向着圆心走，更新会更有效率。 How Feature Scaling概率论知识：标准化。 概率论： 随机变量 $X$ 的期望和方差均存在，且 $ D(X)&gt;0$,令 $X^*=\\frac{X-E(X)}{\\sqrt{D(X)}} $ ，那么 $E(X^)=0,D(X)=1$, $X^$称为X的标准化随机变量。 对所有向量的每一维度，进行标准化处理： $x_{i}^{r} \\leftarrow \\frac{x_{i}^{r}-m_{i}}{\\sigma_{i}} $ （ $m_i$是该维度变量的均值， $\\sigma_i$ 是该维度变量的方差） 标准化后，每一个feature的期望都是0，方差都是1。 Gradient Descent Theory(公式推导)当用Gradient Descent解决 $\\theta^*=\\arg \\min_\\theta L(\\theta)$时，我们希望每次更新 $\\theta $ 都能得到 $L(\\theta^0)&gt;L(\\theta^1)&gt;L(\\theta^2)&gt;…$ 这样的理论结果，但是不总能得到这样的结果。 上图中，我们虽然不能一下知道minima的方向，但是我们希望：当给一个点 $\\theta^0$ 时，我们能很容易的知道他附近（极小的附近）的最小的loss 是哪个方向。 所以怎么做呢？ Tylor Series微积分知识：Taylor Series（泰勒公式）。 Tylor Series:函数 $h(x)$ 在 $x_0$ 无限可导，那么 $\\begin{aligned} \\mathrm{h}(\\mathrm{x}) &=\\sum_{k=0}^{\\infty} \\frac{\\mathrm{h}^{(k)}\\left(x_{0}\\right)}{k !}\\left(x-x_{0}\\right)^{k} \\\\ &=h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{h^{\\prime \\prime}\\left(x_{0}\\right)}{2 !}\\left(x-x_{0}\\right)^{2}+\\ldots \\end{aligned}$ 当 x 无限接近 $x_0$ 时，忽略后面无穷小的高次项， $h(x) \\approx h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right) $ 上图中，用 $\\pi/4$ 处的一阶泰勒展示来表达 $\\sin(x)$ ,图像是直线，和 $\\sin(x)$ 图像相差很大，但当 x无限接近 $\\pi/4$ 是，函数值估算很好。 Multivariable Taylor Series $h(x, y)=h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right) +\\text{something raleted to} (x-x_x^0)^2 \\text{and} (y-y_0)^2+…$ 当 $(x,y)$ 接近 $(x_0,y_0)$ 时， $h(x,y)$ 用 $(x_0,y_0)$ 处的一阶泰勒展开式估计。 $ h(x, y) \\approx h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right)$ Back to Formal Derivation 当图中的红色圆圈足够小时，红色圆圈中的loss 值就可以用 $(a,b)$ 处的一阶泰勒展开式来表示。 $ \\mathrm{L}(\\theta) \\approx \\mathrm{L}(a, b)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}\\left(\\theta_{1}-a\\right)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}}\\left(\\theta_{2}-b\\right) $ $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$ ,d 足够小。 用 $s=L(a,b)$ , $ u=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}, v=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} $ 表示。 最后问题变成： $L(\\theta)\\approx s+u(\\theta_1-a)+v(\\theta_2-b)$ 找 $(\\theta_1,\\theta_2)$，且满足 $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$，使 $L(\\theta)$ 最小。 变成了一个简单的最优化问题。 令 $\\Delta \\theta_1=\\theta_1-a$ , $\\Delta\\theta_2=\\theta_2-b$ 问题简化为： $\\text{min}:u \\Delta \\theta_1+v\\Delta\\theta_2$ $\\text{subject to}:{\\Delta\\theta_1}^2+{\\Delta\\theta_2}^2\\leq d^2$ 画出图，就是初中数学了。更新的方向应该是 $(u,v)$ 向量反向的方向。 所以： $\\left[\\begin{array}{l} \\Delta \\theta_{1} \\\\ \\Delta \\theta_{2} \\end{array}\\right]=-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $\\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $ \\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}} \\\\ \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} \\end{array}\\right] $ Limitation of Gradient Descent Gradient Descent 可能会卡在local minima或者saddle point（鞍点：一个方向是极大值，一个方向是极小值，导数为0） 实践中，我们往往会在导数无穷接近0的时候停下来（&lt; 1e-7)，Gradient Descent 可能会停在plateau(高原；增长后的稳定) Reference[1] 待补充的其他Gradient","link":"/2020/03/01/Gradient/"},{"title":"「LeetCode」：String","text":"LeetCode String 专题记录。 9月毕。 「我祝福你有时有坏运气，你会意识到概率和运气在人生中扮演的角色，并理解你的成功并不完全是你应得的，其他人的失败也并不完全是他们应得的。」 「不想要刚好错过的悔恨，那就要有完全碾压的实力。」 String28-Implement strStr()28-Implement strStr() Problem: 返回第一个字串出现的下标 Solution： Python就暴力匹配。 12345678class Solution: def strStr(self, haystack: str, needle: str) -&gt; int: n1 = len(haystack) n2 = len(needle) for i in range(0, n1-n2+1): if haystack[i:i+n2] == needle: return i return -1 14-Longest Common Prefix14-Longest Common Prefix Problem: 返回串的公共最长前缀。 Solution： 暴力匹配长度就好。 1234567891011121314151617from typing import Listclass Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: LCP = 0 n = len(strs) if n == 0: return &quot;&quot; while True: for i in range(0, n): if LCP &lt; len(strs[i]) and strs[i][LCP] == strs[0][LCP]: continue else: return strs[0][0: LCP] LCP += 1 58-Length of Last Word58-Length of Last Word Problem: 单词串由字母和空格组成，返回最后一个单词的长度。 Solution： 注意串最后的空格。 12345678910111213class Solution: def lengthOfLastWord(self, s: str) -&gt; int: n = len(s) length = 0 end = n-1 while end-1 &gt;= 0 and s[end] == ' ': end -= 1 for i in range(end, -1, -1): if s[i] == ' ': return length else: length += 1 return length 58-First Unique Character in a StringProblem: 找第一个没有重复出现的字符下标。 Solution： 暴力。 12345678910class Solution: def firstUniqChar(self, s: str) -&gt; int: a_ascii = ord('a') cnt = [0]*(26+5) for i in s: cnt[ord(i)-a_ascii] += 1 for idx in s: if cnt[ord(i)-a_ascii] == 1: return s.index(i) return -1 寻找子串开始索引： str.find(substr, beg=0, end=len(string)) substr: 字串 beg: 开始索引 end: 结束索引，默认字符串长度。 如果字符串不包含子串，则返回-1 str.index(str, beg=0, end=len(string)) 和find差不多，如果不包含子串会抛出异常。 383-Ransom Note383-Ransom Note Problem: 给两个字符串，判断串1的字符能否由串2的字符组成。 Solution： 字典计数。 12345678910111213141516class Solution: def canConstruct(self, ransomNote: str, magazine: str) -&gt; bool: ransomDir = {} magazineDir = {} for ch in ransomNote: ransomDir.setdefault(ch, 0) ransomDir[ch] += 1 for ch in magazine: magazineDir.setdefault(ch, 0) magazineDir[ch] += 1 for (k, v) in ransomDir.items(): if k not in magazineDir: return False if ransomDir[k] &gt; magazineDir[k]: return False return True 初始化字典的值：dic.setdefault(ch, 0) 344-Reverse String344-Reverse String Problem: in-place 反转字符串 with O(1) 的额外空间。 Solution： 前后两个指针交换。 123456789101112131415from typing import Listclass Solution: def reverseString(self, s: List[str]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify s in-place instead. &quot;&quot;&quot; n = len(s) l = 0 r = n-1 while l &lt; r: s[l], s[r] = s[r], s[l] l += 1 r -= 1 151-Reverse Words in a String151-Reverse Words in a String Problem: 反转字符串word by word.(结果中单词间只能有一个空格) Solution： 把单词存入列表，再输出。 1234567891011121314151617class Solution: def reverseWords(self, s: str) -&gt; str: n = len(s) i = 0 li = [] while i &lt; n: while i &lt; n and s[i] == ' ': i += 1 l = i while i &lt; n and s[i] != ' ': i += 1 r = i if r != l: li.append(s[l:r]) ans = ' '.join(li[-1::-1]) return ans Python连接字符串总结 加号连接：'a' + 'b' 逗号连接，只能用于print打印: print(a, b) 直接连接: print('a' 'b') 使用 % 格式化字符串：'%s %s' % ('hello', 'world') format 格式化字符串：'{}{}'.format('hello', 'world') join 内置方法：用字符来连接一个序列，数组或列表等：'-'.join(['aa', 'bb', 'cc']) f-string 方法：aa, bb = 'hello', 'world' , f'{aa} {bb}' * 操作符：字符串乘法。 反转列表：[-1: : -1] 186-Reverse Words in a String II186-Reverse Words in a String II Problem: 反转单词in-places. Solution: 两次反转，第一次整体反转，第二次再单词反转。 （不额外开个数组来逐个赋值AC不了，不知道为啥q w q) 123456789101112131415161718class Solution: def reverseWords(self, s: List[str]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify s in-place instead. &quot;&quot;&quot; temp = s[-1::-1] n = len(temp) i = 0 while i &lt; n: l = i while i &lt; n and temp[i] != ' ': i += 1 r = i temp[l:r] = list(reversed(temp[l:r])) i += 1 for index in range(n): s[index] = temp[index] Python 反转列表的方法： list(reversed(a)) , reversed(a)返回的是迭代器，转换成list。 a[::-1] Python 字符串(str)和列表(list)互相转换： str 转换为 list list() 转换为单个字符列表 str.split() 或者str.split(' ') 空格分割转换 1234567891011121314str1 = &quot;123&quot;list1 = list(str1)print list1# ['1', '2', '3']str2 = &quot;123 sjhid dhi&quot;list2 = str2.split() #or list2 = str2.split(&quot; &quot;)print list2# ['123', 'sjhid', 'dhi']str3 = &quot;www.google.com&quot;list3 = str3.split(&quot;.&quot;)print list3# ['www', 'google', 'com'] list转换为str: &quot;&quot;.join(list) 无空格连接 &quot;.&quot;.join(list) 345-Reverse Vowels of a String345-Reverse Vowels of a String Problem: 反转字符串中的元音字母。 Solution： 元音字母，包括大写元音字母和小写元音字母。 1234567891011121314151617181920class Solution: def reverseVowels(self, s: str) -&gt; str: n = len(s) s = list(s) dic = {'a': 1, 'e': 1, 'i': 1, 'o': 1, 'u': 1} rev = [0] * n for index in range(n): if s[index] in dic or s[index].lower() in dic: rev[index] = 1 l = 0 r = n - 1 while l &lt; r: while l &lt; r and rev[l] == 0: l += 1 while l &lt; r and rev[r] == 0: r -= 1 s[l], s[r] = s[r], s[l] l += 1 r -= 1 return ''.join(s) Python大小写转换： 所有字符转换为大写：str.upper() 所有字符转换为小写：str.lower() 第一个字母转换为大写字母，其余小写：str.capitalize() 把每个单词的第一个字母转换为大写，其余小写。 12345678910str = &quot;www.runoob.com&quot;print(str.upper()) # 把所有字符中的小写字母转换成大写字母print(str.lower()) # 把所有字符中的大写字母转换成小写字母print(str.capitalize()) # 把第一个字母转化为大写字母，其余小写print(str.title()) # 把每个单词的第一个字母转化为大写，其余小写# WWW.RUNOOB.COM# www.runoob.com# Www.runoob.com# Www.Runoob.Com Python中string是不可变对象，不能通过下标的方式（如str[0]='a' )改变字符串。 205-Isomorphic Strings205-Isomorphic Strings Problem: 判断是否同构字符串。 Solution： 字符到字符的映射，必须是单射。 12345678910111213141516class Solution: def isIsomorphic(self, s: str, t: str) -&gt; bool: n = len(s) dic = {} vSet = set() # satisfy single map for idx in range(n): ch = s[idx] # single map if ch not in dic and t[idx] not in vSet: dic[ch] = t[idx] vSet.add(t[idx]) continue if ch in dic and dic[ch] == t[idx]: continue return False return True Python 集合的操作： 创建空集合：set() 创建有初值的集合：SET = {v0, v1, v2} 或者SET = set(v0) 判断元素是否在集合中：x in SET 集合运算： a-b :属于a集合不属于b集合 a|b :属于a集合或属于b集合 a&amp;b :集合a和集合b都包含的元素 a^b : 不同时包含于集合a和集合b的元素 集合中添加元素：s.add(x) 集合中添加元素，且参数可以是列表、元组、字典(是每个元素都添加进去）等：s.update(x) 移除元素：s.remove(x) ，如果元素不存在，则会发生错误。 移除元素：s.discard(x) ，如果元素不存在，不会发生错误。 随机删除集合中的一个元素：s.pop() （原理：对集合无序排序，然后删除无序排列集合的第一个） 计算集合元素的个数：len(s) 清空集合s.clear() List Comprehension &amp;&amp; Set Comprehension &amp;&amp; Dictionary Comprehension 这个相当于数学中的 $S={2\\cdot x\\mid x\\in \\left[0,9\\right)}$ 的表达。 List Comprehension 如果用数学中的这个表达来看下面的式子，就很显而易见了。 1arr = [i for i in range(10)] 再看看加了其他限制的例子 12345678# filter the elementsarr1 = [x for x in arr if x % 2==0]# add more conditionsarr2 = [x**2 for x in arr if x &gt;= 3 and x % 2]# use nested for loopsarr3 = [(x, y) for x in range(3) for y in range(4)] 使用List Comprehension不仅优美，而且效率也会很高。 Set Comprehension 同样的 1s = {x for x in range(100) if x%2 != 0 and x%3 != 0} Dictionary Comprehension Syntax：{expression(variable): expression(variable) for variable, variable in input_set [predicate][, …]} 1234567891011121314# [(set_k), (set_v)]&gt;&gt;&gt; {k: v for k, v in [(1, 2), (3, 4)]}{1: 2, 3: 4}&gt;&gt;&gt; {n: n for n in range(2)}{0: 0, 1: 1}&gt;&gt;&gt; {chr(n): n for n in (65, 66, 66)}{'A': 65, 'B': 66}# ((k1, v1), (k2, v2))&gt;&gt;&gt; {k: v for k, v in (('I', 1), ('II', 2))}{'I': 1, 'II': 2}&gt;&gt;&gt; {k: v for k, v in (('a', 0), ('b', 1)) if v == 1}{'b': 1} 68-Text Justification68-Text Justification Problem: 文本对齐，总结下来有以下几点要求。 如果不是最后一行，且该行不止一个单词，则要求左右对齐。 左右对齐：尽可能让单词间的空格均匀分布，如果不能均匀分布，则单词左边的空格应该比右边的空格多。 贪心的思想：应该尽可能的多放单词。 如果是最后一行，或者该行只有一个单词，则要求左对齐。 Solution： 分两种情况处理，判断是左对齐，还是右对齐。 左对齐：该行有x个单词 前x-1个单词的后面都应该只有一个空格。 最后一个单词后面就应该补齐所有空格。 左右对齐：该行有x个单词，有x-1个空格间隙。 计算得到该行的空格数w，则如果能均匀分配，则每个间隙应该有aver = w // (x-1) 个空格。 但也许不会均匀分配，因此，可能会多出m个空格（m &lt; x-1 ) 即前m个单词，单词的后面应该有（aver+1)个空格，后面的(x-1) - m个单词应该有aver个空格。 最后一个单词的后面没有空格。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from typing import Listclass Solution: def __init__(self): self.words = None self.maxWidth = None self.sum = None def leftJustify(self, l: int, r: int) -&gt; str: wordsNum = r - l + 1 lengthNum = self.sum[r] if l == 0 else self.sum[r] - self.sum[l - 1] spaceNum = self.maxWidth - lengthNum temp = &quot;&quot; for i in range(l, r): temp += self.words[i] + &quot; &quot; temp += self.words[r] + &quot; &quot;*(spaceNum - (wordsNum - 1)) return temp def leftRightJustify(self, l: int, r: int) -&gt; str: wordsNum = r - l + 1 lengthNum = self.sum[r] if l == 0 else self.sum[r] - self.sum[l - 1] spaceNum = self.maxWidth - lengthNum temp = &quot;&quot; averSpace = spaceNum // (wordsNum - 1) moreSpace = spaceNum - averSpace*(wordsNum - 1) for i in range(moreSpace): temp += self.words[l+i] + &quot; &quot; * (averSpace + 1) for i in range(l+moreSpace, r): temp += self.words[i] + &quot; &quot; * averSpace temp += self.words[r] return temp def fullJustify(self, words: List[str], maxWidth: int) -&gt; List[str]: self.words = words self.maxWidth = maxWidth n = len(words) sum = [0]*n sum[0] = len(words[0]) # sum prefix length of words for i in range(1, n): sum[i] = sum[i - 1] + len(words[i]) self.sum = sum l = 0 ans = [] while l &lt; n: r = l lengthNum = len(self.words[l]) while r+1 &lt; n and lengthNum + len(self.words[r+1]) + 1 &lt;= maxWidth: lengthNum += len(self.words[r+1]) + 1 # space r += 1 # only one word or the last line if r - l + 1 == 1 or r == n - 1: ans.append(self.leftJustify(l, r)) else: ans.append(self.leftRightJustify(l, r)) l = r + 1 return ans Python的三元运算符： #如果条件为真，返回真 否则返回假condition_is_true if condition else condition_is_false 12is_fat = Truestate = &quot;fat&quot; if is_fat else &quot;not fat&quot; Python的整除是：\\\\ ，实数除是：\\ Reference Python中字符串的连接方法总结： https://segmentfault.com/a/1190000015475309","link":"/2020/09/29/LeetCode_String/"},{"title":"「LeetCode」：Array","text":"8月某司实训+准备开学期末考，我可太咕了q w q…dbq，（希望）高产博主我.我..又回来了。 LeetCode Array专题，持久更新。（GitHub) Array27-Remove Elements27-Remove Elements Solution 12345678910111213141516from typing import Listclass Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: tot = 0 for element in nums: if element == val: continue else: nums[tot] = element tot = tot + 1 return tot Python的参数传递和函数返回值： 1def removeElement(self, nums:List[int], val:int) -&gt; int: 题目要求： “remove all instances of that value in-place and return the new length.” “Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.” “Confused why the returned value is an integer but your answer is an array? Note that the input array is passed in by reference, which means modification to the input array will be known to the caller as well.” 题目要求是在原数组上删除数值，不能额外开新的空间存储数组。 意思就是说，虽然函数返回的是一个数值，但实际返回答案是一个数组。 因为数组的传递是指针传递，返回的是数组长度，则相当于返回了这个in-place的新数组。 26-Remove Duplicates from Sorted Array26-Remove Duplicates from Sorted Array Solution： 1234567891011121314151617from typing import Listclass Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: tot = 0 before = None for index in range(len(nums)): if nums[index] != before: nums[tot] = nums[index] before = nums[index] tot = tot + 1 else: continue return tot 第一次提交的时候before = nums[0] - 1 报错了，原因是传入数组长度为0，下标越界。 注意空数组的下标越界问题。 80-Remove Duplicates from Sorted Array II80-Remove Duplicates from Sorted Array II Solution: 123456789101112131415161718192021from typing import Listclass Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: before = None before_cnt = 0 length = 0 for index in range(len(nums)): if nums[index] != before: nums[length] = nums[index] before = nums[index] length += 1 before_cnt = 1 else: before_cnt += 1 if before_cnt &lt;= 2: nums[length] = nums[index] length += 1 return length 189-Rotate Array（S123）189-Rotate Array Problem: 简述题目大意，给一个列表nums，一个 $k$ 值，要求原址让列表循环右移 $k$ 位。 Solution: 其实以下三种做法时间空间复杂度差别不大，主要看个思路吧。 S Runtime Memory Language S1 64ms 15.2MB pyhon3 S2 64ms 15.1MB python3 S3 116ms 15.1MB python3 S1-简单做法：空间换时间时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ ，循环右移时多开了一个数组。 12345678910111213from typing import Listimport queueclass Solution: def rotate(self, nums: List[int], k: int) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; length = len(nums) a = [0] * length for index in range(length): a[(index + k)%length] = nums[index] nums[:] = a S2-利用数学同余关系时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ ，循环右移时只多开了一个变量。 原理： 定理[1]： 设 $m$ 是正整数，整数 $a$ 满足 $(a,m)=1$ ，$b$ 是任意整数。若 $x$ 遍历模 $m$ 的一个完全剩余系，则 $ax+b$ 也遍历模 $m$ 的一个完全剩余系。 由以上定理可以得知，设 $n$ 为列表长度， $x$ 是列表的下标，遍历 $n$ 的一个完全剩余系。 如果 $(k,n)=1$ ， $kx$ 也遍历 $n$ 的一个完全剩余系。这种情况，列表下标通过 $k$ 的倍数的顺序连成一个环。 ：只需要额外一个变量 $temp$ 存储移动占用的值。 如果 $(k,n)\\neq 1$ ，那么 $kx$ 不会遍历一个 $n$ 的完全剩余系，会出现下图的情况（如绿色的线的元素的 $idx = kx+0$ ，红色线的元素都是 $idx=kx+1$ ），会在 $k$ 的某个剩余类一直循环。 ：遍历每个 $k$ 的剩余类。 在每次循环移位时，需要记录该次循环的起始位，防止重复。 123456789101112131415161718class Solution: def rotate(self, nums: List[int], k: int) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; length = len(nums) k %= length start = 0 cnt = 0 while cnt &lt; length: current, prev = start, nums[start] while True: current = (current + k) % length prev, nums[current] = nums[current], prev cnt += 1 if current == start: break start += 1 S3-利用反转列表的思路时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ 原理： 1234Original List : 1 2 3 4 5 6 7After reversing all numbers : 7 6 5 4 3 2 1After reversing first k numbers : 5 6 7 4 3 2 1After revering last n-k numbers : 5 6 7 1 2 3 4 --&gt; Result 123456789101112131415161718from typing import List# Solution 3: Reverseclass Solution: def reverse(self, nums: list, begin: int, end: int) -&gt; None: while begin &lt; end: nums[begin], nums[end] = nums[end], nums[begin] begin += 1 end -= 1 def rotate(self, nums: List[int], k: int) -&gt; None: n = len(nums) k %= n self.reverse(nums, 0, n-1) self.reverse(nums, 0, k-1) self.reverse(nums, k, n-1) Python用引用管理对象。 12int a1 = 1, *p = &amp;a1;int a2 = 2, &amp;b = a2; 指针：指针变量是一个新变量，这个变量存储的是（变量a1的）地址，该地址指向一个存储单元。（该存储单元存放的是a1的值）。 引用：引用的实质是变量的别名，所以a2和b实际是一个东西，在内存中占有同一个存储单元。 所以python中交换对象可以直接a,b = b,a Python 列表的操作：切片。 41-First Missing Positive41-First Missing Positive Solution: 排一下序，维护一个expect变量就行了。 时间复杂度：$\\mathcal{O}(n\\log{n})$ ，题目没有卡常。 Runtime: 36 ms, faster than 70.96% of Python3 online submissions for First Missing Positive. 空间复杂度：$\\mathcal{O}(n)$ Memory Usage: 13.8 MB, less than 69.19% of Python3 online submissions for First Missing Positive. 12345678910111213from typing import Listclass Solution: def firstMissingPositive(self, nums: List[int]) -&gt; int: nums.sort() expect = 1 for element in nums: if element == expect: expect += 1 elif element &gt; expect: return expect return expect 299-Bulls and Cows299-Bulls and Cows Problem: 题目大意是：给定两个相同长度的字符串，计算这两个字符串有多少个对应位数字相同，和多少个位置不对应但数字相同的个数。 Solution: 应用字符0-9本身数字的性质。 时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ 123456789101112131415161718192021class Solution: def getHint(self, secret: str, guess: str) -&gt; str: # 0-9 cnt for guess (expect the same digit) cnt = [0] * 10 bulls = cows = 0 g = lambda a: ord(a) - ord('0') for si, gi in zip(secret, guess): if si == gi: bulls += 1 else: cnt[g(gi)] += 1 for si, gi in zip(secret, guess): if si == gi: continue elif cnt[g(si)] &gt; 0: cows += 1 cnt[g(si)] -= 1 output = &quot;{}A{}B&quot;.format(bulls, cows) return output ord()函数和chr()函数 ord()返回字符的ASCII码，chr函数返回ASCII码对应的字符。 浅析lambda表达式，匿名函数，类似于C语言的宏。 格式：lambda [arg1[, arg2,...]] : expression 双变量同时遍历使用zip()函数 134-Gas Station（S12）134-Gas Station Problem： 题目大意是：有N个环形加油站，每个加油站能加油gas[i]，一个汽车起始油量为0，且从i个站开到第i+1个站需要花费cost[i]的油量。找出这个车能顺时针跑完一圈的起始点（如果有，则唯一），如果不能返回-1。 Solution： Solution Runtime Memory Language S1-简单做法 3244ms 14.9MB python3 S2-原理优化 104ms 14.8MB python3 S1-简单解法时间复杂度：$\\mathcal{O}(n^2)$ ，实际远达不到 $n^2$ ，算有一点贪心叭。 空间复杂度：$\\mathcal{O}(n)$ 该汽车从起点i能跑完的必要条件： 起始点 gas[i] - cost[i] &gt;= 0 。并且维护一个数组存放 gas[i] - cost[i] ，即这个站自给自足的油量余量。 如果从满足条件1的起点开始跑一圈， 要求路程中的油量必须大于等于0。维护一个汽车当前总油量 $S$ （用前缀和维护），每跑过一段路程，都要求 $S&gt;=0$ 。 123456789101112131415161718192021from typing import Listclass Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: n = len(gas) remain = [] for g, c in zip(gas, cost): remain.append(g - c) for i in range(n): if remain[i] &lt; 0: continue else: S = 0 for j in range(n): S += remain[(i + j) % n] if S &lt; 0: break if S &gt;= 0: return i return -1 S2-对问题分析进行再简化时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ 12345 i : 0 1 2 3 4g[i]: 1 2 3 4 5c[i]: 3 4 5 1 2g-c :-2-2-2 3 3sum :-2-4-6-3 0 如果 $S=\\sum_{i=0}^{n-1} g[i]-c[i],S&lt;0$ 那么一定无解， $S$ 称为总积累油量。 如果 $S&gt;=0$，如果有找出最优解的方法，则一定有解。 起点满足 $g[i]-c[i]&gt;=0$ ，把这些点称为正余量点。 用 $\\mathcal{O}(1)$ 算出从第 $i$ 个点出发到第 $n$ （n就是第0个点） 个点所积累的油量： $res[i] =S-sum[i-1]$ .即用总积累油量减去前 $i-1$ 段路程能积累的油量（一般积累为负）。(sum数组就是 g-c的前缀和) 对于满足起点要求 $g[i]-c[i]&gt;=0$ 的所有点，计算从第 $i$ 个点出发到第 $n$ 个点到油量积累。那么有最大油量积累的点即为最优起始点。（题目规定如果存在，则点唯一） 因此，因为 $S$ 固定，只需要找到 $sum$ 数组中的最小值的下标，下标+1即是结果。 证明其正确性： 如果满足 $g[i]-c[i]&gt;=0$ 的上述点 $i,j（i&lt;j)$ ，如果 $res[i]&gt;=res[j]$ ，说明从 $i$ 到 $j$ 是正油量积累，贪心的思想，那肯定积累的油量越多越好， $i$ 比 $j$ 优。 如果 $res[i]&lt;=res[j]$ ，说明从 $i$ 到 $j$ 是负油量积累，如果从 $i$ 点出发，到 $j$ 点就负油量了；如果从 $j$ 点出发，该车最后再跑 $i$ 到$j$ 段，因为保证了总积累油量是正，所以最后一定有足够的油量能跑完 $i$ 到 $j$ 段。 再证只要 $S&gt;=0$ 则一定有解。是动态尝试起始点，( $i,j$ 都满足 $g[i]-c[i]&gt;=0$ ）从点 $i$ 开始，跑到点 $j$ 时，如果该途中途有出现油量不够，那就把 $i$ 到 $j$ 的这段路程放到路途的后面来跑，等油量积累够了再跑这段。 Code: 123456789101112131415161718from typing import Listclass Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: n = len(gas) sum = 0 remain = [] for g, c in zip(gas, cost): remain.append(g - c) sum += g - c if sum &lt; 0: return -1 for i in range(1, n): remain[i] += remain[i - 1] min_idx = remain.index(min(remain)) return (min_idx + 1) % n 前缀和 list.index(value) 找出list中值为value 的第一个下标。 min(list) 返回list中的最小值。 118-Pascal’s Triangle118-Pascal’s Triangle Problem: 给定一个数字，输出如下规则的值。 Solution： 注意边界吧。（不太喜欢这种题qwq 123456789101112from typing import Listclass Solution: def generate(self, numRows: int) -&gt; List[List[int]]: ans = [] for i in range(0, numRows): temp = [1] * (i+1) for j in range(1, i): temp[j] = ans[i-1][j-1] + ans[i-1][j] ans.append(temp) return ans Python 中的append会出现值被覆盖的情况：变量在循环外定义，但在循环中对该变量做出一定改变，然后append到列表，最后发现列表中的值都是一样的。 因为Python中很多时候都是以对象的形式管理对象，因此append给列表的是一个地址。 119-Pascal’s Triangle II119-Pascal’s Triangle II Problem： 给定一个数字，输出某一行。 Solution： 123456789101112from typing import Listclass Solution: def getRow(self, rowIndex: int) -&gt; List[int]: temp = [1] for i in range(0, rowIndex): for j in range(i, 0, -1): temp[j] += temp[j-1] temp.append(1) return temp 169-Majority Element169-Majority Element Problem: 给一串数字，找到出现次数大于 n/2 的数字。 Solution： 用字典计数。 12345678910111213from typing import Listclass Solution: def majorityElement(self, nums: List[int]) -&gt; int: n = len(nums) cnt = {} for ele in nums: if ele in cnt: cnt[ele] += 1 else: cnt[ele] = 1 return max(cnt, key=cnt.get) 返回值最大/最小的键/索引。 列表： 最大值的索引：list.index(max(list)) 最小值的索引：list.index(min(list)) 字典： 最大值的键：max(dict, key=dict.get) 最小值的键：min(dict, key=dict.get) 229-Majority Element II229-Majority Element II Problem: 给一串数字，返回出现次数大于 n/3 的数字。 Solution： 1234567891011121314151617from typing import Listclass Solution: def majorityElement(self, nums: List[int]) -&gt; int: n = len(nums) cnt = {} ans = [] for ele in nums: if ele in cnt: cnt[ele] += 1 else: cnt[ele] = 1 for (k, v) in cnt.items(): if v &gt; n/3: ans.append(k) return ans 字典的实用方法： 操作 实现方法 删除字典元素 del dict['Name'] 清空字典所有条目 dict.clear() 删除字典 del dict 返回指定键的值，如果值不存在返回default的值 dict.get(key, default) 如果键不存在字典中，添加键并将值设为default,于get类似 dict.setdefault(key, default=None) 判读键是否存在 1. if k in dict 2. dict.has_key(key) 存在返回true 以列表返回可遍历的（键，值）元祖数组 dict.items() 以列表返回一个字典的所有键 dict.keys() 以列表返回字典中的所有值 dict.values() 返回最大值的键值 max(dict, key=dict.get) 返回最小值的键值 min(dict, key=dict.get) 遍历字典的方法： 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-dict={&quot;a&quot;:&quot;Alice&quot;,&quot;b&quot;:&quot;Bruce&quot;,&quot;J&quot;:&quot;Jack&quot;}# 实例一：键循环for i in dict: print &quot;dict[%s]=&quot; % i,dict[i]# 结果:# dict[a]= Alice# dict[J]= Jack# dict[b]= Bruce# 实例二：键值元组循环for i in dict.items(): print i# 结果:# ('a', 'Alice')# ('J', 'Jack')# ('b', 'Bruce')# 实例三：键值元组循环for (k,v) in dict.items(): print &quot;dict[%s]=&quot; % k,v# 结果:# dict[a]= Alice# dict[J]= Jack# dict[b]= Bruce 274-H-Index274-H-Index Problem: 给出研究人员论文的论文引用次数，计算它的H指数（有h篇论文的引用次数至少为h，剩下N-h篇论文的引用次数不超过h）。 Solution： 时间复杂度：$\\mathcal{O}(n\\log{n})$ 空间复杂度：$\\mathcal{O}(n)$ 排序后，再二分。（感觉自己的二分写的有点丑qwq 还有一种思路是，排序完，从最大的h开始递减遍历，满足条件就返回。反正排序也要$\\mathcal{O}(n\\log{n})$ 的复杂度… 12345678910111213141516171819from typing import Listclass Solution: def hIndex(self, citations: List[int]) -&gt; int: n = len(citations) citations.sort() begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 h = n - mid if citations[mid] &gt;= h: end = mid if begin == end: return h else: begin += 1 return n-begin 非递归写二分：while begin &lt;= end 275-H-Index II275-H-Index II Problem: 和274一样，给了递增的论文引用数，希望能用指数时间返回H指数。 Solution： 啊，就二分鸭。 123456789101112131415161718from typing import Listclass Solution: def hIndex(self, citations: List[int]) -&gt; int: n = len(citations) begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 h = n - mid if citations[mid] &gt;= h: end = mid if begin == end: return h else: begin += 1 return n - begin 243-Shortest Word Distanceqwq 这道题还收费来着，于是于是就开了个中国区的会员（中国区的会员便宜好多啊！！） 243-Shortest Word Distance Problem： 给定一串单词，单词1和单词2，计算单词1单词2在单词列表中的距离。 Solution： Solution Runtime Memory Language S1-二分查找 44ms 15.7MB python3 S2-线性维护 40ms 15.6MB python3 S1-二分查找时间复杂度：$\\mathcal{O}(n\\log{n})$ 空间复杂度：$\\mathcal{O}(n)$ （最开始还很疑惑啥是单词距离…单词1和单词2可能在单词列表中重复出现） 计算出单词1和单词2在单词列表中出现的索引值列表，是递增有序的。 对于单词1索引列表中的每个值，在单词2索引列表中查找该值的lower_bound，计算距离。 同理，对于单词2索引列表中的每个值，也同样计算距离。 找出最小距离。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from typing import Listdef lower_bound(a: list, x: int) -&gt; int: n = len(a) begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 if a[mid] &gt;= x: end = mid if begin == end: return begin else: begin += 1 return -1class Solution: ans = None def findShortest(self, li1: list, li2: list) -&gt; None: for idx in li1: min_dis_idx = lower_bound(li2, idx) if min_dis_idx == -1: continue else: self.ans = min(self.ans, li2[min_dis_idx] - idx) if self.ans == 1: return def shortestDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) li1 = [] li2 = [] self.ans = n for idx in range(n): # li1 and li2 are ordered if words[idx] == word1: li1.append(idx) if words[idx] == word2: li2.append(idx) self.findShortest(li1, li2) if self.ans &gt; 1: self.findShortest(li2, li1) return self.ans S2-线性维护：时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ 问题还能再简化，线性扫描单词列表，维护两个变量，单词1出现的最近索引，单词2出现的最近索引。扫描时计算距离，每当单词1或单词2出现时，就用另一个单词的最近索引计算。 12345678910111213141516171819from typing import Listclass Solution: def shortestDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) lst1 = None lst2 = None ans = n for idx in range(n): if words[idx] == word1: lst1 = idx if lst2 is not None: ans = min(ans, lst1-lst2) if words[idx] == word2: lst2 = idx if lst1 is not None: ans = min(ans, lst2-lst1) return ans C++中： lower_bound(begin, end, num)：返回num的下界，即大于等于num的第一个索引位置。 upper_bound(begin, end, num)：返回num的上界，即大于num的第一个索引位置。 Python中用二分实现这两个函数。 244-Shortest Word Distance II244-Shortest Word Distance II Problem: 和上题题干类似，计算单词距离，但是此问是每一个单词列表，可能有多个询问。 Solution： 对每一个单词列表，都可能有多个询问。 因此，之前243的解法每次询问都会遍历一遍单词列表。如果对每个单词列表询问数为 $M$ ，那么时间复杂度为 $\\mathcal{O}(NM)$ ，会超时，所以希望能将单词列表的有关信息存下来，再用常数时间处理每一个询问。 这里的解法是用一个字典把每个单词出现的index列表存下来，键是单词，值是index列表。这个列表相对于单词列表的数目应该是远远小于的，因此用二重循环应该也能过吧（没有尝试二重循环解法） 这里有两种思路，一种是自己想的归并思路，还有一种是官方解答的思路，官方思路比归并的思路更优雅一些，问题抽象的更好。（代码差距不大，时间差距也不太大） Solution Runtime Memory Language S1-归并思路查询 96ms 20.8MB python3 S2-交叉跳跃查询 80ms 20.4MB python3 双指针：$i$ 指向列表1，$j$指向列表2. S1-归并思路归并思路：列表的值都是有序的，再把两个列表的值按归并的思想再排序，可以想成把点一个一个有序放在数轴上。 $i$指针前进的情况：（排序时，取列表1的下一个数字） $i+1&lt;len1$ and $li1[i+1] &lt; li[j]$ $i+1&lt;len1$ and $li1[i+1] &lt; li[j+1]$ $i+1 &lt; len1$ and $j+1 == len2$ ($j$ 已无法移动) 其余情况：$j$ 移动。 1234567891011121314151617181920212223242526272829303132333435from typing import Listclass WordDistance: def __init__(self, words: List[str]): self.words = words self.len = len(words) self.dict = {} for index in range(self.len): word = words[index] if word in self.dict: temp = self.dict[word] temp.append(index) else: temp = [index] self.dict[word] = temp def shortest(self, word1: str, word2: str) -&gt; int: ans = self.len li1 = self.dict[word1] li2 = self.dict[word2] len1 = len(li1) len2 = len(li2) i = j = 0 while i &lt; len1 and j &lt; len2: ans = min(ans, abs(li1[i] - li2[j])) if ans == 1: return ans # i goes ahead if i + 1 &lt; len1 and ((li1[i + 1] &lt; li2[j]) or (j+1 == len2) or (j + 1 &lt; len2 and li1[i + 1] &lt; li2[j + 1])): i += 1 else: j += 1 return ans S2-交叉比较对于当前指向列表1和列表2的两个元素 $li1[i]$ 和 $li2[j]$ ，对 $li1[i]$来说，只需要和旁边的属于列表2的元素比较，对 $li2[j]$ 同理。 因此，当 $li1[i]&gt;li2[j]$ 时，下一次的比较应该让 $j$ 指针前移一位，继续计算指针 $i$ 所指元素和其旁边的列表2的元素。同理，当 $li1[i]&lt;li2[j]$ 时，下一次的比较应该让 $i$ 指针前移一位，继续计算指针 $j$ 和其旁边的列表1的元素。 具体移动如下图。 1234567891011121314151617181920212223242526272829303132333435from typing import Listclass WordDistance: def __init__(self, words: List[str]): self.words = words self.len = len(words) self.dict = {} for index in range(self.len): word = words[index] if word in self.dict: temp = self.dict[word] temp.append(index) else: temp = [index] self.dict[word] = temp def shortest(self, word1: str, word2: str) -&gt; int: ans = self.len li1 = self.dict[word1] li2 = self.dict[word2] len1 = len(li1) len2 = len(li2) i = j = 0 while i &lt; len1 and j &lt; len2: ans = min(ans, abs(li1[i] - li2[j])) if ans == 1: return ans # i goes ahead if li1[i] &lt; li2[j]: i += 1 else: j += 1 return ans 277-Find the Celebrity277-Find the Celebrity Problem: 已有know(i, j) API，判断i是否知道j，i是名人的充要条件是其他所有人知道i，而i不知道其他所有人。 Solution： 两种思路，第一种较为直观，使用二重循环，但剪枝多，远达不到 $\\mathcal{O}(n^2)$ ，第二种稍做优化。因此两种解法差距不太大。 提交时间 运行时间 内存消耗 语言 S几秒前 1896ms 13.6MB python3 5 分钟前 1772ms 13.6MB python3 S1-直观思路时间复杂度：远不到 $\\mathcal{O}(n^2)$ 用了二重循环，但剪枝很多，所以远达不到 $\\mathcal{O}(n^2)$ 12345678910111213141516171819202122232425class Solution: def findCelebrity(self, n: int) -&gt; int: for i in range(n): fg = True # i knows j ? for j in range(n): if i == j: continue if knows(i, j): fg = False break if not fg: continue # j knows i ? for j in range(n): if i == j: continue if not knows(j, i): fg = False break if not fg: continue else: return i return -1 S2-排除法时间复杂度：$\\mathcal{O}(n)$ 排除i：根据know(i, j)=True 可以认为i不是名人，j可能是名人。 对于n-1个关系，最后从n个人中选出一个可能的人，再根据名人的充要条件去判断他是否是名人。 123456789101112131415class Solution: def findCelebrity(self, n: int) -&gt; int: celebrity = 0 for i in range(1, n): if knows(celebrity, i): celebrity = i continue for i in range(n): if celebrity == i: continue if (not knows(celebrity, i)) and knows(i, celebrity): continue else: return -1 return celebrity 245-Shortest Word Distance III245-Shortest Word Distance III Problem: 题意增加了两个单词可能相同，分两种情况就好了。 Solution： 123456789101112131415161718192021222324from typing import Listclass Solution: def shortestWordDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) ptr1 = None ptr2 = None ans = n if word1 == word2: for idx in range(n): if words[idx] == word1: ptr1, ptr2 = idx, ptr1 if (ptr1 is not None) and (ptr2 is not None): ans = min(ans, ptr1-ptr2) else: for idx in range(n): if words[idx] == word1: ptr1 = idx if words[idx] == word2: ptr2 = idx if (ptr1 is not None) and (ptr2 is not None): ans = min(ans, abs(ptr1-ptr2)) return ans 217-Contains Duplicate[E]217-Contains Duplicate Problem: 判断数组中有无重复元素出现。 Solution： 12345678class Solution: def containsDuplicate(self, nums: List[int]) -&gt; bool: S = set() for i in nums: if i in S: return True S.add(i) return False 219-Contains Duplicate II[E]219-Contains Duplicate II Problem: 判断数组中是否有两个相同的值，他们的索引之差小于等于k。 Solution： 存放出现该值的最近的索引，扫一遍。 123456789101112class Solution: def containsNearbyDuplicate(self, nums: List[int], k: int) -&gt; bool: dict = {} n = len(nums) for i in range(n): if nums[i] not in dict: dict.setdefault(nums[i], i) else: if i - dict[nums[i]] &lt;= k: return True dict[nums[i]] = i return False 4-Median of Two Sorted Arrays[H] （2S）4-Median of Two Sorted Arrays[H] Problem: 给两个排好序的数组，返回一个中位数 Solution： S 运行时间 内存消耗 S1 $\\mathcal{O}(m+n)$ ：92ms 14.3MB S2 $\\mathcal{O}(\\log(m+n))$ ：52ms 13.3MB S1:归并排序的做法。 时间复杂度：$\\mathcal{O}(m+n)$ 12345678910111213141516171819202122232425262728293031from typing import Listclass Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: n1 = len(nums1) n2 = len(nums2) median1 = median2 = None i = j = 0 tot = 0 while i &lt; n1 or j &lt; n2: if (i &lt; n1 and j &lt; n2 and nums1[i] &lt;= nums2[j]) or (i &lt; n1 and j &gt;= n2): tot += 1 if tot == (n1 + n2)//2: median1 = nums1[i] if tot == (n1 + n2)//2 + 1: median2 = nums1[i] break i += 1 else: tot += 1 if tot == (n1 + n2) // 2: median1 = nums2[j] if tot == (n1 + n2) // 2 + 1: median2 = nums2[j] break j += 1 if (n1 + n2) % 2 == 1: return median2 else: return (median1 + median2)/2 S2:二分的思路 时间复杂度：$\\mathcal{O}(\\log(m+n))$ 一、首先讨论一个数组的中位数，数组有n个元素，如果n为奇数，则第(n+1)/2个是中位数；如果n为偶数，则第(n+1)/2和第(n+2)/2的平均值为中位数。 回到本题，因为是两个数组，如果根据奇偶性分类讨论就过于麻烦了，所以将两种情况统一以简化解题思路。 即无论n是奇数还是偶数，数组的中位数都是第(n+1)/2和第(n+2)/2的平均数。 回到本题，设数组1有n个元素，数组2有m个元素，则中位数为两个数组的有序序列的第(n+m+1)/2个和第(n+m+2)/2个的平均数。 二、因此，题目需要求解的问题改为求这两个有序数组的有序序列的第k个数。 二分思想：两个数组分别找第k/2个数，（假设都存在），比较，如果第一个数组的这个数小于第二个数组，说明第k个数肯定不在第一个数组的前k/2个数中，因此就可以直接去掉数组1的前k/2个元素，查找有序序列的第k-k/2个数；同理，如果大于，则说明第k个数肯定不在第二个数组的前k/2个数中，去掉数组2的前k/2个元素。 使用一个数组起始指针l1和l2来实现数组的“去掉”前k/2个元素。 设数组1的元素个数为n，数组2的元素个数为m。 递归函数Find(l1, l2, k)：查找起始指针为l1, l2的两个有序数组的第k个数。 讨论边界情况，有数组为空的情况。即 l1 == n 或者 l2 == m . 如果第一个数组已为空，则直接返回第二个数组的第k个数； 同理，如果第二个数组为空，则直接返回第一个数组的第k个数。 两个数组都不为空的情况。即 l1 &lt; n 或者 l2 &lt; m . 递归边界： k == 1 ,即返回 nums1[l1] 和 nums2[l2] 中较小的那一个。 数组长度边界：即有数组的剩余元素个数小于 k/2 ，那么拿出来比较的就应该是数组的最后一个元素。 维护两个值 k1 和 k2 来分别表示用两个数组的第 k1和k2个来比较。 (k1 k2都小于等于k/2) 123# to avoid the rest length of nums1/nums2 is shorter than k//2k1 = k//2 if l1+k//2 &lt;= n else n-l1k2 = k//2 if l2+k//2 &lt;= m else m-l2 比较nums1[l1+k1-1] 和 nums2[l2+k2-1] 的大小，递归： 相等： 如果 k-k1-k2 == 0 说明nums1的前k1个和nums2的前k2个就是有序序列的前k个，返回 nums1[l1+k1-1] 。 否则，（即某一个数组的剩余长度小于k/2），分别去掉两个数组的前k1和k2个数，递归调用 Find(l1+k1, l2+k2, k-k1-k2) 。 nums1[l1+k1-1] &gt; nums2[l2+k2-1] 说明可以去掉数组2的前k2个数，递归调用 Find(l1, l2+k2, k-k2) nums1[l1+k1-1] &gt; nums2[l2+k2-1] 说明可以去掉数组1的前k1个数，递归调用 Find(l1+k1, l2, k-k1) Code： 123456789101112131415161718192021222324252627282930313233343536class Solution: def __init__(self): self.nums1 = None self.nums2 = None def findKthOfTwo(self, l1: int, l2: int, k: int) -&gt; int: nums1 = self.nums1 nums2 = self.nums2 n = len(nums1) m = len(nums2) # nums1 is empty if l1 == n: return nums2[l2+k-1] # nums2 is empty if l2 == m: return nums1[l1+k-1] # both not empty if k == 1: return nums1[l1] if nums1[l1] &lt;= nums2[l2] else nums2[l2] # to avoid the rest length of nums1/nums2 is shorter than k//2 k1 = k//2 if l1+k//2 &lt;= n else n-l1 k2 = k//2 if l2+k//2 &lt;= m else m-l2 if nums1[l1+k1-1] == nums2[l2+k2-1]: return nums1[l1+k1-1] if k-k1-k2 == 0 else self.findKthOfTwo(l1+k1, l2+k2, k-k1-k2) elif nums1[l1+k1-1] &gt; nums2[l2+k2-1]: return self.findKthOfTwo(l1, l2+k2, k-k2) else: return self.findKthOfTwo(l1+k1, l2, k-k1) def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: n = len(nums1) m = len(nums2) self.nums1 = nums1 self.nums2 = nums2 # median: the average of (n+m+1)//2 th and (n+m+2)//2 th return (self.findKthOfTwo(0, 0, (n+m+1)//2) + self.findKthOfTwo(0, 0, (n+m+2)//2)) / 2 reference 《信息安全数学基础》 2.2同余类和剩余系。","link":"/2020/09/15/LeetCode_array/"},{"title":"「LeetCode」：Math","text":"LeetCode Math 专题记录。 10月初。 Albert Einstein: “I believe that not everything that can be counted counts, and not everything that counts can be counted” 「并非所有重要的东西都是可以被计算的，也并不是所有能被计算的东西都那么重要。」 7. Reverse Integer[E]7. Reverse Integer[E] Problem: 反转32bits的有符号数字，如果反转后会溢出则返回0. Solution： 直观的解决它，先算出反转后的数字，用比较大小来看是否溢出。（最开始还想着转换为bit串来看，就复杂了） 123456789101112131415161718class Solution: def reverse(self, x: int) -&gt; int: n_min = -(2 ** 31) n_max = 2 ** 31 - 1 s = 0 flag = True if x &lt; 0: flag = False x = -x while x != 0: r = x % 10 s = s * 10 + r x //=10 if flag is False: s = -s if s &lt; n_min or s &gt; n_max: return 0 return s 十进制转换为二进制、八进制、十六进制： 二进制：bin(a) ,也可以直接赋二进制的值0b10101 八进制：oct(a) ，赋值八进制的值0o263361 十六进制：hex(a) ,赋值十六进制0x1839ac29 165. Compare Version Numbers[M]165. Compare Version Numbers[M] Problem： 比较版本号。 Solution： 直观～Easy～ 123456789101112131415class Solution: def compareVersion(self, version1: str, version2: str) -&gt; int: li1 = version1.split('.') li2 = version2.split('.') n_1 = len(li1) n_2 = len(li2) n = max(n_1, n_2) for i in range(n): a = int(li1[i]) if i &lt; n_1 else 0 b = int(li2[i]) if i &lt; n_2 else 0 if a &gt; b: return 1 elif a &lt; b: return -1 return 0 Python中的强制转换： 字符串转换为int : int_value = int(str_value) int转换为字符串：str.value = str(int_value) int转换为unicode： unicode(int_value) unicode转换为int：int(unicode_value) 字符串转换为unicode：unicode(str_value) unicode转换为字符串：str(unicode_value) Java中的强制转换： 字符串String转化为int：int_value = String.parseInt(string_value) 或者 (int)string_value) int转化为字符串String：string_value = (String)int_value 66. Plus One[E]66. Plus One[E] Problem: 用列表表示一个正数，返回正数+1的列表结果。 Solution： 记录一个最高位的进位情况。 123456789101112131415161718192021from typing import Listclass Solution: def plusOne(self, digits: List[int]) -&gt; List[int]: c_bit = 0 n = len(digits) i = n - 1 digits[i] += 1 while i &gt;= 0: if digits[i] &lt; 10: break digits[i] %= 10 if i == 0: c_bit = 1 else: digits[i-1] += 1 i -= 1 if c_bit == 1: digits.insert(0, c_bit) return digits Python中list添加元素的集中方法：（append(); extend(); insert(); +加号） append() ：在List尾部追加单个元素，只接受一个参数，参数可以是任意数据类型。 extend() ：在list尾部追加一个列表，将该参数列表中的每个元素连接到原列表。 12345&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; b = [3,4,5]&gt;&gt;&gt; a.extend(b)&gt;&gt;&gt; a[1, 2, 3, 3, 4, 5] insert(index, object)：将一个元素插入到列表中，第一个参数是插入的索引点，第二个是插入的元素。 +加号：将两个list相加，返回一个新的list对象。 区别：前三种方法(append, extend, insert)可以对列表增加元素，没有返回值，是直接修改原数据对象，而+加号是需要创建新的list对象，需要消耗额外的内存。","link":"/2020/10/10/Leetcode-math/"},{"title":"「MPC-Mike Rosulek 」：Overview of Secure Computation and Yao&#39;s Protocol","text":"本系列是总结Mike Rosulek教授在上海期智研究院的密码学学术讲座。 这是Mike教授的第一个分享：Overview of Secure Computation and Yao’s Protocol Roadmap Secure computation: Concepts &amp; definitions Yao’s protocol: semi-honest secure computation for boolean circuits 主要内容包括安全多方计算的整体介绍及其应用场景、如何定义安全多方计算的security、Yao的混淆电路协议 (garbled circuits protocol)。 （咕咕咕博客选手回来了，学长说：与其担心有没有学读，不如多学学密码学，泪目，我觉得他说的对！） Secure computationConcepts什么是安全计算？ Mutually distrusting parties, each with a private input. 【参与方之间互不信任，且都有一个隐私输入】 Learn the result of agreed-upon computation. 【参与方希望能得到一个各方商定的计算结果】 安全计算能保证什么？ Privacy (“learn no more than” prescribed output) 【隐私性：参与方除了得到商定的计算结果，其他都不能得到】 Input independence 【各参与方的输入无关】 Output consistency 【各参与方的输出是一个商定一致的计算结果】 以上性质及时在某些参与方勾结欺骗的情况下，也是成立的。 scenariosExample 1: Sugar Beets——甜菜定价 Farmers make bids (“at price X, I will produce Y amount”) 【Farmers：会根据甜菜的定价决定产量，价格对产量的影响如上图蓝色的线】 Purchaser bids (“at price X, I will buy Y amount”) 【Purchaser：会根据甜菜的定价觉得购买量，价格对购买量的影响如上图红色的线】 Market clearing price (MCP): price at which total supply = demand 【双方希望甜菜价格能达到市场出清价，即产量=需求量】 但是双方不希望透露自己心中的定价，因为这会破坏经济市场的balabala 因此，在2009年，通过双方可以通过安全计算得到市场出请价。 Example 2: Ad conversion——广告投放 特斯拉存储了到店购买汽车的用户邮箱，而谷歌希望能个性化的投放汽车广告。 因此，如果双方公开数据库中的数据，就可以通过上图的SQL语句筛选出目标用户。 但这样会侵犯用户的用户的隐私，所以特斯拉和谷歌都不会直接公开用户的信息。由此，双方可以通过安全计算得到广告投放的目标用户。 Example 3: Wage Equity Study——薪资公平性研究 研究人员想研究Boston的薪资水平是否是性别平等的，但各企业不会愿意直接公开薪资。 而通过安全计算，可以在保护企业薪资隐私的情况下，得到相关研究结果。 Definitions“securely” compute f ?如何理解“安全地“计算f？ 可以先从安全性需求/威胁来理解什么是不安全。 What if adversary learns more than f(x, y)? 【如果攻击者学到了除商定计算结果f(x,y)以外的信息？】 What if adversary learns f(x, y) but then prevents honest party from learning it too? 【如果攻击者能够得到计算结果，但是阻碍诚信的参与方得到正确的计算结果？】 What if adversary forces several parties to have inconsistent outputs? 【如果攻击者通过某种手段使得一些参与方得到不一致的计算结果】 What if adversary’s choice of input depends on honest party’s input? 【如果攻击者的输入会依赖一些诚信参与方的输入？】（不太理解这是一个什么样的情况？） Security in ideal world在理想情况下，假设有一个可信第三方，Alice和Bob把自己的输入x, y 告诉她，可信第三方计算f(x, y) ，再把计算结果分别分发给Alice和Bob。 在理想情况中，如果有一个恶意的参与方，结果会怎么样？（你看图，变坏的人变丑了2333 corrupt party’s view: 随意选择一个输入y 除了知道f(x, y) ，其他信息都不知道。 致使诚信参与方得到一致的f(x, y) 可见，在理想情况下，即使有恶意的参与方，也能达到之前提到的“安全地”计算f(x, y) 的要求。 Security goal: real protocol interaction is as secure as the ideal-world interaction For every “attack” against real protocol, there is a way to achieve “same effect” in ideal world 【因此，现实情况的安全计算目标应该是达到和理想情况中的同等安全性。】 【对于每一个尝试攻击真实安全计算协议的攻击者来说，总有一种方式达到 和攻击理想情况的 相同影响。】 首先，先解释在real protocol中，恶意参与方会产生什么样的effect？ 下图是一个现实的MPC协议交互图（有恶意参与方）： effect： Something the adversary learns / can compute about honest party 【恶意参与方通过MPC得到的计算结果，可能能计算一些关于可信参与方的信息】 Some influence on honest party’s output 【恶意参与方能够影响可信参与方得到的计算结果】 Defining security 完整的安全定义: RanCanetti13 Universally Composable Security: A New Paradigm for Cryptographic Protocols Security definition: For every real-world adversary $\\mathcal{A}$, there exists an ideal adversary $\\mathcal{A’}$ s.t. joint distribution (HonestOutput,AdvOutput) is indistinguishable. 【对于每一个真实协议交互下的攻击者 $\\mathcal{A}$ ，都存在一个在理论协议情况下的攻击者 $\\mathcal{A’}$ ，这两个攻击者产生的effect是相同的，两种情况下的诚信参与方的输出和恶意参与方的输出的 联合分布 是不可区分的。】 现在，来解释如何产生上文提到的：For every “attack” against real protocol, there is a way to achieve “same effect” in ideal world. 假设在理想情况中有一个simulator：simulates real-world interaction in ideal world 【可以在理想情况中模拟现实协议的交互。】 对于该模拟器来说，他可以扮演两种角色： Send protocol messages that look like they came from honest party Demonstrates that honest party’s messages leak no more than f(x, y) 【和恶意参与方交互，模拟现实协议中的诚信参与方：对模拟器来说，他除了知道计算结果f(x, y) 外】 Extract an f-input by examining adversary’s protocol messages “Explains” the effect on honest party’s output in terms of ideal world 【在理想协议中，模拟器提取来自恶意参与方的输入信息，模拟理想情况中的参与方。】 Q: yuyu老师问，这里的extract是什么意思？ A：在现实安全计算的协议交互中，不管恶意参与方提交什么输入，总能商定一个结果。因此在理想情况中，也许恶意参与方只是上传了一些garbage bit，模拟器需要从这些garbage bits中提取一些信息，作为参与交互的输入。 Semi- Honest security有一种特殊的情况，即攻击者是semi-honest： Adversary assumed to follow the protocol on a given input 【攻击者在给定输入下遵守协议】 Adversary may try to learn information based on what it sees 【但攻击者会尝试从得到的信息中学到一些其他信息】 No need to extract, only simulate transcript given ideal input+output 【因为攻击者会遵守协议，所以模拟器不需要extract来自半诚实的攻击者，只需要传送来自半诚实参与方的输入和可信第三方的计算输出结果。】 Yao’s protocolYao’s protocol : semi-honest secure computation for boolean circuits. warm-up protocol: garbled truth tableAlice方：会计算一个混淆真值表（garbled truth table） Write truth table of function f 【写下函数f的真值表，该函数对于Alice方有4种输入，对Bob方有4种输入】 For each possible input, choose random cryptographic key 【对于函数的每一种输入，Alice都选择一个随机的密钥（也叫label），对于Alice方的输入有4种密钥，对于Bob方的输入有4种密钥。】 Encrypt each output with corresponding keys 【再用对应的输入密钥加密每一个函数的输出】 Randomly permute ciphertexts, send to Bob 【对第三步的加密真值表随机置换，得到garbled truth table，再发给Bob。】 而对于Bob方：（先不考虑如何得到正确的密钥Ax和By）Bob得到了密钥Ax和By，对garbled truth table进行尝试性解密，就只能得到f(x, y) 对于Bob来说，是通过OT来得到正确的密钥Ax和By。后面会解释OT的简易版本和更为常用的做法。 Q：Bob如何判断这是正确的解密？ A：在对字符加密前，可以对字符加一些标记，比如前导0，或者特殊字符串之类。 在Bob收到garbled truth table和garbled input后，Bob的视角可以模拟成下图： 并且，对于distinguisher来说，只要密钥{A, B}中有一个不知道，那么 $\\mathbb{E}{A, B}(C)\\approx \\mathbb{E}{A’, B’}(C’)$ ，（即和其他不知道的输入的加密是不可区分的）这样的模拟就是不可区分的。 Extending warm-up protocol但这样的协议还存在一些问题： Problem: Cost scales with the truth table size of f 【对于复杂函数来说，真值表的大小会很大，计算开销和存储开销都会很大】 How does Bob magically learn “correct” Ax, By? 【之前忽略的假设，Bob如何才能得到Alice制定的密钥Ax和By】 这里会先解决第一个问题，想法是把这个复杂函数拆分成多个garbled truth table，可以用一个garbled table 的garbled outputs作为另一个garbled table的keys，就像这样： 第二个问题，关于OT的部分，后文会继续解释。 Garbled circuit framework对于garbled circuit framework的提出，出自姚期智院士86年的论文。 用boolean电路表示一个函数，电路由一些门组成： (Alice) Garbling a circuit: Pick random labels W0, W1 on each wire 【对于每一个wire，都会选择两个label来表示true和false，比如选择A0是true，A1是false，这里的label其实就是上文提到的加密密钥。】 “Encrypt” truth table of each gate 【然后，对每一个门，都用对应的label加密真值表】 Garbled circuit = all encrypted gates Garbled encoding = one label per wire 【这样就把一个混淆电路变成一些加密真值表，同样的，把混淆电路的编码变成了每个wire的一个label】 (Bob) Garbled evaluation: Bob收到了混淆电路（garbled circuit）和混淆输入（garbled input），就可以进行garbled evaluation Only one ciphertext per gate is decryptable 【对于Bob来说，他只有输入wire的单个label，因此对于每一个门，他都只能解密加密真值表中的一项】 Result of decryption = value on outgoing wire 【输出门的值就是最后的计算结果】 对于接受方Bob来说，它的任务就是用garbled input来evaluate garbled circuits。他只能学习到每个wire的一个label，计算上不可能的去猜测这个wire的另一个label是什么。而对于输出门的输出wire，即使揭露输出wire的两个label也只会泄漏电路的输出。 在BellareHoangRogaway12中有对gabled circuit做一些正式的符号定义： Privacy: (F,X,d) reveals nothing beyond f and f(x) Obliviousness: (F,X) reveals nothing beyond f Authenticity: given (F, X), hard to find Y‘ that decodes $\\notin$ {f(x), ⊥} Oblivious transfer回到之前说的第二个问题，对于evaluator，也就是Bob，如何才能得到garbled circuit的garbled input？ 对于下图的garbled circuit，需要Alice输入A和B，需要Bob输入C和D。 Garbler’s input: Alice 作为garbler，她知道每个wire的两个label（比如A0, A1)，也知道label对应的真值关系(比如A0是false，A1是true），那Alice只需要把她的输入的对应的那个label发送给Bob就好了。 Evaluator’s input: 我们需要用OT（oblivious transfer）来实现，让Bob从Alice手中获得他想要的label，但又不让Alice知道他想要的是哪一个label。 如何构造不经意传输(OT)？ 下图是一种简单实现的OT，不是最优的协议，后面的文章会继续介绍常用的OT。 Alice有两个label：W0和W1，Bob想要得到其中一个，假设是Wc ( $c=0,1$ ) 。 Bob需要用KeyGen算法生成一对公私钥 $(sk_c, pk_c)$ ，同样还需要BlindKeyGen算法生成一个不知道（或者硕士无法知道）其对应私钥的公钥 $pk_{1-c}$ 。 Bob将两个公钥 $pk_0, pk_1$ 发给Alice Alice用 $pk_0$ 加密W0，用 $pk_1$ 加密W1，把两个密文发送给Bob 因为Bob只知道Wc的私钥，而无法计算得到W1-c的私钥。所以Bob可以从Alice手中得到想要的label。 Need public-key encryption that supports blind key generation: sample a public key without knowledge of secret key E.g.: ElGamal (sample group element without knowing discrete log) 需要用一些支持blind key生成的公钥加密算法，比如ElGamal，群中的某些元素没有对应的离散对数，也就无法知道其对应的私钥。 Yao’s protocol: overviewYao协议： Gabler生成每个wire的label，把garbled circuit f（加密的真值表）、garbled input x（Alice输入对应的labels)和output wire labels（输出wire对应的labels）发送给Evaluator Evaluator用n次OT，在和Alice交互中，得到Bob每一个输入wire的label，每一次OT，Alice都不知道Bob选择的输入是什么。 对于Evaluator来说：garbled f + garbled inputs + all output labels ⇒ Bob learns only f(x, y)","link":"/2021/07/03/MPC1-Yaos-protocol/"},{"title":"「MPC-Mike Rosulek 」：Advanced Techniques and Optimizations for Garbled Circuits","text":"本系列是总结Mike Rosulek教授在上海期智研究院的密码学学术讲座。 这是Mike教授的第二个分享：Advanced Techniques and Optimizations for Garbled Circuits Roadmap Optimizations: How did garbled boolean circuits get so small? New frontiers: How to garble arithmetic circuits? 在这篇文章中，会介绍在garble boolean circuits时的优化技术：包括point-and-permute, row-reduction, free-XOR和half gates。此外，这篇文章还会介绍如何garble arithmetic circuits。 Optimizing garbled circuits在上一篇文章中，介绍了garbled circuits的核心思想： Given garbled gate + one wire label per input wire: can learn only one output label (authenticity) cannot learn truth value of labels (privacy) garbled circuits的计算性能和速度主要取决于他的大小（ciphertext的数量），但现实中garbled circuits的计算是非常快的。 因为现有的garbled boolean circuits使用了一些优化技术，使得garbled gate的ciphertext表示数量大幅下降。 Ciphertext expansion在介绍这些优化技术之前，我们先来分析Yao’s protocol中garbled circuits的大小。 每个boolean gate的wire有两种输入（0或者1），对每个输入都随机选择一个label，再用对应的label（密钥）对输出加密，得到下图的garbled gates（ciphertext）。 但是这样的排列方式会泄漏语意信息，因此需要打乱ciphertext的顺序。 而evaluator在evaluate时，只能用得到的labels一个一个尝试性解密。 为了让evaluator在解密时能清楚知道解密出的label是正确的，就需要使用带有ciphertext expansion的加密方案。（比如在加密时，如果label是4位的，可以通过在label前加4个前导0）这样得到的garbled gate的ciphertext长度是原label长度的一倍。 Point-and-permute首先介绍的是Point-and-permute技术： Assign color bits red &amp; blue to wire labels. Association between (red, blue ) &lt;-&gt; (True, Flase) is random for each wire. 【color bits 可以看作是一个bit mask，或者说是一个1-bit one-time pad。在为每一个wire的labels分配color bits时，是随机分配的：随机选择一个bit r (0或者1)，对于wire a，label的color bits的计算是 $a\\oplus r$ 。】 A wire label reveals its own color(e.g. as last bit) 【可以在label后再附加一个bit作为color bit，来表示该wire label的颜色】 Order the 4 ciphertexts canonically, by color of keys. 【然后对表示garbled gate的4个ciphertexts按照指定的颜色组合排序】 Evaluate by decrypting ciphertext indexed by your colors 【这样evaluator在evaluate混淆电路时，就可以通过已知labels的颜色组合索引出正确的密文，再进行解密，不再需要对每一个密文进行尝试性解密。】 通过Point-and-permute技术，evaluator在评估电路时不再需要对每个密文尝试性解密，garbler在混淆电路时也不再需要做ciphertext expansion。 同时，该技术支持简单的one-time pad加密，即 $\\mathbb{E}_{A, B}(C)=H(A, B)\\oplus C$ ，$H$ 可以是任意的一种加密方式，比如AES。 我们将Point-and-permute与Yao’s protocol进行比较： 假定单个label大小是 $\\lambda$ ，garbled circuits的大小表示为ciphertexts的大小 。garble cost的值代表garbler加密时的开销，eval cost的值表示evaluator解密时的开销。 对于Yao的方案，因为有ciphertext expansion，所以garbled circuits的大小为 $8\\lambda$ 。对每个garbled gate，都需要生成4个ciphertexts。而evaluator在评估电路时，每个ciphertext都有 1/4的概率是正确的，所以开销为 $2.5 = 1\\times 1/4 + 2 \\times 1/4 + 3\\times 1/4 + 4\\times 1/4$ . 对于Point-and-permute的方案，不需要ciphertext expansion，所以garbled circuits的大小为 $4\\lambda$ 。在evaluator评估电路时，可以通过labels的颜色组合来索引密文，因此只需要1的开销。 Garbled Row ReductionRow Reduction[NaorPinkasSumner99] 是在Point-and-permute方案上进一步优化： 相较于Point-and-permute方案中的随机生成输出wire的labels，Row Reduction方案在输出wire的labels生成中，使用了一个trick：label $C_0$ 随机生成，而label $C_1$ 是一个能让P&amp;P方案中的第一个ciphertext为 $0^n$ ，即 $C_1 = H(A_0, B_1)$ 。虽然$C_1 = H(A_0, B_1)$ ，但对除garbler以外的人来说，$C_1$ 和random bits是不可区分的，即 $C_1$ 看起来和随机生成的label一样。 对garbler来说，每个garbled gate就可以只用3个ciphertexts表示。 但对evaluator来说，他只需要把 $0^n$ 加上，重构为4个ciphertexts，后面的操作和P&amp;P方案相同。 同样的，我们把Row Reduction的方案和前两种方案进行比较： GRR3方案相对于P&amp;P方案，可以将garbled circuits的数量从4降为3。 Free XORFree XOR[KolesnikovSchneider08] 是对异或门的优化技术。 Define offset of a wire ≡ XOR of its two labels 【Free XOR对wire定义了一种offset $\\Delta$ ，该值其实等于两个labels的异或】 Choose all wires in circuit to have same (secret) offset ∆ 【Free XOR方案中，对所有的wires都选择了一个相同的保密的offset】 Choose false output = false input ⊕ false input 【相较于之前方案的随机生成output wire的label，Free XOR让output wire的false output = false input ⊕ false input】 【也就是令 $C = A \\oplus B$ (A: false , B: false, C: false)】 Evaluate by xoring input wire labels (no crypto) 【如此定义output wire的label，output wire的label可以通过input wire的label直接计算出来，而不需要再对其加密解密】 【false xor false = false ｜ $A \\oplus B = C$ 】 【false xor true = true ｜ $A \\oplus B \\oplus \\Delta = C\\oplus \\Delta$ 】 【true xor false = true ｜ $A \\oplus \\Delta \\oplus B = C\\oplus \\Delta$ 】 【true xor true = false ｜ $A \\oplus \\Delta \\oplus B \\oplus \\Delta = C$ 】 同样，我们把XOR的方案和之前方案比较，由于XOR方案只适用于XOR gate，因此把XOR gate和AND gate分开： Row reduction $\\times$ 2Row reduction $\\times$ 2[GueronLindellNofPinkas15] 方案是对Row Reduction方案的进一步优化，Row Reduction方案对输出wire的label选择上：$C_0$ 随机生成，$C_1$ 的值是能让P&amp;P方案第一个ciphertext 为 $0^n$ 的方案，即 $C_1 = H(A_0, B_1)$ （式子只是针对下图的样例）。 而Row reduction $\\times$ 2方案规定label $C_0 = H_{00}\\oplus H_{11} \\oplus H_{10}$ ，这样能让剩下三个ciphertext的XOR值为 $0^n$ ，同样的 $C_0$ 对除garbler之外的人来说和random bits不可区分。 因此，对garbler来说，每一个garbled gate只需要用两个ciphertexts表示： 而对evaluator来说，他只需要用这两个ciphertexts重构出原本的garbled circuits即可： 同样，把GRR2方案和其他方案比较，主要和P&amp;P和GRR3方案比较，每个garbled gate都只需要用两个ciphertexts即可表示： 这里需要注意的是，Free XOR技术和GRR2技术是不可兼容的。因为在GRR2中，$C_0 = H_{00}\\oplus H_{11} \\oplus H_{10}$ ，$C_1 = H(A_0, B_1)$ ，无法保证 $C_0\\oplus C_1=\\Delta$ 。因此在一个电路中，那么使用Free-XOR方案，要么使用GRR2方案。 Half GatesHalf Gates 方案出自Samee Zahur, Mike Rosulek, David Evans的15年的文章：Two Halves Make a Whole: Reducing Data Transfer in Garbled Circuits using Half Gates. (Eurocrypt 2015) 这个方案garbled circuits的设计是借用了Free-XOR的部分设计：即所有的wire都选择了一个相同的offset。但output wire label $C$与input wire的label无关。 Garbler: half gate如果garbler提前知道了input wire a的值： $a = 0$ 如果$a = 0$, 这个门就变成一个一元门： $b \\rightarrow 0$ 可以用2个ciphertexts来表示这个门，evaluator解密后都得到output wire 的label $C$。 $a = 1$ 如果$a = 1$, 这个门就变成一个一元门：$b\\rightarrow b$ 同样可以用2个ciphertexts表示这个门，evaluator可以用label $B$解密得到output wire 的 label $C$，可以用 label$B\\oplus \\Delta$ 解密得到output wire 的label $C\\oplus \\Delta$ 。 结合两种情况： 可以归纳为下式，garbler可以根据已知a的值来生成对应的ciphertexts，同样用point-and-permute的color bit的思想来重排列ciphertexts。 同样，也可以再用row reduction的技术来选择output wire的false label $C=H(B)$ ，使得第一个ciphertext为 $0^n$ ，这样对于garbler知道一个input wire真值的garbled half gate就可以用一个ciphertext来表示： Evaluator: half gate如果evaluator提前知道了input wire b的值，即知道label的对应真值： Evaluator has B (knows label B is false): should obtain C (false) Evaluator has $B\\oplus \\Delta$ (knows label $B\\oplus \\Delta$ is true): should be able to transfer truth value from “a” wire to “c” wire. 【即如果evaluator知道wire b的label是true，那evaluate的结果的真值应该和wire a上label对应的真值相同（即使她不知道wire a上实际的真值是什么）】 Evaluator只要知道 $A\\oplus C$ 的值，就可以实现 “transfer truth value from “a” wire to “c” wire.” false($A$) -&gt; false ($C$) : $A\\oplus A \\oplus C = C$ true($A\\oplus \\Delta$ )-&gt; true ($C\\oplus \\Delta$ ): $A\\oplus \\Delta \\oplus A \\oplus C = C\\oplus \\Delta$ 同样的，可以使用row reduction的技术，令false label $C = H(B)$ ，使得garbled circuits的第一个ciphertext为 $0^n$ ，这样对于evaluator知道一个input wire真值的garbled half gate就可以用一个ciphertext表示。 Two halves make a whole对于在复杂电路中的AND门，他的input wire可能和双方输入都相关，因此garbler不知道某个input wire的真值到底是什么，evaluator更不知道。 如何才能将上文中的half gate应用到一般AND门上？ 对于garbler来说，在garble AND gate时，即然不知道某个inpu wire的真值，那我们可以引入一个随机bit r（0或者1），该值其实就是在计算label的color bit时引入的，而bit $a\\oplus r$ 就是label $A$ 的color bit。 对于garbler来说，garbler知道bit $r$的真值，true or false。 对于evaluator来说，他在evaluate这个AND gate时，他会得到wire a的label，虽然不知道这个label实际是对应着true还是false（即不知道 $a$ 的值），但是他知道这个label的color bit是什么（像P&amp;P中提到的那样，可以把color bit放在label的最后一位），而这个color bit其实就是bit $a\\oplus r$ 的真值，所以evaluator在evaluate时是知道 bit $a\\oplus r$ 的真值的。 现在我们通过引入了 bit $r$ ，实现了half gate的两个假设，一个是让garbler知道AND gate其中一个的输入真值，一个是让evaluator知道AND gate其中一个的输入真值。 那如何通过 bit $r$ ，把一个一般AND gate转变为half gates 呢？ 把 $a\\wedge b$ 写成如下式：$$\\begin{align}a\\wedge b &amp;= (a\\oplus r \\oplus r)\\wedge b \\&amp;=[(a\\oplus r)\\wedge b]\\oplus [r\\wedge b]\\end{align}$$ 对于左边的 $[(a\\oplus r)\\wedge b]$ ：evaluator知道 $a\\oplus r$ 的真值，可以用上面提到的第二种half gate。 对于右边的 $[r\\wedge b]$ : garbler知道 $r$ 的真值，可以用上面提到的第一种 half gate。 两个式子的xor运算：就可以使用Free-XOR 技术。 因此，总开销 = 2 “half gates” + “1 XOR gate” = 2 ciphertexts 同样的，把Half gates的方案和其他技术进行比较，AND gate的大小可以降为2，同时Half gates技术和Free-XOR技术是兼容的，因此在half gates方案中，XOR gate的开销为0. Garbling arithmetic circuitsGeneralized Free XOR将Free XOR的思想迁移到 $\\mathbb{Z}_m$ 上。 Free XOR Gerneralized Free XOR Wire carries a truth value from ${0, 1}$ Wire carries a truth value from $\\mathbb{Z}_m$ Wire labels are bit strings ${0, 1}^\\lambda$ . Wire labels are tuples $(\\mathbb{Z}_m)^\\lambda$. Global wire-label-offset $\\Delta\\in{0, 1}^\\lambda$ Global wire-label-offset $\\Delta\\in(\\mathbb{Z}_m)^\\lambda$ false wire labe lis $A$ ；true wire label is $A\\oplus \\Delta$ Wire label encoding truth value $a\\in \\mathbb{Z}_m$ is $A+a\\Delta$ ⊕ is componentwise addition mod 2 + is componentwise addition mod m Generalized Free XOR的核心就是：用wire label $A+a\\Delta\\in (\\mathbb{Z}_m)^\\lambda$ 来编码 $a\\in \\mathbb{Z}_m$ 。 这样编码后，evaluator可以直接通过对label进行模 $m$ 的加法操作来完成evaluate： Garbling unary gates一元门 $\\phi$ （unary gates）: 只有一个input wire $\\in \\mathbb{Z}_m$， 一个output wire $\\in \\mathbb{Z}_l$，注意两个wire可以属于不同的有限域。 Generalized Free XOR除了可以混淆模 $m$的加法电路，还可以混淆一元门。 用input wire label $A+a\\Delta_m\\in (\\mathbb{Z}_m)^\\lambda$ 来编码 $a\\in \\mathbb{Z}_m$ ，用output wire label $C+a\\Delta_l \\in (\\mathbb{Z}_l)^\\lambda$ 来编码 $c\\in \\mathbb{Z}_l$ ，因为两个wire可以属于不同的有限域，所以注意是不同的offset $\\Delta$ 。 因此，使用generalized free XOR技术garble unary gates需要$m$个ciphertexts。 当然如果加上row reduction技术，只需要 $m-1$ 个ciphertexts。 因为作用在 $\\mathbb{Z}_m$ 上，所以为了减少evaluator的开销，一般化point-and-permute技术， color bits $\\in \\mathbb{Z}_m$ 。 Generalized garbling tool通过generalized free xor的思想，我们可以高效混淆有如下特征的电路： Each wire has a preferred modulus $\\mathbb{Z}_m$ Wire-label-offset $\\Delta_m$ global to all $\\mathbb{Z}_m$ -wires Addition gates: all wires touching gate have same modulus Garbling cost: free Mult-by-constant gates: input/output wires have same modulus Garbling cost: free Unary gates: $\\mathbb{Z}_m$ input and $\\mathbb{Z}_l$ output Garbling cost: $m-1$ ciphertexts Arithmetic computations为什么要引入arithmetic circuits，因为他往往会比传统布尔电路高效，以一个32-bit的数为例，使用half-gates技术来混淆一些常见运算，结果如下： 而如果使用上文提到的generalized garbling tool，加法操作和乘以常数操作的开销都降为0，而模 $m$ 乘法操作的开销为 $2m-2$ ciphetextes（讲座中没有详细讲解，方法是generalization of half-gates)，同样以32-bit的数为例，和传统布尔电路技术对比： 可见对于32-bit的乘法、开方操作，使用generalized garbling技术开销非常大，因为这是和 $m$ 相关的。 CRT form而这个可以通过中国剩余定理(Chinese remainder theorem, CRT)来进行优化。 相较于之前在 $\\mathbb{Z}_{32}=\\mathbb{Z}_{4294967296}$ 进行算数运算，通过CRT可以在 $\\mathbb{Z}_{2\\cdot 3\\cdot 5\\cdot 7\\cdot 11\\cdots 29}=\\mathbb{Z}_{6469693239}$ 下进行高效的乘法、开方操作。 通过把一个32-bit的数 $x$ 表示为 $(x\\%2, x\\%3,x\\%5,\\cdots,x\\%29)$ ，对每个余数单独进行算数运算，最后通过中国剩余定理，即可求解出结果。 将使用CRT表示方法的系统与上述方法比较： 可见，用CRT的表示方法能大幅降低garbled circuits的开销。但是CRT的表示方法同样也带来了一些问题： Not so good: Converting from binary to CRT is not so good. Getting CRT valurs into the citcuit via OT is not so good. Kinda bad: (room for improvement) Comparing two CRT-encoded values Converting from CRT to binaty Integer division Modular reduction different than the CRT composite modulus (e.g., garbled RSA) 下面会主要介绍如何比较两个CRT表示方法的数字。 Comparing CRT values下表是CRT的表示方法，可以看出CRT的表示方法不能直接对数字进行比较： 但如果将CRT转换为一种叫PMR (Primorial Mixed Radix)的表示方法，就可以直接对数字进行比较了： 转换方法如下图所示： 要实现上述的转换方法，只需要实现 $(x\\%p, x\\%q)\\mapsto \\lfloor \\frac{x}{p}\\rfloor\\%q$ 模块的garbled circuits。 实现该模块的具体步骤可以参考下图： Subtract x%3 − x%5 (mod 7 is fine)【相减】 “Project” x%3 and x%5 to $\\mathbb{Z}_7$ wires Subtract mod 7 for free Result has the same “constant segments” as what we want Apply unary projection：【再做一个一元映射】 最后分析一下CRT表示方式的比较效率： $(x\\%p, x\\%q)\\mapsto \\lfloor \\frac{x}{p}\\rfloor\\%q$ 模块大约需要 $2p+2q$ 的开销。 而将CRT转换为PMR：对于每一个质数对都需要上述模块。 对于k-bit: CRT比较 $O(k^3)$","link":"/2021/07/05/MPC2-GarbledCircuits/"},{"title":"「Math」：Mersenne Prime","text":"在密码学中，有限域中的运算性能极大影响密码协议的实现。 如果有限域选择梅森素数，得益于它的优良性质，可以极大提高运算效率，特别是有限域下的模运算、乘法操作。 于是近日学习了梅森素数的相关性质，以及如何约减梅森素数域下模运算和乘法运算。 [Mersenne Prime]","link":"/2021/12/23/Mersenne-Prime/"},{"title":"「MPC-Mike Rosulek 」：Oblivious Transfer and Extension","text":"本系列是总结Mike Rosulek教授在上海期智研究院的密码学学术讲座。 这是Mike教授的第三个分享：Oblivious Transfer and Extension Roadmap Precomputation: can compute OTs even before you know your input! OT extension: 128 OTs suffice for everything. OT在多方安全计算中扮演着重要的角色，但OT的实际开销往往很大，因为他不可能使用廉价的加密方法来实现[ImpagliazzoRudich89]。因此在这篇文章中，会介绍一些前沿的方法来提高OT的效率：离线预计算和OT扩展。 Offline PrecomputationRandom OT在提出random OT概念前，我们先回顾一下standard OT: 在standard OT中，Alice选择两个输入：$m_0, m_1$，Bob选择一个输入：$c$。 经过一次OT后，Bob可以得到Alice输入中的一个：$m_c$，而不知道另一个输入 $m_{1-c}$ 。而Alice不知道Bob的输入 $c$ ，也就不知道Bob选择的哪一个。 上述过程中，需要双方选择他们各自的输入，双方的输入确定，Bob得到的结果也是确定的：$m_c$ 。 相对于standard OT的确定性，random OT不需要双方选择输入，而是直接随机采样许多 $m_0, m_1, c$ 作为random OT instances。 Beaver Derandomization Theorem[Beaver91] ： There is a cheap protocol that securely evaluates an instance of satandard OT using an instance of random OT. 【Beaver的去随机化理论提出：存在一些高效的协议能将一个random OT 转化为一个standard OT instance.】 根据Beaver的去随机化理论，在运行OT协议时，可以先离线生成许多random OTs。在online phase：即OT的输入是由Alice和Bob的输入确定的，可以用Beaver的方法高效地将离线生成的random OT去随机化，即变成standard OT。 Beaver DerandomizationBeaver去随机化过程分为两个阶段：离线阶段和在线阶段（standard OT) 在离线阶段，生成大量random OT instances，其中Alice知道 $m_0^\\$, m_1^\\$ $ ，Bob知道 $c^\\$, m_{c^{\\$}}^\\$$ 。 而Beaver的去随机化的核心思想是：用生成的random OT instance的 $m_0^\\$, m_1^\\$ $ 作为one-time pad的密钥，分别加密Alice在线时的输入 $m_0, m_1$ 。而Bob只能用唯一知道的密钥 $ m_{c^{\\$}}^\\$ $ 解密其中一个，而不知道另一个。 Bob在线时的输入： $c=c^\\$$ Bob可以用 $ m_{c^{\\$}}^\\$ $直接解密： $x_c \\oplus m^\\$_{c^\\$} = x_c \\oplus m^\\$_c = m_c$ ，得到想要的 $m_c$ ，而对 $m_{1-c}$ 一无所知。 Bob在线时输入： $c\\neq c^\\$$ 如果Bob还想用 $ m_{c^{\\$}}^\\$ $直接解密得到 $m_c$： $x_c \\oplus m^\\$_{c^\\$} =x_c \\oplus m^\\$_{1\\oplus c} =m_c$ ，Alice在加密时 $m_0, m_1$ 时，必须交换其密钥 $m_0^\\$, m_1^\\$ $ ： 为了Bob能得到正确的 $m_c$ ，Bob应该告诉Alice什么时候要交换密钥，什么时候不用交换密钥。 方法就是用一个1-bit one-time pad，Bob 计算出 $d = c\\oplus c^\\$$ ，由此告诉Alice是否需要交换密钥（$d = 0$ 告诉Alice不需要交换密钥，$d = 1$告诉Alice需要交换密钥），而Alice对 $c, c^\\$$ 都一无所知。 因此Beaver的去随机化的完整过程： Bob计算出 $d = c\\oplus c^\\$$ ，发送给Alice，以此告诉Alice是否需要交换密钥 $m_0^\\$, m_1^\\$ $ 。 Alice输入 $m_0, m_1$ ，根据 $d$ 的值选择密钥 $m_0^\\$, m_1^\\$ $ 来分别加密$m_0, m_1$ : $$ \\begin{align} x_0 &= m_d^\\$ \\oplus m_0 \\\\ x_1 &= m_{1\\oplus d}^\\$ \\oplus m_1 \\end{align} $$ 再将其发送给Bob Bob用唯一已知的密钥 $ m_{c^{\\$}}^\\$ $来解密得到 $m_c$： $x_c \\oplus m^\\$_{c^\\$} =x_c \\oplus m^\\$_{1\\oplus c} =m_c$ ，而对 $m_{1-c}$ 一无所知。 最后分析一下Beaver去随机方法的开销：离线阶段生成一个random OT instance和之前的OT开销相同，但在在线阶段，只需要一些简单的异或运算。 OT ExtensionAn Analogy from Encryption公钥加密（Public-key encryption, PKE）本质上具有不可避免的高昂开销[ImpagliazzoRudich89] ，但是在现实的加密方案中，可以通过混合公钥加密和对称加密的方式，将实现PKE的开销降到最低。 即PKE of λ bits + cheap SKE = PKE of N bits Use (expensive) PKE to encrypt short s 【用PKE来加密共享短密钥，比如DH算法】 Use (cheap) symmetric-key encryption with key s to encrypt long M 【再将短密钥通过密钥扩展算法获得长密钥，作为对称加密的密钥】 OT本质上也具有不可避免的高昂开销[ImpagliazzoRudich89] ，所以如果将OT与公钥加密类比，是否存在一种混合加密方案，也能实现λ instances of OT + cheap SKE = N instances of OT？ Beaver OT ExtensionBeaver利用Yao的两方安全计算，提出了一种OT的扩展协议[Beaver96] 实现$\\lambda$ instances of OT + cheap SKE = $N$ instances of OT. Beaver OT扩展协议其实是一个两方安全计算电路：该电路将Alice和Bob的输入作为伪随机的种子，以此生成$n$ OT instances，包括输出给Alice的random strings和输出给Bob的choice bits+choice strings。 该电路的输入位是 $\\lambda$ bit，因此Alice和Bob需要做 $\\lambda$ 次OTs，而该电路可以生成 $n&gt;&gt;\\lambda$ OTs，由此实现了$\\lambda$ instances of OT + cheap SKE = $N$ instances of OT. 但是用Yao’s garbled circuits的方法实现Beaver OT扩展协议几乎是不可能的，因为这太复杂了。 IKNP Protocol而Yuval Ishai, Joe Kilian, Kobbi Nissim, Erez Petrank在Crypto 2003年发表的论文：Extending Oblivious Transfers Efficiently，提出了一种可实现、更高效的OT扩展协议，以此实现 $\\lambda$ instances of OT + cheap SKE = $N$ instances of OT。 该协议能够用有限的OTs（比如128次OTs）实现足够多次的OTs（比如1 million），下面将详细阐述协议是如何实现的。 IKNP Details Bob: has input $r $ $\\Rightarrow$ extend to matrix 【把输入 $r$ 重复多次扩展为矩阵 $R$，矩阵的每行 $R_i$ （ith row of $R$）要么是全1或者全0，而且该矩阵的高是million级别的，宽度可以为128】 $\\Rightarrow$ secret share as ($T, T’$) 【然后对该矩阵$R$做秘密分享，即拆分为两个矩阵 $T,T’$ 】 【秘密分享的策略是随机生成一个矩阵 $T$ ，而 $T’=T\\oplus R$ ，观察下图可以看出 $R_i = T_i \\oplus T_i’$ ，即两个矩阵的每行要么相同($r=0$)，要么相反($r=1$) 】 Alice： chooses random string $s$ OT for each column $\\Rightarrow$ Alice obtains matrix $Q$ 【从列的角度做OT，Alice每次根据 $s$ 的值从Bob的($T,T’$)选择一列，如果 $s_i=0$ ，从矩阵$T$选择第i列，如果 $s_i=1$ ，从矩阵$T’$选择第i列，得到矩阵 $Q$】 【这里的OT中，Bob是sender，Alice是receiver】 观察Alice经过128次OT得到的矩阵 $Q$ ，可以发现Alice得到的矩阵 $Q$ 和Bob拥有的矩阵 $T$ 有如下关系： Whenever $r_i = 0$, Alice row = Bob row Whenever$r_i =1$, Alice row=Bob row $\\oplus s$ For every i: Bob knows $t_i$ ; Alice knows $q_i$ and $q_i\\oplus s$ . 【再从行的角度观察Alice和Bob得到的信息，Alice每次收到 $q_i$ 后，都可以通过和random string $s$ 异或得到 $q_i$ 和 $q_i\\oplus s$ ，而这其中的一个值与Bob知道的 $t_i$ 相同】 【Alice：因为不知道Bob的 $r_i$ 值，所以不知道Bob选择的 $t_i$ 到底是和 $q_i$ 相等还是和 $q_i\\oplus s$ 相等】 【Bob：因为不知道Alice的 $s$ 值，所以只知道选择的 $t_i$ 等于$q_i$ 和$q_i\\oplus s$ 中的一个，而不知道另一个】 但是从Bob视角重写Alice得到的信息，我们知道得到： $r_i=0: q_i=t_i$ $r_i=1:q_i=t_i\\oplus s$ 至此，我们几乎已经实现了行角度的OT，Alice有两个值：$q_i$ 和 $q_i\\oplus s$ ，Bob根据 $r_i$ 从中选择得到一个，而对另一个一无所知。 Break correlations by applying random oracle 上面我说是“几乎实现了行角度的OT”，为什么是几乎呢？ 因为string s的原因，这些OT instances具有一定的线性相关性。所以为了破坏其中的线性相关性，对这些strings都执行一个任意的密码学函数 $H$ ： $\\Rightarrow$ Random OT instance for each row, using base OT for each column 【由此，通过从列的角度做base OT，得到了足够多的行角度random OT instances】 最后，总结一下IKNP如何实现$\\lambda$ instances of OT + cheap SKE = $N$ instances of OT。 Bob有一个tall matrix($\\lambda$ 列, $n&gt;&gt;\\lambda$ 行)，Alice从列的角度根据秘密字符串 $s$ 做 $\\lambda$ 次base OTs。 最后Alice和Bob得到了行角度的 $n$ 个扩展OTs。 Malicious Bob in IKNP Protocol[KellerOrsiniScholl15] 介绍了如果有恶意的参与方，IKNP受到的威胁 假设Bob是恶意的： Bob： 有一个输入 $r $ $\\Rightarrow$ 对其扩展为矩阵 $R$ ，但是恶意地翻转了 $t_2$ 的第二位bit： $\\Rightarrow$再做秘密共享为 ($T, T’$) ，同样的，$T’$ 中也有一位被翻转了： Alice: 选择一个随机串 $s$ 对每列做OT $\\Rightarrow$ Alice得到矩阵 $Q$ 因为 $r_2=0$ ，本应该有 $q_i = t_i$ 。但因为Bob恶意翻转了一位，所以Alice得到的 $q_i$ 和 $t_i$ 在第二位不同 … 所以在Alice和Bob同时使用 $H(row)$ 破坏线性相关性后，Bob会发现得到的 $H(row#2)$ 值不同，Bob就知道Alice选择的随机串中 $s_2=1$ 。 如此以来，Bob每次都恶意翻转某一列的一个bit，就可以学到 $s$ 的所有比特信息。 Consistency Check所以如何保证Bob没有恶意篡改矩阵 $R_i$ ($R$ 矩阵的第i行)? [KellerOrsiniScholl15] 提出了一种一致性检验方法来检测Bob是否恶意篡改了矩阵。 Alice在执行IKNP中通过多次检验上述方程的一致性，来检测Bob是否恶意篡改了扩展矩阵。 同时，为了保护扩展矩阵的位信息，Bob也使用一个随机矩阵来加密 $R^*$ 。 IKNP可以通过上述一致性检验来实现malicious security，而引入的额外开销很小。 Generalizing IKNPIKNP协议生成了许多二选一的OT instances，这里将介绍如何对IKNP协议一般化，以生成多选一的OT instances。 再次回顾一下IKNP的做法： Bob有一个选择串 $r$ 扩展为矩阵 $R$ ，矩阵的每行是全0或全1 再对矩阵R做秘密分享 从行的角度看矩阵扩展，把0 编码为 000... ，把1 编码为111... 。所以可以把矩阵扩展看作对 $r$ 的bit的一种编码方式（repetition code）。 因此，一般化IKNP的核心思想就是：用具有差错校正的编码方式对 $r$ 的bit进行编码。 Coding view of IKNP 现在，我们从编码的方式看IKNP。 Bob: Bob 有一个输入 $r$ 对 $r$ 的比特进行编码： 在这种编码方式下进行秘密分享 Alice 对每列做OT，得到矩阵$Q$ Bob得到的 $t_i$ 和Alice得到的 $q_i$ 有如下关系：$t_i = q_i\\oplus C(r_i)\\wedge s$ 如果是在编码$C(0)=000…, C(1)=111…$ 下： $r_i=0 \\Rightarrow t_i = q_i\\oplus (000…)\\wedge s=q_i$ $r_i=1 \\Rightarrow t_i=q_i\\oplus (111…)\\wedge s=q_i\\oplus s$ For every i: Bob knows $t_i$; Alice knows $q_i\\oplus C(0)\\wedge s$ and $q_i\\oplus C(1)\\wedge s$ 【此时Alice知道$q_i\\oplus C(0)\\wedge s$ 和 $q_i\\oplus C(1)\\wedge s$ ，Bob知道$t_i$ 】 从Bob的视角重写Alice得到的 $q_i=t_i\\oplus C(r_i)\\wedge s$ 当$C$是一种线性编码时，即满足: $[C(a)\\wedge s]\\oplus [C(b)\\wedge s]=C(a\\oplus b)\\wedge s$ and $C(0)\\wedge s = 000…$ 得到： 化简： Use random oracle to destroy correlations 【再破坏其线性相关性】 Generalizing IKNP刚刚我们考虑的是只对 $\\{0, 1\\}$ 编码，即： $C:\\{0, 1\\}\\rightarrow \\{0, 1\\}^k$ 编码。 现在考虑编码更多的bits，即 $C:\\{0, 1\\}^3\\rightarrow \\{0, 1\\}^k$ ，同样地，$C$是线性编码。 同样的运行IKNP： For every i: Alice can compute 8 (things) 【之前Alice是可以计算$q_i\\oplus C(0)\\wedge s$ 和 $q_i\\oplus C(1)\\wedge s$ 】 现在是对 $\\{0, 1\\}^3$ 编码，就可以计算： $q_i\\oplus C(000)\\wedge s, q_i\\oplus C(001)\\wedge s, \\cdots,q_i\\oplus C(111)\\wedge s$ 同样的，从Bob的角度重写Alice得到的值$q_i=t_i\\oplus C(r_i)\\wedge s$ 并且 $C$ 是线性编码，满足$[C(a)\\wedge s]\\oplus [C(b)\\wedge s]=C(a\\oplus b)\\wedge s$ and $C(0)\\wedge s = 000…$ ，化简后： 对上述Alice计算出的8个串，Bob只知道其中的一个 $t_i$ ，而这个 $t_i$ 等于Alice知道的第几个串，这取决于Bob的选择的 $r_i$ ，使得 $C(r_i\\oplus \\cdots)=000…$ 。 In the random oracle model: $H(t_1 \\oplus c_1 \\wedge s),…H(t_n \\oplus c_n \\wedge s)$ pseudorandom if all $c_i$ have Hamming weight $\\ge \\lambda$ 【在破坏线性相关性时，只有当所有 $c_i$ 的汉明重量（Hamming weight）$\\ge\\lambda$ 时，$H(\\cdot)$ 和随机串才具有不可区分性】 汉明重量（Hamming weight）:汉明重量是一串符号中非0符号的个数。 汉明距离（Hamming distance）：两个字符串对应位置的不同字符的个数。 就此，我们介绍了扩展的IKNP协议，[KolesnikovKumaresan13] 提到使用这样的编码： $C:\\{0, 1\\}^l\\rightarrow \\{0, 1\\}^k$ (编码字符串的汉明距离 $\\ge \\lambda$) 执行 $k$ 次base OT，就能实现从 $2^l$ 中选一个的OT扩展(1-out-of- $2^l$ OT extension)。 1-out-of-256 OT[KolesnikovKumaresan13] : Walsh-Hadamard code $C:\\{0, 1\\}^8\\rightarrow \\{0, 1\\}^k$ (min. dist. 128) 1-out-of- $2^{76}$ OT[OrruOrsiniScholl16] : BCH code $C:\\{0, 1\\}^{76}\\rightarrow \\{0, 1\\}^{512}$ (min. dist. 171) 1-out-of-∞ OT[KolesnikovKumaresanRosulekTrieu16] 这篇文章提出，对于任意的伪随机编码$C:\\{0, 1\\}^*\\rightarrow \\{0, 1\\}^{～480}$ ，只需要满足最小的hamming dist.，而不用要求编码 $C$ 的线性性质，就可以实现1-out-of-∞ OT。 就此，IKNP协议的介绍到此结束。也正是因为IKNP协议，使得OT的效率大幅提高。","link":"/2021/07/07/MPC3-OT/"},{"title":"「机器学习-李宏毅」：Regression","text":"在YouTube上看台大李宏毅老师的课，看完Regression讲座的感受就是： 好想去抓Pokemon！！！ 这篇文章将总结李宏毅老师Regression的讲座，并尝试实现其demo。 Regression（回归）DefineRegression：是找到一个$function$，进行预测。对输入的feature，输出一个$Scalar$(数值，标量)。 Example Application Look for a $function$ Stock Market Forecast（股票预测） $input$：过去的股价信息 $output$：明天的股价平均值（$Scalar$) Self-Driving Car(自动驾驶) $input$：路况信息 $output$：方向盘角度（$Scalar$) Recommendation（推荐系统） $input$：使用者A、商品B $output$：使用者A购买商品B的可能性 可见，$input$都是一些特征信息，$output$都是一个标量数值，这就是Regression。 Regression Case: Pokenmon 看完这节课，感想：好想去抓宝可梦QAQ 预测一个pokemon进化后的CP（Combat Power，战斗力）值。 为什么要预测呐？ 如果进化后的CP值高，就进化他，不然就把他当糖果，因为宝可梦很难抓的。（？没玩过，我也不懂o r z） 上图妙蛙种子的信息(可能的$input$)： $x_{cp}$：CP值 $x_s$:物种 $x_{hp}$:生命值 $x_w$:重量 $x_h$:高度 output：进化后的CP值。 $x_{cp}$：用下标表示一个object的component。 $x^1$：用上标表示一个完整的object。 Step 1: 找一个Model（function set）Model ：$y = b + w \\cdot x_{cp}$ 假设用上式作为我们的Model，那么这些函数： $ \\begin{aligned} &\\mathrm{f}_{1}: \\mathrm{y}=10.0+9.0 \\cdot \\mathrm{x}_{\\mathrm{cp}}\\\\ &f_{2}: y=9.8+9.2 \\cdot x_{c p}\\\\ &f_{3}: y=-0.8-1.2 \\cdot x_{c p} \\end{aligned} $ 等都属于这个集合，但是显然像$f_3$这种函数是bad，CP值不可能是负数。bad functions 很多，所以在下面的步骤，会说明如何判别一个函数的好坏，自动的选出最好的那个 $function$。 把Model 1一般化，得到线代中的 Linear Model：$y = b+\\sum w_ix_i$ $x_i$：x的feature $b$：bias,偏置值 $w_i$：weight，权重 Step 2: 判别Goodness of Function(Training Data)Training Data假定使用Model ：$y = b + w \\cdot x_{cp}$ Training Data：十只宝可梦，用向量的形式表示。 使用Training data来judge the goodness of function.。 Loss Function(损失函数)概率论：做线性回归，一般使用最小二乘法。一般回归，大多使用极大似然估计。 Loss function $L$ ：$L(f)=L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ 其中的 $\\hat{y}^n-(b+w\\cdot x_{cp}^n)$是Estimation error(估测误差) Loss Function的意义：它的 $input$是一个 $function$，它的 $output$体现了how bad it is,这个函数有多糟/好。 Figure the Result 上图横纵坐标是函数 $L$的参数 $w 、b$，图中的每一个point都是一个 $function $。 color：体现函数的输出，越红越大，说明选择的函数越bad。 所以我们要选择紫色区域结果最小的函数。 而这个得到best function的过程是可以通过无数次迭代实现的。（重复的迭代当时是交给计算机做了） Step 3:迭代找出Best Function$L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ 找到Best Function: $f^{*}=\\arg \\min _{f} L(f)$ 也就是找到参数 $w^{*},b^{*}=\\arg \\min_{w,b} L(w,b)=\\arg \\min_{w,b}\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ arg ：argument,变元 arg min：使之最小的变元 arg max：使之最大的变元 据悉，线性回归的参数可以用线性代数的知识，解出closed-form solution（解析解），我先挖个坑QAQ，以后来填这块知识。[1] 在机器学习中，只要$L$函数可微分， 即可用Gradient Descent（梯度下降）的方法来求解。 Gradient Decent（梯度下降）和概率论中的梯度下降估计参数的原理相同，只是计算机不能直接解出方程的解，所以计算机的方法是迭代。 考虑一个参数w*$w^*=\\arg \\min_w L(w)$ 步骤： 随机选取一个初始值 $w^0$ 计算 $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp; &nbsp; $\\begin{equation} w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{0}} \\end{equation}$ 计算 $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp;&nbsp; $\\begin{equation} w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{1}} \\end{equation}$ …until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$ [![3rgGsP.md.png](https://s2.ax1x.com/2020/02/28/3rgGsP.md.png)](https://imgchr.com/i/3rgGsP) **上图迭代过程的几点说明** - $\\begin{equation} \\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}} \\end{equation}$的正负 如果是negative，也就是该点切线斜率是负的，那应该Increse w，以找到最低点。 - Negative $\\rightarrow$ Increase w - Positive $\\rightarrow$ Decrease w - $-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{i}}$：步长 - $\\eta$：learning rate（学习速度），事先设好的值。 - $-$(负号)：如果 $\\begin{equation} \\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}} \\end{equation}$是负的，应该增加w。 - Local optimal：局部最优和全局最优 - 如果是以上图像，则得到的w不是全局最优。 - 但线性回归的损失函数是凸函数，存在一个全局最优，没有局部最优。 ### 考虑多个参数 $w^{*},b^{*}$ 微积分知识：gradient（梯度，向量)： $\\nabla L=\\left[\\begin{array}{l}\\frac{\\partial L}{\\partial w} \\\\frac{\\partial L}{\\partial b}\\end{array}\\right]$ 考虑多个参数和考虑一个参数思路相同，每次迭代，迭代两个参数。 $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ 步骤： 随机选取初值 $w^0,b^0$ 计算 $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ &nbsp; &nbsp; $w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0}} \\quad b^{1} \\leftarrow b^{0}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ 计算 $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ &nbsp; &nbsp; &nbsp; $w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1}} \\quad b^{2} \\leftarrow b^{1}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ …until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$, $ \\frac{{\\rm d}L}{{\\rm d}b}|_{b=b^n}=0$ 上图，坐标为 $L(w,b)$函数的参数，Color代表 $L$的大小，越紫值越小。 每一个点都是一个 $function$，沿着梯度方向（图中法线方向）迭代，找到全局最优点。 再次说明：线性回归中，损失函数是convex（凸函数），没有局部最优解。 $\\frac{\\partial L}{\\partial w}$和 $\\frac{\\partial L}{\\partial b}$的公式推导$L(w, b)=\\sum_{n=1}^{10}\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)^{2}$ 微积分的知识，显然。 数学真香。———我自己 $\\frac{\\partial L}{\\partial w}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)（-x_{cp}^n)$ $\\frac{\\partial L}{\\partial b}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)(-1)$ 实际结果分析Training Data Training Data的Error=31.9，但我们真正关心的是Testing Data的error。 Testing Data 是new Data：另外的Pokemon！。 Testing DataModel 1： $y = b+w\\cdot x_{cp}$ error = 35,比Training Data error更大。 Model 2：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2$ Testing error=18.4，比Model 1 好。 Model 3：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3$ Testing error=18.1，比Model 2好。 Model 4:$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4$ Testing error =28.8,比Model3更差。 Model 5：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4+w_5\\cdot (x_{cp})^5$ Testing error = 232.1,爆炸了一样的差。 Overfiting（过拟合了）从上面5个Model中可以得出，越复杂的函数模型，在Testing data上不一定能得到更好的结果。（过拟合使Training data 的误差越来越小） 所以在选择Model时，需要选择合适的Model。 对模型进行改进如果收集更多的Training Data，可以发现他好像不是一个Linear Model。 Back to step 1:Redesigh the Model从上面那张图，感觉他不是一个Linear Model,而是需要if 是伊布，模型是…，if 是…,可见是和物种有关系。 （很抱歉，我只认识右上角时伊布，QAQ，我也说不出名字） 但用 $\\delta$(微积分学的狄拉克函数)表示条件语句，可以发现，他仍然是一个线性模型。 $\\delta(x_s= \\text{Pidgey)}\\left\\{\\begin{array}{ll}=1 & \\text { If } x_{s}=\\text { Pidgey } \\\\ =0 & \\text { otherwise }\\end{array}\\right.$ $y = b_1\\cdot \\delta_1+w_1\\cdot \\delta_1+b2\\cdot \\delta_2+w_2\\cdot \\delta_2+…$是一个linear model。 拟合出来，Training Data 和Testing Data的error都蛮小的。 如果想让拟合误差更小，还可以考虑其他的feature，重量、高度、HP等。 但同样的，如果函数过于复杂，也会出现Overfitting的情况。 Back to Step 2:Regularization（正则化）对于Linear Model :$y = b+\\sum w_i x_i$ 为什么要正则化？我们希望得到的函数是较平滑的，这样测试时，函数的输出对输入的noise不sensitive，即输入x的细微扰动，并不太会影响输出的结果。 所以当参数越接近0，函数越平滑。因此在原本的loss function后加入 $\\lambda \\sum(w_i)^2$项（ $\\lambda$需手调），可以保证函数较平滑。 正则化： $L = \\sum_n(\\hat{y}^n-(b+\\sum w_i x_i))^2 + \\lambda\\sum(w_i)^2$ $\\lambda $大小的选择 可以得出结论： $\\lambda $越大，Training Error变大了。 当 $\\lambda$更大，损失函数更考虑w参数的取值，更关心函数的平滑程度，而更少的关心拟合的error。 $\\lambda $越大，Testing Error变小了，当 $\\lambda$过大时，又变大。 $\\lambda $较小时，$\\lambda $增大，函数更平滑，能良好适应数据的扰动。 $\\lambda $较大时，函数过于平滑，宛如直线，这显然不能准确预测。 因此，在调节$\\lambda $大小时，也要适当选择。 正则化的一个注意点在regularization中，我们只考虑了w参数，没有考虑bias偏置值参数。 因为正则化是寻找较平滑拟合，而偏置参数只是让函数平移，与平滑无关。 Again：Regularization不考虑bias Fllowing Gradient descent[2] Overfitting and regularization[3] Validation[4] 由于博主也是在学习阶段，学习后，会po上下面内容的链接。 希望能在学习、写博客的过程中，锻炼自己的表达能力，尽量让文风言简意赅又科学严谨。 写博客是为了记录与分享，感谢指正。 Reference[1] “周志华西瓜书p55,待补充” [2] [3] [4]","link":"/2020/02/29/Regression/"},{"title":"「Paper Reading」：AFL++：Combine Incremental Steps of Fuzzing Research","text":"今天给大家分享的论文是一篇基于AFL的工作：AFL++：Combine Incremental Steps of Fuzzing Research，发表在USENIX Workshop。分享时的slides 目录 Introduction State-of-the-Art AFL Smart Scheduling AFLFast MOpt Bypassing Roadblocks LAF-Intel RedQueen Mutate Structured Inputs: AFLSmart AFL++ Evaluation IntroductionFuzzing已经成为软件测试强有力的工具，通过大量的非预期的输入，检测软件的运行状态，以发现软件的隐藏性漏洞。 而AFL是最具盛名的fuzzing工具，基于AFL的研究也层出不穷，但这些技术往往是正交式、独立地发展。 因此： 将这些前沿新颖的fuzzing技术结合起来是一件很困难的事情 而评估这些不同维度的fuzzing技术也是一件很困难的事 AFL++这个工作就致力于解决这个问题： AFL++提供了一个可用的fuzzing工具，结合了许多前沿fuzzing技术 AFL++还提供了一种新颖的可自定义的变异API（Custom Mutator API），研究人员能够轻松将自己设计的Mutator应用到AFL++上，或和其他Mutators结合起来。 这篇论文还评估了许多结合起来的fuzzing技术组合，评估结果体现了fuzzing技术的target-dependency. State-of-the-Art在正式介绍AFL++ 之前，很有必要介绍一下afl++结合的其他fuzzing技术。 除了AFL的主要特点，afl++主要结合了三方面的技术，分别是： 调度上的。包括AFLFast的种子调度和MOpt的变异调度。 绕过fuzzing中的一些roadblocks，包括LAF- Intel和Red Queen。 针对一些复杂的结构化输入的变异。 AFL首先介绍一下AFL的工作原理和主要特征。 AFL是一种基于覆盖率反馈（coverage- guided）的fuzz工具。 AFL的工作流程如图所示： 编译时对源码进行插桩，以记录代码覆盖率（Code Coverage） 初始化一些输入文件，加入到输入队列（queue） 在队列中按照一定策略选择种子（seed），并进行大量的突变（mutation），得到大量的变异文件 如果该变异文件触发了新的执行路径，则将其保存下来，加入到队列中。 goto 2 上述过程会一直进行下去，其中触发的crash会被记录下来，以便后面分析程序漏洞。 Coverage Guided FeedbackAFL是一种使用边覆盖率作为反馈的灰盒fuzzer。 什么是边覆盖率呢？ 在介绍边覆盖率之前，先介绍一下块覆盖率： 如上图，将一个程序划分为一个一个的程序块，一个程序块中的指令要么都执行，要么都不执行。 将上述程序拖到IDA中，得到下图，因此可以用程序块之间的跳转表示边。所以 A -&gt; B -&gt; C -&gt; D -&gt; E (tuples: AB, BC, CD, DE) A -&gt; B -&gt; D -&gt; C -&gt; E (tuples: AB, BD, DC, CE) 这两条执行路径的覆盖率不同。 实现上，就是给每一个块分配一个编译随机值，通过上一个块和当前块的运算的值来表示该边，再进行统计。 MutationAFL中的Mutation主要分为两种，一种是deterministic stage，从下表的变异中选择一个mutation进行连续变异。 而havoc stage 则是每次选择一堆变异，工作作用在seed上。 Fork Server由于AFL会大量运行目标程序，因此为了减少fuzzer的执行开销，AFL使用了一种forkserver的技术。 forkserver的工作原理如下图所示: 当fuzzer在执行目标程序前，会先执行fork()命令，得到一个子进程。 子进程再通过execv()指令执行目标程序，而execv()命令有一个很特别的地方，他会用这个目标程序的映像覆盖掉子进程的映像。 因此，此时的fuzzer fork()出的子进程就变成了插过桩的目标程序，也就是forkserver。 The exec() family of functions replaces the current process image with a new process image. 当fuzzer想要执行目标程序时，就和forkserver通信，将fork目标程序的任务交给forkserver，这样就极大的提高了fuzzer的执行效率。 Persistent ModeAFL另一个提高效率的功能是Persistent Mode。 使用Persistent Mode，只需要对程序做一点小小的改动，即对程序patch一个循环，就像这样： Persistent Mode允许单个进程重复输入，极大减少了fork的开销。 Smart SchedulingAFLFast基于AFL的另一个改进领域是调度问题，一个是种子调度，如何选种子的问题；另一个是变异调度，如何选mutation的问题。 AFLFast主要是从种子调度的方向改进AFL。 AFLFast观察到种子大多数生成的输入都经历了相同的高频路径，因此想要设计一些策略能够focus on一些低频路径。 因此，AFLFast设计了两种策略： Search Strategy：关于在队列中如何选种子的问题，决定种子挑选顺序。 Power Schedule：关于选出的种子可以被fuzz多少次，即可以生成多少变异文件，而这个数量被定义为种子的energy。 seed’s energy: the amount of generated inputs from each seed AFL其实在AFL中也有相应的策略，我们先来看AFL中的种子策略是这样的： 如何挑选种子： 在AFL中，update_bitmap_score 函数中维护了一个变量fav_factor ，这个值越小意味值种子越favored，而这个值其实是由程序的执行时间(exec_us)和种子的长度(len)决定的。 如何给种子分配energy： 在AFL中，calculate_score 函数中维护了一个变量perf_score ，这个值越大意味着会给种子分配更多的energy，这个种子就有更多的机会被fuzz，而这个值主要由执行时间(exec_us)、程序的执行路(bitmap_size)、发现该种子的困难度(handicap)以及种子的深度(depth)共同决定的。 其中handicap，困难度，可以这样来理解，这个值越大，说明发现这个种子经过了很长轮数，来之不易，所以希望能更focus on在这些来之不易的种子上。 AFLFast而AFLFast中，不管是决定如何挑选种子，还是觉得如何给种子分配energy，AFLFast都还考虑了另外两个变量。 对每个种子，定义两个变量： 一个是f(i) ，表示该种子被fuzz的总次数，也叫做频率。 另一个是s(i) ，表示该种子在队列挑选中，被挑选了多少次。 AFLFast-Power Schedule这里主要介绍一下AFLFast的Power Schedule： AFLFast提供了6中Power Schedule： 定义p(i)为分配的energy。 EXPLOIT：p(i) = AFL EXPLOIT模式下的power schedule，就是之前提到的AFL的原生策略。 EXPLORE：p(i) = AFL / const EXPLORE模式下，对AFL中计算出的energy除以了一个常数。 看这样的一个例子： 程序需要依次匹配到这些字符，才可以找到crash。 如果规定每个种子的energy是一个常数，即p = $2^{16}$ ，那总共分配 $2^{18}$ 的energy才能找到crash。 但如果把这个种子的转移过程用马尔可夫链建模，可以发现如果从b*** 转移到ba** ，fuzz的转移概率为 $2^{-10}$ （从4个字符中选择一个字节，每个字节有 $2^{8}$ 中情况） 因此，可以得到从i 种子转移到j 种子需要的energy的期望是 $E[X_{ij}]=\\frac{1}{p_{ij}}$ 那么找到bad! 状态，所需要的总energy的期望和为 : $E[X_{01}]+E[X_{12}]+E[X_{23}]+E[X_{34}]=4 \\cdot 2^{10}=4k$ 因此，我们总是分配所需期望energy更多的值，所以在AFLFast模式，会对energy除以一个常数。 剩下这四种策略都是从不同的方式抑制高频边被fuzz。 MOpt与种子调度相对的是变异调度，MOpt这个工作就是从变异调度的角度提升fuzz的效率。 MOpt工作的主要贡献有： 首先是观察到：很多有效的变异如bitflip，执行的时间却很少。 论文中统计了在deterministic stage不同变异产生的interesting test cases的数量，发现bitlip表现优异。 但从变异执行时间的角度，发现这些变异效率高的变异，执行时间比较短。 所以MOpt的motivation是：希望能花更多的时间在那些变异效率高的变异上。 PSOMOpt使用粒子群优化算法来对问题建模。 定义粒子（particle），即变异（mutation）。粒子的位置，就是该变异被选择的概率。 每一个粒子群（swarm），则是所有mutations的概率分布。 而MOpt与原始PSO算法不同，MOpt使用的是多个群（multiple swarms），每一个群都是一个概率分布。 在MOpt的工作流程中，主要有两个核心模块。 一个是Pilot模块，另一个是Core模块。 Pilot模块：评估每一个粒子群，也就是该mutations的概率分布的fuzz效率。 Core模块：使用Pilot模块评估出的效率最高的mutation策略 来fuzz。 Bypassing Roadblocksfuzzing中有时会遇到一些roadblocks。 LAF- IntelLAF-Intel解决的fuzzing中遇到的一些困难比较语句，如下图： 即使当输入为0xabad1dee时，已经非常接近正确答案了，fuzzer也会认为他是错误的。 因此LAF- Intel的思路是，把这些比较难的、比较一连串字符的比较划分为 多个单字节的比较。 这样可以让程序块的划分粒度更细，当你每匹配到一个字节时，就被认为是interesting，被保存到队列中，以后可以继续fuzz，这样，fuzzer就可以一步一步的解决这个roadblock。 另外，LAF-Intel是基于LLVM架构的，所以LAF- Intel实现了三种Pass： The split-compares-pass：划分为单字节比较，并全部转换为&lt;, &gt;, ==, !=和无符号数的比较。 The compare-transform-pass：重写了strcmp和memcmp，将其全部转换为单字节比较。 The split-switches-pass：将switch比较转换为单字节比较的if串。 RedQueenRedQueen解决的roadblocks： magic number：和上文LAF-Intel解决的roadblocks类似。 nested checksum：而校验和/嵌套校验和的情况就像下图所示： 代码如下图所绘： RedQueen这篇工作的贡献是： 首先他观察到种子的输入，有时是和程序的运行状态直接相关的，这种关联定义为Input-to-State联系。 比如下图： hook住cmp指令，运行时，观察到eax的值为VALU ，与之比较的值为ABCD（都是小端序）。 而VALU在输入中也有出现，所以这里观察到的VALU大概率就是输入的VALU，如果能将输入的VALU换成ABCD，就有较大可能绕过这个roadblock。 RedQueen就是利用这样的Input-to-State的关系来解决这些roadblocks。 Magic Bytes解决Magic Bytes的方法就是上文提到的那样，希望能找到一系列的可替换对。 具体流程为： 跟踪：将所有的cmp指令hook住，尝试运行一下，把指令比较的操作数都提取出来。 变化：对比较指令的操作数做变异操作，比如加一或减一的操作，因为从该条指令并不能得到源码中的比较关系，源码的比较逻辑可能是大于、小于等。 编码：对得到的替换对进行编码操作，如小端序、hex、base-64等，像刚刚的例子就是小端序的编码。 应用：将得到的这些替换对&lt; pattern -&gt; repl &gt;应用到输入中，即在输入中找pattern，替换为repl，试运行。 在执行上述流程之前，RedQueen执行了一个操作，该操作极大提高了绕过的效率。 如果替换对为(0x00, 0x04)，并且输入文件像下面左图这样： 输入文件中出现大量的0x00，就像产生了碰撞一样，其实很多位置并不和那条程序指令相关，这样就会花费大量时间。 如果输入是像上图右边这样的，比较colorful，那RedQueen的效率就会很高。 所以RedQueen在进行绕过之前，会对输入做染色（Colorization）的操作，在保证种子执行路径不变的情况下，增大输入的熵值。 Nested Checksum而对checksum的绕过，这其实是一件很难的事情，因此RedQueen会选择先忽略掉这些困难，后面再来修正。 具体操作： 对输入进行染色 根据指定条件，识别这些像checksum比较的指令，hook住。 然后就用cmp al, al patch原程序，这样就让这些checksum的判断一定为正。 但这样的patch就会带来false positive，即这些输入的执行路径可能并不是这样的。 RedQueen就会在之后进行输入验证，并修复他们。 在fix阶段，其实就是用magic bytes的方法，对checksum的位置进行替换。 不过如果对于嵌套的校验和指令，就需要按照拓扑序（Topological Sort），一个一个的fix。 Mutate Structure Inputs:AFLSmart对于输入复杂的、结构性强的程序，fuzzer通常会生成大量的无效输入。 因此AFLSmart将AFL和Peach结合起来，AFLSmart的输入为Peach pits格式，一种xml文件。 AFLSmart 将种子都表示为Peach pits格式，这样，就可以基于这些块进行变异，而不需要基于比特级的 变异。 AFL++AFL++就将上述提到的诸多fuzzing技术都结合在一起，并提供了一种可供扩展的API。 Seed SchedulingAFL++的Seed Scheduling就是基于AFLFast的种子调度。 AFL++的Power Schedules除了AFLFast提到的6种，还有另外两种，Mmopt和Rare。 Mmopt主要关注那些最新发现的种子 而Rare主要关注那些具有罕见边的种子。 MutatorsAFL++集成了许多mutator，包括RedQueen的Input-to-State mutator，包括Mopt mutator。 因此，AFL++就像一个框架，提供了一个自定义的mutator接口规范，实现这些接口，就可以将自己的mutator缝合到AFL++上，或者将不同的mutator缝合在一起。 AFL++除了提供了mutator的接口规范，还提供了trimming的借口规范。 Input-To-State Mutator这个mutator是基于RedQueen的input-to-state. 这里主要介绍他和RedQueen不同的地方： Colorization RedQueen在染色时是保持程序执行路径不变，即hash of bitmap不变。 而AFL++除了保持程序的执行路径不变，还对程序的执行时间做了一定控制，规定了程序的执行时间下界为2x slowdown Bypass Comparison AFL++采用的是一种probabilistic fuzzing。即如果这个roadblock，使用替换的方法，或者修复的方法失败了，那fuzzer下一次就会以较小概率尝试绕过他。（当下解决不了的困难，先放一放zzzz） 不过其实RedQueen中也有相应的设计，RedQueen是使用的虚拟机断点hook cmp指令。因此，如果这个断电被hit的次数比较少，就将这个断点去掉。 CmpLog Instrumentation RedQueen中采用的是虚拟机断点hook的cmp指令，当断点被hit时，再提取指令操作数。 而AFL++则是使用的一种共享表，每一个指令都记录他的前256次执行的操作数。 MOptAFL++中也缝合了MOpt的Pilot和Core模块。 并且可以和Input-to-State结合。 Instrumentation在插桩上，AFL++首先解决了边hit count溢出的问题。 因为在AFL中，边只会记录到255。 有两种方法可以解决： NeverZero，加一个进位标志。NeverZero可以提高fuzzer的表现性能。 Saturated Counters：当计数超过255时，就停在255。这个做法，不推荐，反而会让fuzzer的性能变差。 除了使用NeverZero，AFL++还使用了Ngram优化边覆盖率的统计。 AFL原生的统计边覆盖率的代码是： 123cur_location = &lt;COMPILE_TIME_RANDOM&gt;; shared_mem[cur_location ^ prev_location]++; prev_location = cur_location &gt;&gt; 1; AFL只考虑了上一个基本块和当前基本块，这样的计算速度更快，但会带来更多的碰撞。 而Ngram则是考虑当前基本块和前N-1个基本块表示该边，这样能部分碰撞，实验结果也表明Ngram能提高实验的性能。 AFL++实现了多种后端的插桩，具体实现的区别如下： Evaluation略，具体可见slides 。 Reference AFL：https://afl-1.readthedocs.io/en/latest/ AFLFast: https://mboehme.github.io/paper/CCS16.pdf https://github.com/mboehme/aflfast RedQueen:https://react-h2020.eu/m/filer_public/6d/86/6d869f98-f544-49cc-8221-b380c593888f/ndss19-redqueen.pdf https://hexgolems.com/talks/redqueen.pdf MOpt:https://www.usenix.org/system/files/sec19-lyu.pdf AFLSmart: https://thuanpv.github.io/publications/TSE19_aflsmart.pdf AFL++:https://aflplus.plus/papers/ https://github.com/AFLplusplus/AFLplusplus","link":"/2021/04/09/aflpp/"},{"title":"「Math」:Entropy, Cross-Entropy and DL-Divergence","text":"在机器学习中，常用cross-entropy来作为模型的损失函数，这篇文章将阐述信息学中的entropy（熵）是什么，cross-entropy（交叉熵）又是什么，KL-Divergence和entropy、cross-entropy的关系是什么？ 如何具象的理解这些概念？ 在开始阅读这篇文章之前，先提及一下香农对bit的定义，香农认为bit是用来消除信息的不确定性的。 bit：uncertainty divided by 2. 原视频 讲的很好，本文只是在此基础上对一些总结，方便理解物质化（马原.jpg）。 公式总概bit：用来消除信息的不确定性 Entropy（熵）： $H(p)=-\\sum_i p_i\\log(p_i)$ 度量概率分布的平均信息量（即不确定性）。值越大，不确定性越大。 Cross-Entropy（交叉熵）： $H(p,q)=-\\sum_i p_i\\log(q_i)$ 度量两个分布的相似程度（一般 $p$ 为真实分布，$q$为预测分布），值越大，两个分布越不相似。 KL-Divergence（KL散度，也叫相对熵） ：$D_{KL}(p|q)=H(p,q)-H(q)$ 度量交叉熵超过熵的那一部分。 Entropy-熵实例1： sunny和rainy的发生的概率都是0.5，天气预报预测明天的天气为sunny，将sunny消息发给用户。 该条消息不管多长，有用的信息其实只有1个比特，即uncertainty divided by 2. 实例2： 有八种不同的天气，发生的概率相同，当天气预报将预测消息发送给用户时。 该条消息能使得uncertainty divided By 8.即有用信息为3个比特。 实例3: sunny发生的概率为0.75，rainy的概率为0.25，如果天气预测明天的天气： 将这个例子理解为抽球游戏，盒子里有3个红球（表示sunny天气），1个白球（rainy天气）。 事件 $X$ 表示为在盒子里抽中球的颜色，可得知抽中红球的概率为0.75，抽中白球的概率是0.25。 抽中哪个球是不确定的，即uncertainty 如果原来是4，即不知道将抽中这四个球中的哪一个。 如果抽中白球，那该信息表示：就是那4个球中的唯一一个白球，uncertainty 从原来的4变为1，即 uncertainty divided by 4.表示该信息，需要有用比特， $\\log_2(4)=\\log_2(1/0.25)=2$ 个比特来表示。即抽打白球的情况的不确定性更大，需要更多的比特来消除不确定性，来表示白球的发生。所以该条信息中只有2个比特是useful information. 如果抽中红球，该信息表示为：是那3个红球中的一个，uncertainty 从原来的4变为3 （如果和抽中白球的情况统一，最后的确定发生的uncertainty都表示为1，即在没有抽之前，抽到红球的uncertainty为 $1/0.75=4/3$ ） 即uncertainty divided by 4/3.表示该信息需要有用 $\\log_2(4/3)=\\log_2(1/0.75)=-\\log_2(0.75)=0.41$ 比特来表示。即抽到红球的情况不确定性没有那么大，只需要较少比特即可消除不确定性，来表示红球的发生。所以该条信息中只有0.41个比特是useful information. 这里也可以看出，如果一个事件的发生的概率越小（越不可能发生），即对该事件发生的不确定性越大，但一旦发生了，所携带的信息量就会很大，因为需要用更多的比特来消除不确定性。 回到本例子： 如果预测天气为rainy，将预测消息发给用户，则该条消息包含2比特（$\\log_2(1/0.25)=-\\log_2(0.25)=2$）的有用信息，即对rainy天气发送的不确定性更大，需要更多的比特来消除不确定性。 如果预测天气为sunny，因为在预测之前，用户对sunny发生的可能性就没有那么大，因此只需要0.41比特（$\\log_2(1/0.75)=-\\log_2(0.75)$)来消除不确定性。 那平均下来，气象局发送的平均信息量为 $0.75\\times 0.41+0.25\\times2=0.81$ bits. 因此我们用 $\\log_2(1/p)=-\\log_2p$ 来表示事件发生时所携带的信息量。（或者说需要这么多信息量来消除事件发生的不确定性） 用 $-\\sum_i{p_i}\\log_2{p_i}$ 来表示该事件的平均信息量（概率分布的不确定性），这就是信息熵（Entropy）。 Entropy：$$H(p)=-\\sum_i pi\\log_2(p_i)$$熵越大，说明携带的平均信息量越多，即不确定性越强，需要越多的比特来消除不确定性。所以熵是用来衡量不确定性的量。 和化学中衡量混乱程度的熵，是类似的。 Cross-Entropy-交叉熵例1： 从上面的实例2来看，即8中天气发生概率相同，对天气表示进行信息编码，为下图： entropy为3bits，而cross-entropy（交叉熵），也就是消息（比特流）的平均长度，为3bits. 例2： 但如果8种天气发生的可能性为下图： 算出来的entropy为2.23bits，即平均信息量为2.23bits。 如果仍是用这样的编码，cross-entropy为3bits，就多出一些冗余信息量。 例3: 如果换一种编码方式： 算出来的cross-entropy为 $0.35 \\times2+0.35\\times2+0.1\\times3+…+0.01\\times5=2.42$ bits，就非常接近entropy=2.23bits。 说明这种编码方式冗余量很小，非常接近真实的概率分布所包含的平均信息。 例4: 如果天气的概率分布变为下图，entropy不变仍然为2.23bits： 那么算出来的cross-entropy为 $0.01\\times2+0.01\\times 2+0.04\\times3+…+0.35\\times5=4.58$ bits，远大于entropy的值。说明这种编码方式冗余量很大。 换一种角度看例4，把信息编码认为是预测的概率分布，例4点编码表示的分布如下： Cross-Entropy：$$H(p,q)=-\\sum_i p_i\\log(q_i)$$所以cross-entropy可以理解为信息/比特流的平均长度。 如果预测的概率分布非常接近真实的概率分布，那比特流的平均长度也会非常接近原分布的平均信息量。 如果预测的概率分布 $q$ 和真实分布 $p$ 完全一样，那么cross-entropy等于entropy。 所以cross-entropy可以用来衡量两个概率分布的相似程度。 用在机器学习中用作评判模型好坏的损失函数，度量模型预测分布和真实分布的相似程度。 KL-Divergence-KL散度而如果预测的概率分布和真实分布不同，那么cross-entropy的值就会大于entropy的值，超过的部分就叫做relative entropy（相对熵），也就是KL-Divergence（Kullback-Leibler Divergence，KL散度） 即可以得到等式：$\\text{Cross-Entropy = Entropy+KL-Divergence}$ 则KL-Divergence：$$\\begin{align}D_{KL}(p|q)=H(p,q)-H(p) &amp;= -\\sum_i p_i\\ln q_i - \\sum_i p_i\\ln p_i \\&amp;= -\\sum_i p_i \\ln \\frac{p_i}{q_i} \\&amp;= \\sum_i p_i \\ln \\frac{q_i}{p_i}\\end{align}$$ Reference 视频链接：https://www.youtube.com/watch?v=ErfnhcEV1O8","link":"/2021/01/16/entropy-and-more/"},{"title":"「机器学习-李宏毅」：Error","text":"这篇文章叙述了进行regression时，where dose the error come from?这篇文章除了解释了error为什么来自bias和variance，还给出了当error产生时应该怎么办？如何让模型在实践应用中也能表现地和测试时几乎一样的好？ Error在中的2.4节，我们比较了不同的Model。下图为不同Model下，testing data error的变化。 可以发现，随着模型越来越复杂，testing data的error变小一些后，爆炸增大。 越复杂的模型在testing data上不一定能得到好的performance。 所以，where dose the error come from? ：bias and variance Bias and Variance of Estimator用打靶作比，如果你的准心，没有对准靶心，那打出的很多发子弹的中心应该离靶心有一段距离，这就是bias。 但把准心对准靶心，你也不一定能打中靶心，可能会有风速等一系列原因，让子弹落在靶心周围，这就是variance。 上图中，可以直观体现出bias 和 variance的影响。 概率论中 ： 一个通过样本值得到了估计量，有三个评判准则：无偏性、有效性和相和性。 这里的无偏性的偏也就是bias。 概率论中定义：设 $\\hat{\\theta}(X_1,X_2,…,X_n)$ 是未知参数 $\\theta$ 的估计量，若 $E(\\hat{\\theta})=\\theta$ ，则称 $\\hat{\\theta}$ 是 $\\theta$ 的无偏估计。 变量 $x$ ，假设他的期望是 $\\mu$ ，他的方差是 $\\sigma^2$. 对于样本： $x^1,x^2,…,x^N$ ，估计他的期望和方差。 概率论的知识： $m=\\frac{1}{N} \\sum_{n} x^{n} \\quad s^{2}=\\frac{1}{N} \\sum_{n}\\left(x^{n}-m\\right)^{2}$ $E(m)=\\mu$ ，所以用 $m$ 是 $\\mu$ 的无偏估计。(unbiased) 但是 $E\\left[s^{2}\\right]=\\frac{N-1}{N} \\sigma^{2} \\quad \\neq \\sigma^{2}$ ，所以这样的估计是有偏差的。(biased) 因此统计学中用样本估算总体方差都进行了修正。 而在机器学习中，Bias和Variance通常与模型相关。 上图中，假设黑色的线是 true function，红色的线是训练得到的函数，蓝色的线是，训练函数的平均函数。 可见，随着函数模型越来越复杂，bias在变小，但variance也在增大。 右下角图中，红色的线接近铺满了，variance已经很大了，模型过拟合了。 对机器学习中模型对bias影响的直观解释 左图的model简单，右图的model复杂。 简单的model，包含的函数集较小，可能集合圈根本没有包括target（true function），因此在这个model下，无论怎么训练，得到的函数都有 large bias。 而右图中，因为函数非常复杂，所以大概率包含了target，因此训练出的函数可能variacne很大，但有 small bias。 what to do with large bias/variance 上图中，红色的线表示bias的误差，绿色的线表示variance的误差，蓝色的线表示观测的误差。 当模型过于简单时：来自bias的误差会较大，来自vaiance的误差较小，也就是 Large Bias Small Variance 当模型过雨复杂时：来自bias的误差会较小，来自variance的误差会很大，也就是 Small Bias Large Variance 2 case : Underfitting ：If your model cannot even fit the training examples, then you have large bias. Overfitting : If you can fit the traning data, but large error on testing data , then you probably have large variance. With Large BiasFor bias, redesign your model. Add more features as input. A more complex model. 考虑更多的feature；使用稍微复杂些的模型。 With Large Variance More data Regularization (在这篇2.5.2文章中有叙述什么是regularization) Model Selection There is usually a trade-off beween bias and variance. Select a model that balances two kinds of error to minimize total error. 选择模型需要在bias和variance中平衡，尽量使得总error最小。 What you should NOT do: 以上，描述的是这样的一个情形：在traning data中，得到了三个自认不错的模型，kaggle的公开的testing data测试，分别得到三个模型的error，认为第三个模型最好！ 但是，当把kaggle用private的testing data 进行测试时，error肯定是大于0.5的，最好的model也不一定是第三个。 同理，当把我们训练出的model拿来实际应用时，可能会发现情况很糟，并且，这个model可能选的是测试中最好的，但在应用中并不是最好的。 Cross Validation什么是Cross Validation(交叉验证)？ 在机器学习中，就是下图过程： 把Traning Set 分成两个部分：Training Set和Validation Set。 在Training Set部分选出模型。 用Validation Set来判断哪个模型好：计算模型在Validate Set的error。 再用模型预测Testing Set(public)，得到的error一定是比Validation Set中大的。 Not recommend : Not用public testing data的误差结果去调整你的模型。 这样会让模型在public的performance比private的好。 但模型在private testing data的performance才是我们真正关注的。 那么当模型预测private testing set时（投入应用时），能尽最大可能的保证模型和在预测public testing data相近。 N-fold Cross ValidationN-fold Cross Validation（N-折交叉验证）的过程如下： 把Training Set 分为3（3-fold）份，每一次拿其中一份当Validation Set，另外两份当作Training Set。 每一次用Train Set来训练。得到了三个Model。 要判断哪一个Model好？ 每一个Model都计算出不同Validation Set的error。 得到一个Average Error。 最后选这个average error最小的model。 最后应用在public traning set，来评估模型应用在private training set的performance。","link":"/2020/03/15/error/"},{"title":"「Web」:HTML and CSS","text":"温故知新：对Web基础知识——HTML和CSS的持续更新。 说在前面B/S 软件结构C/S： Client Server（JavaSE） B/S：Browser Server（JavaEE） 前端开发流程 美术实现：网页设计 前端工程师：设计为静态网页 Java程序员：后端工程师修改为动态页面 网页端组成部分内容：页面中可以看到的数据。一般使用html技术。 表现：内容在页面上的展示形式。一般使用CSS。 行为：页面中的元素与输入设备交互。一般使用javascript技术。 HTML创建HTML文件 创建一个Web静态工程 在工程下创建html页面 12345678910&lt;!DOCTYPE html&gt;&lt;!--声明--&gt;&lt;html lang=&quot;zh_CN&quot;&gt;&lt;!--html中包含两部分：head和body--&gt;&lt;head&gt;&lt;!--head中包含：title标签、CSS样式、js代码--&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;Hello World!&lt;/body&gt;&lt;/html&gt; HTML标签 标签名大小写不敏感 标签有自己的属性 基本属性：修改简单样式 事件属性：设置事件响应后的代码 标签分为单标签&lt;标签/&gt;和双标签&lt;标签&gt;&lt;/标签&gt; 标签的属性必须要有值，属性值加双引号。 显示特殊标签：&lt; &gt; 空格等等，建议查阅文档。 字体标签12345&lt;body&gt; &lt;font color=&quot;red&quot; size=&quot;7&quot;&gt; 哒哒哒。 &lt;/font&gt;&lt;/body&gt; 标题标签：h1 到 h612&lt;h1 align=&quot;center&quot;&gt;标题1&lt;/h1&gt;&lt;h2 align=&quot;left&quot;&gt;标题2&lt;/h2&gt;&lt;!--align：显示位置,默认左--&gt; 超链接123&lt;a href=&quot;https://baidu.com&quot; target=&quot;_self&quot;&gt;百度&lt;/a&gt;&lt;!--_self属性：当前窗口跳转--&gt;&lt;br/&gt;&lt;a href=&quot;https://baidu.com&quot; target=&quot;_blank&quot;&gt;百度&lt;/a&gt;&lt;!--_blank属性：打开新窗口跳转--&gt; 列表标签12345678910111213&lt;ul type=&quot;none&quot;&gt;&lt;!--无序列表--&gt;&lt;!--type属性可以更改列表前的符号--&gt; &lt;li&gt;百度&lt;/li&gt; &lt;li&gt;百度&lt;/li&gt; &lt;li&gt;百度&lt;/li&gt; &lt;li&gt;百度&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;!--有序表格--&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt;&lt;/ol&gt; img标签 属性src:图片等路径位置 JavaSE中路径 相对路径：从工程名字开始算 绝对路径：硬盘中的路径 Web中的路径 相对路径： . ：表示当前文件所在的目录 .. ：表示当前文件所在的上级目录 文件名：表示当前文件所在目录的文件，相当于./文件名 绝对路径：http://ip:port/工程名/资源路径 属性：weight; height; border：设置图片边框大小。 alt：当指定路径找不到图片时，用来代替显示的文本内容。 表格标签：实现跨行跨列1234567891011121314151617181920212223&lt;table border=&quot;1&quot; width=&quot;300&quot;&gt;&lt;!--表格标签--&gt; &lt;!--border：设置边框、width：设置宽度、height：设置高度--&gt; &lt;!--align：设置表格对齐方式--&gt; &lt;!--cellspacing:单元格间距--&gt; &lt;tr&gt;&lt;!--行标签--&gt; &lt;th&gt;h1&lt;/th&gt;&lt;!--表头标签--&gt; &lt;th&gt;h2&lt;/th&gt; &lt;th&gt;h3&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt;&lt;!--单元格标签--&gt; &lt;td align=&quot;center&quot;&gt;1.2&lt;/td&gt; &lt;!--align：设置单元格文本对齐方式--&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;2.1&lt;/td&gt;&lt;!--colspan:列的宽度,实现单元格跨列--&gt; &lt;td rowspan=&quot;2&quot;&gt;2.2&lt;/td&gt;&lt;!--rowspan:行的宽度，实现单元格跨行--&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;3.1&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; iframe框架标签可以在html页面上开辟一个小区域加载单独的页面，实现内嵌窗口。 12345&lt;iframe src=&quot;hello.html&quot; width=&quot;400&quot; height=&quot;600&quot; name=&quot;abc&quot;&gt;&lt;/iframe&gt;&lt;!--name：表示该区域的名字--&gt;&lt;a href=&quot;welcome.html&quot; target=&quot;abc&quot;&gt;欢迎&lt;/a&gt;&lt;!--target：打开窗口显示的位置--&gt;&lt;!--a标签的target属性设置为iframe的name属性，就在开辟的区域打开链接窗口--&gt; 表单标签表单：html中用来收集用户信息的元素集合，将这些信息发送给服务器处理。 1234567891011121314151617181920212223242526272829303132333435&lt;form&gt;&lt;!--表单标签--&gt; 用户名称：&lt;input type=&quot;text&quot; value=&quot;User&quot;/&gt;&lt;br/&gt;&lt;!--input输入框标签--&gt; &lt;!--type：输入类型 value：默认值--&gt; &lt;!--text：文本类型--&gt; 用户密码：&lt;input type=&quot;password&quot; /&gt;&lt;br/&gt; &lt;!--password：密码类型--&gt; 确认密码：&lt;input type=&quot;password&quot;/&gt;&lt;br/&gt; 性别：&lt;input type=&quot;radio&quot; name=&quot;sex&quot; checked=&quot;checked&quot;/&gt;男 &lt;input type=&quot;radio&quot; name=&quot;sex&quot;/&gt;女&lt;br/&gt; &lt;!--radio：单选框; name属性：可对其分组; checked：默认选项--&gt; 兴趣爱好：&lt;input type=&quot;checkbox&quot; checked=&quot;checked&quot;/&gt;Java &lt;input type=&quot;checkbox&quot;/&gt;JavaScript&lt;br/&gt; &lt;!--checkbox：复选框; checked:默认选项--&gt; 国籍： &lt;select&gt;&lt;!--下拉列表框标签--&gt; &lt;option&gt;--请选择国籍--&lt;/option&gt;&lt;!--选项标签--&gt; &lt;option selected=&quot;selected&quot;&gt;中国&lt;/option&gt; &lt;!--selected：默认选择--&gt; &lt;option&gt;美国&lt;/option&gt; &lt;option&gt;日本&lt;/option&gt; &lt;/select&gt;&lt;br/&gt; 自我评价：&lt;textarea rows=&quot;10&quot; cols=&quot;30&quot;&gt;默认值&lt;/textarea&gt;&lt;br/&gt; &lt;!--textarea标签：多行文本输入框；属性 rows:行数; 属性 cols：列数--&gt; &lt;!--textarea起始标签和结束标签中的内容是默认值--&gt; &lt;input type=&quot;reset&quot; value=&quot;重新输入&quot;/&gt;&lt;br/&gt; &lt;!--reset：重置按钮; value属性：更改按钮文本--&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt;&lt;br/&gt; &lt;!--submit：提交按钮--&gt; &lt;input type=&quot;button&quot; value=&quot;按钮&quot;/&gt;&lt;br/&gt; &lt;!--button:按钮--&gt; &lt;input type=&quot;file&quot;/&gt;&lt;br/&gt; &lt;!--file:文件上传--&gt; &lt;input type=&quot;hidden&quot;/&gt;&lt;br/&gt; &lt;!--hidden:隐藏域，需要发送一些不需要用户参与的信息至服务器，可使用隐藏域--&gt;&lt;/form&gt; 表单格式化把表单放入表格，使表单排列整齐。 12345678910111213141516171819202122&lt;form&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名称：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; value=&quot;User&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;用户密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;确认密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;性别：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; checked=&quot;checked&quot;/&gt;男 &lt;input type=&quot;radio&quot; name=&quot;sex&quot;/&gt;女&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; 表单提交的细节以下格式化的表单： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;form action=&quot;https://localhost:8080&quot; method=&quot;get&quot;&gt; &lt;!--form标签属性--&gt; &lt;!--action：设置提交的服务器地址--&gt; &lt;!--method：设置提交的方式，默认GET（也可以是POST）--&gt; &lt;input type=&quot;hidden&quot; name=&quot;action&quot; value=&quot;login&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名称：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; value=&quot;User&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;用户密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;确认密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;性别：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; checked=&quot;checked&quot;/&gt;男 &lt;input type=&quot;radio&quot; name=&quot;sex&quot;/&gt;女&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;兴趣爱好：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;checkbox&quot; checked=&quot;checked&quot;/&gt;Java &lt;input type=&quot;checkbox&quot;/&gt;JavaScript &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;国籍：&lt;/td&gt; &lt;td&gt; &lt;select&gt; &lt;option&gt;--请选择国籍--&lt;/option&gt; &lt;option selected=&quot;selected&quot;&gt;中国&lt;/option&gt; &lt;option&gt;美国&lt;/option&gt; &lt;option&gt;日本&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;自我评价:&lt;/td&gt; &lt;td&gt;&lt;textarea rows=&quot;10&quot; cols=&quot;30&quot;&gt;默认值&lt;/textarea&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;reset&quot; value=&quot;重新输入&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; 格式化后的表单显示为： 表单提交后，url显示为：https://localhost:8080/?action=login&amp;sex=on 该url体现了三部分 提交表单的服务器地址/action属性的值：localhost:8080/ 分隔符：? 请求参数/表单信息：action=login; sex=on 表单提交的时候，数据没有发送给服务器的三种情况： 表单项input标签没有name属性值。 单选、复选输入标签以及下拉列表的option标签，还需要加value属性值，以便发送给服务器具体值，而不是on。 表单项不在提交的form标签中。 修改后的表单代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;form action=&quot;https://localhost:8080&quot; method=&quot;get&quot;&gt; &lt;!--action：设置提交的服务器地址--&gt; &lt;!--method：设置提交的方式，默认GET--&gt; &lt;input type=&quot;hidden&quot; name=&quot;action&quot; value=&quot;login&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名称：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;user&quot; value=&quot;User&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;用户密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; name=&quot;password&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;确认密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; name=&quot;password&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;性别：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; checked=&quot;checked&quot; value=&quot;boy&quot;/&gt;男 &lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;girl&quot;/&gt;女&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;兴趣爱好：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;checkbox&quot; checked=&quot;checked&quot; name=&quot;hobby&quot; value=&quot;Java&quot;/&gt;Java &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;js&quot;/&gt;JavaScript &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;国籍：&lt;/td&gt; &lt;td&gt; &lt;select name=&quot;country&quot;&gt; &lt;option value=&quot;none&quot;&gt;--请选择国籍--&lt;/option&gt; &lt;option selected=&quot;selected&quot; value=&quot;中国&quot;&gt;中国&lt;/option&gt; &lt;option value=&quot;美国&quot;&gt;美国&lt;/option&gt; &lt;option value=&quot;日本&quot;&gt;日本&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;自我评价:&lt;/td&gt; &lt;td&gt;&lt;textarea rows=&quot;10&quot; cols=&quot;30&quot;&gt;默认值&lt;/textarea&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;reset&quot; value=&quot;重新输入&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; 表单提交后的url: https://localhost:8080/?action=login&amp;user=fred&amp;password=123&amp;password=123&amp;sex=girl&amp;hobby=Java&amp;hobby=js&amp;country=中国 表单标签method属性参数的区别 GET： 浏览器的地址栏为：action属性值 + ? + 请求参数 请求参数格式为：name=value&amp;name=value 不安全 有数据长度限制 POST请求的特点： 浏览器上的地址栏为：action属性值（没有请求参数） 相当于GET请求更安全 理论上没有数据长度限制 div和span div 标签：默认独占一行 span 标签：长度是封装数据长度 p 标签：默认在段落的上方或下方各空出一行（如果已有空行则不空） label标签label标签为input元素定义标注。 该标签不会为用户呈现特殊的效果，但为鼠标用户改进了可用性，即在label元素内点击文本，就会触发该控件。即当用户选择该标签时，浏览器会自动将焦点转到和label标签绑定的表单项上。 常见的应用情况是：单选框/复选框，点击文本即可勾选，而不需要去点那个框。 for : 表示该label是为表单中哪个控件服务，for属性点值设置为该元素的id属性值 CSSCSS简介CSS：层叠样式表单，用于增强/控制网页样式，且允许将样式信息和网页内容分离的一种标记性语言。 语法规则： 选择器：浏览器根据选择器决定受CSS样式影响到HTML元素/标签。 属性：属性:值; 形成一个完成的declaration。 CSS中的注释：/**/ CSS与HTML结合方式标签中的style在标签的style属性设置style=&quot;key: value1 value2;&quot; 这种方式可读性差，且没有复用性。 head标签中使用style标签在head标签中，用style标签定义需要的css样式。 style标签中的语句是CSS语法。 123456789&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style&gt; div{ border: 1px solid red; } &lt;/style&gt;&lt;/head&gt; 可以在同一页面复用代码，不能在多个页面复用CSS代码，且维护不方便，需要修改每个页面。 CSS文件把CSS样式写成CSS文件，在html文件的head标签中通过link标签引用。 style.css 123456div{ border: 1px red solid;}span{ border: 1px red solid;} div.html 123456789&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--link标签专门在head中用来引入CSS样式代码--&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;style.css&quot;/&gt; &lt;!--rel:文档间的关系--&gt; &lt;!--type:目标URL的类型--&gt; &lt;!--href:URL--&gt;&lt;/head&gt; 可以在多个页面中复用CSS样式，且维护方便。 CSS选择器标签名选择器1234标签名{ 属性:值; 属性:值;} 标签名选择器决定哪些标签被动的使用这个样式。 id选择器1234#id选择器{ 属性:值; 属性:值;} id选择器通过id属性选择性的使用这个样式。 html文件 12&lt;div id=&quot;id001&quot;&gt;div1&lt;/div&gt;&lt;!--标签的id属性--&gt; &lt;div id=&quot;id002&quot;&gt;div2&lt;/div&gt; CSS文件： 123456789101112&lt;style&gt; #id001{ border: yellow 1px solid; font-size: 30px; color: blue; } #id001{ border: 5px blue dotted; font-size: 20px; color: red; }&lt;/style&gt; class 选择器1234.class属性值{ 属性:值; 属性:值;} class属性多用来分组定义CSS样式。 class选择器通过class属性值选择性使用这个样式。 html文件 12&lt;div class=&quot;class0&quot;&gt;div1&lt;/div&gt;&lt;!--标签的class属性--&gt; &lt;div class=&quot;class0&quot;&gt;div2&lt;/div&gt; CSS文件： 1234567&lt;style&gt; .div{ color: blue; font-size: 30px; border: 1px yellow solid; }&lt;/style&gt; 组合选择器12345.class0, #id001{ color: blue; font-size: 30px; border: 1px yellow solid;} 组合选择器可以让多个选择器共用同样的CSS样式。 常用样式具体可查阅 字体颜色 color : red; color : rgb(33,33,13); color : #00F666; 宽度 width : 19px; width : 20%; 高度 height : 19px; height : 20%; 背景颜色 background-color : #0F2222; 字体大小 font-size : 20px; 边框 border : 1px solid red; DIV居中（相当于页面的居中） 12margin-left : auto;margin -right : auto; 文本居中 text-align : center; 超链接去下划线 text-decoration : none; 表格细线 1234567table{ border : 1px solid black; border-collapse : collapse;/*合并表格边框*/}td,th{ border : 1px, solid black;} 列表去修饰符 list-style : none","link":"/2020/07/22/html-css/"},{"title":"「Cryptography-MIT6875」: Lecture 10","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Today’s topic is Digital Signatures. Topics Covered: Motivation for Digital Signatures. Definition: EUF-CMA Security One-time signatures: Lamport’s Scheme. how to sign a single bit, once how to sign $n$ bits, once Collision-resistant hashing how to sign polynomially many bits, once. Digital SignaturesDigital Signatures vs. MACsMACsIn Lecture 5, we mentioned the applications of PRF and one of them is authentication. Message Authentication Codes(MAC) can be evaluated by PRF, the message taken as input. MAC gives the Authenticity that Bob is able to ensure the messages came from Alice. Yet it needs Alice and Bob to share a secret key beforehand. Then Alice uses the $sk$ to produce the MAC. Digital SignaturesDigital Signatures is the Public-key analog of MACs. The goal: Only Alice can produce signatures. Bob (or anyone else) can verify them. There is a pair of keys, secret key $sk$ and the corresponding (public) verification key $vk$. (Public) verification keys are stored in a “dictionary”. Comparison Signatures MACs 1 n uses require n key-pairs $n$ user require $n^2$ keys 2 Publicly Verifiable Privately Verifiable 3 Transferable Not transferable 4 Provides Non-Repudiation Dose not provide Non-Repudiation Signatures have public verification keys and private secret keys, so $n$ uses require $n$ key-pairs.But MAC have (private) shared keys beforehand, so $n$ users require $n^2$ keys. Anyone can verify the signatures using the public $vk$.But only Bob who has the secret key $sk$ can verify the MACs. Signatures are Transferable. That is, you can take signatures and show it to others. Signatures provides Non-Repudiation. That is, you cannot claim that you didn’t sign it.[*不可抵赖] Transferability and Non-Repudiation whether they are good properties or bad properties depends on the scenario. It is double edged sword. ApplicationsThere are abundant applications of signatures. Certificates, or a public-key directory in practice. Trusted Certificate Authority(CA), e.g. Verisign, Let’s Encrypt. When Alice (=www.google.com) wants to register her public (encryption and signing) keys $pk$ and $vk$, CA first checks that she is Alice. CA issues a “certificate” $\\sigma\\leftarrow Sign(SK_{Verisign},Alice||pk||vk)$.The certificate is essentially the signature.The certificate indicates that $pk$ and $sk$ are indeed Alice’s. CA produces the signature using its sign key $SK_{Verisign}$. Alice can later stores this certificate to prove she “owns” $pk$ and $sk$. Browsers store $VK_{Verisign}$ and check the certificate.$VK_{Verisign}$ is the public verification key of CA. Bitcoin and other cryptocurrencies. I am identified by my verification key $vk$. When I pay you (your verification key = $vk’$), I sign “$x$ paid to $vk’$” with my $sk$. DefinitionThe definition of Digital Signatures consists of a triple of PPT algorithms $(Gen,Sign,Verify)$. $Gen(1^n)\\rightarrow (vk,sk)$ running by Alice(signer)PPT Key Generation algorithm generates a public-private key pair. $Sign(sk,m)\\rightarrow \\sigma$ running by Alice (signer)(possible probabilistic) Signing algorithm uses the secret signing key to produce a signature $\\sigma$.Whether deterministic or probabilistic Signing algorithm makes sense. $Verify(vk,m,\\sigma)\\rightarrow Acc(1)/Rej(0)$ running by Bob (any verifier)Verification algorithm uses the public verification key to check the signature $\\sigma$ against a message $m$. Correctness: For all $vk,sk,m$: $Verify(vk,m,Sign(sk,m))=accept$. EUF-CMA SecurityDefine the adversary first. The adversary after seeing signatures of many messages, should not be able to produce a signature of any new message. Adversary: Power of adversary: Chosen-message attackThe adversary can request for, and obtain, signatures of (poly. many) messages $m_1,m_2,\\dots$ Goal of adversary: Existential ForgeryThe adversary wins if she produces a signature of any new message $m^*\\notin \\{m_1,m_2,\\dots \\}$. Then we give Existentially Unforgeable against a Chosen Message Attack (EUF-CMA) security by a game. Game : The Challenger generates a public-private key pair $(vk,sk)$ and sends the public verification key $vk$ to Eve. Eve can request for poly. many messages $\\{m_1,m_2,\\dots\\}$ and obtain the corresponding signatures $\\{\\sigma_1,\\sigma_2,\\dots\\}$. Eve produce a signature $\\sigma^*$ against a new message $m^*\\notin \\{m_1,m_2,\\dots\\}$. EUF-CMA Definition: Eve wins if $Verify(vk,m^*,\\sigma^*)=1$ and $m^*\\notin\\{m_1,m_2,\\dots\\}$. The signature scheme is EUF-CMA-secure if no PPT Eve can win with probability better than $negl(n)$. The challenger gives Eve a lot of signatures. Now Eve wants to forge a signature of other message right. We say that she wins if she produces a signature right of even one message that was not signed already. We call this an existential forgery because there exists a $m^*$ not in the set for which she produces a signature. It’s a very strong definition. But the definition dose not prevent the adversary from producing a new signature for the same message. Yet she dose not win. In other words, it’s consistent with the definition to allow the adversary to produce a new signature for the same message. We can make the job easier for adversary. She wins as well if she produces a new signature for the same message. Then we can get strong EUF-CMA definition. So the stronger adversary, the stronger security by definition. Lamport (One-time) SignaturesIn this section, we introduce a beautiful signature, Lamport Signature. It’s One-time signature sort of like One-time Pads. The one-time signatures means that the adversary gets a signature of some message once, a single message, and she should not be able to produce a signature of any other different message. How to sign a bitWe only use the signing key to sign once and we are really signing a bit. $Gen(1^n)\\rightarrow (SK,VK)$ Signing Key $SK:[x_0,x_1]$where $x_0,x_1$ are both $n$-bit string. Verification Key $VK:[y_0=f(x_0),y_1=f(x_1)]$where $f$ is a OWF. $Sign(SK,b)\\rightarrow \\sigma$ where $b$ is a bit The signature is $\\sigma=x_b$. $Verify(VK,b,\\sigma)$ Check if $f(\\sigma)\\overset ? = y_b$ The signing keys $SK$ are two random $n$-bit string and the verification keys $VK$ are the corresponding values of OWF, i.e. the signing keys are images of verification keys. The game in Lamport (One-time) Signatures differs from that Eve only gets a signature of a single message. If $b=0$, the challenger gives Eve $x_0$, the signature of $0$, and Eve wants to forge the signature of $1$. So the task of adversary is producing a signature of $1$. That’s $x_1$. She cannot do it because $x_1$ is random. Claim : Assuming $f$ is a OWF, no PPT adversary can produce a signature of $\\bar{b}$ given a signature of $b$. The intuition of proof is easy. There’s no way Eve can produce $x_1$ unless she can invert the $y_1$ she have. How to sign n bitsWe can use the signing keys to sign $n$ bits, once. Just repeat it. $Gen(1^n)\\rightarrow (SK,VK)$ Signing Key $ SK:\\left[\\begin{array}{c}x_{1,0},x_{2,0},\\dots, x_{n,0}\\\\ x_{1,1},x_{2,1} ,\\dots, x_{n,1}\\end{array} \\right]$ where $x_{\\cdot,\\cdot}$ is $n$-bit random string and each column is a pair-key for one bit. Verification Key $VK:\\left[\\begin{array}{c}y_{1,0},y_{2,0},\\dots, y_{n,0}\\\\ y_{1,1},y_{2,1} ,\\dots, y_{n,1}\\end{array} \\right]$ where $f$ is a OWF and $y_{i,c}=f(x_{i,c})$. $Sign(SK,\\vec m)\\rightarrow \\vec\\sigma$ where $m$ is a $n$-bit message $(m_1,\\dots, m_n)$. The signature is $\\vec \\sigma=(x_{1,m_1},\\dots, x_{n,m_n})$. $Verify(VK,\\vec m,\\vec\\sigma)$ Check if $\\forall i:f(\\sigma_i)\\overset ? = y_{i,m_i}$. In the game, Eve can request for a signature once of any $n$-bit message $m$ and she wants to forge the signature of a different message $m’\\ne m$. There are two claims. Claim 1: Assuming $f$ is a OWF, no PPT adversary can produce a signature of $m’$ given a signature of a single message $m\\ne m’$. Claim 2: The adversary can forge signature on any message given the signatures on (some) two messages. We only give the proof of Claim 1. Claim 2 can be comprehended easily after proving Claim1. Proof for Claim 1: Suppose for the contradiction that there is a adversary forging a signature of $m’$ given a signature of a single message $m$ s.t. $m’\\ne m$. We want to construct a OWF Inverter for $y$ so we need to interact with the forger. Interaction with the forger: Inverter gives the verification keys $VK$ to the forgery. The forger request for a signature of a single message $m$. Inverter produces the signature $\\sigma$ she wants. Then the forger promises to give a signature $\\sigma’$ against a new message $m’$ The key idea is we can take $y$ into $VK$ and plop it into one of the two slots in one column. The thing to notice is that there is at least one different bit between $m$ and $m’$. Note: The message $m$ is what the forger wants to obtain its signature The message $m’$ is what the forger wants to forge its signature. For simplicity, we suppose there is only one different bit between $m$ and $m’$. messages $m$ and $m’$ are known in advance: Suppose the Inverter knows $m$ and $m’$in advance.For simplicity, suppose $m=00\\dots 0$ and $m’=10\\dots 0$. The Inverter wants to get the inverse of $y$ against the OWF $f$. So the Inverter interacts with the forger. The Inverter samples $SK$ and generates the verification keys (put $y$ into one slot as follows) $$ VK=\\left[\\begin{array}{c}f(x_{1,0}) &,f(x_{2,0}),\\dots, f(x_{n,0})\\\\ y &,f(x_{2,1}) ,\\dots, f(x_{n,1})\\end{array} \\right] $$ where $x_{\\cdot,\\cdot}$ is known to the Inverter. The forger requests for the signature of $m=00\\dots 0$. The Inverter produces the signature $\\sigma=(x_{1,0},\\dots,x_{n,0})$. The forger promises to produce the signature on $m’=10\\dots0$ from the contradiction which has the inverse of $y$. Done. However, the Inverter dosen’t know the two messages in advance.The Invert only knows there is one different bit between $m$ and $m’$. So the Inverter could guess it. messages $m$ and $m’$ are unknown in advance: The Inverter guesses the different bit locates in $i$-th bit right w.p. $1/n$. Suppose the Inverter plants the trap $y$ into the slot $(i,0)$, $i$-th column and $0$-th row.Then the generated verification keys is as follows. $$ VK=\\left[\\begin{array}{c}f(x_{1,0}) ,\\dots ,&y&,\\dots, f(x_{n,0})\\\\ f(x_{1,1}) ,\\dots ,&f(x_{i,1})& ,\\dots, f(x_{n,1})\\end{array} \\right] $$ where $x_{\\cdot,\\cdot}$ is known to the Inverter. In the forger’s point of view, when she looks at the verification keys $VK$, she has no information about where the trap is. There are two events that could happen even if $m$ differs from $m’$ in $i$-th bit. The message $m$ hits the trap while the $m’$ dose not hit the trap. The message $m$ dosen’t hit the trap while the $m’$ hits the trap. The Inverter can only handle with the second event.The message $m$ hits the opposite location to the trap while the $m’$ hits the trap. The Inverter is able to give the signature of $m$, i.e. $(\\dots,x_{i,1},\\dots)$ The forger promises to produce the signature of $m’$from the contradiction, which have the inverse of $y$. Otherwise, the forger gives something that the Inverter already has known. So the probability of inverting right is $\\varepsilon/2n$ if the advantage of forging is non-negligibly $\\varepsilon$.The $1/n$ is the probability of guessing the location of different bit and the $1/2$ is the probability of guessing whether $m$ hits the trap. Done. Besides, Claim 2 says that once I give you the two signatures, I am actually giving you all the inverses and you can produce signatures of any message you want. This violates one-wayness. So far, the length of message is limited by the size of verification keys. Before proceeding to the next question that how to sign poly. many bits, we take a detour in collision-resistant hash function. Detour: Collision-Resistant Hash FunctionsA compressing function $h:\\{0,1\\}^m\\rightarrow \\{0,1\\}^n$ (where $m&gt;n$ ) for which it is computationally hard to find collisions. It compresses the bits sort of the opposite of PRG. Definition: $h$ is collision-resistant if for every PPT algorithm $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[A(1^n)=(x,y):x\\ne y, h(x,y)]=\\mu(n) $$ In theory, we like to talk about families of functions to handle non-uniform adversaries (who could have hardcoded collision for the fixed function $h$). A compressing family of functions $\\mathcal{H}=\\{\\{0,1\\}^m\\rightarrow \\{0,1\\}^n\\}$ (where $m&gt;n$) for which it is computationally hard to find collisions. Collision-Resistant Definition: $\\mathcal{H}$ is collision-resistant if for every PPT algorithm $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}_{h\\gets \\mathcal{H}}[A(1^n)=(x,y):x\\ne y, h(x,y)]=\\mu(n) $$ How to sign poly. many bitsBack to the question that how to sign poly. many bits with a fixed verification key. The key idea is hashing the message into $n$ bits and sign the hash. $Gen(1^n)\\rightarrow (SK,VK)$ Signing Key $SK:\\left[\\begin{array}{c}x_{1,0},x_{2,0},\\dots, x_{n,0}\\\\ x_{1,1},x_{2,1} ,\\dots, x_{n,1}\\end{array} \\right]$ where $x_{\\cdot,\\cdot}$ is $n$-bit random string and each column is a pair-key for one bit. Verification Key $VK:\\left[\\begin{array}{c}y_{1,0},y_{2,0},\\dots, y_{n,0}\\\\ y_{1,1},y_{2,1} ,\\dots, y_{n,1}\\end{array} \\right]$ where $f$ is a OWF and $y_{i,c}=f(x_{i,c})$. Sample $h\\gets \\mathcal{H}$. $Sign(SK,\\vec m)\\rightarrow \\vec\\sigma$ where $m$ is a $n$-bit message $(m_1,\\dots, m_n)$. Compute the hash $z=h(m)$. The signature is $\\vec \\sigma=(x_{1,z1},\\dots, x_{n,z_n})$. $Verify(VK,\\vec m,\\vec\\sigma)$ Recompute the hash $z=h(m)$ Check if $\\forall i:f(\\sigma_i)\\overset ? = y_{i,z_i}$. Claim : Assuming $f$ is a OWF and $\\mathcal{H}$ s a collision-resistant family, no PPT adversary can produce a signature of $m’$ given a signature of a single $m\\ne m’$. We only give the idea of proof. Intuition of Proof: Suppose for the contradiction. There are two possibilities. Either the adversary picked $m’$ s.t. $h(m’)=h(m)$, in which case she violated collision-resistance of $\\mathcal{H}$. Or she produced a Lamport signature on a “message” $z’\\ne z$, in which case she violated one-time security of Lamport, and therefore the one-wayness of $f$.","link":"/2022/07/29/mit6875-lec10/"},{"title":"「Cryptography-MIT6875」: Lecture 11","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Today’s topic is Many-time Digital Signatures. Topics Covered: Many-time, stateful, signature schemes. Naor-Yung construction: stateless EUF-CMA-secure signature schemes. In last blog was introduced the definition of Digital Signatures and the EUF-CMA Security. Moreover, we introduced Lamport (One-time) Digital Signatures. We can use it to sign polynomially many bits with a fixed verification key. The main idea is hashing the message into $n$ bits and signing the hash. So far, it’s one-time security. The adversary can forge signature on any message given the signatures on (some) two messages. How to achieve Many-time Signature Scheme ? It’s today’s gist. We will achieve many-time signature scheme in four+ steps. Stateful, Growing Signatures.Idea: Signature Chains How to Shrink the signatures.Idea: Signature Trees How to Shrink Alice’s storage.Idea: Pseudorandom Trees How to make Alice stateless.Idea: Randomization (optional). How to make Alice stateless and deterministic.Idea: PRFs. S1: Stateful Many-time SignaturesThe first step is to achieve stateful many-time signatures. The main idea is Signature Chains. sign m1 Alice starts with a secret signing Key $SK_0$.Her public verification Key $VK_0$ is stored in the (public) “directory”. When signing a message $m_1$: Generate a new pair $(VK_1,SK_1)$. Produce signature $\\sigma_1\\leftarrow Sign(SK_0,m_1||VK_1)$. Output $VK_1||\\sigma_1$ Remember $VK_1||m_1||\\sigma_1$ as well as $SK_1$. Alice is going to sign not only the message, but the message together with the next verification key. She uses it to authenticate a new verification. It’s the concatenation of two strings and we can sign polynomially many bits. To verify a signature $VK_1||\\sigma_1$ for message $m_1$: Run $Verify(VK_0,m_1||VK_1,\\sigma_1)$. We use $VK_0$ to verify the signature $\\sigma_1$ that $m_1$is sent from Alice and $VK_1$ is authenticated from Alice. sign m2But how about the next message $m_2$ ? Can we just output $VK_2||\\sigma_2$ as follows ? (NO!) When signing the next message $m_2$: Generate a new pair $(VK_2,SK_2)$. Produce signature $\\sigma_2\\leftarrow Sign(SK_1,m_2||VK_2)$. Output $VK_2||\\sigma_2$ The thing to point is that each signing is independent and everyone only knows the verification key $VK_0$ in that “directory”. The verifier dosen’t know the verification key $VK_1$ for the signature $\\sigma_2$ nor the authentication for $VK_1$ when he receives $VK_2||\\sigma_2$. So Alice needs to send $VK_1$ as well as the authentication for $VK_1$. We should output $VK_1||m_1||\\sigma_1||VK_2||\\sigma_2$ as follows. When signing the next message $m_2$: Generate a new pair $(VK_2,SK_2)$. Produce signature $\\sigma_2\\leftarrow Sign(SK_1,m_2||VK_2)$. Output $VK_1||m_1||\\sigma_1||VK_2||\\sigma_2$. (additionally) Remember $VK_2||m_2||\\sigma_2$ as well as $SK_2$. The first part $VK_1||m_1||\\sigma_1$ is to authenticate the verification key $VK_1$. The verify uses $VK_1$ to verify the message $m_2$ together with the next verification $VK_2$. To verify a signature $VK_1||m_1||\\sigma_1||VK_2||\\sigma_2$ for message $m_2$: Run $Verify(VK_0,m_1||VK_1,\\sigma_1)$ to authenticate $VK_1$. Run $Verify(VK_1,m_2||VK_2,\\sigma_2)$ to authenticate the message $m_2$ together with the next verification key $VK_2$. It’s growing signatures since Alice needs remember the $VK_i||m_i||\\sigma_i$ as well as $SK_i$. And the signature chains is as follows. An optimizationIn fact, Alice stores the $m_i$ just to use $\\sigma_i$ to authenticate $VK_i$. So there is an optimization that need to remember only the past verification keys, not the past messages. Suppose we can split the verification into two halves. We use part of $VK_i$ to sign $m_{i+1}$ and the rest to sign $VK_{i+1}$. The signature chains is as follows. The verifier only needs to verify the past verification keys, not the past messages. ProblemsThere are still two major problems. Alice is stateful.Alice needs to remember a whole lot of things, $\\mathcal{O}(T)$ information after $T$ steps. The signatures grow.Length of the signature of the $T$-th message is $\\mathcal{O}(T)$. S2: How to Shrink the signatures ?The next step is to shrink the signature. The main idea is Signature Trees. Alice starts with a secret signing Key $SK_\\epsilon$.Her public verification Key $VK_\\epsilon$ is stored in the (public) “directory”. Alice generates many random $(VK,SK)$ pairs and arrange them in a tree of depth = security parameter $\\lambda$.There are $2^\\lambda$ leaves and Alice only uses the leaf to sign the message. When signing the first message $m_0$ Alice only uses the leaf to sign the message while use the parent node to sign both children nodes. Use $VK_{000}$ to sign $m_0$. $\\tau_0\\gets Sign(SK_{000},m_0)$ “Authenticate ” $VK_{000}$ using the “signature path”.Alice produces the authentication path for $VK_{000}$ : $(\\sigma_\\epsilon,\\sigma_0,\\sigma_{00})$. Authenticate $VK_0$: use $VK_\\epsilon$ to sign both $VK_0$ and $VK_1$ $\\sigma_\\epsilon\\gets Sign(SK_\\epsilon,VK_{0},VK_{1})$ Authenticate $VK_{00}$: use $VK_{0}$ to sign both $VK_{00}$ and $VK_{01}$$\\sigma_0\\gets Sign(SK_0,VK_{00},VK_{01})$ Authenticate $VK_{000}$: use $VK_{00}$ to sign both $VK_{000}$ and $VK_{001}$ $\\sigma_{00}\\gets Sign(SK_{00},VK_{000},VK_{001})$ Signatures of $m_0$: (Authentication path for $VK_{000}$, $\\tau_0\\gets Sign(SK_{000},m_0)$) When signing the next message $m_1$ Use $VK_{001}$ to sign $m_1$. $\\tau_1\\gets Sign(SK_{001},m_1) $ “Authenticate ” $VK_{001}$ using the “signature path”.Alice produces the authentication path for $VK_{001}$ : $(\\sigma_\\epsilon,\\sigma_0,\\sigma_{00})$. Authenticate $VK_0$: $\\sigma_\\epsilon\\gets Sign(SK_\\epsilon,VK_{0},VK_{1})$ Authenticate $VK_{00}$: $\\sigma_0\\gets Sign(SK_0,VK_{00},VK_{01})$ Authenticate $VK_{000}$: $\\sigma_{00}\\gets Sign(SK_{00},VK_{000},VK_{001})$ Signatures of $m_1$: (Authentication path for $VK_{001}$, $\\tau_1\\gets Sign(SK_{001},m_1)$) The good news is the signatures consist of $\\lambda$ one-time signatures and do not grow with time. But the bad news is the signer generates and keeps the entire ($\\approx 2^\\lambda$-size) signature tree in memory. Besides, the signer also needs to remember the state that what is the last leaf used for signing. S3: How to Shrink Alice’s storage.The main idea is Pseudorandom Trees. Instead of truly random signature trees, Alice uses PRF to build a pseudorandom signature trees. Alice keeps a secret PRF key $K$. Alice populates the nodes with $r_x=PRF(K,x)$ Use $r_x$ to derive the key pair $(VK_x,SK_x)\\gets Gen(1^\\lambda;r_x)$. The thing to notice is that Alice only registers the verification key $VK_\\epsilon$.So the verifier only knows $VK_\\epsilon$, not the PRF key. We can use the pseudorandom signature tree to sign many-time signatures same as above. As a matter of fact, the signer can do lazy evaluation instead of evaluating every node beforehand. So the signer can achieve short signatures and small storage at the same time. However, it’s still stateful. The signer still needs to keep a counter indicating which leaf (which tells her which secret key) to use next. It proceeds to the next step. S4: How to make Alice stateless.The main idea is randomization. We can achieve stateless via randomization. When signing a message $m$ Pick a random leaf $r$. Use $VK_r$ to sign $m$.$\\sigma_r\\gets Sign(SK_r,m)$ Output $(r,\\sigma_r,\\text{authentication path for }VK_r)$. The good news is it’s stateless. But we cannot pick the same leaf twice since we are using the one-time signature scheme. The key idea of security analysis is birthday attack. If the signer produces $q$ signatures, the probability she picks the same leaf twice is $\\le q^2/2^\\lambda$, which is negligible. S5: How to make Alice stateless and deterministic.The key idea is generating $r$ pseudo-randomly. Have another PRF key $K’$ and let $r=PRF(K’,m)$. When signing a message $m$ Pick a pseudorandom leaf $r=PRF(K’,m)$. Use $VK_r$ to sign $m$.$\\sigma_r\\gets Sign(SK_r,m) $ Output $(r,\\sigma_r,\\text{authentication path for }VK_r)$. Security AnalysisFor simplicity, we analyze the randomization stateless scheme. (S4) The many-time digital signature scheme is EUF-CMA secure if the one-time digital signature is one-time secure. Assume for the contradiction that there is an adversary breaking the EUF-CMA security, then we can construct a one-time forger. We have the adversary $\\mathcal{A}$ for EUF-CMA security. Get the verification key $VK$. Request for signatures of $q$ messages. ($q$-time) Request for message $m_i$ Obtain the signature $\\sigma_i$ for $m_i$. Produce $(m^*,\\sigma^*)$ that a signature against a new message $m^*\\notin \\{m_1,m_2,\\dots m_q\\}$ with non-negligible advantage. We want to construct a forger $\\mathcal{B}$ for one-time security. Get the one-time verification key $OVK$. Request for the signatures $\\sigma$ of a single message $m$. (only one-time) Produce $(m’,\\sigma’)$ that a signature against a new message $m’\\ne m$. For simplicity, we condition on the event $E$ where all our random $r$’s are distinct. $\\operatorname{Pr}[\\mathcal{A}\\text{ wins }\\mid E]\\ge q^2/2^\\lambda$ Suppose the probability above dosen’t change very much. $\\ge \\operatorname{Pr}[\\mathcal{A}\\text{ wins }]-\\operatorname{Pr}[E]=1/poly(\\lambda)-negl(\\lambda)$ $\\ge 1/poly(\\lambda)$. So the advantage of $\\mathcal{A}$ is non-negligible even on the condition. So We need the forger $\\mathcal{B}$ to interact with the adversary $\\mathcal{A}$ to win the game. Construction of One-time Forger $\\mathcal{B}$: Plop $OVK$ into a random leaf $r$. $\\mathcal{B}$ get the one-time verification key $OVK$. $\\mathcal{B}$ generates the pseudorandom signature trees and plop the $OVK$ into a random leaf $r_{OVK}$. $\\mathcal{B}$ sends the root verification key $VK_\\epsilon$ to $\\mathcal{A}$. $\\mathcal{A}$ requests for the signatures for $q$ times.For each message $m_i$, there are two cases. If $\\mathcal{B}$ picks the random leaf $r\\ne r_{OVK}$, $\\mathcal{B}$ is able to produce the signature. If $\\mathcal{B}$ picks the random leaf $r= r_{OVK}$, $\\mathcal{B}$ cannot produce the signature. But $\\mathcal{B}$ can request the signature for a single message. $\\mathcal{B}$ requests for the message $m_i$. $\\mathcal{B}$ obtains the signature $\\sigma_i$ for the single message $m_i$. $\\mathcal{B}$ passes the signature $\\sigma_i$ to the $\\mathcal{A}$ as the response. Note: $\\mathcal{B}$ can only picks $r_{OVK}$ once. Besides, it’s necessary to pick it. $\\mathcal{A}$ promises to produce the signature for a new message $m^*\\notin \\{m_1,m_2,\\dots m_q\\}$ The signature consists of $(r^*,\\sigma^*,\\text{authentication path for }VK_{r^*})$ If $r^*= r_{OVK}$ and $\\sigma ^*$ is different from the previous $\\sigma_i$ generated by $\\mathcal{B}$. Then $\\mathcal{B}$ wins. $\\mathcal{B}$ just passes the $(m^*,\\sigma^*)$ as the forgery signature. But it could happen only if $\\mathcal{A}$ picks $r^*$ from one of the leaves $\\{r_1,r_2,\\dots r_q\\}$ given by $\\mathcal{B}$. Claim : If $\\mathcal{A}$ picks one of the leaves $\\{r_1,r_2,\\dots r_q\\}$ when forging, then $\\mathcal{B}$ can produce the one-time forgery w.p. $1/q$. But there is no guarantee that $\\mathcal{A}$ could pick one of the leaves $\\{r_1,r_2,\\dots r_q\\}$ given from $\\mathcal{B}$. Instead, we plop $OVK$ into a node. Plop $OVK$ into the $VK_\\epsilon$ location as follows. $\\mathcal{B}$ get the one-time verification key $OVK$. $\\mathcal{B}$ generates the pseudorandom signature trees and plop the $OVK$ into the root node. So $\\mathcal{B}$ knows all secret key except $SK_\\epsilon$. $\\mathcal{B}$ sends the root verification key $VK_\\epsilon$ to $\\mathcal{A}$. $\\mathcal{A}$ requests for the signatures for $q$ times.For each message $m_i$, there are two cases. Pick a random leaf $r$. Use $VK_r$ to sign $m$.$\\tau_i\\gets Sign(SK_r,m) $ Produce authentication path for $VK_r$.Signatures for message $m_1$: $(r,\\tau_1,(\\sigma_\\epsilon,\\epsilon_0,\\epsilon_{01}))$. $\\mathcal{B}$ cannot produce the signature of $\\sigma_\\epsilon$ since she dosen’t know the $SK_\\epsilon$. But $\\mathcal{B}$ can request the signature $\\sigma_\\epsilon$ for a single message $(VK_0||VK_1)$. $\\mathcal{A}$ promises to produce the signature for a new message $m^*\\notin \\{m_1,m_2,\\dots m_q\\}$ The signature consists of $(r^*,\\tau^*,\\text{authentication path for }VK_{r^*})$ Suppose $\\mathcal{A}$ picks $r^*=1$ as above figure.The authentication path for $VK_{001}$: $(\\sigma_\\epsilon’,\\sigma_0’,\\sigma_{00}’)$. In fact, the output of forgery consists $VK_0’,VK_1’,VK_{00}’,VK_{01}’,VK_{000}’,VK_{001}’$, which could be different from the tree built by $\\mathcal{B}$. If $VK_0||VK_1 \\ne VK_0’||VK_1’$, $\\mathcal{A}$ wins. Plop $OVK$ into the $VK_0$ location.The analysis is the same.If $VK_{00}||VK_{01} \\ne VK_{00}’||VK_{01}’$, $\\mathcal{A}$ wins. The main idea is as above.","link":"/2022/08/02/mit6875-lec11/"},{"title":"「Cryptography-MIT6875」: Lecture 12","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Construction of CRHF from Discrete Log Digital Signatures only from OWF Direct Constructions:Trapdoor Permutation and the Hash-and-Sign Paradigm. Random Oracles. Digital Signature from CRHFWe showed the theorem about digital signature in Lecture 10. Theorem: Assuming the existence of one-way functions and collision-resistant hash function families, there are digital signature schemes. CRHF DefinitionRecall the definition of Collision-Resistant Hash Functions. A compressing family of functions $\\mathcal{H}=\\{h:\\{0,1\\}^m\\rightarrow \\{0,1\\}^n\\}$ (where $m&gt;n$ ) for which it is computationally hard to find collisions. Definition: $\\mathcal{H}$ is collision-resistant if for every PPT algorithm $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}_\\mathcal{h\\gets H}[A(1^n,h)=(x,y):x\\ne y,h(x)=h(y)]=\\mu(n) $$ The function $h$ is given to the adversary. And the advantage of finding a collision is negligible. How can we construct the CRHF ? Construction of CRHF from Discrete LogConstruction: Let $p=2q+1$ be a “safe” prime. Let $\\mathcal{H}=\\{h:(\\mathbb{Z}_q)^2\\rightarrow QR_p\\}$ $\\mathcal{H}$ maps two element in $\\mathbb{Z}_q$ to one element in $QR_p$, the subgroup of quadratic residues in $\\mathbb{Z}_p^*$ with order $q$. Each function $h_{g_1,g_2}\\in \\mathcal{H}$ is parameterized by two generators $g_1$ and $g_2$ of $QR_p$. Define $h_{g_1,g_2}(x_1,x_2)=g_1^{x_1}g_2^{x_2} \\mod p$. This compresses $2\\log q$ bits into $\\log q\\approx \\log {q+1}$ bits. Prove $h_{g_1,g_2}$ is collision-resistant. Proof: Suppose for contradiction that there is an adversary that finds a collision $(x_1,x_2)$ and $(y_1,y_2)$. $g_1^{x_1}g_2^{x_2}= g_1^{y_1}g_2^{y_2}\\mod p$ $g_1^{x_1-y_1}= g_2^{y_2-x_2}\\mod p$ $g_1=g_2^{(y_2-x_2)(x_1-y_1)^{-1}}\\mod p$ (assuming $x_1-y_1\\ne 0$) This turns to a discrete log problem of $DLOG_{g_2}(g_1)$. Turns out to another theorem of digital signature scheme. Theorem: Assuming the hardness of the discrete logarithm problem, there are digital signature schemes. Other Constructions of CRHFSimilarly, we can construct CRHF from the hardness of factoring, lattice problems etc. It’s not known to follow from the existence of one-way functions or even one-way permutations. It’s still a big open problem. “Black-box separations”: Certain ways of constructing CRHF from OWF/OWP cannot work.”Finding collisions on a one-way street”, Daniel Simon, Eurocrypt 1998. Digital Signature from OWFBut it turns out that collision-resistant hashing is not necessary; something weaker called universal one-way hashing (UOWHF) suffices. Furthermore, UOWHFs can be constructed from one-way functions alone. The challenge is different between CRHF and UOWHF. CRHF Give $\\mathcal{A}$ the function $h$ It’s computationally hard for $\\mathcal{A}$ to gives $(x,y)$ such that $h(x)=h(y)$ s.t. $x\\ne y$. UOWHF $\\mathcal{A}$ requests for the hash of $x$. Give $\\mathcal{A}$ the hash $h(x)$ It’s computationally hard for $\\mathcal{A}$ to give $y$ such that $h(x)=h(y)$ s.t. $x\\ne y$. So we can construct Digital Signature only from OWF. Theorem: Digital Signature schemes exist if and only if one-way functions exist. We can construct Digital Signatures from two routes. OWF → UOWHF → Digital Signatures CRHF(+OWF) → Digital Signatures Now we catch the sight of words in crypto. Direct ConstructionsWe will show that “Hash-and-Sign” is secure in random oracle model. “Vanilla” RSA SignaturesWe can construct Digital Signature scheme directly from any trapdoor permutation, e.g. RSA. Vanilla RSA Signatures: $Gen(1^\\lambda)$ Pick primes $(P,Q)$ and let $N=PQ$. Pick $e$ relatively prime to $\\phi(N)$ and let $d=e^{-1} \\pmod {\\phi(N)}$. $SK=(N,d)$ and $VK=(N,e)$ $Sign(SK,m)$ Output signature $\\sigma=m^d \\pmod N$ $Verify(VK,m,\\sigma)$ Check if $\\sigma^e=m\\pmod N$ But it is existentially forgeable and malleable. Problems: Existentially forgeable Attack1: Pick a random $\\sigma$ and output $(m=\\sigma^e,\\sigma)$ as the forgery. Malleable Attack2: Given a signature of $m$, you can produce a signature of $2m,3m,\\dots$ Fundamental issues under the problems: Can “reverse-engineer” the message starting from the signature. (Attack 1) Algebraic structure allows malleability. (Attack 2) How to fix Vanilla RSA ? Fixed Vanilla RSA Signature: $Gen(1^\\lambda)$ Pick primes $(P,Q)$ and let $N=PQ$. Pick $e$ relatively prime to $\\phi(N)$ and let $d=e^{-1} \\pmod {\\phi(N)}$. $SK=(N,d)$ and $VK=(N,e,\\color{blue}{H})$ $Sign(SK,m)$ Output signature: $\\sigma= \\color{blue}{H(m)}^d \\pmod N$ $Verify(VK,m,\\sigma)$ Check if $\\sigma^e=\\color{blue}{H(m)}\\pmod N$ What is $H$ ? $H$ is some very complicated “hash” function. $H$ should be at least one-way. ( to prevent Attack 1) $H$ should be hard to “algebraically manipulate” $H(m)$ into $H(\\text{related } m’)$.(to prevent Attack 2) Collision-resistance dose not seem to be enough. Given a CRHF $H(m)$, you may be able to produce $H(m’)$ for related $m’$. The Random Oracle HeuristicWe want a public $H$ that is “non-malleable”. Given $H(m)$, it is hard to produce $H(m’)$ for any non-trivially related $m’$. Random Oracle Definition: For every PPT adversary $A$ and “every non-trivial relation” $R$, $$\\operatorname{Pr}[A\\left(H(m)\\right)=H(m’):R(m,m’)=1]=negl(\\lambda)$$ The goal of adversary is to come up with the relation $R$ such that you can somehow manipulate $H(m)$ into $H(m’)$. How about the relation $R$ where $R(x,y)=1$ if and only if $y=H(x)$ ? A public $H$ that “behaves like a random function”. We can consider it as a proxy to a random function. (A PRF also behaves like a random function, but $PRF_K$ is not publicly computable. ) The adversary $\\mathcal{A}$ can get the public function $H$ in reality. But in the Random Oracle Heuristic world, the only way to compute $H$, virtually a black box, is by calling the oracle. Claim: The hashed RSA is EUF-CMA secure in the random oracle model. Proof: Assume there is a PPT adversary $\\mathcal{A}$ that breaks the EUF-CMA security of hashed RSA in the random oracle model. Given $\\mathcal{A}$ the verification key. $\\mathcal{A}$ asks the Hash Query for poly. times.(We can model it to split the hash queries and sign queries.) $\\mathcal{A}$ asks the Sign Query for poly. times. $\\mathcal{A}$ gives a forgery $(m^*, \\sigma^*)$. Recall the RSA assumption:given $N,e$ and $y=x^e\\mod N$, hard to compute $x$. Then, there is an algorithm $\\mathcal{B}$ that solves the RSA problem. The task of $\\mathcal{B}$ is to compute $x$ given the $(N,e,y)$. $\\mathcal{B}$ needs to interact with the adversary $\\mathcal{A}$ $\\mathcal{B}$ gives the verification key $VK=(N,e)$ to $\\mathcal{A}$. $\\mathcal{A}$ asks polynomially many Hash Queries. For all hash queries, $\\mathcal{B}$ picks a random $\\tilde{m}$ as the trap. For the trap $\\tilde{m}$, $\\mathcal{B}$ sets the hash $H(\\tilde{m})=y$. For other normal $m$, $\\mathcal{B}$ picks a random $x$ and sets the hash $H(m)=x^e$. $\\mathcal{A}$ asks polynomially many Sign Queries. For each Sign Query for $m$: If $m=\\tilde{m}$, i.e. hits the trap, $\\mathcal{B}$ aborts.Because $\\mathcal{B}$ cannot produce the signature. Otherwise, $\\mathcal{B}$ is able to produce the signature $\\sigma=x$. $\\mathcal{A}$ promises to produce the forgery $(m^*,\\sigma^*)$. The thing to notice is that the message $m^*$ is new to all the messages in Sign Query, not in Hash Query. If $m^*=\\tilde{m}$, hits the trap, then the signature $\\sigma^*=x$ is what she wants. Claim:To produce a successful forgery, $\\mathcal{A}$ must have queried the hash oracle on $m^*$. With probability $1/q$, $m^*$ is the trap. (where $q$ is the number of hash queries) Bottomline: Hashed RSA (SHA-3)In practice, we let $H$ be the SHA-3 hash function. And we believe that SHA-3 acts like a random function. That’s the heuristic. On the one hand, it doesn’t make any sense, but one the other hand, it has served us well so far. There are no attacks against RSA+SHA-3, for example.","link":"/2022/08/04/mit6875-lec12/"},{"title":"「Cryptography-MIT6875」: Lecture 13","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Zero knowledge, definitions and example ZK Proof for QR Language. honest-verifier ZK malicious-verifier ZK In the following lectures, we will introduce much more than communicating securely. Complex Interactions: proofs, computations, games. Complex Adversaries: Alice or Bob, adaptively chosen. Complex Properties: Correctness, Privacy, Fairness. Many Parties: this class, MIT, the Internet. Today’s gist is the proof. It’s very different from the classic proofs. NP ProofsIn classic proofs, prover writes down a string(proof) and verifier checks. Little formally, it can be formalized as below. There is a claim(or theorem), e.g. 15627 is a prime. There are two parties, a prover and a verifier. The prover provide a proof to the verifier. The verifier can accept or reject the proof. NP Language DefinitionThe proofs of $\\mathcal{NP}$ are efficiently verifiable proofs. Nondeterministic Polynomial-time (NP) In computational complexity theory, $\\mathcal{NP}$ (nondeterministic polynomial time) is a complexity class used to classify decision problems.$\\mathcal{NP}$ is the set of decision problems for which the problem instances, where the answer is “yes”, have proofs verifiable in polynomial time by a deterministic Turing machine, or alternatively the set of problems that can be solved in polynomial time by a nondeterministic Turing machine. The prover has no computational bounce and needs to work hard to produce a proof. But the verifier can efficiently verify it in polynomial time. The $\\mathcal{NP}$ proof of a theorem is actually a set of strings which can be written down. Definition of Language Procedure: A language/decision procedure $\\mathcal{L}$ is simply a set of strings. So $\\mathcal{L}\\subseteq \\{0,1\\}^*$. The language is actually a set of strings which represent the true statements. Definition of $\\mathcal{NP}$-language: $\\mathcal{L}$ is an $\\mathcal{NP}$-language if there is a poly-time verifier $V$ where Completeness: True theorems have (short) proofs.For all $x\\in \\mathcal{L}$, there is a $\\texttt{poly}(|x|)\\texttt{-long}$ witness (proof) $w\\in\\{0,1\\}^*$ s.t. $V(x,w)=1$. Soundness: False theorems have no short proofs.For all $x\\notin \\mathcal{L}$, there is no witness. That is, for all polynomially long $w\\in\\{0,1\\}^*$ s.t. $V(x,w)=0$. Look at some examples. e.g.1 N=PQ LanguageTheorem: $N$ is a produce of two prime numbers. Prover: Give the two prime factors as proof. $=(P,Q)$. Verifier: Accept if and only if $N=PQ$ and $P,Q$ are primes. After interaction, Bob, the Verifier knows $N$ is a produce of two primes. Also, the two factors of $N$. e.g.2 QR LanguageTheorem: $y$ is a quadratic residue $\\mod N$ where $N=PQ$. Prover: Give the square root as proof. $=x$ Verifier: Accept if and only if $y=x^2\\mod N$. After interaction, Bob, the Verifier knows $y$ is a quadratic residue $\\mod N$. Also, the square root of $y$. e.g.3 Graph Isomorphism Language Graph Isomorphism Problem Two graphs $G_0$ and $G_1$ are isomorphic graphs if they have the same number of vertices, edges and also the same edge connectivity. The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic.The problem is not known to be solvable in polynomial time nor to be NP-complete, and therefore may be in the computational complexity class NP-intermediate. Theorem: Graphs $G_0$ and $G_1$ are isomorphic. Prover: Give the map of nodes as proof. $=\\pi$ Verifier: Accept iff the edge connectivity is retained after mapping. After interaction, Bob, the Verifier knows $G_0$ and $G_1$ are isomorphic. Also, the isomorphism. e.g.4 Hamiltonian Cycle Language Hamiltonian Path Problem and Hamiltonian Cycle Problem A Hamiltonian path is a path that visits each vertex exactly once.A Hamiltonian cycle is a closed loop on a graph where every vertex is visited exactly once. In the mathematical field of graph theory the Hamiltonian path problem and the Hamiltonian cycle problem are problems of determining whether a Hamiltonian path or a Hamiltonian cycle exists in a given graph. Both problems are NP-complete. Theorem: Graph $G$ has a Hamiltonian cycle. Prover: Give the Hamiltonian circle as proof. Verifier: Accept iff every edge exists. After interaction, Bob, the Verifier knows $G$ has a Hamiltonian cycle. Also, the Hamiltonian cycle itself. Every one of the above can be reduced to $\\mathcal{NP}$-Complete Problem. Besides, every proof above leaks some valuable information more than the proof itself. Is there any other way that the prover only tells the verifier whether $y$ is quadratic residue without telling the square root?(Example 2) The point is that we want to reveal the minimal information. Interactive ProofsBefore proceeding to Zero Knowledge Proof, let’s start with Interactive Proofs. Interactive Proof is not a monologue but a dialogue. There are two necessary new ingredients in Interactive Proofs. Two (Necessary) New Ingredients: Interaction: Rather than passively reading the proof, the verifier engages in a conversation with the prover. Randomness: The verifier is randomized and can make a mistake with a (exponentially small) probability. Note: The randomness of the verifier is necessary. If the verifier is completely deterministic as a function, the prover, since the first message she sends, she knows what the verifier is going to send back. So she can prepare her second message beforehand, then she can prepare her third message… It’s not interactive. The thing to point is that for the prover, she can do no more powerful thing than sending a single message. IdeaLet’s start with solving the Rubik’s Cube. Theorem: There is an $\\le k$ move solution to this cube. The idea is split the process of resolving in half. Interactive Proof: Prover: sends a “random” cube. Verify: sends a Challenge (0 or 1) Prover If Challenge 0: show $k/2$ moves If Challenge 1: show $k/2$ moves The point is that if the prover can do both challenges consistently (or many and many times), then there exists $k$ moves. We do not take it seriously since there are many flaws in the above protocol. We just get some inspiration from it. Definition of IPThe Interactive Proofs for a Language $\\mathcal{L}$ is as follows. There is a claim or a theorem. Two parties are interacting for a language $\\mathcal{L}$. Prover: Computational Unbounded Verifier: Probabilistic Polynomial-time Definition of $\\mathcal{IP}$-Language: $\\mathcal{L}$ is an $\\mathcal{IP}$-language if there is a probabilistic poly-time verifier $V$ where(There are three similar definitions) Definition in words. Completeness: If $x\\in \\mathcal{L}$, $V$ always accepts. Soundness: If $x\\notin \\mathcal{L}$, regardless of the cheating prover strategy, $V$ accepts with negligible probability. Rewrite with probability. Completeness: If $x\\in \\mathcal{L}$, $\\operatorname{Pr}[(P,V)(x)=\\texttt{accept}]=1. $ Soundness: If $x\\notin \\mathcal{L}$, there is a negligible function $negl$ s.t. for every $P^*$, $\\operatorname{Pr}[(P^*,V)(x)=\\texttt{accept}]=negl(\\lambda).$ Relax the probability: Equivalent as long as $c-s\\ge 1/poly(\\lambda)$ Completeness: If $x\\in \\mathcal{L}$, $\\operatorname{Pr}[(P,V)(x)=\\texttt{accept}]\\color{blue}{\\ge c}.$ Soundness: If $x\\notin \\mathcal{L}$, there is a negligible function $negl$ s.t. for every $P^*$, $\\operatorname{Pr}[(P^*,V)(x)=\\texttt{accept}]\\color{blue}{\\le s}$. Completeness is a property of the protocol when $P$ and $V$ are both honest. Soundness is a property of the protocol when $P$ is dishonest. So soundness is also a property of the verifier.The verifier has to be sound because it has to work against an arbitrary $P$. IP for QR LanguageWe can give the interactive proof for QR. $\\mathcal{L}=\\{(N,y): y \\textrm{ is a quadratic residue}\\mod N\\}$ Interactive Proof: Prover: send a random square $s$ Verifier: flip a coin $b$ (randomness) Prover If $b=0$, send $z=r$ If $b=1$, send $z=rx$ where $y=x^2 \\mod N$. Verifier: Accept iff $z^2=sy^b \\mod N$. Completeness: Claim: If $(N,y)\\in \\mathcal{L}$, then the verifier accepts the proof with probability 1. Proof: $z^2=(rx^b)^2=r^2(x^2)^b=sy^b$. So the verifier’s check passes and he accepts. Soundness: Claim: If $(N,y)\\notin \\mathcal{L}$, then for every cheating prover $P^*$, the verifier accepts with probability at most $1/2$. Proof: The challenge is random and the randomness is over the coin. If the probability equals $1/2$, it means that the cheating prover $P^*$ can only solve one of the challenges, Challenge 0. Suppose the verifier accepts with probability $\\ge1/2$. Then, there is some $s\\in \\mathbb{Z}_N^*$, the prover is able to pass both challenges. So the prover can produce both $z_0$ and $z_1$ beforehand. $z_0:z_0^2=s\\mod N$ $z_1:z_1^2=sy\\mod N$ This means $(z_1/z_0)^2=y\\mod N$, which tells us that $(N,y)\\in \\mathcal{L}$. (Contradiction) Moreover, we can make the probability of soundness negligible. Just *repeat the procedure sequentially $\\lambda$ times. * Claim: If $(N,y)\\notin \\mathcal{L}$, then for every cheating prover $P^*$, the verifier accepts with probability at most $(\\frac{1}{2})^\\lambda$. Zero Knowledge ProofActually, the interactive proof for QR language is Zero-Knowledge. But what dose zero-knowledge mean? After the interaction, $V$ knows: The theorem is true; and A view of the interaction (=transcript + coins of $V$)The transcript is all the messages going back and forth. $P$ gives zero knowledge to $V$:When the theorem is true, the view gives $V$ nothing that he couldn’t have obtained on his own without interacting with $P$. That means the view is not going to give any new knowledge that $V$ couldn’t have it on his own. We can consider the knowledge as some stuff that we cannot generate in probabilistic poly-time on our own.If we are given something we can generate by ourselves, it’s zero-knowledge. $(P,V)$ is zero-knowledge if $V$ can generate his view of the interaction all by himself in probabilistic polynomial time. $(P,V)$ is zero-knowledge if $V$ can “simulate” his view of the interaction all by himself in probabilistic polynomial time. We formalize it by the Simulation Paradigm. The Simulation ParadigmThere are two indistinguishable distributions. The real view of the interaction: $\\texttt{view}_V(P,V)=(s,b,z$).It consists of Transcript: $(s,b,z)$ Coins: $b$ The simulated view: $\\texttt{sim}_S(s,b,z)$ We can give the zero-knowledge definition by the simulation paradigm. Zero-knowledge Definition (for honest V) (Honest-verifier) Zero-knowledge Definition: An Interactive Protocol $(P,V)$ is zero-knowledge for a language $\\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\\in \\mathcal{L}$, the following two distributions are indistinguishable. $\\texttt{view}_V(P,V)$ $\\texttt{sim}_S(x,1^\\lambda) $ $(P,V)$ is zero-knowledge interactive protocol if it is complete, sound and zero-knowledge. Completeness is a property of the protocol when $P$ and $V$ are both honest. Soundness is a property of the protocol when $P$ is dishonest. So it’s also a property of the verifier.The verifier has to be sound because it has to work against an arbitrary $P$. Zero-knowledge is a property against the verifier. Actually, we give the definition of zero-knowledge against a honest verifier. We’ll refine it for any arbitrary verifier in the next section. There are some analogous definitions of honest-verifier zero-knowledge. Perfect (Honest-verifier) Zero-knowledge: An Interactive Protocol $(P,V)$ is perfect (honest-verifier) zero-knowledge for a language $\\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\\in \\mathcal{L}$, the following two distributions are identical. $\\texttt{view}_V(P,V) $ $\\texttt{sim}_S(x,1^\\lambda) $ Statistical (Honest-verifier) Zero-knowledge: An Interactive Protocol $(P,V)$ is statistical (honest-verifier) zero-knowledge for a language $\\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\\in \\mathcal{L}$, the following two distributions are statistically indistinguishable. $\\texttt{view}_V(P,V)$ $\\texttt{sim}_S(x,1^\\lambda) $ Computational (Honest-verifier) Zero-knowledge: An Interactive Protocol $(P,V)$ is computational (honest-verifier) zero-knowledge for a language $\\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\\in \\mathcal{L}$, the following two distributions are computationally indistinguishable. $\\texttt{view}_V(P,V) $ $\\texttt{sim}_S(x,1^\\lambda)$ QR Protocol is (honest-V) ZKClaim: The QR protocol is (honest-verifier) perfect zero knowledge. Simulator S works as follows: First pick a random bit $b$. Pick a random $z\\in \\mathbb{Z}_N^*$. Compute $s=z^2/y^b$. Output $(s,b,z)$. The simulator can sort of permute things which is offline. And the verifier is online in the real world. Lemma: The simulated transcript is identically distributed as the real transcript in the interaction $(P,V)$. Proof of Lemma: The thing we need to prove is that the distribution of every component in the transcript is identical. Real transcript: $\\texttt{view}_{V}(P,V)=(s,b,z) $ $s$ is square as same as random $b$ is random coin(since the honest verifier) $z=\\sqrt{sy^b}$ is random Simulated transcript: $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,z) $ $s=z^2/y^b$ is square as same as random ($(N,y)\\in \\mathcal{L}$ so $y$ is square)Besides, the distribution of $s$ hides $b$ perfectly. $b$ is random $z$ is random QED Actually we only prove the QR protocol is honest-verifier Zero-knowledge. When the theorem is true, the view gives the honest verifier $V$ nothing that $V$ couldn’t have it on his own. What if $V$ is NOT honest ? Note: The following malicious part is actually lectured in the Lecture 14.I put it here for the continuous and complete description. Zero-knowledge Definition (for malicious V)In real world, the verifier could be malicious that he can do anything he wants. We hope the view also gives zero-knowledge to the malicious verifier $V^*$. The definition for honest verifier undermines the definition for malicious verifier. Recap: A language $\\mathcal{L}$ is actually a set of strings which represent true statements.The view of $V^*$ is the transcripts and the coins, which contains all the messages going back and forth. Refine the definitions for malicious verifier $V^*$. Perfect Zero-knowledge Definition: An interactive Protocol $(P,V)$ is perfect zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are identical: $\\texttt{view}_{V^*}(P,V^*) $ $\\texttt{sim}_S(x,1^\\lambda)$ Statistical Zero-knowledge Definition: An interactive Protocol $(P,V)$ is statistical zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are statistical indistinguishable: $\\texttt{view}_{V^*}(P,V^*) $ $\\texttt{sim}_S(x,1^\\lambda) $ Computational Zero-knowledge Definition: An interactive Protocol $(P,V)$ is computational zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are computationally indistinguishable: $\\texttt{view}_{V^*}(P,V^*)$ $\\texttt{sim}_S(x,1^\\lambda) $ QR Protocol is (malicious-V) ZKThe ZK proof for QR language we gave above is actually honest-verifier zero-knowledge. In this section, we consider the malicious-verifier zero-knowledge for QR Protocol. Recap the zero-knowledge proof for QR Language. $\\mathcal{L}=\\{(N,y):y \\textrm{ is a quadratic residue }\\mod N\\}$. The view of a malicious verifier $V^*$ is $\\texttt{view}_{V^*}(P,V^*)=(s,b,z)$. When $V^*$ obtains the $s$, the only power that $V^*$ has is to choose the $b$ in a bizarre fashion rather than random. So the distribution of $b$ is not random. Let $b=V^*(s)$ denote the bizarre thing generated by $V^*$ after receiving $s$. The simulator $S$ only gets an instance of $(N,y)$ and wants to generate $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,z)$. In order to produce the same distribution of $b$, the simulator $S$ needs to interact with the malicious verifier $V^*$. It’s sort of a prover which also needs to interact with the verifier. The only distinction is that the prover $P$ has to be online to answer the challenge from the verifier $V^*$ and the simulator $S$ can be offline with the goal of generating the view. Claim: The QR protocol is (malicious-verifier) perfect zero knowledge. Simulator S works as follows: First set $s=z^2/y^b$ for a random $z$ and a random $b$ and “feed” $s$ to $V^*$ Let $b'=V^*(s)$. (generated by $V^*$ in any bizarre fashion rather than random) If $b=b’$, output $(s,b,z)$ and stop. Otherwise, go back to step 1 and repeat. (also called “rewinding”) Lemma: $S$ runs in expected polynomial-time. When $S$ outputs a view, it is identical to the view of $V^*$ in a real execution. Lemma 1 is well proven. The probability of terminating in one iteration is $1/2$ since $b$ is random. So the expected iterations is $2$. Proof of Lemma 2: We need to prove that the distribution of every component is identical. Real transcript: $\\texttt{view}_{V^*}(P,V^*)=(s,b,z) $ $s$ is square as same as random $b$ is generated in any bizarre way, i.e. $b=V^*(s)$. $z=\\sqrt{sy^b}$ is random Simulated transcript: $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,y) $ $s=z^2/y^b$ is square as same as random ($(N,y)\\in \\mathcal{L}$ so $y$ is square)Besides, the distribution of $s$ hides $b$ perfectly. $b$ has the same distribution with $b’=V^*(s)$. $z$ is random QED So far we have proven the QR protocol is (malicious-verifier) zero-knowledge. What made ZK Proof possible ? Each statement had multiple proofs of which the prover chooses one at random.(They are $(s,\\sqrt{s})$ and $(s,\\sqrt{sy})$ as for the QR protocol.) Each such proof is made of two parts (as shown above): seeing either one on its own gives the verifier no knowledge ; seeing both imply 100% correctness. (That’s the completeness) Verifier choose to see either part, at random.(Verifier can choose to see $\\sqrt{s}$ or $\\sqrt{sy}$ at random)The prover’s ability to provide either part on demand convinces the verify. (That’s the soundness)The prover has to prepare both part correctly, otherwise there is a half chance that he’ll get caught.","link":"/2022/08/09/mit6875-lec13/"},{"title":"「Cryptography-MIT6875」: Lecture 14","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Perfect ZK Proof for QR Language Perfect ZK Proof for Graph Isomorphism Comp. ZK Proof for 3Coloring All NP Languages have Comp. ZK Proofs Commitment Schemes Note: The first two sections are also posted at the end of last Lecture, just for the continuous and complete description. ZK DefinitionIn the previous Lecture is introduced the definition of honest-verifier Zero-knowledge. When the theorem is true, the view gives the honest verifier $V$ nothing that $V$ couldn’t have it on his own. In real world, the verifier could be malicious that he can do anything he wants. Refine the ZK definitions for any verifier $V^*$. Recap: A language $\\mathcal{L}$ is actually a set of strings which represent true statements.The view of $V^*$ is the transcripts and the coins, which contains all the messages going back and forth. Perfect Zero-knowledge Definition: An interactive Protocol $(P,V)$ is perfect zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are identical: $\\texttt{view}_{V^*}(P,V^*) $ $\\texttt{sim}_S(x,1^\\lambda)$ Statistical Zero-knowledge Definition: An interactive Protocol $(P,V)$ is statistical zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are statistical indistinguishable: $\\texttt{view}_{V^*}(P,V^*) $ $\\texttt{sim}_S(x,1^\\lambda) $ Computational Zero-knowledge Definition: An interactive Protocol $(P,V)$ is computational zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are computationally indistinguishable: $\\texttt{view}_{V^*}(P,V^*)$ $\\texttt{sim}_S(x,1^\\lambda) $ Perfect ZK Proof for QR LanguageRecap the zero-knowledge proof for QR Language. $\\mathcal{L}=\\{(N,y):y \\textrm{ is a quadratic residue }\\mod N\\}$. We proved in last Lecture that the QR protocol is honest-verifier zero knowledge. Now we only consider the malicious-verifier zero knowledge. The QR protocol is malicious-verifier zero knowledge. The view of a malicious verifier $V^*$ is $\\texttt{view}_{V^*}(P,V^*)=(s,b,z)$. When $V^*$ obtains the $s$, the only power that $V^*$ has is to choose the $b$ in a bizarre fashion rather than random. So the distribution of $b$ is not random. Let $b=V^*(s)$ denote the bizarre thing generated by $V^*$ after receiving $s$. The simulator $S$ only gets an instance of $(N,y)$ and wants to generate $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,y)$. In order to produce the same distribution of $b$, the simulator $S$ needs to interact with the malicious verifier $V^*$. It’s sort of a prover which also needs to interact with the verifier. The only distinction is that the prover $P$ has to be online to answer the challenge from the verifier $V^*$ and the simulator $S$ can be offline with the goal of generating the view. Claim: The QR protocol is (malicious-verifier) perfect zero knowledge. Simulator S works as follows: First set $s=z^2/y^b$ for a random $z$ and a random $b$ and “feed” $s$ to $V^*$ Let $b'=V^*(s)$. (generated by $V^*$ in any bizarre fashion rather than random) If $b=b’$, output $(s,b,z)$ and stop. Otherwise, go back to step 1 and repeat. (also called “rewinding”) Lemma: $S$ runs in expected polynomial-time. When $S$ outputs a view, it is identical to the view of $V^*$ in a real execution. Lemma 1 is well proven. The probability of terminating in one iteration is $1/2$ since $b$ is random. So the expected iterations is $2$. Proof of Lemma 2: We need to prove that the distribution of every component is identical. Real transcript: $\\texttt{view}_{V^*}(P,V^*)=(s,b,z)$ $s$ is square as same as random $b$ is generated in any bizarre way, i.e. $b=V^*(s)$. $z=\\sqrt{sy^b}$ is random Simulated transcript: $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,y)$ $s=z^2/y^b$ is square as same as random ($(N,y)\\in \\mathcal{L}$ so $y$ is square)Besides, the distribution of $s$ hides $b$ perfectly. $b$ has the same distribution with $b’=V^*(s)$. $z$ is random QED So far we have proven the QR protocol is (malicious verifier) zero-knowledge. ZK Proof for Graph Isomorphism Recap the Graph Isomorphism Problem:Two graphs $G_0$ and $G_1$ are **isomorphic graphs** if they have the same number of vertices, edges and also the same edge connectivity. The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. Now we describe the zero-knowledge proof for the claim that $G$ and $H$ are isomorphism graphs. The interaction protocol is as shown below: Prover has the knowledge that she knows a map $\\pi$ such that $H=\\pi(G)$. ZK Proof for Graph-iso: Prover: choose a random permutation $\\rho$ generate a new random graph $K$ such that $K=\\rho(G)$. send the graph $K$ to verifier Verifier: generate a random challenge bit $b$ Prover: has to answer the challenge bit If $b=0$: Prover sends the map $\\pi_0=\\rho$ such that $K=\\pi_0(G)$(prove that she can map $G$ → $K$) If $b=1$: Prover sends the map $\\pi_1=\\pi\\circ \\rho^{-1}$ such that $H=\\pi_1(K)$(prove that she can map $K$ → $H$) Completeness: Completeness is a property of the protocol when $P$ and $V$ are both honest. We prove that the verifier will pass the equations and accept it. If $b=0$: check $K=\\pi_0(G)=\\rho(G)=K$. If $b=1$: check $H=\\pi_1(K)=\\pi\\rho^{-1}(K)=\\pi(G)=H$ Soundness: Soundness is a property of the protocol when $P$ is malicious. The verifier has to be sound because it has to work against an arbitrary $P$. Suppose $G$ and $H$ are non-isomorphic, and the prover could answer both the verifier challenges. Then the prover both prepares the $\\pi_0$ and $\\pi_1$ such that $K=\\pi_0(G)$ and $H=\\pi_1(K)$.(Otherwise, she’ll be caught in half chance.) In other words, the prover can get $H=\\pi_0\\circ\\pi_1(G)$, a contradiction. QED. Perfect Zero Knowledge: The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. The view of an arbitrary verifier $V^*$ is $\\texttt{view}_{V^*}(P,V^*)=(K,b,\\pi_b)$. When $V^*$ receives the graph $K$, the only power that $V^*$ has is to choose the $b$ in a bizarre fashion rather than random. So the distribution of $b$ is not random. Let $b=V^*(K)$ denote the bizarre thing generated by $V^*$ after receiving $K$. The simulator $S$ only gets an instance of $(G,H)$ and wants to generate $\\texttt{sim}_S((G,H),1^\\lambda)=(K,b,\\pi_b)$. Simulator S works as follows: Pick a random permutation $\\rho$ and a random $b$. Generate a graph $K$ such that If $b=0$: $K=\\rho(G)$.Let $\\pi_0=\\rho$. If $b=1$: $K=\\rho(H)$, i.e. $H=\\rho^{-1}(K)$.Let $\\pi_1=\\rho^{-1}$. Feed the graph $K$ to verifier $V^*$ Let $b’=V^*(K)$ If $b=b’$, output $(K, b, \\pi_b)$ and stop. Otherwise, go back to step 1 and repeat. Intuition of my proof: It sort of split the $K$ into two intermediate points, $K_0$ and $K_1$.The prover has the knowledge that could map $G$ to $K$ and map $K$ to $H$.The point is that the simulator can simulate the knowledge that could map $G$ to $K_0$ and map $K_1$ to $H$. Lemma: $S$ runs in expected polynomial-time. When $S$ outputs a view, it is identical to the view of $V^*$ in a real execution. Lemma 1 is well proven. The probability of terminating in one iteration is $1/2$ since $b$ is random. So the expected iterations is $2$. Proof of Lemma 2: We need to prove that the distribution of every component is identical. Real transcript: $\\texttt{view}_{V^*}(P,V^*)=(K,b,\\pi_b)$ $K$ is a random graph since the random $\\rho$. $b$ is generated in any bizarre way, i.e. $b=V^*(s)$. $\\pi_b$ is a random map If $b=0$: $\\pi_0=\\rho$. → random map If $b=1$: $\\pi_1=\\pi\\circ \\rho^{-1}$. → random map. Simulated transcript: $\\texttt{sim}_S((G,H),1^\\lambda)=(K,b,\\pi_b)$ $\\pi_b$ is a random map since $\\rho$ is random. If $b=0$: $\\pi_0=\\rho$. If $b=1$: $\\pi_1=\\rho^{-1}$. $K$ is a random graph. If $b=0$, $K=\\pi_0(G)=\\rho(G)$ If $b=1$, $K=\\pi_1(H)=\\rho^{-1}(H)$ $b$ has the same distribution with $b’=V^*(s)$. QED Efficient Prover (given a Witness)So far we keep saying that the prover is unbounded and the verifier is ppt. But there are no unbounded people. In both these protocols above, the (honest) prover is actually polynomial-time given the NP witness （the square root of $y$ in the case of QR, and the isomorphism in the case of graph-iso). Therefore, the prover and the verifier can both be polynomial-time. The only difference between the (honest) prover and the verifier is that the prover knows some privileged knowledge, a witness or a solution to a problem, that the verifier dose not know. That’s the common way to reduce the zero-knowledge proofs. The thing to point is that soundness is nevertheless against any, even computationally unbounded, prover $P^*$. All NP languages have Comp. ZK ProofsWe shows two languages with perfect ZK proofs, the QR protocol and the Graph-iso protocol. Do all NP languages have perfect ZK proofs ? The theorem[Fortnow’89, Aiello-Hastad’s 87] answered NO, unless bizarre stuff happens in complexity theory. Technically, the polynomial hierarchy collapses. That is NP = P. Nevertheless, we can relax the question. Do all NP languages have ZK proofs ? Theorem: [Goldreich-Micali-Wigderson’87] Assuming one-way permutations exist, all of NP has computational zero-knowledge proofs. It means that every language of NP problem has computational zero-knowledge proof given a witness. Moreover, the assumption can be relaxed to one-way functions. This theorem is amazing and it tells us that everything can be proved (in the sense of Euclid) can be proven in zero knowledge! How to prove the theorem ? We cannot prove every NP problem one by one. Luckily, we can prove NP-Complete Problem to which every other problem in NP can be reduced. It turns out that there are a whole of complete problems. There is a list of 20 odd problems that came up with in the 70s already and this list keeps increasing. So NP-complete problems is sort of a wealth. We are going to pick the Graph Coloring Problem and prove it has Comp. ZK Proofs. ZK Proof for 3ColoringHere is the Graph 3Coloring Problem. Given a graph and three colors, red blue and green, you’re supposed to assign colors to every vertex such that no two adjacent vertexes have the same color. (or every two adjacent vertexes have different colors) Before proceeding to the zero-knowledge proof of the three-coloring, let’s introduce the lead-box model. The Lead-box ModelThe lead-box model is as shown below. Lead-box Model: The sender Alice has a bit $b$. Commit to $b$: put $b$ in a lead-box and locks it, and send the box to receiver. Open $b$: send $b$ together with the key. Then the receiver Bob can check the thing in box is what Alice claims. The lead-box above should be hiding and binding $b$. Properties: Hiding means that the lead-box should completely hide $b$. Blinding means that the sender shouldn’t be able to open to $1-b$. Once the Alice sends the box to Bob, she should not be able to change her mind about what’s inside the box. That’ blinding. That’s a commitment. It can be used for computational zero-knowledge. It can also be used to ensure fairness. We will later show how to implement such a lead-box (as a commitment protocol) using one-way permutations. ZK Proof with Lead-box: Part IThe language $\\mathcal{L}$ is that the graph $G$ is 3-colorable. Given a 3-colorable witness (solution), it can be proven in computational zero-knowledge. The prover is given the graph $G$ and the 3-colorable witness. The verifier is given the graph $G$. The interaction of ZK proof is as shown below. Interactive Protocol for 3COL: Prover: come up with a random permutation of the colors, $\\rho:V\\rightarrow \\{R,G,B\\}$. The color of every vertex is masked by the random permutation. Prover: commit to (the color of) every vertex. Verifier: pick a random edge $(i,j)$ Prover: open $\\rho(i)$ and $\\rho(j)$ Verifier: check Check the openings that $\\rho(i)$ and $\\rho(j)$ are what Alice claims. Chek the $\\rho(i),\\rho(j)\\in \\{R,G,B\\}$ Check: $\\rho(i)\\ne \\rho(j)$ The completeness is well proven. Soundness: Soundness is the property of the protocol against dishonest prover $P$. If the graph is not 3COL, in every 3-coloring (that $P$ commits to), there is some edge whose end-points have the same color. $V$ will catch this edge and reject with probability $\\ge 1/|E|$. In one time: the verifier accepts with probability $\\le1-1/|E|$. Repeat $|E|\\cdot \\lambda$ times: he verifier accepts with probability $\\le (1-1/|E|)^{|E|\\cdot \\lambda}\\le 2^{-\\lambda}$. which is negligible. QED Moreover, the proof is Computational Zero-knowledge. The key reason of zero-knowledge is the prover commits to all colors (of the vertices) but only open two colors (of the vertexes). It leaks nothing to the verifier since the colors have been randomly permuted. So the prover gives zero-knowledge to the verifier. We will elaborate the Comp. Zero-knowledge in the following Part II. More analysis into the first message(the message in the lead-box). If the first message dose not exist, the proof is not sound. The malicious prover can always answer “red” and “blue” because the verifier cannot check what Alice claims without the commitment. If the first message are not in the lead-box, the proof is not zero-knowledge. Commitment SchemesThe lead-box is indeed a commitment protocol. The Commitment Protocol $(S,R)$ works as follows. Commitment Protocol $(S,R)$: There are two parties, sender $S$ and receiver $R$. Sender $S$ commits to a bit $b$, so the protocol is instanced to $(S(b,1^\\lambda),R(1^\\lambda))$. Let $\\texttt{dec}$ be the sender’s output, decommitment. Let $\\texttt{com}$ be the receiver’s output, commitment. Sender $S$ opens $b$ $S$ sends $b$ together with $\\texttt{dec}$. Receiver $R$ checks $b$ using $\\texttt{dec}$. Properties of Commitment Protocol: Completeness: $R$ always accepts in an honest execution. Computational Hiding: For every possibly malicious (PPT) $R^*$, $\\texttt{view}_{R^*}(S(0),R^*)\\approx_c\\texttt{view}_{R^*}(S(1),R^*)$ (the view of $R^*$ is $(\\texttt{com},b,\\texttt{dec})$) Perfect Binding: For every possibly malicious $S^*$, let $\\texttt{com}$ be the receiver’s output in an execution of $(S^*, R)$. There is no pair of decommitments $(\\texttt{dec}_0,\\texttt{dec}_1)$ s.t. $R$ accepts both $(\\texttt{com},0,\\texttt{dec}_0)$ and $(\\texttt{com},1,\\texttt{dec}_1)$. Completeness is the property of the commitment protocol when $S$ and $R$ are honest. Computational Hiding is the property of commitment protocol against malicious $R^*$. Perfect Binding is the property of commitment protocol against malicious $S^*$. A Commitment Scheme from any OWPThere is a commitment scheme starting from any OWP as follows. Commitment Protocol $(S,R)$ from OWP: Sender $S$ commits to bit $b$ Pick a random $r$ as the decommitment, $\\texttt{dec}=r$. Compute the commitment $\\texttt{com}=(f(r),HCB(r)\\oplus b)$. Send the commitment to $R$. Sender $S$ opens $b$ Send $(b,r)$ to $R$, $r$ as the $\\texttt{dec}$. Receiver $R$ checks $b$ using $\\texttt{dec}$. Let $\\texttt{com}=(x,y)$ Check $f(r)=x$. Check $HCB(r)\\oplus b=y$. This commitment scheme has completeness, comp. hiding and perfect binding. Properties of Commitment Scheme: Completeness is well proven. Computational Hiding can be proven by the hardcore bit property. As for any arbitrary receiver $R^*$, it is hard to compute $HCB(r)$ given $f(r)$ since $f$ is OWP. We say that the hardcore bit of $f(r)$ computational hides the bit $b$. The point in Perfect Binding is that $f$ is a permutation. If $x=f(r)$ is fixed, then $r$ is fixed. There is no other $r’$ such that $f(r’)=f(r)$. Then $HCB(r)$ is fixed, and the bit $b$ is fixed since $y$ is fixed. That’s perfect binding. ZK Proof with Commitment: Part IIReplace the lead-box with the commitment protocol. Commitment to $\\rho(k)$ is $\\texttt{com}(\\rho(k);r_k)$ where $r_k$ is the random. Decommitment to $\\rho(k)$ is $r_k$. Why is this protocol zero-knowledge? Comp. Zero-knowledge: We dive into a malicious-verifier zero-knowledge. What can an arbitrary verifier $V^*$ do ? He can see all these commitments to He can pick an arbitrary edge in bizarre fashion. Real transcript of malicious $V^*$: $\\texttt{view}_{V^*}(P,V^*)=\\left(\\{\\texttt{com}_k(\\rho(k);r_k)\\}_{k=1}^{n},(i,j),(\\texttt{dec}_i,\\texttt{dec}_j)\\right)$. The commitment to every color (of the vertex): $\\texttt{com}_k(\\rho(k);r_k)$ The edge chosen in bizarre fashion: $(i,j)=V^*(\\{\\texttt{com}_k\\})$ The decommitment: $(\\texttt{dec}_i,\\texttt{dec}_j)$. Simulator S works as follows: First pick a random edge $(i^*,j^*)$.Color this edge with random, different colors.Color all other edges red. Feed the commitments of the colors to $V^*$ and get edge $(i,j)=V^*(\\{\\texttt{com}_k\\})$. If $(i,j)=(i^*,j^*)$, output the commitments and the openings $r_i$ and $r_j$ as the simulated transcript. If $(i,j)\\ne(i^*,j^*)$, go back to step 1 and repeat. The key reason why it works is that the prover commits to all colors (of the vertices) but only open two colors (of the vertexes).It leaks nothing to the verifier since the colors have been randomly permuted. So the prover gives zero-knowledge to the verifier. Lemma: Assuming the commitments is hiding, $S$ runs in expected polynomial-time. When $S$ outputs a view, it is computationally indistinguishable to the view of $V^*$ in a real execution. Proof of Lemma 2: Analysis the distribution of real transcript and simulated transcript. Real transcript: $\\texttt{view}_{V^*}(P,V^*)=\\left(\\{\\texttt{com}_k\\}_{k=1}^{n},(i,j),(\\texttt{dec}_i,\\texttt{dec}_j)\\right)$ The commitments are computationally random since the computational hiding property. The distribution of $(i,j)$ is in bizarre fashion. The decommitments are random. Simulated transcript: $\\texttt{sim}_{S}(G)=\\left(\\{\\texttt{com}_k\\}_{k=1}^{n},(i^*,j^*),(\\texttt{dec}_i,\\texttt{dec}_j)\\right)$ The commitments are computationally random since the computational hiding property. The distribution of $(i^*,j^*)$ is same as $(i,j)=V^*(\\{\\texttt{com}_k\\})$. The decommitments are random. Examples of NP Assertions My public key is well-formed.e.g. in RSA, prove the public key is $N$, a product of two primes together with an $e$ that is relatively to $\\varphi(N)$. Encrypted bitcoin (or Zcash): “I have enough money to pay you.”e.g. I will publish an encryption of my bank account and prove to you that my balance is $\\ge \\$X$. Running programs on encrypted inputs: Given $Enc(x)$ and $y$, prove that $y=\\textrm{PROG}(x)$.","link":"/2022/08/11/mit6875-lec14/"},{"title":"「Cryptography-MIT6875」: Lecture 15","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Sequential vs Parallel Repetition: reduce soundness error Proof of Knowledge PoK of DLOG Non-Interactive ZK(NIZK) NIZK in The Random Oracle Model NIZK for 3COL NIZK in The Common Random String Model (Lecture 16) RecapRecap NP Proofs. Give NP Proofs for the NP-complete problem of graph 3-coloring. Prover $P$: has a witness, the 3-coloring of $G$. $P$ gives the proof, the solution to 3-coloring of $G$, to $V$. Verifier $V$ checks: only 3 colors are used any two vertices connected by an edge are colored differently. The verify learned the graph $G$ is 3-colorable and the 3-coloring solution. So NP proofs reveal too much information. With Zero-knowledge (Interactive) Proofs, the verifier can only learns the graph $G$ is 3-colorable without knowing the solution. Prover $P$: permute the colors commit to each color send all the commitments to the verifier. Verifier $V$: pick a random edge Prover $P$: open the vertices of the edge. Verifier $V$ checks the openings &amp; the colorings of two vertices are different. Besides, we proved the 3COL Protocol is completeness, soundness and zero-knowledge in previous blog. Completeness: For every $G\\in 3COL$, $V$ accepts $P$’s proof. Soundness: For every $G\\notin 3COL$ and any cheating $P^*$, $V$ rejects $P^*$’s proof with probability $\\ge 1-neg(n)$. Zero-knowledge: For every cheating $V^*$, there is a PPT simulator $S$ such that for every $G\\in 3COL$, $S$ simulates the view of $V^*$. Sequential vs Parallel RepetitionThe 3COL protocol has a large soundness error of $1-1/|E|$, the probability that $V$ accepts even though $G\\notin 3COL$. Reducing Soundness Error Theorem: Sequential Repetition reduces soundness error for interactive proofs, and preserves the ZK property. But it brings about the problem that it costs a lot of rounds. An alternative way is parallel repetition. Theorem [Goldreich-Krawczyk’90]: Parallel Repetition also reduces soundness error for interactive proofs. It is also honest-verifier ZK, but dose not, in general, preserve the ZK property. Note: Preserving the ZK property in general means that it is ZK against malicious verifier. There is an intuitive interpretation to the theorem.[Goldreich-Krawczyk’90] The interaction in parallel repetition is $P$ sends all first message in parallel and $V$ response at once with all second messages … If $V$ is honest verifier, he indeed dose not look at the commitments, and just picks the random edges independently, which is the same with the sequential repetition. But when $V^*$ is malicious verifier, there is no reason that $V^*$ picks the edge independently.$V^*$ can apply a giant hash function and do some bizarre thing to pick these dependent edges. Intuitively, it’s harder to simulate such a thing. The simulator’s strategy in parallel repetition: $S$ feeds some made up first messages to $V^*$. $V^*$ picks the edges in bizarre manners. $S$ only can answer exactly one challenge. The key reason is the challenge space is exponentially large and the probability of hitting that made up challenge is negligible.So this simulation strategy goes down the drain. This theorem tells that some protocols in parallel repetition is not zero-knowledge against malicious verifier. And the following theorem tells us that the parallel repetition of 3COL protocol is not zero-knowledge if we run it in many and many times in parallel. Theorem [Holmgren-Lombardi-Rothblum’s21]: Parallel Repetition of the (Goldreich-Micali-Wigderson) 3COL protocol is not zero-knowledge. Fortunately, we have zero-knowledge protocols in const rounds with exponentially small soundness error, rather in a million rounds. Theorem [Goldreich-Kahan’95]: There is a constant-round ZK proof system for 3COL (will exponentially small soundness error), assuming discrete logarithms are hard. Proofs of KnowledgeSo far, we focus on the decision problem: $y\\in \\mathcal{L}$ or $y\\notin \\mathcal{L}$. (e.g. $y$ is quadratic residue $\\mod N$ or it is not.) Here is a different scenario that Alice has the knowledge, the discrete log of $y$ assuming $g$ is a generator. And Alice wants to convince Bob the discrete log of $y$ always exists. In this scenario the prover wants to convince the verifier that she knows a solution to a problem, e.g. that she knows the discrete log of $y$. It is difficult to formulate it as the decision problem. It is Proof of Knowledge. Likewise, we can define the completeness, soundness and zero-knowledge. Completeness: When Alice and Bob run the protocol where Alice has input $x$, Bob outputs accept. Soundness: How to define soundness that Alice dose not have the knowledge ?It is difficult to formulate the leak of knowledge. Zero-knowledge: There is a simulator that, given only $y$, outputs a view of Bob that is indistinguishable from his view in an interaction with Alice. ExtractorThe main idea of Goldreich is that if Alice knows $x$, there must be a way to “extract it from her”. It’s not about putting diodes on her brain. [*diode 二极管] It’s sort of talking to Alice. Definition of Proof of Knowledge: For any cheating$P^*$, if the prover can convince the verifier that the discrete log of $y$ always exist such that $\\operatorname{Pr}[\\langle P^*,V\\rangle(y)=\\textrm{accept}]\\ge \\varepsilon$, then there exists an extractor $E$ such that $\\operatorname{Pr}[E^{P^*}(y)=x \\text{ s.t. }y=g^x]\\ge \\varepsilon'\\approx \\varepsilon$. The extractor is indeed the expected ppt adversary. The definition of PoK is proposed in On Defining Proofs of Knowledge by Mihir Bellare and Oded Goldreich. We will not dig into the definition but give an example of PoK. ZK Proof of Knowledge of Discrete Log.The protocol is as follows. ZK Proof of Knowledge of DLOG: Prover: Pick a random $r$ and send $z=g^r$ to Verifier. Verifier: Pick a random challenge $c$ Prover: Answer the challenge If $c=0$: send $s=r$ If $c=1$: send $s=r+x$ Verifier: Accept iff $g^s=z\\cdot y^c$. The above protocol is completeness, soundness and zero-knowledge. The completeness and zero-knowledge is well proven. Completeness: If the prover has the discrete log of $y$, the verifier accepts with probability 1. $g^s=g^{r+cx}=g^r\\cdot (g^{x})^c=z\\cdot y^c$ Zero-knowledge: The real view of $V^*$ is $\\texttt{view}_{V^*}=(z,c,s)$ The simulator works as follows Generate $z=g^s/y^c$ for a random $s$ and a random $c$. Feed $z$ to verifier and get the challenge $c^*=V^*(z)$. If $c^*=c$, output as the simulated transcript. If $c^*\\ne c$, back to step 1 and repeat. The simulated view is identical to the view in real execution. Soundness: The key is to construct an extractor by the contradiction. If the protocol is of soundness, the cheating prover $P^*$ can convince the verifier with probability $1/2$. Assume for the contradiction that $P^*$ convinces the verifier with probability $\\ge 1/2+1/poly$. Then the prover $P^*$ should prepare for both challenges. It’s easy to extract the discrete log of $y$ form $P^*$. Runs $P^*$ with $c=0$ and gets $s_0$. Rewind $P^*$ to the first message. Runs $P^*$ with $c=1$ and gets $s_1$. By contradiction, $g^{s_0}=z$ and $g^{s_1}=zy$ with probability $1/poly$. That is $g^{s_1-s_0}=y$ w.p. $1/poly$. So $s_1-s_0$ is the discrete log of $y$ w.p. $1/poly$. It’s known as Schnorr proof, or Schnorr Signature. Efficient Signature Generation by Smart Cards Non-Interactive ZKLet’s proceed to the next topic. Can we make proofs non-interactive again ? The advantages of Non-Interactive ZK (NIZK): $V$ dose not need to be online during the proof process. Proofs are not ephemeral, can stay into the future.[*ephemeral 短暂的] NIZK is ImpossibleFirstly, we claim that NIZK is impossible. Suppose there were an NIZK proof system for 3COL. The NIZK proof is of completeness and zero-knowledge, but NOT sound. Proof: Completeness: When $G$ is in 3COL, $V$ accepts the proof $\\pi$. Zero Knowledge: PPT simulator $S$, given only G​ in 3COL, produces an indistinguishable proof $\\tilde{\\pi}$.In particular, $V$ accepts $\\tilde{\\pi}$. Imagine running the Simulator $S$ on a $\\underline{G\\notin 3COL}$.It produces a proof $\\tilde{\\pi}$ which the verifier still accepts! Because $S$ and $V$ are PPT. They together cannot tell if the input graph is 3COL or not without the witness. Therefore, $S$ is indeed a cheating prover!It can produce a proof for a $G\\notin 3COL$ that the verifier nevertheless accepts. Ergo, the proof system is NOT SOUND. Two Roads to NIZKBut we can achieve NIZK under some models. There are two roads to NIZK. Random Oracle Model &amp; Fiat-Shamir Transform Common Random String Model NIZK: Random Oracle ModelAs discussed before randomness of verifier for ZK proofs is necessary. Otherwise, it is not interactive. More discussion about randomness.Give an example in ZK proof of knowledge for discrete log.The protocol is not sound if $P^*$ knows the random challenge $c$ beforehand. If $P^*$ knows $c=0$ beforehand, it’s useless. If $P^*$ knows $c=1$ beforehand, Send $z=g^s/y$ for random $s$. Send $s$. NIZK for 3COLConsider NIZK Proof for 3COL. Start with the parallel repetition of 3COL protocol. It is complete, has exponentially small soundness error, and is hones-verifier ZK. Similarly, the randomness is necessary. Otherwise, the cheating prover can make up message $a$, $z$ beforehand. However, the protocol can be non-interactive in the random oracle model. Recap Random Oracle Model [Lecture 12] In random oracle model, the only way to compute $H$ is by calling the oracle.We can consider it as a very complicated public function, e.g. SHA-3.Moreover, we can consider the public function as a proxy to a random function. But in the Random Oracle Heuristic world, the only way to compute $H$, virtually a black box, is by calling the oracle. Fiat and Shamir (1986): Let $c=H(a)$. Now the prover can compute the challenge herself! It is potentially harmful for soundness. But in random oracle model for $H$, it can prove soundness.","link":"/2022/08/16/mit6875-lec15/"},{"title":"「Cryptography-MIT6875」: Lecture 16","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: IP for Quadratic Non-Residuosity Non-interactive ZK NIZK in The Common Random String(CRS) Model Construction in CRS Model: Blum-Feldman-Micali’88 (quadratic residuosity) NIZK for QNR NIZK for 3SAT Proofs vs. Argument At the end of last blog, we said that Non-Interactive Zero-Knowledge (NIZK) is achievable in random oracle model. Today, we move to the NIZK in the common random string model. The Power of Interactive ProofsBefore proceeding to NIZK, let’s focus on the power of Interaction Proofs(IP). Is IP more powerful than NP ? We have been using interaction to get zero knowledge proofs for NP. Indeed, interaction is necessary. But, never mind zero knowledge for a moment. Can you prove more stuff with interactive proofs than with traditional (i.e. NP) proofs ? The thing to point is that we know there is NP proof for quadratic residues, i.e. proof = the square root, but there is no NP proof for quadratic non-residues. In Lecture 14, we gave interactive proofs for quadratic residuosity. Indeed, we can also give an (honest-verifier perfect ZK) interactive for quadratic non-residuosity. IP for QNRThe prover wants to convince the verifier that the $y$ is a quadratic non-residuosity. The interactive protocol is as shown below. ZK Proof for Quadratic Non-residue: Verifier: pick a random $r$ pick a random $b\\gets{0,1}$ send $s=r^2y^b$ Prover: guess $b’$ Verifier Check: $b=b’$ The protocol is complete, sound and (honest-verifier perfect) zero-knowledge. Completeness: Recall that the completeness is the property of the protocol when the prover and the verifier are both honest. If $y$ is non-square, the prover should be able to win. The thing to point is that the prover knows $y$ is non-square, so maybe the she is unbounded or she knows the factorization of $N$ since there is no NP proof for QNR. Hence, if $y$ is non-square, then the prover can tell $s$ is square or non-square to answer $b’$. Soundness: Recall that the soundness is the property of the protocol against malicious prover. If $y$ is a square, then $r^2y^b$ is also a square. Hence, the prover cannot cheat the verifier with probability better than $1/2$. Then we can use sequential repetition or parallel repetition to reduce the soundness error. Honest-Verifier Perfect Zero Knowledge: Recall that the zero-knowledge is the property of the protocol against verifier. The view of $V$ is $(r,b,b’)$, so the simulator can do exactly what $V$ dose. The simulated view is $(r,b,b’=b)$ for a random $r$ and a random $b$. Similarly, although there is no NP proof for graph non-isomorphism, there is an (honest-verifier perfect) interactive proof for graph non-isomorphism. It turns out that IP can prove more stuff than NP. Indeed, the IP = PSPACE &gt;&gt; NP. PSPACE is the set of things that we can decide if we have polynomial memory to work with but potential exponential time. It’s surprising that we can prove such a language, i.e. QNR, using interactive proofs. IP = PSPACE >> NP[Lund-Fortnow-Karloof-Nisan’89, Shamir’90] https://www.cs.princeton.edu/courses/archive/spring09/cos522/BabaiEmail.pdf NIZK: The Common Random String ModelIn last blog is introduced that we can achieve NIZK in the random oracle model. In this section, we will introduce another way, the common random string model. In this model, there is an angle coming up with random sequence of polynomial bits. The angle could be the sunspots. Completeness: For every $G\\in 3COL$, $V$ accepts $P$’s proof. Soundness: For every $G\\notin3COL$ and any “proof” $\\pi^\\star$, $V(CRS,\\pi^\\star)$ accepts with probability $\\le neg(n)$. Zero Knowledge: There is a PPT simulator such that for every $G\\in 3COL$, $S$ simulates the view of the verifier $V$. $$ S(G)\\approx(CRS\\gets D, \\pi \\gets P(G,\\text{colors}) $$ The view of verifier is $(CRS, \\pi)$. The simulator has to produce the simulated view without knowing the witness and the CRS. So the simulator has to fake potentially the common random and the proof. If we sort of change the definition that the simulator gets a common random string and has to produce the view with a fixed CRS, then this definition is impossible to achieve.For the same reason that we prove the NIZK is impossible (Lecture 15), it dose not satisfy the soundness. Similarly, the Common Reference String Model is that there is an angle coming up the random product of two primes, not a sequence of random bits. And it cannot be achieved by the sunspots. Construct NIZK in the CRS ModelThere are several constructions of NIZK in the CRS model. Blum-Feldman-Micali’88 (quadratic residuosity) Feige-Lapidot-Shamir’90 (factoring) Groth-Ostrovsky-Sahai’06 (bilinear maps) Canetti-Chen-Holmgren-Lombardi-Rothblum^2-Wichs’19 and Peikert-Shiehian’19 (learning with errors) In this Lecture, we will introduce the first construction. Blum-Feldman-Micali’88 (quadratic residuosity) Review our number theory hammers &amp; polish them. Construct NIZK for a special NP language, namely quadratic non-residuosity. Bootstrap to NIZK for 3SAT, an NP-complete language. Quadratic Residuosity More introduction is referred in Lecture 9 Let $N=pq$ be a product of two large primes. Jacobi symbol ($Jac$) divides $\\mathbb{Z}_N^*$ evenly unless $N$ is a perfect square. $Jac_{-1}$: non-squares $Jac_{+1}$: pseudo-squares A surprising fact is Jacobi symbol $\\left(\\frac{x}{N}\\right)=\\left(\\frac{x}{P}\\right) \\left(\\frac{x}{Q}\\right)$ is computable in poly. time without knowing $p$ and $q$. (using Law of Quadratic Reciprocity.[* 二次互反定理]) $x$ is square mod $N$ iff $x$ is square mod $p$ and square mod $q$, so we can even $Jac_{+1}$. $QR_N$: the set of squares mod $N$. $QNR_N$: the set of non-squares mod $N$. (but with Jacobi symbol $+1$) Note: The following are new claims: We call $N$ good if exactly half the elements of $\\mathbb{Z}_N^*$ with Jacobi symbol $+1$ are squares. Exactly half residues even iff $N=p^iq^j$ is odd, and $i,j\\ge 1$, not both even. If $N$ is good, there is an important property is if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$. (This property will be used in the following NIZK for QNR) But if $N$ has three or more prime factors, the fraction of residues is smaller. Analysis of $N=pqr$: $x$ is square mod $N$ iff $x$ is square mod $p$, mod $q$ and mod $r$. If $N=pqr$, the fraction of residues in $Jac_{+1}$ is $1/4$ as shown in table below. Jac (x/p) (x/q) (x/r) 1 1 1 1 1 1 -1 -1 1 -1 1 -1 1 -1 -1 1 -1 1 1 -1 -1 1 -1 1 -1 1 1 -1 -1 -1 -1 -1 Besides, the property, “ if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$”, is no longer satisfied. Quadratic Residuosity Assumption (QRA): Let $N=pq$ be a product of two large primes. No PPT algorithm can distinguish between a random element of $QR_N$ from a random element of $QNR_N$ given only $N$. NIZK for QNRDefine the NP language GOOD with instance $(N,y)$ where $N$ is good; and $y\\in QNR_N$ (that is, $y$ has Jacobi symbol $+1$ but is not a square mod $N$) The non-interactive protocol is as follows. The prover wants to convince the verifier that $(N,y)$ is good. NIZK for QNR: $CRS = (r_1,r_2,\\dots,r_m)$ where each $r_i$ is sampled from $Jac_N^{+1}$. The proof is $\\forall i: \\sqrt{r_i}$ or $\\sqrt{yr_i}$ The verifier check $N$ is odd $N$ is not a prime power $N$ is not a perfect square (Fact: If the preceding three passes, then at most half of $Jac_N^{+1}$ are squares.) I received either a mod-$N$ square root of $r_i$ or $yr_i$. Completeness: If $N$ is good and $y\\in QNR_N$, the prover can compute either $\\sqrt{r_i}$ or $\\sqrt{yr_i}$. The prover has a knowledge that $y$ is non-residuosity. So, maybe she is unbounded or knows the factorization of $N$. If $r_i\\in QR_N$: the prover can compute $\\sqrt{r_i}$. If $r_i\\in QNR_N$: $yr_i$ is square by the property above so the prover can compute $\\sqrt{yr_i}$. Soundness: What if $N$ has more than $2$ prime factors ? No matter what $y$ is, for half the $r_i$ , both $r_i$ and $yr_i$ are not quadratic residues. So the prover cannot compute either $\\sqrt{r_i}$ or $\\sqrt{yr_i}$ for half the $r_i$. Suppose $N=pqr$ The fraction of quadratic residues in $Jac_{+1}$ is $1/4$. Besides, the property, “ if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$”, is no longer satisfied. The thing to notice is that the proof only proves $N$ is good, not $N=pq$. Perfect Zero Knowledge Simulator S: First pick the proof $\\pi_i$ to be random in $\\mathbb{Z}_N^*$. Then reverse-engineer the CRS, letting $r_i=\\pi_i^2$ or $r_i=\\pi_i^2/y$ randomly. The distribution of simulated view is identical to the real view. Warning:We define $CRS = (r_1,r_2,\\dots,r_m)$ where each $r_i$ is sampled from $Jac_N^{+1}$. The CRS depends on the instance $N$. Not good.An alternative solution is Let CRS be random numbers. Interpret them as elements of $\\mathbb{Z}_N^*$ and both the prover and the verifier filter out $Jac_N^{-1}$. NIZK for 3SAT From Wiki: SAT:In logic and computer science, the Boolean satisfiability problem (sometimes called propositional satisfiability problem and abbreviated SATISFIABILITY, SAT or B-SAT) is the problem of determining if there exists an interpretation that satisfies a given Boolean formula.SAT is the first problem that was proven to be NP-complete.This means that all problems in the complexity class NP, which includes a wide range of natural decision and optimization problems, are at most as difficult to solve as SAT. 3SAT:Like the satisfiability problem for arbitrary formulas, determining the satisfiability of a formula in conjunctive normal form where each clause is limited to at most three literals is NP-complete also; this problem is called 3-SAT, 3CNFSAT, or 3-satisfiability. Literal:In mathematical logic, a literal is an atomic formula (atom) or its negation. Boolean Variables: $x_i$ can be either true(1) or false(0). A Literal is either $x_i$ or $\\overline{x_i}$ A Clause is a disjunction of literals. A Clause is true if any one of the literals is true. E.g. $x_1\\vee x_2\\vee \\overline{x_5}$ is true as long as $(x_1,x_2,x_5)\\ne(0,0,1)$ A 3-SAT formula is a conjunction of many 3-clauses. A 3-SAT formula $\\Psi$ is satisfiable if there is an assignment of values to the variables $x_i$ that makes all its clauses true. $\\Psi=\\left(x_{1} \\vee x_{2} \\vee \\overline{x_{5}}\\right) \\wedge\\left(x_{1} \\vee x_{3} \\vee x_{4}\\right)\\wedge\\left(\\overline{x_{2}} \\vee x_{3} \\vee \\overline{x_{5}}\\right)$ Cook-Levin Theorem: It is NP-complete to decide whether a 3-SAT formula $\\Psi$ is satisfiable. Recall NIZK for QNR. We saw a way to show that a pair $(N,y)$ is GOOD. That is: the following is the picture of $\\mathbb{Z}_N^*$ and for every $r\\in Jac_{+1}$, either $r$ or $ry$ is a quadratic residue. The prover wants to convince the verifier that $\\Psi$ is satisfiable. The input $\\Psi=\\left(x_{1} \\vee x_{2} \\vee \\overline{x_{5}}\\right) \\wedge\\left(x_{1} \\vee x_{3} \\vee x_{4}\\right)\\wedge\\left(\\overline{x_{2}} \\vee x_{3} \\vee \\overline{x_{5}}\\right)…$ with $n$ variables and $m$ clauses. The non-interactive protocol is as follows. NIZK for 3SAT: $CRS=(r_1,r_2,…)$ where each $r_i$ is sampled from $Jac_N^{+1}$.Note: Similarly, we can Let CRS be random numbers and interpret them as elements of $\\mathbb{Z}_N^*$, and both the prover and the verifier filter out $Jac_N^{-1}$. Prover picks an $(N,y)$ and proves that it is GOOD. (as defined above) Prover encodes the satisfying assignment and sends the encode variables $(y_1,\\dots,y_n)$ to $V$.We should hide the values to achieve zero knowledge so the encoding is indeed a commitment. $y_i\\gets QR_N$ if $x_i$ is false (encryption of $0$ is a residue) $y_i \\gets QNR_N$ if $x_i$ is true (encryption of $1$ a non-residue) For the literal $x_i$, $Enc(x_i)=y_i$ $Enc(\\overline{x_i})=yy_i$ exactly one of the $Enc(x_i)$ or $Enc(\\overline{x_i})$ is a non-residue. A residue is indeed the encryption of 0(false) A non-residue is indeed the encryption of 1(true) Prove that (encoded) assignment satisfies each clause. For each clause, say $x_1\\vee x_2\\vee \\overline{x_5}$, we WANT to SHOW: $x_1$ OR $x_2$ OR $\\overline{x_5}$ is true. Let $(a_1=y_1,b_1=y_2,c_1=yy_5)$ denote the encoded variables. So, each of them is either $y_i$ (if the literal is a var) or $yy_i$ (if the literal is a negated var). Now, we WANT to SHOW: $a_1$ OR $b_1$ OR $c_1$ is a non-residue. A terrible way is to prove $a_1$ is non-residue or $b_1$ is non-residue. If the prover convinces the verifier that $a_1$ is non-residue, $P$ indeed tells $V$ that $x_1$ is true(1). Hence, we want to prove one of them is a non-residue without telling the verifier that which one is non-residue. WANT to SHOW: $a_1$ OR $b_1$ OR $c_1$ is a non-residue Equivalently, the signature of $(a_1,b_1,c_1)$ is NOT (QR, QR, QR). A clever idea is to generate seven additional triples. The original triple is $(a_1,b_1,c_1)$. This 8 triples span all possible QR signatures. There is one triple is (QR, QR, QR), e.g. the second triple, and reveal the square roots of this triple. Then we can prove the origin triple is NOT (QR, QR, QR). Proof consists of two parts Proof of Coverage : show that the 8 triples span all possible QR signatures. For each of poly. many triples $(r,s,t)$ from CRS, show one of the 8 triples has the same signature. That is, there is a triple $(a_i,b_i,c_i)$ s.t. $(ra_i,sb_i,tc_i)$ is (QR, QR, QR) Show one triple (except the original triple) is (QR, QR, QR) and reveal the square roots. Hence, the full proof is as follows. Prover picks an $(N,y)$ and proves that it is GOOD. Prover encodes the satisfying assignment and sends the encode variables $(y_1,\\dots,y_n)$ to $V$. Prove that (encoded) assignments satisfy each clause. For each clause, construct the proof $\\rho$ = (7 additional triples, square root of the second triples, proof of coverage). Completeness and Soundness can be inherited from NIZK for QNR. Computational Zero Knowledge Simulator S: Simulator picks $(N,y)$ where $y$ is a quadratic residue. The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. Simulated view of $(N,y, \\pi)$ Pick the proof $\\pi_i$ to be random in $\\mathbb{Z}_N^*$. Reverse-engineer the CRS, letting $r_i=\\pi_i^2$ or $r_i=\\pi_i^2/y$ randomly. Then $r_i=\\pi_i^2$ and $r_i=\\pi_i^2/y$ are both residues. The distribution of simulated view is computationally indistinguishable to the real view. Simulated view of encoded assignments $(y_1,\\dots, y_n)$ $y_i \\gets QNR_N$ whether $x_i$ is true or false (encryptions are both non-residues) $Enc(x_i)=y_i$ (encryption of $1$ is a non-residue) $Enc(\\overline{x_i})=yy_i$ (encryption of $0$ is a non-residue) Encoding of ALL literals can be set to true. Proofs vs. ArgumentBefore proceeding to the evolution of proofs, let’s discuss the difference between proofs and arguments. The main difference is the definition in soundness. In Lecture 14, we gave the definition of soundness in Proofs that nobody can convince you a false statement. We mentioned that the soundness holds agains unbounded provers. Arguments is where the soundness holds against only computational bounded provers. It’s actually weakening the notion of proof. Why do we still introduce the the argument ? It allows us to get perfect zero knowledge for all of NP.Recall perfect ZK proofs do not exist for NP-complete languages, unless the polynomial hierarchy collapses. It turns out that if we weaken the notion of proofs, then we can construct the perfect zero knowledge systems. It allows us to get very short proof = succinct arguments or SNARGs.SNARKs : succinct arguments of knowledge. The Evolution of ProofsProofs have been evolving over generations. CLASSIC Proofs (Complexity class: NP) Prover writes down a string (proof); And Verifier checks. INTERACTIVE Proofs (Complexity class: IP = PSPACE &gt;&gt; NP) Prover and verifier talk back and forth. In last blog, we introduced the zk proof for quadratic non-residue. PROBABILISTICALLY CHECKABLE Proofs (Complexity class: NEXP &gt;&gt; PSPACE) Non-interactive, but verifier only looks at 3 bits of proof. MULTIPROVER interactive proofs Proofs in the wild: ZCash","link":"/2022/08/23/mit6875-lec16/"},{"title":"「Cryptography-MIT6875」: Lecture 1","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Everything you’ve ever wanted is on the other side of fear. Topics: Introduction to cryptography Secure Communication and Shannon’s definitions of perfect secrecy Perfect Indistinguishability definitions. The One-time pad construction Shannon’s lower bound. Introduction6.875 is about Crypto, not Cryptocurrencies. 6.875 is about foundations, including Digital Signatures, Zero-knowledge Proofs, Public-key Encryption, Homomorphic Encryption, Threshold Cryptography, Pseudorandomness and so on. The Intellectual OriginsClaude E. Shannon The paper, “Communication Theory of Secrecy Systems”(1945), gave a way to define some terms in Secure Communication. The paper, “A mathematical Theory of Communication”(1948), founded Information Theory. Alan M. Turing He was engaged on Cryptanalysis of Enigma Machine in 1938-39. The paper, “On Computable Numbers, with an Application to the Entscheidungsproblem”(1936), give birth to Computer Science. Modern CryptographyModern Cryptography is a Practice to Theory and Back. The Definitions in CRYPTO are important. It solves the problems of Security, Privacy, Integrity(etc.) using Encryption, Digital Signatures, Pseudorandom Functions(etc.). It makes use of the ideas in Math and Theoretical CS to construct some tools, such as Interactive Proofs, Probabilistically checkable Proofs, Locally decodable Codes etc. 6.875 Themes The Omnipresent, Worst-case, Adversary. The central idea is to model the adversary.What they know? What they can do? What their goals are? Definitions will be our friend.If you cannot define something, you cannot achieve it. A key takeaway from 6.875.Cryptographic(or, adversarial) thinking.【这门课最重要的是培养一种从攻击者角度的思考方式，如何才能抵抗哪些未知的攻击】 Computational Hardness will be our enabler. The central theme is to use cryptographic leash.Use computational hardness to “tame” the adversary.【计算难度将是有利的工具】 Number theory is a classic source of hard problem.【数论是困难问题的经典来源】 [G.H.Hardy, “A Mathematician’s Apology”] ”Both Gauss and lesser mathematicians may be justified in rejoicing that there is one such science [number theory] at any rate, whose very remoteness from ordinary human activities should keep it gentle and clean”【高斯和一些数学家都会认为应该保持数论的纯洁性，即使它远离一般的人类活动。】 More recently, people get inspiration from geometry, coding theory, combinatorics.【现在，也会从几何学、编码理论、组合学中获得困难问题】 Security Proofs via Reductions. The proof usually comes in a from of reduction.【安全证明往往会规约为一种困难问题的求解】” If there is an (efficient) adversary that breaks scheme A w.r.t definition D, then there is an (efficient) adversary that factors large number” The reductions in 6.875 will be probabilistic and significantly more involved than the NP-hardness reductions.【这门课所涉及的规约将是概率性的，比NP难度中的规约更复杂】 6.875 Topics6.875 will cover these topics. Pseudorandomness【伪随机性】 Secret-key Encryption and Authentication【对称加密和认证】 Public-key Encryption and Digital Signatures【公钥加密和数字签名】 Cryptographic Hashing【哈希】 Zero-knowledge Proofs【零知识证明】 Secure Multiparty Computation【安全多方计算】 Private Information Retrieval【隐私信息检索】 Homomorphic Encryption【同态加密】 Advanced topics： Threshold Cryptography, Differential Privacy, …【前沿课题：门限密码，差分隐私等】 Secure CommunicationThe scenario in Secure Communication is that Alice wants to send a message M to Bob without revealing it to Eve. It’s actually very difficult. However, it can be achievable in such a setting which Alice and Bob meet beforehand to agree on a secret key k. Symmetric-key EncryptionIn order to achieve Symmetric-key Encryption, it is necessary to design three (possibly probabilistic) polynomial-time algorithm. Key Generation Algorithm Gen: $k\\leftarrow \\mathrm{Gen}(1^n)$ It has to be probabilistic (or, random).n is the desired output length.w Encryption Algorithm Enc: $c\\leftarrow \\mathrm{Enc}(k,m)$ Decryption Algorithm Dec: $m\\leftarrow \\mathrm{Dec}(k,c)$ The Worst-case AdversaryWhat dose the worst-case adversary looks like? It’s actually an arbitrary computationally unbounded algorithm EVE.【实际上是一个计算能力无限的任意算法】 It knows Alice and Bob’s algorithms Gen, Enc and Dec but does not know the key nor their internal randomness.【算法公开，只有密钥是未知的】 Kerckhoff’s principle or Shannon’s maxim: - Gen, Enc and Dec are public algorithms. - The key is private. It can see the ciphertexts going through the channel.【能够看到信道里的密文】(but cannot modify them…) Consequently, Security Definition actually defines what the adversary is trying to learn? What dose the security definition looks like? (intuitive)EVE should learning nothing about m from ciphertext c. $$ \\forall \\mathrm{EVE}, \\mathrm{Pr}[\\mathrm{EVE(Dec}(k,c)=m)]\\le \\frac{1}{\\mathcal{M}} $$ where k is generated by $k\\leftarrow \\mathrm{Gen}(1^n)$ and m is sampled in $m\\leftarrow \\mathcal{M}$ supposing $\\mathcal{M}$ is probabilistic uniform distribution. It’s important to note that $\\mathcal{M}$ is an arbitrary distribution as a matter of fact. The only thing we can control is the distribution of k.【M的分布实际上是任意的，但我们能控制k的分布】 Two Equiv. Def.s of SecurityIn this section, I will introduce two equivalent definitions of security, Shannon’s perfect Secrecy Definition and Perfect Indistinguishable Definition. It will be easier to prove one of them in some cases. Shannon’s Perfect Secrecy Def.The main idea in Shannon’s perfect secrecy definition is the posteriori of attacker is equal to the priori of the attacker, i.e. A-posteriori= A-priori. 【香农安全定义的主要思想是，攻击者的后验概率等于先验概率，也就是说攻击者看到密文后获得的信息（后验）和看到密文前得到的信息（先验）是一样的】 ** Shannon’s Perfect Secrecy Definition: ** $$ \\begin{array}{l}\\forall\\mathcal{M}, \\forall m\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\\\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]=\\operatorname{Pr}[\\mathcal{M}=m]\\end{array} $$ where $\\mathcal{M,K,C}$ are variables in message space, key space and ciphertext space respectively. There are two worlds, real world and ideal world. A-posteriori represents the real world.In real world, the attacker can see the ciphertext in the channel.So the probability that the attacker knows the message is m when the ciphertext c is known is $\\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]$ . A-priori represents the ideal world.In ideal world, the attacker can only guess the message when the ciphertext is unknown. So the probability is $\\operatorname{Pr}[\\mathcal{M}=m]$ . Perfect Indistinguishability Def.Perfect indistinguishability is a formalizaton of a turing test. There are two worlds, world 0 and world 1. The two worlds both sample the key $k$ from the key space $\\mathcal{K}$, and the world 0 uses $k$ to encrypt $m$ while world 1 uses $k$ to encrypt $m’$. The attacker is a distinguisher that gets the $c$ and tries to guess which world she’s in. 【有两个世界，一个世界用随机密钥加密$m$，另一个用随机密钥加密$m’$，攻击者希望能根据密文区分她在哪一个世界】 ** Perfect Indistinguishability Definition: ** $$ \\begin{array}{l}\\forall\\mathcal{M}, \\forall m,m'\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\\\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c\\right]=\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m')=c\\right]\\end{array} $$ where $\\mathcal{M,K,C}$ are variables in message space, key space and ciphertext space respectively. The Two Def.s are Equiv.The two definitions are equivalent. ** Theorem: ** An encryption scheme (Gen, Enc, Dec) satisfies secrecy IFF it satisfies perfect indistinguishability. The proof is simple use of Bayes’ Theorem. Indistinguishability → SecrecyHere we prove that if it satisfies perfect indistinguishability then it satisfies perfect secrecy. We know indistinguishability(IND) $$ \\begin{array}{l}\\forall\\mathcal{M}, \\forall m,m'\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\\\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c\\right]=\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m')=c\\right]\\end{array} $$ We want secrecy(SEC) $$ \\begin{array}{l}\\forall\\mathcal{M}, \\forall m\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\\\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]=\\operatorname{Pr}[\\mathcal{M}=m]\\end{array} $$ ** Proof: ** Suppose tha the Probability is equal to $\\alpha$ in IND. There is a key observation.$\\forall m \\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K,M})=c]=\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]$ The key and the message are both random in left while the message is fixed in right. Proof: $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K,M})=c]$ = $\\sum\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K,M})=c\\mid \\mathcal{M}=m]; \\operatorname{Pr}[\\mathcal{M}=m]$ = $\\sum\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]; \\operatorname{Pr}[\\mathcal{M}=m]$ = $\\alpha \\sum\\operatorname{Pr}[\\mathcal{M}=m]$ = $\\alpha$ We can use Bayes’ theorem and the key observation to deduce the SEC. $\\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]$ = $\\frac{\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\mid \\mathcal{M}=m \\right] \\operatorname{Pr}[\\mathcal{M}=m]}{\\operatorname{Pr}\\left[ \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]}$ (Bayes) = $\\frac{\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c \\right] \\operatorname{Pr}[\\mathcal{M}=m]}{\\operatorname{Pr}\\left[ \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]}$ = $\\frac{\\alpha \\operatorname{Pr}[\\mathcal{M}=m]}{\\alpha}$ （key observation) = $\\operatorname{Pr}[\\mathcal{M}=m]$ Secrecy → IndistinguishabilityHere we prove that if it satisfies perfect secrecy then it satisfies perfect indistinguishability. We know secrecy(SEC) $$\\begin{array}{l}\\forall\\mathcal{M}, \\forall m\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]=\\operatorname{Pr}[\\mathcal{M}=m]\\end{array}$$ We want indistinguishability(IND) $$\\begin{array}{l}\\forall\\mathcal{M}, \\forall m,m’\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c\\right]=\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m’)=c\\right]\\end{array}$$ ** Proof: ** Just prove the above key observation. $\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c\\right]$ = $\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\mid \\mathcal{M}=m \\right]$ = $\\frac{\\operatorname{Pr}\\left[ \\mathcal{M}=m\\mid\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c \\right]\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c ]}{\\operatorname{Pr}[\\mathcal{M}=m]}$ (Bayes) From SEC, we know:$\\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]=\\operatorname{Pr}[\\mathcal{M}=m]$ = $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c ]$ = $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K}, m’)=c ]$ One-time PadPerfect Secrecy is achievable using one-time pad. Perfect Secrecy is AchievableThe One-time Pad Construction: Gen: choose an n-bit string k at random, i.e. $k\\leftarrow {0,1}^n$ Enc(k,m), where M is an n-bit message: Output $c=m\\oplus k$ Dec(k, c): Output $m=c\\oplus k$ xor property: uniformityX is a uniform variable and $Y$ is a random variable.Then $Z=X\\oplus Y$ is a uniform variable. randomness$X$ is a fixed value and $Y$ is a random variable.The $Z=X\\oplus Y$ is a random variable. Correctness can be easily verified. $c\\oplus k=(m\\oplus k)\\oplus k=m$. ** Claim: ** One-time Pad achieves Perfect Indistinguishability (and therefore perfect secrecy) ** Proof: ** It requires that for any $m,m’$, $c\\in{0,1}^n$, $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]=\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m’)=c]$ $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]$ = $\\operatorname{Pr}[\\mathcal{K}\\oplus m=c]$ = $\\operatorname{Pr}[\\mathcal{K}= m\\oplus c]$ = $1/2^n$ $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m’)=c]$ = $\\operatorname{Pr}[\\mathcal{K}\\oplus m’=c]$ = $\\operatorname{Pr}[\\mathcal{K}= m’\\oplus c]$ = $1/2^n$ QED. Reusing a One-time Pad?Is it secure when we reuse a one-time Pad? The answer is absolutely no. ** Claim: ** One-time Pad does not achieve Perfect Indistinguishability (and therefore not perfect secrecy). ** Proof: ** It requires that for all pairs $(m_0,m_1),(m_0’,m_1’),(c_0,c_1)\\in{0,1}^{2n}$,$\\operatorname{Pr}[\\operatorname{Enc}(k, m _0)=c_0 \\text { and } \\operatorname{Enc}(k, m_1)=c_1]=\\operatorname{Pr}[\\operatorname{Enc}(k, m _0’)=c_0 \\text { and } \\operatorname{Enc}(k, m_1’)=c_1]$ We want to pick $(m_0,m_1),(m_0’,m_1’),(c_0,c_1)\\in{0,1}^{2n}$ which makes the left of the equation not equal to the right. Pick $m_0=m_1=m,m_0’\\ne m_1’$ and $c_0=c_1=c$ The left side $\\operatorname{Pr}[\\operatorname{Enc}(k, m _0)=c_0 \\text { and } \\operatorname{Enc}(k, m_1)=c_1]$ = $\\operatorname{Pr}[\\operatorname{Enc}(k, m )=c]$ = $1/2^n$ The right side $\\operatorname{Pr}[\\operatorname{Enc}(k, m _0’)=c_0 \\text { and } \\operatorname{Enc}(k, m_1’)=c_1]$ = 0 QED. A Serious LimitationPerfect secrecy has its price. A serious limitation of perfect secrecy is that the key space must be greater than or equal to the message space. ** Theorem: ** For any perfectly secure encryption scheme, $\\mathcal{|K|\\ge|M|}$. ** Proof(by picture): ** Assume for contradiction that $\\mathcal{|K|&lt;|M|}$. If we pick any $c\\in \\mathcal{C}$, there are at most $|\\mathcal{K}|$ possible messages using distinct keys.Suppose the possible message space is the bluer part in the left ellipse which the size is at most $|\\mathcal{K}|$ and less than $\\mathcal{|M|}$ . Choose the message $m$ inside the bluer part. $m$ is the possible message.So the probability is $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]&gt;0$. Choose the message $\\tilde{m}$ outside the bluer part. $\\tilde{m}$ can’t be the possible message. So the probability is $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},\\tilde{m})=c]=0$ There is a contradiction. SolutionSo, what are we to do? The solution is to RELAX the definition. The adversary we mentioned above is an arbitrary computationally unbounded algorithm EVE. Now, we relax the adversary to an arbitrary computationally bounded algorithm.","link":"/2022/06/30/mit6875-lec1/"},{"title":"「Cryptography-MIT6875」: Lecture 17","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Definition of IND-CCA Security Application of NIZK: Construction of CCA-secure encryption scheme In last post, we saw a NIZK for 3SAT, and then we can get a NIZK for all of NP in the CRS model. Moreover, we mentioned that if we weaken the notion of proofs into arguments, then we can construct perfect zk for all of NP. Today, we will discuss the application of NIZK, non-malleable and chosen ciphertext secure encryption scheme. Active Attacks against CPA-secure EncryptionRecall the public encryption schemes. Bob generate a pair of keys, a public key $pk$, and a private (or secret) key $sk$. Bob “publishes” $pk$ and keeps $sk$ to himself. Alice encrypts $m$ to Bob using $pk$. Bob decrypts using $sk$. In Lecture 8, we gave the definition of IND-CPA security for public encryption scheme and mentioned the IND-CPA-secure is achievable with randomness. But there are two active attacks against the public encryption scheme above. MalleabilityThe first active attack is malleability. Consider the scenario for bidding. Alice wants to bid ¥100 and she encrypts her bid with public key. There is a malicious attacker that wants to win the bidding and he can modify the encryption of ¥100​ into an encryption of ¥101 as his bidding. The attack can always modify the encryption, which is always one more dollar than Alice’s although the attack dose not know what Alice’s bid is. So the adversary could modify (“maul”) an encryption of $m$ into an encryption of a related message $m’$. Chosen-Ciphertext AttackAnother active attack is chosen-ciphertext attack. If the first bit of the message is 0, we define it’s the encryption of a valid message. If the first bit of the message is 1, we define it’s the encryption of an invalid message. Then the adversary may have access to a decryption “oracle” and can use it to break security of a “target” ciphertext $c^*$ or even extract the secret key! In fact, Bleichenbacher showed how to extract the entire secret key given only a “ciphertext verification” oracle. Bleichenbacher IND-CCA SecurityAfter defining the stronger active attackers, we can define the Indistinguishable Chosen-Ciphertext Attack Security, or IND-CCA. Recall the game in IND-CPA secure definition. Game in IND-CPA (one-message): The Challenger generates a pair of key $(pk, sk)$ and publishes the $pk$. Eve sends two single messages, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$. The challenger samples $b$ from ${0,1}$ and encrypts the message $m_b$ using $pk$.And send the ciphertext to Eve. Eve guesses which message is encrypted and output $b’$. Eve wins if $b’=b$. Let’s move to the game in IND-CCA. As has been said, the adversary has the power of accessing to a decryption “oracle”. The adversary can query for the decryption in polynomial many times. It can happen both before and after the challenge. Note that the adversary can query for the decryption, before the challenge, of any ciphertext. But after the challenge, he cannot query for the decryption of the challenge. Otherwise, the challenge is meaningless. So there are additional two phases in the game of IND-CCA. Game in IND-CCA: The Challenger generates a pair of key $(pk, sk)$ and publishes the $pk$. Eve asks for the decryption of any ciphertext in poly. many times. Eve sends two single messages, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$. The challenger samples $b$ from ${0,1}$ and encrypts the message $m_b$ using $pk$.And send the ciphertext to Eve. Eve asks for the decryption of any ciphertext (except the challenge $c^\\star$) in poly. many times. Eve guesses which message is encrypted and output $b’$. Eve wins if $b’=b$. IND-CCA Security Definition The encryption scheme is IND-CCA secure if no PPT Eve can win with probability $&gt;1/2+negl(\\lambda)$. Constructing CCA-Secure Encryption In the light of the CCA-secure definition, we can construct the CCA-secure encryption scheme. Our goal is that the adversary is hard to modify an encryption of $m$ into an encryption of a related message, say $m+1$. Intuitionally, the proof of knowledge and the signatures should help against the malleability for CAP-secure encryption. With NIZK proofs of knowledge, the idea is that the encryption party attaches an NIZK proof of knowledge of the underlying message to the ciphertext. Therefore, the encryption consists of CPA-encryption and the NIZK proof of knowledge of the message. $C:(c=\\text{CPAEnc}(m;r),\\text{ proof }\\pi \\text{ that “I know }m\\text{ and }r\\text{ “})$. This idea will turns out to be useful, but NIZK proofs themselves can be malleable. So the active attack turns to create the CPA-encryption of $(m+1,r)$ and the NIZK proof of $(m+1,r)$. Start with Digital SignaturesLet’s start with digital signatures. We construct the CCA-secure encryption, which contains the signature of the CPA-secure encryption. Note that it is malleable. $C:(c=\\text{CPAEnc}(pk,m;r),\\text{Sign}_{sgk}(c),vk)$ where the encryption produces a signing/verification key pair by running $(sgk,vk)\\gets \\text{Sign.Gen}(1^n)$. It is actually not CCA-secure. (or it’s malleable) If the adversary changes $vk$, all bets are off. The picture below explains the reason vividly. Consequently, the lesson is that we need to “tie” the ciphertext $c$ to $vk$ in a “meaningful” way. IND-CPA → “Different-Key Non-malleability”One observation is we can reduce IND-CPA to Different-Key Non-malleability(NM). The Different-Key NM predicates that given (independent) $pk,pk’$ and the encryption $\\text{CPAEnc}(pk,m;r)$, the adversary cannot produce $\\text{CPAEnc}(pk’,m+1;r)$. It’s a reduction that suppose the adversary could produce the different-key encryption, then she can break the IND-CPA security of $\\text{CPAEnc}(pk,m;r)$. Proof: Suppose for contradiction that the adversary has the power of producing the different-key encryption, $\\text{CPAEnc}(pk’,m+1;r)$. The interaction with the Diff-Key NM adversary is as follows. Interaction with Diff-Key NM Adv. Pick a random pair $(pk’, sk’)$ and give the two public keys $pk,pk’$ . Give the CPA-secure encryption of $m$ using $pk$. The Diff-Key NM adv. promises to produce the CPA-secure encryption of $m+1$ using the different key $pk’$. (w.r.t. contradiction) Then we can construct a CPA adversary by using the Diff-Key NM adversary, as shown below. Break CPA-secure Encryption The challenge is to decrypt the message given CPA-secure encryption.(as shown on the left of the picture) Given $pk$, then we pick a random pair $(pk’,sk’)$ and send $pk,pk’$ to the adversary. Give the CPA-secure encryption from CPA challenge. The adversary promises to produce an encryption using the different key $pk’$. Then we can decrypt it with $sk’$ and subtract 1 to get $m$. Hence, if the adversary can break the Different-Key NM game, then she can break CPA security. CCA-Secure Encryption SchemeWe can get non-malleable and CCA-secure encryption putting CPA-secure encryption, digital signature and NIZK proofs together. As has been said, we need to tie the ciphertext $c$ to the verification key $vk$. NM Encryption Scheme CCA Encryption (only non-malleable): We define $2n$ public keys of the CPA scheme as the CCA public key. CCA Public Key: $\\left[\\begin{array}{llll}p k_{1,0} &amp; p k_{2,0} &amp; \\dots &amp; p k_{n, 0} \\ p k_{1,1} &amp; p k_{2,1} &amp; \\dots &amp; p k_{n, 1}\\end{array}\\right]$ where $n=|vk|$. First, pick a signing/verification key pair $(sgk, vk)$. Then use the CCA public key, based on the bits of $vk$, to produce the ciphertext. $CT=[ct_{1,vk_1},ct_{2,vk_2},\\dots,ct_{n,vk_n}]$ where $ct_{i,j}\\gets \\text{CPAEnc}(pk_{i,j},m)$. This ties the ciphertext $CT$ to the verification key $vk$ in meaningful way that the encryption is under the public key indexed by the bits of $vk$. For each ciphertext in slot $i$ of $CT$: If the $i$-th bit of $vk$ is 0, then use $pk_{i,0}$ to produce the ciphertext. If the $i$-th bit of $vk$ is 1, then use $pk_{i,1}$ to produce the ciphertext. Output $(CT,vk,\\sigma=\\text{Sign}_{sgk}(CT))$ The encryption scheme above is non-malleable. Non-malleability rationale: If the adversary keeps the $vk$ the same, she needs to produce $(CT’,vk,\\sigma_{sgk}(CT’))$ for the related message $m’$, which has to break the signature scheme. $CT’$ is encrypted under the same public key as $CT$. If the adversary changes the $vk$, she has to break the Different-Key Non-malleability game, and therefore CPA security. The adversary needs the produce $(CT’,vk’,\\sigma_{sgk’}(CT’))$ for the related message $m’$. $CT’$ is encrypted under the different public key, which is indexed by $vk’$. Hence, for each different bit of $vk’$: The original $ct_{i,j}\\gets \\text{CPAEnc}(pk_{i,j},m)$. The adversary needs to produce $ct_{i,j}’\\gets \\text{CPAEnc}(pk_{i,1\\oplus j},m’)$. Turns out the Different-Key NM, which can be reduced to CPA security. CCA-Secure Encryption SchemeWe are not done!!Adversary could create ill-formed ciphertexts, e.g. different $ct$s encrypt different messages, and uses it for Bleichenbacher-like attack. Hence, it has to prove that the ciphertext is well-formed. CCA Encryption (non-malleable and CCA-secure): We define $2n$ public keys of the CPA scheme as the CCA public key. CCA Keys: PK = $\\left[\\begin{array}{llll}p k_{1,0} &amp; p k_{2,0} &amp; \\dots &amp; p k_{n, 0} \\ p k_{1,1} &amp; p k_{2,1} &amp; \\dots &amp; p k_{n, 1}\\end{array}\\right]$, CRS where $n=|vk|$. SK = $\\left[\\begin{array}{c}sk_{1,0} \\ sk_{1,1}\\end{array}\\right]$ To achieve NIZK proof, it has to have (public) CRS. The secret key contains only one pair since it needs to be proven well-formed. First, pick a signing/verification key pair $(sgk, vk)$. Then use the CCA public key, based on the bits of $vk$, to produce the ciphertext. $CT=[ct_{1,vk_1},ct_{2,vk_2},\\dots,ct_{n,vk_n}]$ where $ct_{i,j}\\gets \\text{CPAEnc}(pk_{i,j},m)$. Generate NIZK proof $\\pi$ = “CT is well-formed”. Output $(CT,{\\color{blue}\\pi},vk,\\sigma=\\text{Sign}_{sgk}({\\color{blue}{CT,\\pi}}))$ CCA Decryption: Check the signature Check the NIZK proof Decrypt with $sk_{1,vk_1}$ Now, this encryption scheme is CCA-secure and non-malleable. Proof of CCA-security: Proof Sketch Suppose for contradiction that there is an CCA adversary. Play the CCA game with the adversary.Note that we will define several hybrid distributions to argument and play the CCA game in one Hybrid. Then we can use her to break either the NIZK soundness/ZK, the signature scheme or the CPA-secure scheme. CCA game is as follows. I drew this picture on my own. since the proof is omitted in the lecture. Corrections and advice are welcome. Hybrid 0: Play the CCA game as prescribed. Hybrid 1: Observe that $vk_i\\ne vk^*$. (Otherwise break signature. ) Observe that this means each query ciphertext-tuple involves a different public-key from the challenge ciphertext. Then we can use the “different private-key” to decrypt. (If the adversary sees a difference, she broke NIZK soundness.) It means that the adversary produces a ill-formed ciphertext and she cheats successfuly. Hybrid 2: Now change the CRS/$\\pi$ into simulated CRS/$\\pi$. (It’s OK by ZK) The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. If we want to use the adversary to break CPA security, it has to win the CPA game as follows. We can plop $pk$, given in CPA game, into one slot of $PK$. We plop ciphertext, given in CPA game, into $c^*$ with the corresponding slot. For other slots in $c^*$, we can generate the ciphertext for random message. But we cannot generate NIZK proof $\\pi$ since we don’t have the witness.It says that the $c^*$ is ill-formed. But we cannot since we don’t have the witness , that is $m^*$, the challenge of CPA game. Hence, in Hybrid 2, we change the CRS/$\\pi$ into simulated CRS/$\\pi$. It’s zero-knowledge. But more importantly, we can generate simulated proof. Consequently, if the adversary wins in this hybrid, she breaks IND-CPA as shown below. I drew this picture on my own since the proof is omitted in the lecture. Corrections and advice are welcome.","link":"/2022/09/03/mit6875-lec17/"},{"title":"「Cryptography-MIT6875」: Lecture 2","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Science wins either way. Topics: How to circumvent Shannon’s lower bound: the computational adversary Definition of computational security The definition of pseudorandom generators(PRG) RecapIn the previous blog, we define two equivalent definitions of security, Perfect Secrecy and Perfect Indistinguishability, with the setting of an arbitrary computationally unbounded adversary. Perfect Secrecy: A posteriori = A prioriFor all $m, c$: $\\operatorname{Pr}[\\mathcal{M}=m\\mid \\operatorname{E}(\\mathcal{K,M)}=c] = \\operatorname{Pr}[\\mathcal{M}=m]$ Perfect Indistinguishability:For all $m_0, m_1, c$: $\\operatorname{Pr}[ \\operatorname{E}(\\mathcal{K,m_0)}=c] = \\operatorname{Pr}[\\operatorname{E}(\\mathcal{K,m_1)}=c]$ Although the definitions above are equivalent, sometime one is more convenient to work with than the other. We mentioned that the perfect secrecy is achievable using one-time pad. However, the keys are as long as messages in one-time pad. Worse still, in Shannon’s theorem, for any perfectly secure scheme, the key space must be larger than or equal to the message space, i.e. $\\mathcal{|K|\\ge|M|}$. This is known as Shannon’s conundrum. Therefore, the gist of this blog is how we can overcome Shannon’s conundrum. Computational SecurityBefore that, I’ll introduce two mathematical definitions to instantiate Perfect Indistinguishability. Perfect IndistinguishabilityAs mentioned in the previous blog, Perfect Indistinguishability is substantially a formalization of the Turing test. There are two worlds, world 0 and world 1. The two worlds both sample the key $k$ from the key space $\\mathcal{K}$, and the world 0 uses $k$ to encrypt $m$ while world 1 uses $k$ to encrypt $m’$. The adversary called EVE is a distinguisher that gets the $c$ and tries to guess which world she’s in. Perfect Indistinguishability: For all $m_0, m_1, c$: $\\operatorname{Pr}[ \\operatorname{E}(\\mathcal{K,m_0)}=c] = \\operatorname{Pr}[\\operatorname{E}(\\mathcal{K,m_1)}=c]$ There are two mathematical definitions to instantiate Perfect Indistinguishability. The First Def: The probability that EVE think she is in World 0 is the same. $\\begin{array}{l}\\text{For all EVE and all }m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; c=E(k,m_0):\\mathrm{EVE}(c)=0] = \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; c=E(k,m_1):\\mathrm{EVE}(c)=0]\\end{array}$ The Second Def: The probability of EVE guessing correctly which world she is in is exactly half. $\\begin{array}{l}\\text{For all EVE and all }m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\};c=E(k,m_b):\\mathrm{EVE}(c)=b] = 1/2\\end{array}$ Key Idea: Computationally Bounded AdversariesNow, back to the question of how to overcome Shannon’s conundrum. The key idea is we relax the adversary to an arbitrary computationally bounded adversary. Computational Indistinguishability (take 1)In order to define the computationally bounded adversary (or, Computational Indistinguishability), I’ll introduce the axiom of modern crypto that Feasible Computation = Probabilistic Polynomial-time. The Axiom of Modern Crypto: p.p.tFeasible Computation = Probabilistic polynomial-time p.p.t. is the abbreviation for Probabilistic Polynomial-time It’s a polynomial in a security parameter n (in next subsection). So, in the scenario of Secure Communication mentioned above, Alice and Bob are fixed p.p.t. algorithms while Eve is any p.p.t. algorithm. Toy Definition with p.p.t.Let’s attempt to define Computational Indistinguishability with p.p.t.. There are two worlds again, world 0 and world 1.The only difference from the unbounded case is that the adversary is a p.p.t. distinguisher. Write it as a mathematical definition. Toy Def: $\\begin{array}{l}\\text{For all p.p.t EVE and all }m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; c=E(k,m_0):\\mathrm{EVE}(c)=0] = \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; c=E(k,m_1):\\mathrm{EVE}(c)=0]\\end{array}$ Not FeasibleHowever, it’s still subject to Shannon’s impossibility which means the key space must be larger than or equal to the message space for any scheme s.t. the Toy Def. Prove it by contradiction same with the proof of Shannon’s Theorem. Proof: Assume the contradiction that the key is n bits and the message is n+1 bits, i.e. $\\mathcal{|K|&lt;|M|}$. If we pick any $c\\in \\mathcal{C}$ and decrypt $c$ with all distinct keys, there are at most $|\\mathcal{K}|$ possible messages.The possible message space is the blue part in the left ellipse which the size is at most $|\\mathcal{K}|$ and less than $\\mathcal{|M|}$. Assume $m_0$ is inside the possible message space (inside the blue part) while $m_1$ is outside the possible message space. Consider Eve that picks a random key k and output 0 if $\\mathrm{D}(k,c)=m_0$ (w.p $\\ge 1/2^n$)It’s possible because $m_0$ is inside the blue part. output 1 if $\\mathrm{D}(k,c)=m_1$ (w.p $=0$)It’s impossible. output a random bit if neither holds. (w.p $=1/2$) Hence, the probability of EVE guessing correctly which world she is in is more than half. $\\begin{array}{l}\\text{There exist EVE and }m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\};c=E(k,m_b):\\mathrm{EVE}(c)=b] = 1/2 + 1/2^n\\end{array}$ Computational Indistinguishability (take 2)For the purpose of circumventing the Shannon’s lower bound, I’ll introduce a new notion. Negligible FunctionsInformally, Negligible Functions are functions that grow slower than $1/p(n)$ for any polynomial $p$. Definition of Negligible Functions: A function $\\mu :\\mathbb{N} \\rightarrow \\mathbb{R}$ is negligible if for every polynomial function $p$, there exists an $n_0$ s.t. for all $n&gt;n_0$: $$\\mu(n) &lt; 1/p(n)$$ The key property of negligible function is that events that occur with negligible probability look to poly-time algorithms like they never occur. Examples:Is $\\mu$ negligible ? $\\mu(n)=1/n^{100}$ if $n$ is prime.No, choose $p(n)=n^{200}$. $\\mu(n)=1/2^n$Yes. Let $\\mu(n)$ be a negligible function and $q(n)$ a polynomial function.Then $\\mu(n)q(n)$ is negligible function. ( Run the event $q(n)$ times） Security Parameter: nSecurity Parameter $n$ (sometimes $\\lambda$) always appears in papers. What the hell is it ? Security Parameter is used to measure how secure your system is. Security Parameter is sort of an input to all these algorithms to instantiate them. So runtimes &amp; success probabilities are measured as a function of $n$. We want honest parties run in time (fixed) polynomial in $n$. We want adversaries run in time (arbitrary) polynomial in $n$, and should have success probability negligible in $n$. DefinitionNow we can define Computational Indistinguishability with p.p.t and negligible function. Definition: $ \\begin{array}{l}\\text { For all p.p.t EVE, there is a negligible function } \\mu \\text { s.t. for all } m_{0}, m_{1}: \\\\ \\operatorname{Pr}\\left[k \\leftarrow \\mathcal{K} ; b \\leftarrow\\{0,1\\} ; c=E\\left(k, m_{b}\\right): \\operatorname{EVE}(c)=b\\right] \\leq 1 / 2+\\mu(n)\\end{array} $ Hence, the probability of EVE guessing correctly which world she is in is substantially half, which is half plus a negligible function of security parameter. Pseudorandom Generators (PRG)In the section, I’ll introduce our first crypto tool: Pseudorandom Generators (PRG). Before that, I want to ask a question. Can you take 10 random bits as input and deterministically generate 20 bits of randomness as output ? No, you cannot generate randomness out of nothing. Informally, Pseudo-random Generators are deterministic programs that stretch a “truly random” seed into a (much) longer sequence of “seeming random” bits. There are more questions after having the informal definition. How to define “seeming random” ? Can such a G exist ? 3 Equiv. Definitions of PRGFor the first question that how to define a strong pseudo-random number generator, there are three equivalent definitions. Def 1 [Indistinguishability] “No polynomial-time algorithm can distinguish between the output of a PRG on a random seed vs. a truly random string.” = “as good as” a truly random string for all practical purpose. Def 2 [Next-bit Unpredictability] “No polynomial-time algorithm can predict the (i+1)th bit of the output of a PRG given the first i bits, better than chance 1/2.” Def 3 [Incompressibility] “No polynomial-time algorithm can compress the output of the PRG into a shorter string” All three definitions are equivalent! In this blog, I’ll just elaborate[*详尽说明; explain] the first definition of indistinguishability. The other two definitions and the proof will be elaborated in following blog. Def 1: IndistinguishabilityDef 1 [Indistinguishability] A deterministic polynomial-time computable function ${G}:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ is a PRG if: It is expanding: $m&gt;n$ and for every p.p.t. algorithm D (called a distinguisher or a statistical test) if there is a negligible function ${\\mu}$ such that: $$ |\\operatorname{Pr}[D({G(U_n)})=1]-\\operatorname{Pr}[D({U_m})=1]|={\\mu(n)} $$ Notation: $U_n$(resp. $U_m$) denotes the random distribution on $n$-bit (resp. $m$-bit) strings; $m$ is shorthand for $m(n)$. First of all, PRG is a deterministic polynomial-time computable function which can expand $n$ bits to $m$ bits. $m$ is shorthand for $m(n$) and $m&gt;n$. There are two worlds, world 1 and world 2. World 1 is the pseudorandom world which samples a truly random $n$-bit string from $U_n$ and expands it to a $m$-bit string using PRG $G$, and output the expanded $m$-bit string as $y$. World 0 is the truly random world which sample a truly random $m$-bit string from $U_m$, and output the sampled $m$-bit string as $y$. The adversary is a p.p.t distinguisher that gets the $y$ and tries to tell which world she’s in. Why good ?Why is this a good definition ? It’s good for all applications. As long as we can find truly random seeds, can replace true randomness by the output of PRG(seed) in any (polynomial-time) application. If the application behaves differently, then it constitutes a (polynomial-time) statistical test between PRG(seed) and a truly random strings. PRG can Overcome Shannon’s ConundrumPRG can overcome Shannon’s conundrum. That is, we can encrypt $n+1$ bits using an $n$-bit key. How to achieve it ?The scheme consists of three algorithms. $Gen(1^n)$: generate a random n-bit key $k$ $Enc(k,m)$ where $m$ is a $m(n)$-bit message: Expand $k$ into a $(n+1)$-bit pseudorandom string $k’=G(k)$ One time pad with $k’$: $c = k’\\oplus m$ $Dec(k, c)$: output $G(k)\\oplus c$ $G$ is a public and deterministic p.p.t. function, so Alice and Bob can both calculate the one-time pad key $k’=G(k)$. Correctness: $Dec(k,c)=G(k)\\oplus c = G(k)\\oplus G(k)\\oplus m = m$ The correctness is easily proved. The scheme is under the assumption that $G$ is a pseudorandom generator. Hence, if there exists a PRG, we can move beyond Shannon’s. Security AnalysisThe most important thing is to prove the scheme is computational secure. Recall the definitions of computational indistinguishability and pseudorandom generator. Def of Computational Indistinguishability: $\\begin{array}{l}\\text{For all {p.p.t} EVE, {there is a negligible function} }{\\mu }\\\\ \\text{s.t. for all } m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\};c=E(k,m_b):\\mathrm{EVE}(c)=b] \\le 1/2 + \\mu(n)\\end{array}$ Def of PRG [Indistinguishability]: A deterministic polynomial-time computable function ${G}:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ is a PRG if: $\\begin{array}{l}\\text{For all {p.p.t} EVE, {there is a negligible function} }{\\mu }\\text{ s.t. for all } m_0, m_1:\\\\ |\\operatorname{Pr}[D({G(U_n)})=1]-\\operatorname{Pr}[D({U_m})=1]|={\\mu(n)}\\quad ,m>n\\end{array}$ The following proof is maybe your first reduction. The scheme is under the assumption that $G$ is a pseudorandom generator. Hence, the thing we want to prove is if $G$ is a pseudorandom generator, then the scheme is computational secure. Proof: We want to prove the contradiction for simplicity that if the scheme doesn’t satisfy the computational indistinguishability, then the assumption doesn’t hold, i.e. $G$ is not a PRG. Suppose for contradiction that there is a p.p.t. $\\operatorname{EVE}$, a polynomial function $p$ and $m_0, m_1$ s.t. $\\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\};c=Enc(k,m_b):\\mathrm{EVE}(c)=b] \\ge 1/2 + p(n)$ Case 1: If $\\operatorname{EVE}$ is a distinguisher for the scheme, instantiate it. $\\rho = \\operatorname{Pr}[k\\leftarrow \\{0,1\\}^n; b\\leftarrow \\{0,1\\};c=G(k)\\oplus m_b:\\mathrm{EVE}(c)=b] \\ge 1/2 + p(n)$ The probability of $\\operatorname{EVE}$ guessing correctly which world she is in is more than half. As a result, $\\operatorname{EVE}$ is able to distinguish case 1. Case 2: If EVE is a distinguisher for one-time pad, instantiate it. $\\rho' = \\operatorname{Pr}[k'\\leftarrow \\{0,1\\}^{n+1}; b\\leftarrow \\{0,1\\};c=k'\\oplus m_b:\\mathrm{EVE}(c)=b] = 1/2 \\;(+\\mu(n))$ The probability of $\\operatorname{EVE}$ guessing correctly which world she is in is substantially half. As a result, $\\operatorname{EVE}$ cannot distinguish case 2. Then we can construct a distinguisher $\\operatorname{EVE’}$ for $G$ using distinguisher $\\operatorname{EVE}$. A distinguisher for PRG is to get $y$ and try to tell whether $y$ is from the pseudorandom world and the truly random world. Construct $\\operatorname{EVE}’$:Get as input a string $y$, run $\\operatorname{EVE}(y\\oplus m_b)$ for a random $b$, and let $\\operatorname{EVE}$’s output be $b’$. Output “PRG” if $b=b’$ and “RANDOM” otherwise. Run $\\operatorname{EVE}(y\\oplus m_b)$ for a random $b$ Let $b’:=\\operatorname{EVE}(y\\oplus m_b)$ $\\operatorname{EVE}'(y)=\\begin{cases} \\text{PRG},& b=b'\\\\ \\text{RANDOM},& \\text{otherwise}\\end{cases}$ If $y$ is from the pseudorandom world, $\\operatorname{EVE}$ is trying to distinguish the Case 1 when running $\\operatorname{EVE}(y\\oplus m_b)$ for a random $b$. The probability of $\\operatorname{EVE}(y\\oplus m_b)$ guessing $b$ correctly, i.e. $b’=b$, is more than half. $\\operatorname{Pr}[\\operatorname{EVE’}\\text{ outputs PRG}\\mid y\\text{ is pseudorandom}]=\\rho \\ge 1/2 + 1/p(n)$ If $y$ is from the truly random world, $\\operatorname{EVE}$ is trying to distinguish the Case 2 when running $\\operatorname{EVE}(y\\oplus m_b)$ for a random $b$. The probability of $\\operatorname{EVE}(y\\oplus m_b)$ guessing $b$ correctly, i.e. $b’=b$, is substantially half. $\\operatorname{Pr}[\\operatorname{EVE’}\\text{ outputs RANDOM}\\mid y\\text{ is random}]=\\rho’ = 1/2$ Hence, the advantage of $\\operatorname{EVE}’$ distinguishing for $G$ is $\\operatorname{Pr}[\\operatorname{EVE'}\\text{ outputs PRG}\\mid y\\text{ is pseudorandom}]-\\operatorname{Pr}[\\operatorname{EVE'}\\text{ outputs RANDOM}\\mid y\\text{ is random}]\\ge 1/p(n)$ , which proves $G$ is not a PRG. QED. So far we have proven it that $G$ is not a PRG if the scheme is not (computational) secure which also means the scheme is (computational) secure if there exists a PRG $G$. We make a reduction from whether the scheme is (computational) secure to whether PRGs exist. It’s a simple but typical reduction. The reductions will be more intricate but the principle remains same. Two Questions to PRGAs a consequence, the proof of security comes back to PRGs. There are two questions to PRGs Q1: Do PRGs exist ? Q2: How do we encrypt longer messages or many messages with a fixed key ? The next subsection will answer the first question Q1 while Q2 will be answered in following blog. Q1: Do PRGs Exist ?Do PRGs exist ? If P=NP, PRGs do not exist. I believe P$\\ne$NP so I believe PRGs do exist. There are two methodologies to construct PRGs, the practical methodology and the foundational mehodology. Rigndael (AES) is designed by the practical methodology, but this course is more about the foundational methodology. The Practical Methodology Start from a design framework.A practical truth is appropriately chosen functions composed appropriately many times look random. For example, if you repeat the knot many and many times, then you will get a random line ball. Come up with a candidate construction.e.g. the construction of Rijndael (AES) . Do extensive cryptanalysis. The Foundational MethodologyThe maxim is reducing to simpler primitives. Construct one-way function (OWF) from well-studied and average-case hard problems. Then create the thriving world of cryptography based on OWF. “Science wins either way” ——Silvio Micali There is a PRG candidate from the average-case hardness of subset-sum. A PRG Candidate from the average-case hardness of Subset-sum: $G(a_1,\\dots,a_n,x_1,\\dots,x_n)=(a_1,\\dots,a_n,\\sum_{i=1}^n x_i a_i \\mod{2^{n+1}})$ where $a_i$ are random $(n+1)$-bit numbers, and $x_i$ are random bits. Nevertheless, there exists a better way to construct PRG. Beautiful Function: If $G$ is one-way function, then $G$ is a PRG. If lattice problems are hard on the worst-case, $G$ is a PRG.","link":"/2022/07/02/mit6875-lec2/"},{"title":"「Cryptography-MIT6875」: Lecture 5","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics covered: Applications of PRFs Note: The second half of Lecture 5, Number Theory, is contained in Lecture 6. RecapIn last blog, we introduced the security definition of the stateless secret-key encryption. Then we elaborated on the theorem of PRF, if there are PRGs, then there are PRFs, and gave the Goldreich-Goldwasser-Micali (GGM) construction. Applications of PRFsIn this blog, we will show some applications of PRF. Identification Protocols Authentication Application to Learning Theory. Identification ProtocolsPRF is widely used in identification protocols. Think about the situation that you take your ID card and put it on a RFID to pay the money. The ID card has a secret embedded in it that is specific to your cost right. So it’s sort of a key that the card contains. The device has access to a database which stores all the students keys. The student need the ID card to authenticate to the device. However, there is an eavesdropper in the channel, who is listening to the communications and wants to impersonate Tim. PRFs can prevent the adversary from impersonating. Unpredictability of PRFBefore that, let’s introduce a simple lemma about unpredictability of PRF. Let $f_s:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m$ be a pseudorandom function. Consider an adversary who requests and obtains $f_s(x_1),\\dots,f_s(x_q)$ for a polynomial $q=q(n)$. Can she predict $f_s(x^*)$ for some $x^*$ of her choosing where $x^*\\notin \\{x_1,\\dots, x_q\\}$? Lemma: If she succeeds with probability $\\frac{1}{2^m}+1/p(n)$, then she broke PRF security.This $(\\frac{1}{2^m})$ is negligible in $n$ is $m$ is large enough. It’s easy to comprehend that the adversary cannot even distinguish it and certainly she cannot predict it. So indistinguishability implies unpredictability and it turns out the lemma is right. In [the lecture 3], we proved that the Unpredictability = Indistinguishability for bits (or, for PRG). However, for PRF, Indistinguishability → Unpredictability, but not vice versa. Challenge-Response ProtocolThe secret in the ID card is the PRF key $s$. The device has access to the database which stores all the students’ mapping relation, (ID number $ID$, PRF Key $s$). Whenever Tim wants to authenticate to the device, the protocol is as follows: The device sends a random $r$ to the ID card. The ID card use the embedded key $s$ to evaluate $f_s(r)$, and sends the pair $(ID, f_s(r))$ to the device. According to the lemma of unpredictability, it’s easy to prove the security of the protocol. The adversary collects $(r_i, f_s(r_i))$ for poly. many $r_i$(potentially of her choosing). She eventually has to produce $f_s(r^*)$ for a fresh random $r^*$ when she is trying to impersonate. This is hard as long as the input and output lengths of the PRF are long enough. AuthenticationAnother scenario is to use PRF as the Message Authentication Code(MAC). Consider the initial secure communication. Alice and Bob have an agreed key $k$, and they use the one-time pad to encrypt the message. The adversary can learning nothing from the ciphertext according to Perfect Secrecy. But one-time pad (and encryption schemes in general) are malleable. The adversary can toggle between $m$ and $m’$ easily. She can change the valid ciphertext $m\\oplus k$ to a totally different ciphertext $m’\\oplus k$. Likewise, the adversary can change the valid ciphertext $(r, f_k(r)\\oplus m)$ to a totally different ciphertext $(r,f_k(r)\\oplus m’)$ in the stateless secret-key encryption. MACIt is of importance to use Message Authentication Codes. Alice and Bob have an agreed key $k$ to specify a pseudorandom function $f_k$. MAC is evaluated by the pseudorandom function $f_k$, the message $m$ taken as input. People can see a bunch of messages and tags (MAC) and cannot come up with the new message and the corresponding tag. MACs give us integrity, but not privacy. There is a solution to guarantee the privacy, Encrypt, then MAC. Note that there are two difference PRF $f_k$ and $f_{k’}$ since the input length is different. Learning TheoryThere is a negative results in learning theory. Theorem [Kearns and Valiant 1994]: Assuming PRFs exist, there are hypothesis classes that cannot be learned by polynomial-time algorithms. It’s showed that the function cannot be learned if PRFs exist. But it gives a counterpoint to machine learning that says everything can be learned. You just throw the samples to a deep neural network.","link":"/2022/07/12/mit6875-lec5/"},{"title":"「Cryptography-MIT6875」: Lecture 4","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics: Stateless Secret-key Encryption with PRF. The Goldreich-Goldwasser-Micali (GGM) PRF construction. RecapIn the previous blog, we introduced the second definition of PRG, Next-bit Unpredictability. Using hybrid argument, we achieved the predicting-to-distinguishing reduction to prove Next-bit Unpredictability = Indistinguishability for PRGs. We elaborated the theorem of PRG Length Extension and used it to construct stateful secret-key encryption scheme. For the purpose of stateless encryption, we brought about a new notion, Pseudorandom Functions (PRFs). AgendaIn the first place, we’ll finish up the the stateless secret-key encryption scheme. The last blog elaborated the theorem of PRG Length Extension, if there is a PRG that stretched by one bit, there is one that stretched by poly. many bits. In this blog, we’ll introduce the theorem of PRF, if there is a PRG, then there is a PRF. In addition, we will elaborate the Goldreich-Goldwasser-Micali (GGM) construction. PRFDefinitionRecall the definition of PRF. Collection of the Pseudorandom Functions: Consider the collection of pseudorandom functions $\\mathcal{F}_l=\\{f_k:\\{0,1\\}^l\\rightarrow\\{0,1\\}^m\\}_{k\\in\\{0,1\\}^n}$, each of which maps $l$ bits to $m$ bits. indexed by a key $k$. $n$ : key length, $l$: input length, $m$: output length. Independent parameters, all poly(sec-param)=poly(n). #functions in $\\mathcal{F}_l\\le 2^n$ (single exponential in $n$) Collection of ALL Functions: Consider the collection of ALL functions $ALL_l=\\{f:\\{\\ 0,1\\}^l\\rightarrow \\{0,1\\}^m\\}$, each of which is maps $l$ bits to $m$ bits. #functions in $ALL_l\\le 2^{m2^l}$ (doubly exponential in $l$) The #functions in the pseudorandom collection is much less than that in the random collection. But the pseudorandom functions should be “indistinguishable” from random. There are two worlds, the pseudorandom world and the random world. The pseudorandom world Sample a function $f$ from $\\mathcal{F}_l$.The function $f$ is picked one and stays fixed for all. The oracle is instantiated once the $f$ is picked. The oracle responses $f(x)$ for each query $x$.You can think there is a truth table of $f$ in the oracle.It responses the same $f(x)$ for the same query $x$ since the function $f$ is fixed. The random world sample a function $f$ from $ALL_l$.Likewise, the function $f$ is picked one and stays fixed for all. The oracle is instantiated once the $f$ is picked. The oracle responses $f(x)$ for each query $x$.Likewise, it responses the same $f(x)$ for the same query $x$ since the function $f$ is fixed. The Distinguisher $D$ has the power of querying the oracle many poly. times and try to guess which world she is in, the pseudorandom world or the random world. Definition of PRF: For all p.p.t. $D$, there is a negligible function $\\mu$ s.t. $$|\\operatorname{Pr}[f\\leftarrow \\mathcal{F}_l:D^f(1^n)=1]-\\operatorname{Pr}[f\\leftarrow ALL_l:D^f(1^n)=1]|\\le \\mu(n)$$ Notation: Actually, $D$ dose not have a challenge since she gets nothing as input except the secure parameter, but she has the power of querying the oracle many poly. times. The Relation in PRG and PRFPonder over the relation in PRG and PRF. They both expand a few random bits into many pseudorandom bit. PRG expands $n$ bits into $p(n)$ bits while PRF expands $n$ bits into $2^l$ bits that $l=p(n)$. With a PRG, accessing $2^l$-th bit takes time $2^l$ time. With a PRF, this can be done in time $l$. So, a PRF = locally accessible (or, random-access) PRG. Stateless-key EncryptionWe can design stateless secret-key encryption from PRF. $Gen(1^n)$: Generate a random $n$-bit key $k$ that defines $f_k:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m$.The domain size of $f_k$, $2^l$, had better be super-polynomially large in $n$. $Enc(k,m)$: Pick a random $x$ and let the ciphertext $c$ be the pair $(x,y=f_k(x)\\oplus m)$.It’s a polynomial time to evaluate $f_k(x)$ since $f_k$ are random accessible. $Dec(k,c=(x,y))$: Output $f_k(x)\\oplus y$. Instead of remembering the whole truth table of $f$, we remember just the key $k$. Notation: We cannot pick up the same $x$ for the different messages since one-time pad cannot be reused. Therefore, the domain size of $f_k$ is supposed be super-polynomially large in $n$ to prevent collision of $x$. Security of Secret-Key Enc. (for one msg)In [the second blog] of this series, we defined the computationally indistinguishability of the secret-key encryption scheme. There are two mathematical forms of Computational Indistinguishability. We can use them to define the security of the secret-key encryption from PRF respectively. There are two worlds, the left world and the right world. They both sampled a key $k$ using $Gen(1^n)$. The left world uses the key to encrypt $m_0$ while the right world uses the key to encrypt $m_1$ using $Enc(k,m)$. The distinguisher $D$ gets the ciphertext $c=Enc(k,m)$ and tries to guess which world she is in. We can get the definitions. Definition 1 $\\begin{array}{l}\\text{For all }m_0,m_1,\\text{and all p.p.t. }D,\\text{there is a negligible function }\\mu \\text{ s.t.} \\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\}:D(Enc(k,m_b))=b] \\le 1/2 + \\mu(n)\\end{array}$ Definition 2 $\\begin{array}{l}\\text{For all }m_0,m_1,\\text{and all p.p.t. }D,\\text{there is a negligible function }\\mu \\text{ s.t.} \\\\ \\mid \\operatorname{Pr}[k\\leftarrow \\mathcal{K}:D(Enc(k,m_0))=1] -\\operatorname{Pr}[k\\leftarrow \\mathcal{K}:D(Enc(k,m_1))=1]\\mid \\le \\mu(n)\\end{array}$ The two definitions are substantially equivalent. The first emphasizes the probability of $D$ guessing correctly while the second emphasized the probability of $D$ guessing she is the world 1. However, the definition indicates that the distinguisher only sees the ciphertext of one message, $m_0$ or $m_1$, which contradicts the setting that distinguisher has the power of querying the oracle poly. many times. So this definition only defines the secret-key encryption scheme for one message, which encrypts different messages with different keys, such as one-time pad. Security of Secret-Ket Enc. (for many msgs)We need to modify it for many messages since the oracle is an interactive oracle. There are two oracles. The Left Oracle $Left(\\cdot,\\cdot)$ Sample a key $k$ .The oracle is instantiated once is key is picked. For each query $(m_L, m_R)$, the left oracle uses the key $k$ to encrypt the left message $m_L$ by $c:=Enc(k,m_L)$ and responses with $c$. The Right Oracle $Right(\\cdot,\\cdot)$ Sample a key $k$.The oracle is instantiated once is key is picked. For each query $(m_L,m_R)$, the right oracle uses the key $k$ to encrypt the right message $m_R$ by $c:=Enc(k,m_R)$ and responses with $c$. The distinguisher $D$ has the power of asking the oracle many poly. times.For each query, $D$ gets the ciphertext and tries to guess with which oracle she is interacting (or which message is encrypted). We can get the secure definitions of the secret-key encryption for many messages. Definition : For all p.p.t. $D$, there is a negligible function $\\mu$ s.t.$$\\mid \\operatorname{Pr}[k\\leftarrow \\mathcal{K}:D^{Left(\\cdot,\\cdot)}(1^n)=1] -\\operatorname{Pr}[k\\leftarrow \\mathcal{K}:D^{Left(\\cdot,\\cdot)}(1^n)=1]\\mid\\le \\mu(n)$$ Crucial Supplement: Actually, the definitions of security we gave above are not achievable including for one messages and for many messages. How can we attack them ? Length Attack. We can use messages of different lengths. For the secret-key encryption for one message, if $|m_0|\\ne|m_1|$, the adversary can distinguish easily only from the length of ciphertext. For the secret-key encryption for many messages, if $|m_L\\ne m_R|$, the adversary can distinguish it easily as well. It’s a crucial point that the scheme is secure only if it hides all possible information, which many people often overlook in practice. Therefore, the supplements for these definitions are secure definition for one message: $|m_0|=|m_1|$. secure definition for many messages: $|m_L|=|m_R|$ Security AnalysisWe’ll prove the definition by hybrid argument. Proof: We know two oracles. Left oracle: $c=(x,y=f_k(x)\\oplus m_L)$ Right oracle: $c=(x,y=f_k(x)\\oplus m_R)$ The thing we want to prove is $D$ gets $c$ and cannot distinguish with which oracle she is interacting. We want to change the oracle a little bit to define a sequence of hybrid distributions.Consider the ciphertext as the distribution. Hybrid 0: $D$ gets access to the Left oracle.$c=(x,y=f_k(x)\\oplus m_L)$ Hybrid 1: Replace $f_k$ by a random function.$c=(x,y=r_x\\oplus m_L)$ Hybrid 2: Replace $f_k$ by a random function.$c=(x,y=r_x)$ Hybrid 3: Replace $f_k$ by a random function (like Hybrid 1).$c=(x,y=r_x\\oplus m_R)$ Hybrid 4: $D$ gets access to the Right oracle (like Hybrid 0).$c=(x,y=f_k(x)\\oplus m_R)$ The thing we want to prove is that Hybrid 0 and Hybrid 4 are indistinguishable. Prove Hybrid 0 = Hybrid 1 (and Hybrid 4 = Hybrid 3). It can be proved by PRF security. The definition of PRF indicates $D$ cannot distinguish from the pseudorandom world and the random world. Prove Hybrid 1 = Hybrid 2 (and Hybrid 3 = Hybrid 2). It can be proven by birthday paradox. Suppose $D$ has asked the oracle for $q$ times and gets $q$ ciphertexts. The oracle in Hybrid 2 only outputs the function value $r_x$ without $\\oplus$ operation.If $D$ gets two same ciphertexts, $D$ knows that she is interacting with the oracle in Hybrid 2. The oracle in Hybrid 1 is one-time pad.If $D$ gets two ciphertexts with the same $x$ but different $y$, $D$ knows that she is interacting with the oracle in Hybrid 1. The probability of $D$ distinguishing from Hybrid 1 and Hybrid 2 is up to the collision probability of $x$. The domain size of $x$ is $2^l$ that $l$ is super-polynomially large in $n$. The probability of picking the same $x$ is at most $q^2/2^l$, which is negligible.There are $q^2$ possible pairs to match any value in the domain size of $x$. The adjacent hybrid distributions are all indistinguishable, so Hybrid 0 and Hybrid 4 are indistinguishable. QED. Theorem of PRFFinally, we can get into the theorem of PRF. (Tired but Happy ^ - ^) PRG Length ExtensionBefore start, let’s look back at PRG Length Extension. Theorem Let $G:\\{0,1\\}^n \\rightarrow \\{0,1\\}^{n+1}$ be a PRG. Then, for every polynomial $m(n)$, there is a PRG $G':\\{0,1\\}^n\\rightarrow\\{0,1\\}^{m(n)}$.Recall the construction of the last blog. Construction: $G(s)=G_0(s)||G_1(s)$ where $G_0(s)$ is 1 bit and $G_1(s)$ is $n$ bits. We parse the output of PRG as 1 bit and $n$ bits. The 1 bit is as the output and the $n$ bits are as the seed of next call. Write the process as a tree. The problem is accessing the $i$-th output bit takes time linear in $\\approx i$. Goldreich-Goldwasser-Micali PRF Theorem: Let $G$ be a PRF.Then, for every polynomials $l=l(n),m=m(n)$, there exists a PRF family $\\mathcal{F}_l=\\{f_s:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m\\}_{s\\{0,1\\}^n}$. Note: We will focus on $m=l$. The output length could be made smaller (by truncation) or larger (by expansion with a PRG). ConstructionConstruction: Let $G(s)=G_0(s)||G_1(s)$ where $G_0(s)$ and $G_1(s)$ are both $n$ bits each. So $G:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{2n}$ is a PRG that stretches $n$ bits into $2n$ bits and parses the output in half. Notation: $G_0(s),G_1(s)$ are not functions in $s$. $G_0(s)$ represents the first $n$ bits of $G(s)$’s output and $G_1(s)$ represents the second $n$ bits of $G(s)$’s output. Write it as a tree as well. The depth is $l$ so there are $2^l$ leaves or paths. So each path/leaf labeled by $x\\in\\{0,1\\}^l$ corresponds to $f_s(x)$. The pseudorandom function family $\\mathcal{F}_l$ is defined by a collection of functions $f_s$ where: $$f_s(x_1x_2\\dots x_l)=G_{x_l}(G_{x_{l-1}}(\\dots G_{x_1}(s)))$$ If $G:\\{0,1\\}\\rightarrow \\{0,1\\}^2$ $f_s$ define $2^l$ presudorandom bits. The $x$-th bit can be computed using $l$ evaluations of the PRG $G$.(as opposed to $x \\approx 2^l$ evaluations as before) Security Analysis of GGM PRFPRG Repetition LemmaBefore proving the security of GGM PRF, let’s introduce a PRG Repetition Lemma, which is the component of the following proof. Lemma: Let $G$ be a PRG. Then, for every polynomials $L=L(n)$, the following two distributions are computationally indistinguishable: $$(G(s_1),G(s_2),\\dots,G(s_L)\\approx (u_1,u_2,\\dots,u_L)$$ This lemma can be proved easily by hybrid argument since the hybrid distributions are easy to construct. If there is a p.p.t. distinguisher between the two distributions with distinguishing advantage $\\varepsilon$, then there is a p.p.t. distinguisher for $G$ with advantage $\\ge \\varepsilon/L$. By hybrid argument, we can achieve the PRG reduction. Step 1-Hybrid ArgumentProve it by contradiction. Assume that there is a p.p.t. $D$ and a poly. function $p$ s.t.$|\\operatorname{Pr}[f\\leftarrow \\mathcal{F}_l:D^f(1^n)=1]-\\operatorname{Pr}[f\\leftarrow ALL_l:D^f(1^n)=1]|\\ge 1/p(n)$ Under the assumption, we can derive a contradiction to the security of PRG. The key idea is argument by levels of the tree. Very ingenious! For simplicity, we consider the PRG of GGM PRF is $G:\\{0,1\\}\\rightarrow \\{0,1\\}^2$, so the GGM PRF generate $2^l$ pseudorandom bits. Consider the hybrid distributions of the $2^l$ bits generated by a tree. Hybrid 0 (The Pseudorandom World): In the pseudorandom world, we consider the distribution of $2^l$ bits generated by a GGM tree as Hybrid 0. For each query $x$, the oracle responses with $b_x$ by $l$ evaluations of PRG. Hybrid 1: We can get Hybrid 1 by changing a little to the GGM tree in Hybrid 0. The difference is the first level of the tree in Hybrid 1, $s_0$ and $s_1$, are random while they are generated by PRG in Hybrid 0. Hybrid 2: We can get Hybrid 2 by changing Hybrid 1 a little. The second level of tree in Hybrid 2, $s_{00}, s_{01}, s_{10}, s_{11}$, are random while they are generated by PRG in Hybrid 1. … Hybrid $l$ (The Random World): We can define a sequence of hybrids slowly by changing a level of tree each time. At last, we get the hybrid $l$, the random world, the $l$-th level of which are all random bits. These hybrids can be efficiently computable using lazy evaluation. Lazy evaluation means that there is no need to evaluation all bits $b_1b_2\\dots b_{2^l}$, the oracle only evaluates $b_x$ for each query $x$. Let $p_i=\\operatorname{Pr}[f\\leftarrow H_i:D^f(1^n)=1]$. We know $p_0-p_l\\ge \\varepsilon$. By a hybrid argument, there is some $i$ s.t. $p_i-p_{i+1}\\ge \\varepsilon/l$. Step 2-ReductionNow we’ll prove that if a distinguisher with advantage $\\varepsilon/l$ between hybrids implies a distinguisher with advantage $\\ge \\varepsilon/ql$ for the PRG. (where $q$ is the number of queries that $D$ makes) Assume there is a p.p.t. $D$ distinguishing from Hybrid $i$ and Hybrid $i+1$ with advantage $\\varepsilon/l$. We will construct a Repeated Repetition Breaker $D’$ from the distinguisher $D$ so as to get a PRG Breaker from the PRG Repetition Lemma. The challenge of $D’$ : She gets a tuple of $(y_1,y_2,\\dots, y_q)$ and tries to guess whether they are all pseudorandom or all random. $q$ is the upper bound number of queries that $D$ makes and $q=q(n)$. $(y_1,y_2,\\dots, y_q)$ are either all pseudorandom or all random. Suppose each $y_i$ are $2n$ bits. Construction of Repeated PRG Breaker $D’$ Parse each $y_i$ in half: $y_i=(y_{i,L},y_{i,R})$. Construct a oracle by putting these $y_i$ into the $i$-th level of the tree in order.It’s a GGM tree only starting from the $i$-th level. If $(y_1,y_2,\\dots, y_q)$ are all pseudorandom, this oracle implies Hybrid $i$.Each pair of $y_i$, $(y_{i,L},y_{i,R})$, actually has an implicit parent in the tree.We don’t know it but it exists. If $(y_1,y_2,\\dots, y_q)$ are all random, this oracle implies Hybrid $i+1$. This oracle can use lazy evaluation as well.It only responses with $f(x)$ for the each coming query $x$. Let the distinguisher $D$ interact with the oracle we build.$D$ can make at most $q$ queries. If $D$ distinguishes it as Hybrid $i$, $D’$ outputs ‘Pseudorandom’. If $D$ distinguishes it as Hybrid $i+1$, $D’$ outputs ‘Random’. $D’$ has advantage with $\\varepsilon/l$ for distinguishing repeated PRG. There is a distinguisher with advantage $\\varepsilon/lq$ for PRG from the PRG Repetition Lemma. Nits of GGM PRFAlthough the construction of GGM PRF is elegant, there are some nits. Expensive: $l$ invocations of a PRG. Sequential: bit by bit, $l$ sequential invocations of a PRG. Loss in security reduction: break PRF with advantage $\\varepsilon$ → break PRG with advantage $\\varepsilon/ql$, where $q$ is an arbitrary polynomial. $q =$ # queries of the PRF distinguisher. So some new questions come into mind. Is there tighter reduction ? How to avoid the loss ?","link":"/2022/07/08/mit6875-lec4/"},{"title":"「Cryptography-MIT6875」: Lecture 3","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics: The Hybrid Argument. An application: PRG length extension. The notion of pseudorandom functions: Definition, motivation, discussion and comparison with PRGs. PRG implies (stateful) secret-key encryption. PRFs imply (stateless) secret-key encryption. RecapIn the previous blog, we defined Computational Indistinguishability that a new definition of security for secret-key encryption, which can overcome Shannon’s impossibility. Then we brought about Pseudorandom Generator which can encrypt a single message longer than the key. We gave the definition of PRG from the Indistinguishability and proved the secret-key encryption scheme by contradiction. At the end, we saw a construction of PRG based on subset sum, and answered the question of whether there is a PRG. AgendaWe already know PRG can encrypt a single message longer than the key. The last blog left the second question to PRG. Q2: How to encrypt poly. many messages with a fixed key ? This is the gist of this blog. We’ll introduce two methods. PRG length extension.We can achieve stateful encryption of poly. many messages. Theorem: If there is a PRG that stretched by one bit, there is one that stretched by poly. many bits. Another new notion: Pseudorandom Functions (PRF)We can achieve stateless encryption of poly many messages. Theorem(next blog): If there is a PRG, then there is a PRF. More importantly, we’ll introduce a new proof technique, Hybrid Arguments. PRGIn the last blog, we mentioned that there are three equivalent definitions of PRG. Def 1 [Indistinguishability] “No polynomial-time algorithm can distinguish between the output of a PRG on a random seed vs. a truly random string.” = “as good as” a truly random string for all practical purpose. Def 2 [Next-bit Unpredictability] “No polynomial-time algorithm can predict the (i+1)th bit of the output of a PRG given the first i bits, better than chance 1/2.” Def 3 [ Incompressibility] “No polynomial-time algorithm can compress the output of the PRG into a shorter string” Def 1 [Indistinguishability]We elaborated the first definition of Indistinguishability last blog. Definition 1 [Indistinguishability]: A deterministic polynomial-time computable function ${G}:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ is a indistinguishable (or, secure against any statistical test) if: for every p.p.t. algorithm D (called a distinguisher or a statistical test) if there is a negligible function ${\\mu}$ such that: $$|\\operatorname{Pr}[D({G(U_n)})=1]-\\operatorname{Pr}[D({U_m})=1]|={\\mu(n)}$$ Notation: $U_n$(resp. $U_m$) denotes the random uniform distribution on $n$-bit (resp. $m$-bit) strings; $m$ is shorthand for $m(n)$. So $G$ is a PRG if $D$ cannot distinguish from the output of the $G$ and a truly random string. Def 2 [Next-bit Unpredictability]Today I will introduce the second definition of Next-bit Unpredictability. Definition 2 [Next-bit Unpredictability]: A deterministic polynomial-time computable function ${G}:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ is a next-bit unpredictable if: for every p.p.t. algorithm D (called a next-bit predictor) and every $i\\in\\{0,\\dots,m\\}$ , if there is a negligible function ${\\mu}$ such that: $$\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_1y_2\\dots y_{i-1})=y_i]=\\frac{1}{2}+\\mu(n)$$ Notation: $y_1,y_2,\\dots,y_m$ are the bits of the m-bit string $y$. So $G$ is a PRG if the probability of $P$ predicting the right $i$th bit of $G$ is substantially half given the first $(i-1)$ bits of $G$. Ind. = NBUThere are two theorems that state the equivalence of Def 1( Indistinguishability) and Def 2(NBU). Theorem: A PRG $G$ is distinguishable if and only if it is next-bit unpredictable. Theorem: A PRG $G$ passes all (poly-time) statistical tests if and only if it passes (poly-time) next-bit tests. Next-bit Unpredictability(NBU) is seemingly a much weaker requirement. It only says that the next bit predictors, a particular type of distinguishers which takes a prefix of a string and try yo predict the next bit, cannot succeed. Yet, surprisingly, Next-bit Unpredictability (NBU) = Indistinguishability (Ind.). In addition, NBU is much more easier to use. Ind. → NBUThe first and foremost, the thing we want to prove is that if a PRG $G$ is indistinguishable, then it is next-bit unpredictable, i.e. Ind. → NBU. Proof: The main idea of contradiction is that if there exists a p.p.t. predictor $P$, then we can construct a p.p.t distinguisher $D$ from $P$, i.e. NOT NBU → NOT Ind. So we can Suppose for contradiction $G$ is NOT Next-bit Unpredictable, i.e. there is a p.p.t. predictor $P$, a polynomial function $p$ and an $i\\in\\{1,\\dots, m\\}$ s.t. $$ \\operatorname{Pr}[y\\leftarrow G(U_n):P(y_1y_2\\dots y_{i-1})=y_i]\\ge \\frac{1}{2}+1/p(n) $$ Then, I claim that $P$ essentially gives us a distinguisher $D$. The probability of $P$ predicting the right bit is more than half. Consequently, the PRG $G$ is NOT Next-bit Unpredictable. Construct a distinguisher $D$ from predictor $P$.Consider $D$ which gets an $m$-bit string $y$ and does the following:(If $P$ is p.p.t. so is $D$.) Run $P$ on the $(i-1)$-bit prefix $y_1y_2\\dots y_{i-1}$ If $P$ returns the $i$-th bit $y_i’$, then output 1 (=PRG) else output 0 (=Random). $D(y)=\\begin{cases} \\text{1(=PRG)},& y_i'=y_i\\\\ \\text{0(=Random)},& \\text{otherwise}\\end{cases}$ The task of $D$ is to guess which world she is in, the pseudorandom world or the truly random world. The task of $P$ is to predict the next-bit given the first $(i-1)$ bits of $y$. By the construction of $D$, the probability of $D$ guessing correctly is equal to the probability of $P$ predicting the right bit regardless the distribution of $y$, i.e. $\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1]=\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_1y_2\\dots y_{i-1})=y_i]$ $\\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]=\\operatorname{Pr}[y\\leftarrow U_m:P(y_1y_2\\dots y_{i-1})=y_i]$ Therefore, we want to show that $D$ is able to distinguish the PRG $G$ which means $G$ is NOT Indistinguishable, i.e. there is a polynomial $p’$ s.t. $|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p’(n)$ If $y$ is from the pseudorandom world, the probability of $D$ guessing correctly is equal to the probability of $P$ predicting the right bit by the construction of $D$. $|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1]|$ = $\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_1y_2\\dots y_{i-1})=y_i]\\ge \\frac{1}{2}+1/p(n)$ (by assumption) If $y$ is from the truly world, the probability of $P$ predicting the right bit is 1/2 since $y$ is random. $|\\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|$ =$\\operatorname{Pr}[y\\leftarrow U_m:P(y_1y_2\\dots y_{i-1})=y_i]=\\frac{1}{2}$ ($y$ is random) Hence, the advantage of the distinguisher $D$ guessing correctly is$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n)$ QED. NBU → Ind.Furthermore, we want to prove that if a PRG $G$ is next-bit unpredictable, then it is indistinguishable, i.e. NBU → Ind.. Equally, suppose for contradiction that there is a distinguisher $D$, and a polynomial function $p$ s.t. $$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n)$$ But how to construct a next-bit predictor $P$ out of the distinguisher $D$ ? It is imperative that we need a new proof technique, HYBRID ARGUMENT. Using the technique, we can prove it by the following steps: Hybrid Argument From Distinguishing to Predicting Proof of NBU → Ind.Step 1: Hybrid Argument Hybrid Argument is a proof technique used to show that two distributions are computationally indistinguishable. ——wiki The contradiction is that there is a distinguisher $D$, and a polynomial function $p$ s.t.$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n)$ We define the advantage of $D$ guessing correctly is $\\varepsilon:=1/p(n)$. A PuzzleBefore that, let’s discuss a simple puzzle. Lemma: Let $p_0,p_1,p_2,\\dots,p_m$ be real numbers s.t. $\\color{blue}{p_m-p_0\\ge \\varepsilon}$ Then, there is an index $i$ such that $\\color{blue}{p_i-p_{i-1}\\ge \\varepsilon/m}$. Proof: Write it $p_{m}-p_{0}=\\left(p_{m}-p_{m-1}\\right)+\\left(p_{m-1}-p_{m-2}\\right)+\\cdots+\\left(p_{1}-p_{0}\\right) \\ge \\varepsilon$ At least one of the $m$ terms has to be at least $\\varepsilon/m$ (averaging). Hybrid DistributionsWe define a sequence of hybrid distributions $H_0,H_1,\\dots,H_m$, which $H_0:=U_m$ and $H_m:=G(U_n)$. $H_0$ is the distribution that all bits are random while $H_m$ is the distribution that all bits are pseudorandom. Others are the distributions that some bits are random and others are pseudorandom. In addition, there is only one different bit between the adjacent hybrid distributions as shown the figure above. According to the assumption, $D$ distinguishes the $y$ between the pseudorandom world and the truly random world with advantages $\\varepsilon$, which means $D$ distinguishes the distribution between $H_m$ and $H_0$ with advantage $\\varepsilon$, i.e. $$\\operatorname{Pr}[D(H_m)=1] - \\operatorname{Pr}[D(H_0)=1]\\ge \\varepsilon$$ With reference to the puzzle, there exists $i$ such that $D$ distinguishes the distribution between $H_{i}$ and $H_{i-1}$ with advantage $\\ge \\varepsilon/m$, i.e. $\\exists i$ s.t. $$\\operatorname{Pr}[D(H_i)=1] - \\operatorname{Pr}[D(H_{i-1})=1]\\ge \\varepsilon/m$$ Random bit v.s. Pseudorandom bitBy the hybrid argument, what information can we get from $D$ distinguishing between $H_{i}$ and $H_{i-1}$with advantage $\\ge \\varepsilon/m$ ? Let’s summarize it: Define $p_i = \\operatorname{Pr}[D(H_i)=1]$.$p_0 = \\operatorname{Pr}[D(U_m)=1]$ and $p_m = \\operatorname{Pr}[D(G(U_n))=1]$. By the hybrid argument, we have $p_i-p_{i-1}\\ge \\varepsilon /m$. The key intuition is $D$ output ‘1’ more often given a pseudorandom $i$-th bit than a random $i$-th bit. Therefore, $D$ gives us a “signal” as to whether a given bit is the correct $i$-th bit or not. Right bit v.s. Wrong bitLet’s dig a bit more. $H_{i-1}$ is the hybrid distribution the $i$-th bit of which is a random bit $u$. $H_i$ is the hybrid distribution the $i$-th bit of which is the $i$-th pseudorandom bit $y_i$ generated by $G$. We define a new hybrid distribution $\\overline{H_i}$, the $i$-th bit $\\overline{y_i}$ of which is the opposite bit $y_i$ in $H_i$. With reference to $p_i=\\operatorname{Pr}[D(H_i)=1]$, we define $\\overline{p_i}=\\operatorname{Pr}[D(\\overline{H_i})=1]$. We know: $p_i-p_{i-1}\\ge \\varepsilon/m$. Claim: $p_{i-1}=(p_i+\\overline{p_i})/2$.Proof: The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. In hybrid distribution $H_{i-1}$, $u$ is a random bit. So, $\\operatorname{Pr}[u=0]=0.5$ and $\\operatorname{Pr}[u=1]=0.5$. $\\operatorname{Pr}[D(H_{i-1})=1]$ = $\\operatorname{Pr}[D(H_{i-1})=1\\wedge u=0]+[D(H_{i-1})=1\\wedge u=1]$ = $\\operatorname{Pr}[D(H_{i-1})=1\\mid u=0]\\operatorname{Pr}[u=0]+[D(H_{i-1})=1\\mid u=1]\\operatorname{Pr}[u=1]$ =$(\\operatorname{Pr}[D(H_{i-1})=1\\mid u=0]+[D(H_{i-1})=1\\mid u=1])/2$ If $u=0$ in $H_{i-1}$, we can use $H_i$ with $y_i=0$ and $\\overline{H_{i}}$ with $\\overline{y_i}=0$ to express $H_i$. $\\operatorname{Pr}[D(H_{i-1})=1\\mid u=0]$ = $\\operatorname{Pr}[D(H_{i-1})=1\\wedge y_i=0]+\\operatorname{Pr}[D(\\overline{H_i})=1\\wedge \\overline{y_i}=0]$ If $u=1$ in $H_{i-1}$, we can use $H_i$ with $y_i=1$ and $\\overline{H_{i}}$ with $\\overline{y_i}=1$ to express $H_i$. $\\operatorname{Pr}[D(H_{i-1})=1\\mid u=1]$ = $\\operatorname{Pr}[D(H_{i-1})=1\\wedge y_i=1]+\\operatorname{Pr}[D(\\overline{H_i})=1\\wedge \\overline{y_i}=1]$ Sum it up. $\\operatorname{Pr}[D(H_{i-1})=1]$ = $(\\operatorname{Pr}[D(H_i)=1\\wedge y_i=0] +\\operatorname{Pr}[D(H_i)=1\\wedge y_i=1] \\ + \\operatorname{Pr}[D(\\overline{H_i})=1\\wedge \\overline{y_i}=0] +\\operatorname{Pr}[D(\\overline{H_i})=1\\wedge \\overline{y_i}=1])/2$ = $(\\operatorname{Pr}[D(H_i)=1]+\\operatorname{Pr}[D(\\overline{H_i})=1])/2$ QED. We illustrate the claim as the following figure.$p_{i-1}$ is the midpoint of $p_i$ and $\\overline{p_i}$. Corollary: $p_i-\\overline{p_i}\\ge 2\\varepsilon/m$ (w.r.t. the claim) What can we learn from $p_i-\\overline{p_i}\\ge 2\\varepsilon/m$ ? The takeaway is that $D$ says ‘1’more often when fed with the ‘right bit’ than the ‘wrong bit’. Step 2: From Distinguishing to Predicting By the hybrid argument, we get the takeaway: $p_i-\\overline{p_i}\\ge 2\\varepsilon/m$. ( $p_i=\\operatorname{Pr}[D(H_i)=1]$ ) The distinguisher $D$ outputs ‘1’ more often when fed with the ‘right bit’ than the ‘wrong bit’. The Predictor $P$The predictor is given the first $i-1$ pseudorandom bits (call it $y_1y_2\\dots y_{i-1}$) and needs to guess the $i$-th bit. The Predictor $P$ works as follows: $$ P(y_1y_2\\dots y_{i-1})=\\begin{cases} b,& \\text{if }D(y_1y_2\\dots y_{i-1}|b| u_{i+1}\\dots u_m)=1\\\\ \\overline{b},& \\text{otherwise}\\end{cases},b\\leftarrow\\{0,1\\} $$ Pick a random bit $b$ Feed $D$ with input $y_1y_2\\dots y_{i-1}|b| u_{i+1}\\dots u_m$ ($u$’s are random). If D says ‘1’, output $b$ as the prediction for $y_i$ and if $D$ says ‘0’, output $\\overline{b}$ as the prediction for $y_i$. Analysis of Predictor $P$ The probability of $P$ predicting the right bit$\\operatorname{Pr}[x\\leftarrow {0,1}^n;y=G(x):P(y_1y_2\\dots\\ y_{i-1})=y_i]$ consists of two cases where $D$ outputs 1 and $D$ outputs 0. = $\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}b\\dots)=1 \\wedge b=y_i]+\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}b\\dots)=0\\wedge b\\ne y_i]$ ($y_i=b \\text{ or } y_i=\\overline{b}$) = $\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}b\\dots)=1 \\mid b=y_i]\\operatorname{Pr}[b=y_i] \\ +\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}b\\dots)=0\\mid b\\ne y_i]\\operatorname{Pr}[b\\ne y_i]$ = $\\frac{1}{2}(\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}y_i\\dots)=1]+\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}\\overline{y_i}\\dots)=0)$ (since b is random) = $\\frac{1}{2}(\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}y_i\\dots)=1]+1-\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}\\overline{y_i}\\dots)=1)$ = $\\frac{1}{2}(1+\\varepsilon/m)\\ge \\frac{1}{2} + 1/p(n)$ (since $m$ is also a polynomial in $n$) QED. SUMMARIZE: We want to prove that if the $G$ is next-bit predictable, then $G$ is indistinguishable. Hybrid Argument Suppose the contradiction that there is a $D$ distinguishing from the pseudorandom world and the truly random world with advantage $\\varepsilon$.$\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]\\ge \\varepsilon$ By defining a sequence of hybrid distributions, we know that $D$ outputs ‘1’ more often when fed with a pseudorandom bit than fed a random bit.$p_i-p_{i-1}\\ge \\varepsilon/m \\qquad(p_i=\\operatorname{Pr}[D(H_i)=1])$ Dig it more. We get the takeaway that $D$ says ‘1’ more often when fed a right bit than a wrong bit.$p_i-\\overline{p_i}\\ge 2\\varepsilon/m \\qquad(\\overline{p_i}=\\operatorname{Pr}[D(\\overline{H_i})=1])$ From distinguishing to predicting Construct a predictor $P$ from the distinguisher $D$ $P(y_1y_2\\dots y_{i-1})=\\begin{cases} b,& \\text{if }D(y_1y_2\\dots y_{i-1}|b| u_{i+1}\\dots u_m)=1\\\\ \\overline{b},& \\text{otherwise}\\end{cases},b\\leftarrow\\{0,1\\}$ Analysis the probability of $P$ predicting the right bit. By the hybrid argument, we can deduce $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):P(y_1y_2\\dots\\ y_{i-1})=y_i]\\ge1/2+1/p(n)$. So the probability of $P$ predicting the right bit is more than half. Proof of PBU = Ind.We have proven that Next-bit Unpredictability = Indistinguishability. Actually, Previous-bit Unpredictability(PBU) is equal to Indistinguishability, i.e. PUB = Ind.. I write it down just to exercise. The proof is similar with the proof of NBU=IND, so you can skip reading it. Proof: Prove PUB → Ind. Suppose for the contradiction that there is a p.p.t. predictor $P$, a polynomial function $p$ and an $i\\in\\{1,2,\\dots,m\\}$ s.t.$\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_{i+1}y_{i+2}\\dots y_m)=y_i]\\ge 1/2+p(n)$ Construct a distinguisher $D$ from $P$ $D(y)=\\begin{cases} \\text{1(=PRG)}&, P(y_{i+1}y_{i+2}\\dots y_m)=y_i\\\\ \\text{0(=Random)}&, \\text{otherwise}\\end{cases}$ Analysis the probability of $D$ distinguishing $y$. If $y$ is from pseudorandom world$\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] \\ = \\operatorname{Pr}[y\\leftarrow G(U_n):P(y_{i+1}y_{i+2}\\dots y_m)=y_i]\\ge 1/2+p(n)$ If $y$ is from the truly random world$|\\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]| \\ =\\operatorname{Pr}[y\\leftarrow U_m:P(y_{i+1}y_{i+2}\\dots y_m)=y_i]= 1/2$ The advantage of $D$ distinguishing $y$ is non-negligible.$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n)$ QED. Prove Ind. → PUB Suppose for the contradiction that there is a p.p.t distinguisher $D$, a polynomial function $p$ s.t.$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n):=\\varepsilon$ Hybrid Argument Define a sequence of hybrid distributions $H_0:=U_m,H_1,\\dots,H_m=G(U_n)$Define $p_i=\\operatorname{Pr}[D(H_i)=1]$ We know $p_m-p_0\\ge \\varepsilon$ from the contradiction. Define $\\overline{p_i}=\\operatorname{Pr}[D(\\overline{H_i})=1]$ where the $i$-th bit in $\\overline{H_i}$ is the opposite bit of the $i$-th bit in $H_i$. We claim $p_{i-1}=(p_i+\\overline{p_i})/2$ . (The proof is same with the above) Get the corollary $p_i-\\overline{p_i}\\ge 2\\varepsilon/m$.Takeaway: $D$ output ‘1’ more often when fed with a right bit than a wrong bit. From distinguishing to predicting Construct a predictor $P$ from the $D$ $P(y_{i+1}y_{i+2}\\dots y_m)\\begin{cases} b,& \\text{if }D(u_1u_2\\dots u_{i-1}|b| y_{i+1}y_{i+2}\\dots y_m)=1\\\\ \\overline{b},& \\text{otherwise}\\end{cases}\\\\b\\leftarrow\\{0,1\\},u\\text{ is random.}$ Analyze the probability of $P$ predicting. $\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_{i+1}y_{i+2}\\dots y_m)=y_i]$ = $\\operatorname{Pr}[D(\\dots b y_{i+1}y_{i+2}\\dots y_m)=1 \\wedge b=y_i]+\\operatorname{Pr}[D(\\dots b y_{i+1}y_{i+2}\\dots y_m)=0\\wedge b\\ne y_i]$($y_i=b \\text{ or } y_i=\\overline{b}$) =$\\frac{1}{2}(\\operatorname{Pr}[D(\\dots y_i y_{i+1}y_{i+2}\\dots y_m)=1]+\\operatorname{Pr}[D(\\dots \\overline{y_i} y_{i+1}y_{i+2}\\dots y_m)=0)$ (since b is random) =$\\frac{1}{2}(\\operatorname{Pr}[D(\\dots y_i y_{i+1}y_{i+2}\\dots y_m)=1]+1-\\operatorname{Pr}[D(\\dots \\overline{y_i} y_{i+1}y_{i+2}\\dots y_m)=1)$ =$\\frac{1}{2}(1+\\varepsilon/m)\\ge \\frac{1}{2} + 1/p(n)$ QED. PRG Length ExtensionLet $G:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{n+1}$ be a pseudorandom generator. The goal is to used $G$ to generate poly. many pseudorandom bits. The construction $G’$ consists of poly. many calls of $G$. The input of $G’$ is $s_0$ ($n$-bit). The output of $G$ is parsed by $b_i||s_i$, which $b_i$ is 1-bit and $s_i$ is $n$-bit. The output of $G’$ are poly. many pseudorandom bit. It’s also called a stream cipher by the practitioners. Security AnalysisTheorem: If there is a PRG that stretched by one bit, there is one that stretched by poly. many bits. The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. $G:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{n+1}$ is a PRG that stretched by one bit. Define $G’(s_0)=b_1b_2\\dots b_L$ with the above construction, which is a PRG that stretched by poly. many bits.$s_0$ is $n$-bit random string and $L$ is a polynomial in $n$. The thing we want to prove is that if $G:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{n+1}$ is a secure PRG, then $G':\\{0,1\\}^n\\rightarrow \\{0,1\\}^{n+L}$ is a secure PRG. There are many definitions of PRG, but it’s easy to work with Previous-bit Unpredictability here. Proof: Suppose for the contradiction that there is a p.p.t. predictor $P$, who can predict the previous-bit, and a polynomial function $p$ and an $i\\in\\{1,2,\\dots,L\\}$ s.t.$\\operatorname{Pr}[b\\leftarrow G’(U_n):P(b_{i+1}b_{i+2}\\dots b_L)=b_i] \\ge 1/p(n):=\\varepsilon$ Then the predictor $P$ essentially gives us an adversary $A$ against $G$. The task of $A$ is to predict $b_i$ given $s_i$ that $b_i$ is the first bit of $G(s_{i-1})=b_i||s_i$. The construction of $A$ Run $G’$ on the seed $s_i$.We can get $b_{i+1},b_{i+2},\\dots,b_L$ from the output of $G’(s_i)$. Feed $P$ with $b_{i+1},b_{i+2},\\dots,b_L$. $A$ returns the output of $P$. Analysis of the adversary $A$ predicting the previous bit. $\\operatorname{Pr}[b_i||s_i\\leftarrow G(U_n):A(s_i)=b_i]$ = $\\operatorname{Pr}[(b_{i+1},b_{i+2},\\dots,b_L)\\leftarrow G’(s_i):P(b_{i+1},b_{i+2},\\dots,b_L)=b_i]$ = $\\operatorname{Pr}[b\\leftarrow G’(U_n):P(b_{i+1},b_{i+2},\\dots,b_L)=b_i]\\ge 1/p(n)$ QED. Stateful Secret-key EncryptionFrom the PRG length extension, we can encrypt poly. many messages with a fixed key. The procedure is as follows. Alice and Bob have an agreed key $s_0$ as the initial state of $G’$ Alice wants to encrypt a 1-bit $m$.Then Alice and Bob uses $G’(s_0)$ to generate 1 bit $b_1$ as the one-time pad key, and their states convert to $s_1$. Alice wants to encrypt a 3-bit $m’$.Then Alice and Bob uses $G’(s_1)$ to generate 3 bits $b_2b_3b_4$ as the one-time pad key, and their states convert to $s_4$. Now Bob wants to encrypt a 1-bit $m’’$.Then Alice and Bob uses $G’(s_4)$ to generate 1 bit $b_5$ as the one-time pad key, and their states convert to $s_5$. It’s achievable that Alice and Bob can keep encrypting as many bits as they wish. However, Alice and Bob have to keep their states in perfect synchrony. They cannot transmit simultaneously. Otherwise, correctness goes down the drain, so does security. PRFIt’s stateful encryption using PRG length extension. How to be stateless ? There is an idea that Alice and Bob can generate (poly.) many and many bits, such as $n^{100}$ bits. If Alice want to encrypt 1-bit $m$, she can use a random key $b_x$ indexed by the random number $x$ she picks up and send $(x,m\\oplus b_x$) to Bob. Yet, it dose not work. Because there is a collision of Alice using the same one-time key with non-negligible probability.$\\operatorname{Pr}[\\text{Alice’s first two indices collide}]\\ge 1/n^{100}$ To prevent the collision, there is another idea that Alice and Bob can generate $2^n$ bits and the probability of collision is negligible.$\\operatorname{Pr}[\\exists \\text{ collision in }t=p(n)\\text{ indices}]\\le t^2/2^n=neg(n)$ But it brings about another problem that Alice and Bob are not poly-time. Although it dose not work, there is some inspiration. The goal is never compute the exponentially long string explicitly since it’s not poly-time. Instead, we want a function $f_k(x)=b_x$, that $b_x$ is the $x$-th bit in the implicitly defined (pseudorandom) string. It is computable in polynomial time $p(|x|)=p(n)$ that $|x|$ is the length of $x$. And $f_k(x_1),f_k(x_2)\\dots$ are computationally indistinguishable from random bits for random $x_1,x_2,\\dots$ Hence, there is no need to store the state after each encryption since you can get the encryption key directly by computing the function. Consequently, it is the stateless encryption of poly. many messages. The functions are called Pseudorandom Functions. DefinitionConsider these two collections of functions. Collection of the Pseudorandom Functions: Consider the collection of pseudorandom functions $\\mathcal{F}_l=\\{f_k:\\{0,1\\}^l\\rightarrow\\{0,1\\}^m\\}_{k\\in\\{0,1\\}^n}$, each of which maps $l$ bits to $m$ bits. indexed by a key $k$. $n$ : key length, $l$: input length, $m$: output length. Independent parameters, all poly(sec-param)=poly(n). #functions in $\\mathcal{F}_l\\le 2^n$ (single exponential in $n$) Every (pseudorandom) function is indexed by the key $k$, so if the $k$ is fixed, the function $f_k$ is fixed. So the number of functions in the pseudorandom world is up to the number of the keys, that is $2^n$. Collection of ALL Functions: Consider the collection of ALL functions $ALL_l=\\{f:\\{\\ 0,1\\}^l\\rightarrow \\{0,1\\}^m\\}$, each of which is maps $l$ bits to $m$ bits. #functions in $ALL_l\\le 2^{m2^l}$ (doubly exponential in $l$) For a fixed $m$-bit string in the output space, there are at most $2^l$ possible inputs with different functions. So the number of the functions in the random world is at most $(2^m)^{2^l}=2^{m2^l}$, which is doubly exponential. The #functions in the pseudorandom world is much less than that in the random world. But the pseudorandom functions should be “indistinguishable” from random. There are two worlds, the pseudorandom world and the random world. The pseudorandom world Sample a function $f$ from $\\mathcal{F}_l$.The function $f$ is picked one and stays fixed for all. The oracle is instantiated once the $f$ is picked. The oracle responses $f(x)$ for each query $x$.You can think there is a truth table of $f$ in the oracle.It responses the same $f(x)$ for the same query $x$ since the function $f$ is fixed. The random world sample a function $f$ from $ALL_l$.Likewise, the function $f$ is picked one and stays fixed for all. The oracle is instantiated once the $f$ is picked. The oracle responses $f(x)$ for each query $x$.Likewise, it responses the same $f(x)$ for the same query $x$ since the function $f$ is fixed. The Distinguisher $D$ has the power of querying the oracle many poly. times and try to guess which world she is in, the pseudorandom world or the random world. Definition: For all p.p.t. $D$, there is a negligible function $\\mu$ s.t. $$|\\operatorname{Pr}[f\\leftarrow \\mathcal{F}_l:D^f(1^n)=1]-\\operatorname{Pr}[f\\leftarrow ALL_l:D^f(1^n)=1]|\\le \\mu(n)$$ Notation: You can consider the subscript $f$ as an interactive C program, which fed with an input and outputs the result. The distinguisher $D$ actually gets nothing as input except the secure parameter. Stateless Secret-key EncryptionWe can define the stateless secret-key encryption scheme from PRF. $Gen(1^n)$: Generate a random $n$-bit key $k$ that defines $f_k:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m$ The domain size of $f_k$, $2^l$, had better be super-polynomially large in $n$. $Enc(k,m)$: Pick a random $x$ and let the ciphertext $c$ be the pair $(x,y=f_k(x)\\oplus m)$. $Dec(k,c=(x,y))$: Output $f_k(x)\\oplus y$ Correctness $Dec(k,c)=f_k(x)\\oplus y=f_k(x)\\oplus f_k(x)\\oplus m=m$. Alice and Bob agree with the key $k$, which defines the pseudorandom function $f_k$. When decrypting, Bob computes the one-time key directly through $f_k(x)$. They don’t need to store $f_k$( or, the giant truth table), the only thing they need to store is the key $k$. In the next blog, we’ll introduce the theorem that if there is a PRG, then there is a PRF.","link":"/2022/07/06/mit6875-lec3/"},{"title":"「Cryptography-MIT6875」: Lecture 6 - Number Theory","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ The motif of this blog is Number Theory, including the second half of Lecture 5 and the Lecture 6. It’s an excellent opportunity to learn Number Theory in manner of English. An important point in this blog is that we focus more on the statements, which is useful in later lectures, rather than the proof. About 70% of the content in this blog is originally and literally from the lecture notes. I just organize and refine it according to the logic of the professor’s narration since the lecture note is awesome. The rest is my own understanding and derivation of some theorems. And I will be learning Number Theory and completing the omitted proof. So this blog will be updated continuously. Topics covered: Groups, Order of a group and the Order of an element, Cyclic Groups. The Multiplicative Group $\\mathbb{Z}_N^*$ and $\\mathbb{Z}_P^*$ for a prime $P$. Generators of $\\mathbb{Z}_P^*$. Primes, Primality Testing. The Discrete Logarithm (DLOG) problem and a candidate OWF. Diffie-Hellman assumptions: DDH and CDH. GroupsAn Abelian group $\\mathbb{G}=(S,\\star)$ is a set $S$ together with a operation $\\star :S\\times S\\rightarrow S$ which satisfies Identity: There is an element $\\mathcal{I}\\in S$ such that for $a\\in S$, $a\\star \\mathcal{I}=\\mathcal{I}\\star a=a$. Inverse: For every $a\\in S$, there is an element $b\\in S$ such that $a\\star b =b\\star a=\\mathcal{I}$. Associativity: For every $a,b,c\\in S$, $a\\star (b\\star c)=(a\\star b)\\star c$. Commutativity: For every $a,b\\in S$, $a\\star b=b\\star a$. Order of a group and the order of an element The order of a group is the number of elements in it, namely $|S|$. The order of an element $g\\in S$ is the minimal number of times one has to perform the group operation on $g$ to get to the identity element $\\mathcal{I}$.That is, $\\mathrm{ord}(g)=\\min_{i>0}\\{g^i=\\mathcal{I}\\}$. Theorem 1 (Lagrange’s Theorem): The order of any element divides the order of the group. Generator of a GroupA generator of a group $\\mathbb{G}$ is an element of order $|\\mathbb{G}|$. In other words, $$ \\mathbb{G}=\\{g,g^2,\\dots,g^{|\\mathbb{G}|}=\\mathcal{I}\\} $$ Cyclic groupA group $\\mathbb{G}$ is called cyclic if it has a generator. Theorem 2: Every group whose order is a prime number is cyclic.Moreover, every element other than the identity is a generator. Proof[2] Discrete LogarithmsLet $\\mathbb{G}$ be a cyclic group. We know that $\\mathbb{G}$ has a generator $g$, and that every $h\\in \\mathbb{G}$ can be written as $h=g^x$ for a unique $x\\in\\{1,2,\\dots,|\\mathbb{G}|\\}$. We write $$x=\\operatorname{dlog}_g(h)$$ to denote the fact that $x$ is the discrete logarithm of $h$ to the base $g$. We will look for groups where computing the group operation is easy (namely, polynomial time) but computing discrete logarithms is hard (namely, exponential or sub-exponential time). Our source for such groups will come from number theory. Discrete logarithms in $\\mathbb{Z}_N$ are, for better or worse, easy. Baby (Computational) Number TheoryThe complexity of basic operations with numbers. $n$ denotes the input length for each of these operations. Greatest Common Divisors: The greatest common divisor (gcd) of positive integers $a$ and $b$ is the largest positive integer $d$ that divides both $a$ and $b$. $a$ and $b$ are relatively prime if their gcd is 1. The Multiplicative Group $\\mathbb{Z}_N^*$The multiplicative group of numbers mod $N$, denoted $\\mathbb{Z}_N^*$, consists of the set $$ S=\\{1\\le a< N:\\operatorname{gcd}(a,N)=1\\} $$ with multiplication mod $N$ being the operation. Some further facts about $\\mathbb{Z}_N^*$ The order of $\\mathbb{Z}_N^*$, the number of positive integers smaller than $N$ that are relatively prime to it, is called the Euler totient function of $N$ denoted $\\varphi(N)$. $\\varphi(p)=p-1$ if $p$ is prime. $\\varphi(p^k)=p^k-p^{k-1}$ if $p$ is prime. $\\varphi(pq)=\\varphi(p)\\varphi(q)$ if $\\operatorname{gcd}(p,q)=1$. If $N=\\prod _i p_i^{\\alpha_i}$ is the prime factorization of $N$, then $\\varphi(N)=\\prod_i p_i^{\\alpha_i-1}(pi-1)$. The Multiplicative Group $\\mathbb{Z}_p^*$ $\\mathbb{Z}_p^*$ is Cyclic. The following theorem is a very important property of $\\mathbb{Z}_p^*$ when $P$ is prime. Theorem 4 : If $P$ is prime, then $\\mathbb{Z}_p^*$ is a cyclic group. Proof[2] Note: It is very tempting to prove this theorem by appealing the Theorem 2 which says that every group with prime order is cyclic. Be careful, and note that the order of $\\mathbb{Z}_p^*$ is $P-1$, which is decidedly not prime. There are several followup questions. How many generators are there for $\\mathbb{Z}_p^*$ ? How to tell (efficiency) if a given element $g$ is a generator ? How to sample a random generator for $\\mathbb{Z}_p^*$ ? $\\mathbb{Z}_p^{*}$ and $\\mathbb{Z}_{P-1}$ Before proceeding further, let us note the following structural fact about $\\mathbb{Z}_P^*$. There are two groups are isomorphic with an isomorphism $\\phi$ that maps $x\\in\\mathbb{Z}_{P-1}$ to $g^x\\in \\mathbb{Z}_P^*$. [isomorphic: 同构的] In particular, consider $\\phi(x)=g^x \\pmod P$. We have $\\phi(x+y)=\\phi(x)\\cdot \\phi(y)$. The isomorphism is efficiently computable in the forward direction (exponentiation, using the repeated squaring algorithm) but not known to be efficiently computable in the reverse direction. For example, consider $\\mathbb{Z}_7^*$ and $\\mathbb{Z}_6$: $\\mathbb{Z}_7^* = \\{1,2,3,4,5,6\\}$ and there is a generator $g=5$. You can get $\\{g,g^2,g^3,g^4,g^5,g^6\\} =\\{5, 4,6,2,3,1\\}$.When you perform the multiplication on $g$, you will wrap around the group.That’s an intuitive reason why discrete logarithm is hard in $\\mathbb{Z}_p^*$. $\\mathbb{Z}_6=\\{1,2,3,4,5,6\\}$ and the generator $g=1$. The group operation in $\\mathbb{Z}_6$ is addition.You can get $\\{g,g^2,g^3,g^4,g^5,g^6\\}=\\{1,2,3,4,5,6\\}$.When you perform the addition on $g$, you just walk along the group. We can know that there is a one-to-one mapping $\\phi$ from $\\mathbb{Z}_6$ to $\\mathbb{Z}_7^*$, that is $\\mathbb{Z}_6\\cong \\mathbb{Z}_7^*$ . Here is another quick application of this isomorphism: Lemma : Let $P$ be an odd prime. If $g$ is a generator of $\\mathbb{Z}_P^*$, then so is $g^x$ as long as $x$ and $P-1$ are relatively prime. Proof: The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. There is a generator $g’$ in $\\mathbb{Z}_{P-1}$corresponding to $g$ in $\\mathbb{Z}_P^*$ from the isomorphism $\\mathbb{Z}_P^*\\cong \\mathbb{Z}_{P-1}$. $x$ and $P-1$ are relatively prime, and there are two statements. In group $\\mathbb{Z}_P^*$, if $g$ is a generator, then $g^x$ is a generator. In group $\\mathbb{Z}_{P-1}$, if $g’$ is a generator, then $xg’$ is a generator. From the isomorphism, we can turn the question in $\\mathbb{Z}_P^*$ to a relative question in $\\mathbb{Z}_{P-1}$. It’s easy to prove that if $x$ is a generator in $\\mathbb{Z}_{P-1}$ (which can iterate all elements), then $ax$ is also a generator in $\\mathbb{Z}_{P-1}$ (which can also iterate all elements) where $\\operatorname{gcd}(a, P-1)=1$. The main idea is to suppose for a contradiction that there are two different $x_1$ and $x_2$,i.e. $x_1\\ne x_2 \\pmod{P-1}$, satisfying $ax_1=ax_2 \\pmod{P-1}$. Then we know $P-1\\mid a(x_1-x_2)$. From $\\operatorname{gcd}(a, P-1)=1$, we know $P-1\\mid x_1-x_2$. So $x_1= x_2 \\pmod{P-1}$. (contradiction) QED. As a corollary, we immediately derive the fact that $\\phi(P-1)$ elements of $\\mathbb{Z}_p^*$ are generators. $\\mathbb{Z}_p^*$ has lots of generatorsFor the first question, $\\mathbb{Z}_p^*$ has lots of generators. Theorem 5: The number of generators in $\\mathbb{Z}_p^*$ is $\\varphi(P-1)$. But how large is $\\varphi(P-1)$ asymptotically? This is answered by the following classical theorem. Theorem 6: For every integer $N$, $\\varphi(N)=\\Omega(N/\\log \\log N)$. In other words, if you pick a random element of $\\mathbb{Z}_p^*$, you will see a generator with probability $$\\varphi(P-1)/(P-1)=\\Omega(1/\\log\\log P)$$ which is polynomial in $1/\\log P$. So, reasonably often.[asymptotical: 渐进的] Difference between Big $\\mathcal{O}$ and Big $\\Omega$:Big $\\mathcal{O}$ and Big $\\Omega$ function are used in computer science to describe the performance or complexity of an algorithm. Big $\\mathcal{O}$ is used to describe the worst case running time for an algorithm. But, Big $\\Omega$ notation, on the other hand, is used to describe the best case running time for a given algorithm. If you want to get a generator in $\\mathbb{Z}_p^*$, you will hit a generator very quickly just keeping picking random in $\\mathbb{Z}_p^*$. Then it comes to the second question. How to tell a generator of $\\mathbb{Z}_p^*$ ?The answer is that you can tell in poly. time whether $g\\in \\mathbb{Z}_p^*$ is a generator given the factorization of $P-1$. Given the factorization of P-1​At first, we know that a generator is an element of order $P-1$, so it’s easy to check whether $g^{P-1}=1 \\pmod P$. Then we need to check if there is some smaller power of $g$ that equals $1$ since the order is the minimal power. However, there are a large number of divisors of $P-1$, roughly $P^{1/\\log \\log P}$ which is not polynomial (in $\\log P$). It turns out, you do not need to check all divisors, but rather only the terminal divisors (or, the maximal factors less than $P-1$). If $P-1=\\prod_i q_i^{\\alpha_i}$, the terminal divisors are $(P-1)/q_i$ for each $i$. For example: If $P-1=p_1^2p_2^3p_3^2$. Suppose $g=p_1^2p_2^3$, then $g^{p_1^2p_2^3}=1$.You can get some powers of $g^{p_1^2p_2^3}$ that equals $1$.And the power to $g^{p_1^2p_2^3}$ is the multiplication to the exponent $p_1^2p_2^3$. You can get $g^{p_1^2p_2^3p_3}=1$ which is smaller power than $P-1$. You can also get $g^{p_1^2p_2^3p_3^2}=1$ which is useless since $P-1=p_1^2p_2^3p_3^2$. So you only hit one terminal divisor, that is $(P-1)/p_3$. Suppose $g=p_1p_2^2p_3$, then $g^{p_1p_2^2p_3}=1$.Similarly, you can get some powers of $g^{p_1p_2^2p_3}$ that equals $1$. You can get $g^{p_1p_2^3p_3^2}=1$ by power $p_2p_3$.You hit one terminal divisor $(P-1)/p_1$. You can get $g^{p_1^2p_2^2p_3^2}=1$ by power $p_1p_3$.You hit one terminal divisor $(P-1)/p_2$. You can get $g^{p_1^2p_2^3p_3}=1$ by power $p_1p_2$.You hit one terminal divisor $(P-1)/p_3$. So you can hit 3 terminal divisors. The conclusion is that if there is a smaller power of $g$ that equals 1, then you can definitely hit some powers of terminal divisor that equals 1, which are maximal factors of $P-1$. So you can only check all the terminal divisiors. Algorithm: The following algorithm works on $g$ and the prime factorization of $P-1$. For each $i$, check if $g^{(P-1)/q_i}\\overset{?}= 1\\pmod P$. If yes for any $i$, say “not a generator”. Otherwise, say “generator”. Given only g​ and ​P​That’s nice. But can one tell if $g$ is a generator given only $g$ and $P$ ?(as opposed to the prime factorization of $P-1$ which is in general hard to compute) We don’t know, so there are some ways around it. Solution 1: Pick a random $P$ together with its prime factorization of $P-1$. This, it turns out, can be done due to a clever algorithm of Kalai [6]. Solution 2: Pick $P=2Q+1$ where $Q$ is prime. Such primes like $P$ are called safe primes, and $Q$ is called a Sophie-Germain prime after the famous mathematicians. While there are infinitely many primes, it has only been conjectured that there are infinitely many Sophie-Germain primes. This remains unproven. Moreover, the Sophie-Germain primes are considered as the hardest class of primes for discrete logarithms. Solution 2 is what people typically use in practice. In practice, you just pick a random prime as $Q$ and check whether $P=2Q+1$ is a prime. It’s efficient as long as the Sophie-Germain primes are in dense distribution. The above solution 2 brings about new questions. How to sample a random prime and how to test primality ? PrimesThere are some questions about primes. How many primes of $n$-bit length ? How to test primality ? How to sample a $n$-bit random prime ? How many primesThe prime number theorem tells us that there are sufficiently many prime numbers. In particular, letting $\\pi(N)$ denote the number of prime numbers less than $N$, we know that $\\pi(N)=\\Omega(N/\\log N)$. Thus, if you pick a random number smaller than $N$, with probability $1/\\log N$ (which is $1$/polynomial in bit-length $n$ in question) you have a prime number at hand. Primality TestingHow to recognize that a given number is prime ? This has been the subject of extensive research in computational number theory with many polynomial-time algorithms, culminating with the deterministic polynomial-time primality testing algorithm of Agrawal, Kayal and Saxena[1] (a.k.a. AKS) in 2002. Sample a primeHow to sample a $n$-bit random prime ? The above two facts put together tell us how to generate a random $n$-bit prime number. Just pick a random number less than $2^n$. And test if it is prime. In expected $n$ iterations of this procedure, you will find a $n$-bit prime number, even a random prime number. One-way FunctionsOne-way function is the atom of cryptography. A one-function is a function $f$ that is easy to compute but hard to invert on average. How to define a one-way function ? Take 1 (Wrong): For every p.p.t. algorithm $A$ and every chosen $x$,there is a negligible function $\\mu$, s.t.$$\\operatorname{Pr}[A(f(x))=x]&lt;\\mu(n)$$It is hard for $A$ to guess the inverse of $f(x)$ for every chosen $x$. It seems right. How about the function $f(x)=0$ which maps any $x$ to $0$ ? The probability of guessing the inverse of $0$ is negligible if the domain size of $x$ is sufficiently large. Because it is essentially random. But $f(x)=0$ is not one-way function obviously. Take 2: For every p.p.t. algorithm $A$ and every chosen $x$, there is a negligible function $\\mu$,s.t.$$\\operatorname{Pr}[A(f(x))\\in \\tilde{f}(f(x))]&lt;\\mu(n)$$where $\\tilde{f}$ is the inverse function. The difference of the new definition is that $A$ is trying to guess some possible inverses of $f(x)$ rather than the exactly chosen $x$. In this definition, $f(x)=0$ is not one-way function. Candidate: DLOGLet us present an informal one-way function candidate. $$f(P,g,x)=(P,g,g^x \\pmod P)$$ Computing this function can be done in time polynomial in the input length. However, inverting is the discrete logarithm problem which is conjectured to be hard. Defined formally below. Discrete Log Assumption (DLOG): For a random $n$-bit prime $P$ and random generator $g$ of $\\mathbb{Z}_P^*$, and a random $x\\in \\mathbb{Z}_{P-1}$, there is no polynomial (in $n$) time algorithm that computes $x$ given $P,g,g^x \\pmod P$. Shorthand: Easy: $P,g,x\\rightarrow g^x$ Hard: $P,g,g^x\\rightarrow x$ In fact, this is not only a one-way function, but can also be made into a family of one-way permutations. More on that later, we will see that one-way permutations can be used to build pseudorandom generators; and as we saw already, pseudorandom generators can be sued to build pseudorandom functions and stateless secret-key encryption and authentication. We can do all the crypto we saw so far based on the hardness of the discrete logarithm problem. Route: OWF→PRG→PRF. However, going via this route may not be the most efficient. So, we will look at related problems and try to build more efficient PRGs and PRFs. The Diffie-Hellman AssumptionsThere is another route to build PRGs and PRFs efficiently, that is Diffie-Hellman Assumptions. Route: DH→PRG. Route: DH→PRF. Given $g^x$ and $g^y\\mod P$, you can easily compute $g^{x+y}=g^x\\cdot g^y \\pmod P$. But it is hard to compute $g^{xy}\\pmod P$. If you can compute discrete logarithms, then you can compute $x$ from $g^x$, and raise $g^y$ to $x$ to get $(g^y)^x=g^{xy} \\pmod P$. But discrete log is hard, so this isn’t an efficient way to solve the problem. CDH AssumptionThe problem, called the computational Diffie-Hellman (CDH) problem, appears to be computationally hard, in fact as hard as computing discrete logarithms! Computational Diffie-Hellman Assumption (CDH): For a random $n$-bit prime $P$ and random generator $g$ of $\\mathbb{Z}_P^*$, and random $x,y\\in \\mathbb{Z}_{P-1}$, there is no polynomial (in $n$) time algorithm that computes $g^{xy}\\pmod P$ given $P,g,g^x\\pmod P,g^y\\pmod P$. Shorthand: Easy: $P,g,g^x,g^y\\rightarrow g^{x+y}$ Hard: $P,g,g^x,g^y\\rightarrow g^{xy}$ Moreover, it appears hard to even tell if you are given the right answer or not! DDH AssumptionBut this requires some care to formalize. At first, one may think that given $P,g,g^x,g^y$, it is hard to distinguish between the right answer $g^{xy} \\pmod P$ versus a random number $u \\mod P$. Let us call the assumption that this decisional problem is hard the decisional Diffie-Hellman (DDH) assumption. Decisional Diffie-Hellman Assumption (first take): For a random $n$-bit prime $P$ and random generator $g$ of $\\mathbb{Z}_P^*$, and random $x,y\\in \\mathbb{Z}_{P-1}$ and a random number $u\\in \\mathbb{Z}_P^*$, there is no polynomial (in $n$) time algorithm that distinguishes between $(P,g,g^x\\pmod P, g^y \\pmod P, g^{xy} \\pmod P)$ and $(P,g,g^x\\pmod P,g^y \\pmod P, u\\pmod P)$. However, the assumption turns out to be false. DDH is False in $\\mathbb{Z}_P^*$However, the DDH assumption is false in $\\mathbb{Z}_P^*$. It seems awfully strong on first look. It says not only it is hard to compute $g^{xy}$ from $g^x$ and $g^y$, but also that not even a single bit of $g^{xy}$ can be computed (with any polynomial advantage beyond trivial guessing). However, there are some information about $g^{xy}$ indeed dose leak from $g^x$ and $g^y$. Before that, let us take a quick detour in Quadratic Residues. $$h=g^2 \\pmod P$$ Some facts about Quadratic Residues: $h$ is a QR if $h=g^2\\pmod P$. 1/2 of $\\mathbb{Z}_P^*$ are QR. We can tell efficiently if $h$ is a QR by evaluating $h^{(P-1)/2}\\overset{?}=1 \\pmod P$. $h=g^x$ is a QR if and only if $x$ is even. Use the facts of quadratic residue in $\\mathbb{Z}_P^*$, we know the following statements are equivalent. $g^{xy}$ is a QR. iff $xy$ is even. iff $x$ is even or $y$ is even. iff $g^x$ is a QR or $g^y$ is a QR. Now, we can distinguish between $(P,g,g^x,g^y,g^{xy})$ and $(P,g,g^x,g^y,u)$. $g^{xy}$ is a quadratic residue with probability $3/4$ while $u$ is a quadratic residue with probability $1/2$. So the advantage for distinguishing is $1/4$. Thus, we need to refine our assumption. Looking at the core reason behind the above attack, we see that there is a 1/2 chance that $g^x$ falls into a subgroup (the subgroup of quadratic residues, to be precise) and once that happens, $g^{xy}$ is also in the subgroup no matter what $y$ is. These properties are furthermore detectable in polynomial-time which led us to attack. A solution is to work with subgroup of $\\mathbb{Z}_P^*$ of prime order. In particular, we will take $P=2Q+1$ to be a safe prime and work with $\\mathcal{QR}_P$, the subgroup of quadratic residues in $\\mathbb{Z}_P^*$. $\\mathcal{QR}_P$ has order $(P-1)/2=Q$ which is indeed prime! By virtue of this, every non-identity element of $\\mathcal{QR}_P$ is its generator w.r.t. theorem 2. With this change, we can state the following DDH assumption which is widely believed to be true. Decisional Diffie-Hellman Assumption (final): Let $P=2Q+1$ be a random $n$-bit safe prime and let $\\mathcal{QR}_P$ denote the subgroup of quadratic residues in $\\mathbb{Z}_P^*$. For a random generator $g$ of $\\mathcal{QR}_P$, and random $x,y\\in \\mathbb{Z}_Q$ and a random number $u\\in \\mathcal{QR}_P$, there is no polynomial (in $n$) that distinguishes between $(P,g,g^x\\pmod P,g^y \\pmod P, g^{xy}\\pmod P)$ and $(P,g,g^x\\pmod P, g^y \\pmod P,u\\pmod P)$. Shorthand: Hard: distinguish between $(P,g,g^x,g^y,g^{xy})$ and $(P,g,g^x,g^y,u)$. We know DLOG→CDH→DDH but no implications are known in the reverse directions. References[1] M. Agrawal, N. Kayal, and N. Saxena. Primes is in p. Annals of mathematics, pages 781–793, 2004. [2] D. Angluin. Lecture notes on the complexity of some problems est number theory. 1982. [6] A. Kalai. Generating random factored numbers, easily. Journal of Cryptology, 16(4):287–289, 2003.","link":"/2022/07/14/mit6875-lec6/"},{"title":"「Cryptography-MIT6875」: Lecture 8","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Public-key Encryption and Key exchange. Definitions: IND-Security (IND-CPA) Trapdoor permutations. Key Agreement ProblemThere is a remaining problem in secret-key encryption (or symmetric encryption), the Key Agreement Problem. How did Alice and Bob get the same $sk$ to begin with ? Can Alice and Bob, who never previously met, exchange messages securely ? Merkle’s [1974]The Merkle’s idea is based on one-way function. Assume that $H:[n^2]\\rightarrow [n^2]$ is an injective OWF. (or one-to-one). Injective Function: In mathematics, an injective function (also known as injection, or one-to-one function) is a function $f$ that maps distinct elements to distinct elements. That is, $f(x_1) = f(x_2)$ implies $x_1 = x_2$. Equivalently, $x_1\\ne x_2$ implies $f(x_1) \\ne f(x_2)$ in the equivalent contrapositive statement. Alice picks $n$ random numbers $x_1,\\dots,x_n$ Bob picks $n$ random numbers $y_1,\\dots, y_n$ There is a common number since birthday paradox.That says $x_i=y_j$ with high probability. Alice and Bob can detect it in time $\\mathcal{O}(n)$, and they set it as their shared key. How long dose it take Eve, the adversary to compute the shared key ? She knows $i$ and $j$ since she can see the ciphertexts in the channel, but she needs to inver the OWF. Assuming the OWF is very strong, that is $\\Omega(n^2)$. But the Merkle’s only protects against quadratic-time Eves although it’s still an excellent idea. Fascinating HistoryThe Public-key Encryption has a fascinating history. Merkle (1974):“Secure Communications Over Insecure Channels.” Only protects against quadratic-time adversary. Diffie &amp; Hellman (1976):“New Direction in Cryptography” Turing Award 2015. Marked the birth of public-key cryptography. Invented the Diffie-Hellman key exchange.(conjectured to be secure against all poly-time attackers unlike Merkle) Used to this day (e.g., TLS 1.3) albeit with different groups than what DH had in mind. Rivest, Shamir &amp; Adleman (1987):“A Method for Obtaining Digital Signatures and Public-Key Cryptosystems” Turing Award 2002. Invented the RSA trapdoor permutation, public-key encryption and digital signatures. RSA Signatures used to this day (e.g., TLS 1.3) in essentially the original form it was invented. Goldwasser &amp; Micali (1982):“Probabilistic Encryption” Turning Award 2012. Defined what is now the gold-standard of security of public-key encryption(two equivalent definitions: indistinguishability and semantic security) GM-encryption: based on the difficulty of the quadratic residuosity problem, the first homomorphic encryption. Public-Key EncryptionIt’s also called Asymmetric Encryption. The goal is : Anyone can encrypt to Bob Bob, and only Bob can decrypt. Public-key Encryption SchemeThe public encryption scheme works as follows: Bob generate a pair of keys, a public key $pk$, and a private (or secret) key $sk$. Bob “publishes” $pk$ and keeps $sk$ to himself. Alice encrypts $m$ to Bob using $pk$. Bob decrypts using $sk$. Hence, there are a triple of PPT algorithms $(Gen, Enc, Dec)$ s.t. $Gen(1^n)\\rightarrow (pk,sk)$: PPT Key generation algorithm generates a public-private key pair. $Enc(pk,m)\\rightarrow c$: Encryption algorithm uses the public key to encrypt message $m$. $Dec(sk,c)\\rightarrow m$: Decryption algorithm uses the private key to decrypt ciphertext $c$. Correctness:For all $pk,sk,m$: $Dec(sk,Enc(pk,m))=m$. Define the AdversaryBut how to define security ? What dose Eve know ? Eve knows Bob’s public key $pk$ Eve sees polynomially many ciphertexts $c_1,c_2,\\dots$ of messages $m_1,m_2,\\dots$ So the challenge is that Eve should not get any partial information about the set of messages. IND-Security (IND-CPA)We define the Indistinguishability Security, or Indistinguishability-CPA. CPA is the abbreviation of Chosen Plaintext Attack.That is, the adversary has the power of obtaining the encryption of arbitrary message of his choice. We give the definition by a game, which is actually the same as the Turning Test. Many Message SecurityDefine the Game: The Challenger generates a pair of key $(pk, sk)$ and publishes the $pk$. Eve sends two vectors of message, $\\vec{m_0}$ and $\\vec{m_1}$s.t. $|m_0^i|=|m_1^i|$ for all $i$.(A important restrict is $|m_0^i|=|m_1^i|$ for all $i$, or there is a length attack.) The challenger samples $b$ from ${0,1}$ and encrypts all the messages in vector $\\vec{m_b}$ using $pk$.And send the sequence of the ciphertext to Eve. Eve guesses which vector of message is encrypted and output $b’$. Eve wins if $b’=b$. IND-security Definition (Many-message) (unachievable): The encryption scheme is IND-secure if no PPT EVE can win in this game with probability better than $1/2 + \\text{negl}(n)$. Or written in more traditional (and cumbersome) notation as below.For all PPT pair of algorithms $(M,A)$, there is a negligible function $\\mu$ s.t.:[cumbersome:笨重的；不方便的]$$\\operatorname{Pr}\\left[\\begin{array}{c} (pk,sk)\\gets Gen(1^n); \\ (\\vec {m_o},\\vec{m_1},state)\\gets M(pk) s.t. |m_0^i|=|m_1^i|; \\ \\vec{c}\\gets Enc(pk,\\vec{m_b}) :A(state,\\vec{c})=b\\end{array}\\right] \\le \\frac{1}{2} +\\mu(n)$$ Yet the definition is unachievable. There is a simple way to win the game. You can construct two vector of message in which one is composed of the same message and the other is composed of the different message. Hence, it has to be randomized. In [Lecture 1], we gave two security definitions in Symmetric-key Encryption, Shannon’s Perfect Secrecy and Perfect Indistinguishability. Similarly, in Asymmetric-key Encryption, there is an alternative definition, Semantic Security, corresponding to the Indistinguishability Security. The Semantic Security is the computational analog of Shannon’s Shannon’s Perfect Secrecy, and it turns to be equivalent to Indistinguishability Security. i.e. Semantic Security = IND-Security. But the proof is more complex. We will stick to IND-security as it’s easy to work with. One Message SecurityIt’s cumbersome to define with many messages. Actually, the definition can be simplified to One Message Security. The only difference is that Eve can only see one ciphertext rather than a sequence of ciphertexts in the game. Define the Game: The Challenger generates a pair of key $(pk, sk)$ and publishes the $pk$. Eve send two single messages, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$. The challenger sample $b$ from ${0,1}$ and encrypt the message $m_b$ using $pk$.And send the ciphertext to Eve. Eve guesses which message is encrypted and output $b’$. Eve wins if $b’=b$. One-message IND-security Definition (unachievable): The encryption scheme is single-message-IND-secure if no PPT EVE can win in this game with probability better than $1/2 + \\text{negl}(n)$. Similarly, it’s unachievable. In public-key encryption, Eve knows the $pk$ as well. So she has the power of generating any ciphertext for arbitrary message of her choice, which is the meaning of CPA. When she gets the ciphertext from the Challenger, she can easily distinguish it. Because she can generate the ciphertexts of $m_0$ and $m_1$ using $pk$. Hence, only in public-key encryption, the many message security implies one message security. It dose not work with secret-key encryption. Theorem: A public-key encryption scheme is IND-secure iff it is single-message IND-secure. The proof is the simple use of Hybrid Argument. We’ll show four constructions in following blogs. Trapdoor Permutations (RSA) Quadratic Residuosity/Goldwasser-Micali Diffie-Hellman/El Gamal Learning with Errors/Regev In this blog, we introduce the Trapdoor Permutations. Trapdoor FunctionsWe know one-way function (family) is easy to compute but hard to invert. But Trapdoor One-way Function (family) is easy to invert given a trapdoor. Besides, it’s Trapdoor One-way Permutation when domain = range. DefinitionDefinition: A function (family) $\\mathcal{F=\\{F_n\\}}_{n\\in\\mathbb{N}}$ where each $\\mathcal{F_n}$ is itself a collection of functions $\\mathcal{F_n}=\\{F_i:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{m(n)}\\}_{i\\in I_n}$ is a trapdoor one-way function family if Easy to sample function index with a trapdoor.There is a PPT algorithm $Gen(1^n)$ that outputs a function index $i\\in I_n$ together with a trapdoor $t_i$. Easy to compute $F_i(x)$ given $i$ and $x$. Easy to compute an inverse of $F_i(x)$ given $t_i$. It is one-way.That is, for every p.p.t. $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}\\left[\\begin{array}{c}(i, t) \\leftarrow \\operatorname{Gen}\\left(1^{n}\\right) ; x \\leftarrow\\{0,1\\}^{n} ; y=F_{i}(x) ; \\\\ A\\left(1^{n}, i, y\\right)=x^{\\prime}: y=F_{i}\\left(x^{\\prime}\\right)\\end{array}\\right] \\leq \\mu(n) $$ Public-key Encryption ConstructionWe can construct IND-Secure Public-key Encryption from Trapdoor Permutations. There are three p.p.t. algorithms. Not IND-Secure (without randomization): $Gen(1^n)$: Sample function index $i$ with a trapdoor $t_i$.The public key is $i$ and the private key is $t_i$. $Enc(pk=i,m)$: Output $c=F_i(m)$ as the ciphertext. $Dec(sk=t_i,c)$: Output $F_i^{-1}(c)$ computed using the private key $t_i$. However, it is NOT IND-secure since it could reveal partial information about $m$. IND-Security is unachievable without randomization as we mentioned before in the IND-security definition. In last lecture [(Lecture 7)], we introduced GL Theorem that every one-way function has a hardcore bit . Besides, we showed one-way permutation implies PRG. (In fact, one-way function implies PRG as well.) Hence, we can construct a PRG from the trapdoor permutation, which is pseudorandom. IND-Secure (with PRG from trapdoor permutations): $Gen(1^n)$: Sample function index $i$ with a trapdoor $t_i$.The public key is $i$ and the private key is $t_i$. $Enc(pk=i,m)$ where $m$ is a bit. Pick a random $r$. Output $c=(F_i(r),HCB(r)\\oplus m)$ as the ciphertext. $Dec(sk=t_i,c)$: Recover $r$ using the private key $t_i$. Decrypt $m$ using $HCB(r)$. Notation: If the message is $k$-bit, it has to run the encryption $k$ times, each of which is encrypting one bit. This public-key encryption is IND-secure. It looks familiar with the stateless secret-key encryption with PRF. “Stateless Secret-key Encryption with PRF:” in Lecture 4 $Gen(1^n)$: Generate a random $n$-bit key $k$ that defines $f_k:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m$. The domain size of $f_k$, $2^l$, had better be super-polynomially large in $n$. $Enc(k,m)$: Pick a random $x$ and let the ciphertext $c$ be the pair $(x,y=f_k(x)\\oplus m)$.It’s a polynomial time to evaluate $f_k(x)$ since $f_k$ are random accessible. $Dec(k,c=(x,y))$: Output $f_k(x)\\oplus y$. The proof is by hybrid argument. (It is also familiar with the proof of secret-key encryption using PRF in Lecture 4.) Proof: The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. From the (One-message) IND-Security definition,we want to prove the following two ciphertexts are indistinguishable. ciphertext of $m_0$: $c=(F_i(r),HCB(r)\\oplus m_0)$ ciphertext of $m_1$: $c=(F_i(r),HCB(r)\\oplus m_1)$ We define a sequence of hybrid distributions by changing the ciphertext a little bit.Consider the ciphertext as the distribution.Direction: Hybrid 0 → Hybrid 3 ← Hybrid6 Hybrid 0: $D$ gets the ciphertext of $m_0$$c=(F_i(r),HCB(r)\\oplus m_0)$ Hybrid 1: replace with the hardcore bit$c=(F_i(r),HCB(r))$ Hybrid 2: replace with a random bit $r_b$$c=(F_i(r),r_b)$ Hybrid 3: replace with a random $r_x$ s.t. $|r_x|=|F_i(r)|$$c=(r_x,r_b)$ Hybrid 4: replace with a random bit $r_b$ (like Hybrid 2)$c=(F_i(r),r_b)$ Hybrid 5: replace with the hardcore bit (like Hybrid 1)$c=(F_i(r),HCB(r))$ Hybrid 6: $D$ gets the ciphertext of $m_1$$c=(F_i(r),HCB(r)\\oplus m_1)$ The thing we want to prove is that Hybrid 0 and Hybrid 6 are indistinguishable. Prove Hybrid 0 = Hybrid 1 (and Hybrid 6 = Hybrid 5) by birthday paradox.The probability of $D$ distinguishing from Hybrid 0 and Hybrid 1 is up to the collision probability of $r$. Prove Hybrid 1 = Hybrid 2 (and Hybrid 5 = Hybrid 4) by HCB security.The probability of $D$ distinguishing from Hybrid 1 and Hybrid 2 is determined by the advantage of computing the $HCB(r)$ from $F_i(r)$, which is negligible. Prove Hybrid 2 = Hybrid 3 ( and Hybrid 4 = Hybrid 3) by birthday paradox.The probability of $D$ distinguishing from Hybrid 0 and Hybrid 1 is up to the collision probability of $r$. QED. Trapdoor Permutation CandidatesTrapdoor Permutations are exceedingly rare. There are two candidates. (both need factoring to be hard) The RSA (Rivest-Shamir-Adleman) Function. The Rabin/Blum-Williams Function This blog only show the RSA Trapdoor Permutation. RSA Trapdoor PermutationLet’s review some number theory from Lecture 6. Let $N=pq$ be a product of two large primes. Facts: $\\mathbb{Z}_N^* =\\{a\\in \\mathbb{Z}_N^*:\\operatorname{gcd}(a,N)=1\\}$ is a group. group operation is multiplication mod $N$. inverses exist and are easy to compute. the order of the group is $\\phi(N)=(p-1)(q-1)$ Let $e$ be an integer with $\\operatorname{gcd}(e,\\phi(N))=1$. Then, the map $F_{N,e}(x)=x^e\\mod N$ is a trapdoor permutation. The key fact is given $d$ such that $ed=1 \\mod \\phi(N)$, then it is easy to compute $x$ given $x^e$. This gives us the RSA trapdoor permutation collection $\\{F_{N,e}:\\operatorname{gcd}(e,N)=1\\}$. Function index is $(N,e)$ Trapdoor for inversion is $d=e^{-1} \\mod \\phi(N)$ The hardness of inversion without trapdoor = RSA assumption: Given $N,e$ (as above) and $x^e\\mod N$, hard to compute $x$.","link":"/2022/07/23/mit6875-lec8/"},{"title":"「Cryptography-MIT6875」: Lecture 9","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Quadratic Residue and Quadratic Residuosity Assumption Goldwasser-Micali Encryption and Homomorphism Diffie-Hellman Key Exchange El Gamal Encryption In last blog is showed one of the constructions of public-key encryption using Trapdoor Permutations, RSA encryption. The gist in this blog is the remaining three constructions of public-key encryption. Constructions of Public-Key Encryption: Trapdoor Permutations (RSA) Quadratic Residuosity/Goldwasser-Micali Diffie-Hellman/El Gamal Learning with Errors/Regev Quadratic ResidueQR mod PLet $P$ be prime. We saw (in Lecture 6) that exactly half of $\\mathbb{Z}_P^*$ are squares. Define Legendre symbol $\\left(\\frac{x}{P}\\right)=1$ is $x$ is square, $-1$ if $x$ is not a square, and $0$ if $x=0\\mod P$. We can tell efficiently whether $x$ is a quadratic residue mod P using Legendre symbol. $$ \\left(\\frac{x}{P}\\right)=x^{(P-1) / 2}=\\begin{cases} 1 &,\\text{ if }x \\text{ is square} \\\\ -1 &,\\text{ if }x \\text{ is non-square} \\\\ 0 &,\\text{ if }x \\text{ is 0}\\end{cases} $$ Then we split $\\mathbb{Z}_P^*$ in half, one is square and the other is non-square. It is easy to compute square roots mod $P$. Besides, it’s an explicit formula for the case where $P=3\\pmod 4$. Claim: The square roots of $x \\mod P$ are $\\pm x^{(P+1) / 4}$. Proof: $\\left(\\pm x^{(P+1)/4}\\right)^2=x^{(P+1)/2}=x\\cdot x^{(P-1)/2}=x\\mod P$. QR mod NNow, let $N=PQ$ be a product of two primes and look at $\\mathbb{Z}_N^*$. Define Jacobi symbol $\\left(\\frac{x}{N}\\right)=\\left(\\frac{x}{P}\\right) \\left(\\frac{x}{Q}\\right)$ to be $+1$ if $x$ is a square mod both $P$ and $Q$ or a non-square mod both $P$ and $Q$. Jacobi Symbol — — from Wiki The Jacobi symbol is a generalization of the Legendre symbol.For any integer $a$ and any positive odd integer $n$, the Jacobi symbol $(\\frac{a}{n})$ is defined as the product of the Legendre symbols corresponding to the prime factors of $n$: $$ \\left(\\frac{a}{n}\\right)=\\left(\\frac{a}{p_1}\\right)^{\\alpha_1} \\left(\\frac{a}{p_2}\\right)^{\\alpha_2}\\dots \\left(\\frac{a}{p_k}\\right)^{\\alpha_k} $$ where $n=p_1^{\\alpha_1}p_2^{\\alpha_2}\\dots p_k^{\\alpha_k}$ is the prime factorization of $n$.Some properties: Fix the bottom argument: $\\left(\\frac{ab}{n}\\right)=\\left(\\frac{a}{n}\\right)\\left(\\frac{b}{n}\\right)$ Fix the the top argument: $\\left(\\frac{a}{mn}\\right)=\\left(\\frac{a}{m}\\right)\\left(\\frac{a}{n}\\right)$ So we can also split $\\mathbb{Z}_N^*$ in half, and the Jacobi symbol for one half is $+1$ and the other is $-1$. A surprising fact is Jacobi symbol $\\left(\\frac{x}{N}\\right)=\\left(\\frac{x}{P}\\right) \\left(\\frac{x}{Q}\\right)$ is computable in poly. time without knowing $P$ and $Q$. The key is Law of Quadratic Reciprocity. Law of Quadratic Reciprocity: For all $x, N$: $\\left(\\frac{x}{N}\\right)=\\left(\\frac{N}{x}\\right) \\cdot (-1)^{(N-1)\\cdot (x-1)/4}$ $\\left(\\frac{x}{N}\\right)=\\left(\\frac{x\\mod N}{N}\\right)$ $\\left(\\frac{2}{N}\\right)=\\left(-1\\right)^{\\frac{\\left(n-1\\right)^2}{8}}$ $\\left(\\frac{-1}{N}\\right)=(-1)^{\\frac{(n-1)}{2}}$ The one thing to notice is that the Legendre symbol$\\left(\\frac{x}{P}\\right)$ tells you exactly whether $x$ is the square mod $P$ or not. But the Jacobi symbol $\\left(\\frac{x}{N}\\right)=1$ only gives you partial information that is square mod both $P$ and $Q$ or non-square mod both $P$ and $Q$. But $x$ is square mod $N$ iff $x$ is square mod $P$ and it is a square mod $Q$. Claim: $x$ is square mod $N$ iff $x$ is square mod $P$ and it is a square mod $Q$. Hence, there are two cases for $Jac_{+1}$. $QR_N$: the set of squares mod $N$. $QNR_N$: the set of non-squares mod $N$. (but with Jacobi symbol $+1$) The Jacobi symbol can be $+1$ even though $x$ is not square mod $N$, so it’s pseudo-square. The conjecture is that distinguishing between $QR_N$ and $QNR_N$ is a hard problem. We cannot distinguish from squares and pseudo-squares. Can we use this hardness? Finding Square Roots Mod NThe fact is finding square roots mod $N$ is as hard as factoring $N$. Suppose we know P and Q.Suppose we know $P$ and $Q$, and we want to find the square root of $x \\mod N$. Before that, let’s introduce the Chinese Reminder Theorem(CRT). It’s a nice morphism.Informally, we can get $\\mathbb{Z}_N^*\\cong \\mathbb{Z}_P^* \\cdot \\mathbb{Z}_Q^*$. In forward direction, we can map $\\mathbb{Z}_N^*$ to $(\\mathbb{Z}_P^* ,\\mathbb{Z}_Q^*)$ uniquely. That is: $x \\rightarrow(x\\mod P, x\\mod Q)$. In back direction, we can map $(\\mathbb{Z}_P^* ,\\mathbb{Z}_Q^*)$ to $\\mathbb{Z}_N^*$ uniquely. That is: $(x_1, x_2)\\rightarrow c_Px_1+c_Qx_2\\mod N$.where $c_P$ and $c_Q$ are CRT coefficients. We can find the square root of $x\\mod N$ by following algorithm. Algorithm: Find the square roots of $y\\mod P$ and $y\\mod Q$. $x=y_P^2\\mod P$ $x=y_Q^2\\mod Q$ Let $y=c_Py_P+c_Qy_Q$ where the CRT coefficients. $c_P=1\\mod P \\text{ and }0 \\mod Q$ $c_Q=0\\mod P \\text{ and }1 \\mod Q$ Then $y$ is a square root of $x\\mod N$. Proof: The proof is easy using CRT. $y^2 = (c_Py_P + c_Qy_Q)^2\\mod N$ We can map $y^2$ to pair $(y^2\\mod P, y^2\\mod Q)$ Then we get pair $(y_P^2\\mod P,y_Q^2 \\mod Q)$ That is $x$. Moreover, we can get $x=y^2\\mod N \\longleftrightarrow \\begin{array}{c}x=y^2\\mod P \\\\x=y^2 \\mod Q\\end{array}$. Therefore, the takeaway is if $x$ is a square, it has $4$ distinct square roots $\\mod N$. Because it respectively has $2$ distinct square roots $\\mod P$ and $\\mod Q$. Suppose we know square root.Suppose we have a box that computes square roots $\\mod N$. The thing to notice is that the box only returns one square $y$ as it has $4$ distinct square roots. So if we feed the box $x=z^2\\mod N$ for a random $z$ of our choice, the box can return other square root. We can use the box to factor $N$. Feed the box $x=z^2 \\mod N$ for a random $z$. Claim: With probability $1/2$, $\\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$. Proof: Notation: $z$ denotes the square root of $x$, which is known. $x$ denotes the input of box s.t. $x=z^2 \\mod N$. $y$ denotes the output of box s.t. $x=y^2\\mod N$, which could be different with $z$ . We know $y^2=z^2\\mod N$. $N\\mid y^2-z^2$ $N\\mid (y-z)(y+z)$ There are three cases. If $N\\mid (y-z)$, we can get $y=z\\mod N$. (Useless) If $N\\mid (y+z)$, we can get $y=-z\\mod N$. (Useless) If $N\\nmid (y-z)$ and $N\\nmid (y+z)$, what can we get ? $(y-z)$ and $(y+z)$ respectively have a factor of $N$ since $N=PQ$. So $\\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$. Hence, $\\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$ if $y\\ne z$ and $y\\ne-z$.The probability is $1/2$. QED. It’s a very clever trick to factor. I know a solution to a problem that turns it into an Oracle. The Oracle could actually help me in solving another problem. It’s an example of the reduction. Quadratic Residuosity Assumption(QRA)Finding square roots is as hard as factoring $N$. Moreover, recognizing squares $\\mod N$ also seems hard as we mentioned above. Quadratic Residuosity Assumption (QRA): Let $N=PQ$ be a product of two large primes. No PPT algorithm can distinguish between a random element of $QR_N$ from a random element of $QNR_N$ given only $N$. Goldwasser-Micali (GM) EncryptionGoldwasser-Micali (GM) Encryption is under the Quadratic Residuosity Assumption. GM Scheme $Gen(1^n)$: Generate random $n$-bit prime $p$ and $q$ and let $N=pq$. Let $y\\in QNR_N$ be some quadratic non-residue with Jacobi symbol $+1$. How to sample a quadratic non-residue with Jacobi symbol $+1$ ? Sample a random number $y$. Check $\\left(\\frac{y}{P}\\right)\\overset?{=} -1$ and $\\left(\\frac{y}{Q}\\right)\\overset?{=} -1$ Then we can get a quadratic non-residue with $\\left(\\frac{y}{N}\\right)=1$. Let $pk=(N,y)$. Let $sk=(p,q)$. $Enc(pk,b)$ where $b$ is a bit: Generate random $r\\in\\mathbb{Z}_N^*$.(randomness) Output $\\begin{cases}r^2\\mod N &, \\text{if }b =0 \\\\r^2y\\mod N &,\\text{if }b=1 \\end{cases}$. $Dec(sk,c)$: Check if $c\\in\\mathbb{Z}_N^*$ is a quadratic residue using $p$ and $q$. If yes, output $0$ else $1$. If $b=0$, the encryption is $r^2$, which is quadratic residue with Jacobi symbol $+1$. $\\left(\\frac{r^2}{N}\\right)=\\left(\\frac{r}{N}\\right)^2=1$. ( $r\\in \\mathbb{Z}_N^*$ so $r\\ne0$) If $b=1$, the encryption is $r^2y$, which is quadratic non-residue with Jacobi symbol $+1$. $\\left(\\frac{r^2y}{N}\\right)=\\left(\\frac{r^2}{N}\\right)\\left(\\frac{y}{N}\\right)=1$. Although you know the Jacobi symbol is $+1$, you have no idea that the ciphertext is square or pseudo square. Hence, IND-security follows directly from the quadratic residuosity assumption. GM is a Homomorphic EncryptionGiven a GM-ciphertext of $b$ and a GM-ciphertext of $b’$, I can compute a GM-ciphertext of $b+b’\\mod 2$ without knowing anything about $b$ or $b’$. $Enc(pk,b)$ where $b$ is a bit:Generate random $r\\in\\mathbb{Z}_N^*$ and output $r^2y^b\\mod N$. Claim: $Enc(pk,b)\\cdot Enc(pk,b’)$ is an encryption of $b\\oplus b’=b+b’\\mod2$. Proof: Consider $c_1=r_1^2y^b$ and $c_2=r_2^2y^{b’}$. $c_1\\cdot c_2 = r_1^2r_2^2 y^{b+b’}$ . $c_1\\cdot c_2$ is QR if $b+b’=0\\mod 2$. The takeaway here is $QR\\cdot QR=QR$ and $QNR\\cdot QNR=QR$. Diffie-Hellman Key ExchangeThe main idea is the commutativity in the exponent, $(g^x)^y=(g^y)^x$, where $g$ is an element of some group. So you can compute $g^{xy}$ given either $g^x$ and $y$, or $g^y$ and $x$. We elaborated the Diffie-Hellman Assumption in Lecture 6. Diffie-Hellman Assumption(DHA): Hard to compute $g^{xy}$ given only $g,g^x$and $g^y$. Diffie-Hellman Key Exchange: Let $p=2q+1$ be a safe prime, and $g$ be the generator of $QR_p$. Alice picks a random number $x\\in \\mathbb{Z}_q$ and sends $g^x\\mod p$ to Bob. Bob picks a random number $y\\in\\mathbb{Z}_q$ and sends $g^y\\mod p$ to Alice. Alice and Bob have the shared key $k=g^{xy}\\mod p$ Alice computes it by $g^y$ and $x$. Bob computes it by $g^x$ and $y$. El Gamal Encryption $Gen(1^n)$: Generate an $n$-bit safe prime $p=2q+1$ and a generator $g$ of $\\mathbb{Z}_p^*$. Let $h=g^2\\mod p$ be a generator of $QR_p$.Choose a random number $x\\in \\mathbb{Z}_q$. Let $pk=(p,h,h^x)$. Let $sk=x$.(Finding $sk$ from $pk$ is the discrete logarithm problem.) $Enc(pk,m)$ where $m\\in QR_p$. Generate random $y\\in\\mathbb{Z}_q$. (randomness) Output $(h^y,h^{xy}\\cdot m)$. $Dec(sk=x,c)$ Compute $h^{xy}$ using $h^y$ and $x$. Divide the second component to retrieve $m$. Decisional Diffie-Hellman Assumption (DDHA):Hard to distinguish between $g^{xy}$ and a uniformly random group element, given $g,g^x$ and $g^y$.That is the following two distributions are computationally indistinguishable:$(g,g^x,g^y,g^{xy})\\approx (g,g^x,g^y,u)$ From DDH assumption, we know It’s hard to distinguish between $(h,h^x,h^y,h^{xy})$ and $(h,h^x,h^y,u)$. Moreover, it’s hard to distinguish between $(h,h^x,h^y,h^{xy})$ and $(h,h^x,h^y,h^{xy}\\cdot m)$ since $m$ is random, so is $m\\cdot h^{xy}$. So it’s hard to distinguish between $(h,h^x,h^y,h^{xy}\\cdot m)$ and $(h,h^x,h^y,u)$. Hence, DH/El Gamal is IND-secure under the DDH assumption. The source of hardness is different in RSA and GM. There is no factoring and only DH problem. Which Group to UseQuadratic Residue Group: We used the $QR_P$ group for a safe prime $P=2Q+1$ where $Q$ is prime so far. The order of the group is $Q$. But it is not used in practice. Because discrete log can be broken in sub-exponential time $2^{\\sqrt{\\log P\\log\\log P}}$. It’s better than $poly(P)$ but worse than $poly(\\log P)$. And the $poly(\\log P)$ is what we’re referring to $poly(n)$. Elliptic Curve Groups: In practice, we use Elliptic Curve Groups in DH/El Gamal. It is the set of solutions $(x,y)$ to the equation $y^2=x^3+ax+b\\pmod P$ together with a very cool group addition law. The best known discrete log algorithm is $\\mathcal{O}(\\sqrt{P})$ time! That says that we can use much smaller keys. We can use 160-bit $P$ to suffice “80-bit security”. SummaryWe have elaborated three constructions of public-key encryption. Constructions of Public-Key Encryption: Trapdoor Permutations (RSA) Quadratic Residuosity/Goldwasser-Micali Diffie-Hellman/El Gamal Learning with Errors/Regev If factoring is easy, RSA is broken (and that’s the only known way to break RSA). If factoring is easy, Goldwasser-Micali is broken conjecturing that finding square roots or recognizing the squares are both as hard as factoring. Moreover, the discrete problem is similarly broken to factoring in quantum computer. Hence, the preceding three constructions are dead if the quantum computer comes out. Yet learning with errors is post-quantum secure as far as we know. We will see more when we do homomorphic encryption. Practical ConsiderationsThere are some practical considerations. How do I know the public key? Public-key Infrastructure: a directory of identities together with their public keys. But it needs to be “authenticated”.Otherwise Eve could replace Bob’s $pk$ with her own. Public-key encryption is orders of magnitude slower than secret-key encryption. We mostly showed how to encrypt bit-by-bit! Super-duper inefficient. Exponentiation takes $O(n^2)$ time as opposed to typically linear time for secret key encryption (AES). The $n$ itself is large for PKE (RSA: $n\\ge 2048$) compared to SKE (AES: $n=128$).(For Elliptic Curve El-Gamal, it’s $320$ bits. We can solve problem 1 and minimize problems 2&amp;3 using hybrid encryption. Hybrid Encryption To encrypt a long message $m$ (think 1GB) Pick a random key $K$ (think 128 bits) for a secret-key encryption. Encrypt $K$ with the $PKE$: $PKE.Enc(pk,K)$. Encrypt $m$ with the $SKE$: $SKE.Enc(K,m)$. To decrypt: Recover $K$ using $sk$. Then using $K$, recover $m$.","link":"/2022/07/27/mit6875-lec9/"},{"title":"「机器学习-李宏毅」:HW1-Predict PM2.5","text":"在本篇文章中，用手刻Adagrad完成了「机器学习-李宏毅」的HW1-预测PM2.5的作业。其中包括对数据的处理，训练模型，预测，并使用sklearn toolkit的结果进行比较。有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub Task Descriptionkaggle link 从中央气象局网站下载的真实观测资料，必须利用linear regression或其他方法预测PM2.5的值。 观测记录被分为train set 和 test set, 前者是每个月前20天所有资料；后者是从剩下的资料中随机取样出来的。 train.csv: 每个月前20天的完整资料。 test.csv: 从剩下的10天资料中取出240笔资料，每一笔资料都有连续9小时的观测数据，必须以此观测出第十小时的PM2.5. Process Datatrain data如下图，每18行是一天24小时的数据，每个月取了前20天（时间上是连续的小时）。 test data 如下图，每18行是一笔连续9小时的数据，共240笔数据。 最大化training data size 每连续10小时的数据都是train set的data。为了得到更多的data，应该把每一天连起来。即下图这种效果： 每个月就有： $20*24-9=471$ 笔data 123456789# Dictionary: key:month value:month datamonth_data = {}# make data timeline continuousfor month in range(12): temp = np.empty(shape=(18, 20*24)) for day in range(20): temp[:, day*24: (day+1)*24] = data[(month*20+day)*18: (month*20+day+1)*18, :] month_data[month] = temp 筛选需要的Features : 这里，我就只考虑前9小时的PM2.5，当然还可以考虑和PM2.5等相关的氮氧化物等feature。 training data 1234567# x_data v1: only consider PM2.5x_data = np.empty(shape=(12*471, 9))y_data = np.empty(shape=(12*471, 1))for month in range(12): for i in range(471): x_data[month*471+i][:] = month_data[month][9][i: i+9] y_data[month*471+i] = month_data[month][9][i+9] testing data 12345# Testing data featurestest_x = np.empty(shape=(240, 9))for day in range(240): test_x[day, :] = test_data[18*day+9, :]test_x = np.concatenate((np.ones(shape=(240, 1)), test_x), axis=1) Normalization 123456789101112# feature scale: normalizationmean = np.mean(x_data, axis=0)std = np.std(x_data, axis=0)for i in range(x_data.shape[0]): for j in range(x_data.shape[1]): if std[j] != 0: x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]for i in range(test_data.shape[0]): for j in range(test_data.shape[1]): if std[j] != 0: test_data[i][j] = (test_data[i][j] - mean[j])/std[j] Training手刻Adagrad 进行training。（挖坑：RMSprop、Adam[1] Linear Pseudo code 123456Declare weight vector, initial lr ,and # of iterationfor i_th iteration : y’ = the product of train_x and weight vector Loss = y’ - train_y gradient = 2*np.dot((train_x)’, Loss ) weight vector -= learning rate * gradient 其中的矩阵操作时，注意求gradient时矩阵的维度。可参考下图。 Adagrad Pseudo code 123456789Declare weight vector, initial lr ,and # of iterationDeclare prev_gra storing gradients in every previous iterations for i_th iteration : y’ = the inner product of train_x and weight vector Loss = y’ - train_y gradient = 2*np.dot((train_x)’, Loss ) prev_gra += gra**2 ada = np.sqrt(prev_gra) weight vector -= learning rate * gradient / ada 注：代码实现时，将bias存在w[0]处，x_data的第0列全1。因为w和b可以一同更新。（当然，也可以分开更新） Adagrad training 123456789101112131415161718192021# train-adagradbatch = x_data.shape[0] # full batchepoch = 400# some parameters for trainingdim = x_data.shape[1]+1w = np.zeros(shape=(dim, 1)) # concatenate bias = w[0]lr = np.full((dim, 1), 0.8) # learning rategrad = np.empty(shape=(dim, 1)) # gradient of loss to every paragradsum = np.zeros(shape=(dim, 1)) # sum of gradient**2x_data = np.concatenate((np.ones(shape=(x_data.shape[0], 1)), x_data), axis=1)loss_his = np.empty(shape=(epoch, 1))for T in range(epoch): L = y_data - np.dot(x_data, w) loss_his[T] = np.sum(L**2) / x_data.shape[0] grad = (-2)*np.dot(np.transpose(x_data), L) gradsum = gradsum + grad**2 w = w - lr*grad/(gradsum**0.5) Testing1answer = np.dot(test_x, w) Draw and Analysis在每次迭代更新时，我将Loss的值存了下来，以便可视化Loss的变化和更新速度。 Loss的变化如下图：(红色的是sklearn toolkit的loss结果) 此外，在源代码中，使用sklearn toolkit来比较结果。 结果如下： 123456789101112131415161718192021222324252627v1: only consider PM2.5Using sklearnLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)bias= [21.37402689]w= [[ 0.00000000e+00] [-5.54801503e-01] [-4.32873874e-01] [ 3.63669814e+00] [-3.99037687e+00] [-9.07364636e-01] [ 8.83495803e+00] [-9.51785135e+00] [ 1.32734655e-02] [ 1.81886444e+01]]In our modelbias= [19.59387132]w= [[-0.14448468] [ 0.39205748] [ 0.26897134] [-1.02415371] [ 1.21151411] [ 2.21925424] [-5.48242478] [ 4.01080346] [13.56369122]] 发现参数有一定差异，于是我在testing时，也把sklearn的结果进行预测比较。 一部分结果如下： 1234567891011121314151617['id', 'value', 'sk_value']['id_0', 3.551092352912313, 5.37766865368331]['id_1', 13.916795471648756, 16.559245678900034]['id_2', 24.811333478647043, 23.5085950470451]['id_3', 5.101440436158914, 6.478306159981166]['id_4', 26.7374726797937, 27.207516152986663]['id_5', 19.43735346531517, 21.916809502961648]['id_6', 22.20460696285646, 24.751295357256392]['id_7', 29.660872382552682, 30.24344042612033]['id_8', 17.5964527734513, 16.64242443764712]['id_9', 56.58017426943178, 59.760988216575115]['id_10', 13.767504260132299, 10.808372404511037]['id_11', 11.743000466164233, 11.526958393801682]['id_12', 59.509878887026105, 64.201008247897]['id_13', 53.19824337746267, 54.3856368053018]['id_14', 21.97191108867921, 24.530720709840974]['id_15', 10.833283625735444, 14.350345549104446] Code有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137########################## Date: 2020-4-4# Author: FredLau# HW1: predict the PM2.5##########################import sysimport numpy as npimport pandas as pdimport csvfrom sklearn import linear_modelimport matplotlib.pyplot as plt###################### process data# process train dataraw_data = np.genfromtxt('data/train.csv', delimiter=',')data = raw_data[1:, 3:]data[np.isnan(data)] = 0 # process nan# Dictionary: key:month value:month datamonth_data = {}# make data timeline continuousfor month in range(12): temp = np.empty(shape=(18, 20*24)) for day in range(20): temp[:, day*24: (day+1)*24] = data[(month*20+day)*18: (month*20+day+1)*18, :] month_data[month] = temp# x_data v1: only consider PM2.5x_data = np.empty(shape=(12*471, 9))y_data = np.empty(shape=(12*471, 1))for month in range(12): for i in range(471): x_data[month*471+i][:] = month_data[month][9][i: i+9] y_data[month*471+i] = month_data[month][9][i+9]# process test datatest_raw_data = np.genfromtxt('data/test.csv', delimiter=',')test_data = test_raw_data[:, 2:]test_data[np.isnan(test_data)] = 0# feature scale: normalizationmean = np.mean(x_data, axis=0)std = np.std(x_data, axis=0)for i in range(x_data.shape[0]): for j in range(x_data.shape[1]): if std[j] != 0: x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]for i in range(test_data.shape[0]): for j in range(test_data.shape[1]): if std[j] != 0: test_data[i][j] = (test_data[i][j] - mean[j])/std[j]# Testing data featurestest_x = np.empty(shape=(240, 9))for day in range(240): test_x[day, :] = test_data[18*day+9, :]test_x = np.concatenate((np.ones(shape=(240, 1)), test_x), axis=1)################################# train-adagradbatch = x_data.shape[0] # full batchepoch = 400# some parameters for trainingdim = x_data.shape[1]+1w = np.zeros(shape=(dim, 1)) # concatenate bias = w[0]lr = np.full((dim, 1), 0.8) # learning rategrad = np.empty(shape=(dim, 1)) # gradient of loss to every paragradsum = np.zeros(shape=(dim, 1)) # sum of gradient**2x_data = np.concatenate((np.ones(shape=(x_data.shape[0], 1)), x_data), axis=1)loss_his = np.empty(shape=(epoch, 1))for T in range(epoch): L = y_data - np.dot(x_data, w) loss_his[T] = np.sum(L**2) / x_data.shape[0] grad = (-2)*np.dot(np.transpose(x_data), L) gradsum = gradsum + grad**2 w = w - lr*grad/(gradsum**0.5)f = open('output/v1.csv', 'w')sys.stdout = fprint('v1: only consider PM2.5\\n')################################ train by sklearn linear modelprint('Using sklearn')reg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print('bias=', reg.intercept_)print('w=', reg.coef_.transpose())print('\\n')# In our modelprint('In our model')print('bias=', w[0])print('w=', w[1:])############################ draw change of lossplt.xlim(0, epoch)plt.ylim(0, 10)plt.xlabel('$iteration$', fontsize=16)plt.ylabel('$Loss$', fontsize=16)iteration = np.arange(0, epoch)plt.plot(iteration, loss_his/100, '-', ms=3, lw=2, color='black')sk_w = reg.coef_.transpose()sk_w[0] = reg.intercept_sk_loss = np.sum((y_data - np.dot(x_data, sk_w))**2) / x_data.shape[0]plt.hlines(sk_loss/100, 0, epoch, colors='red', linestyles='solid')plt.legend(['adagrad', 'sklearn'])plt.show()# plt.savefig('output/v1.png')f.close()############### test (sklearn vs our adagradf = open('output/v1test.csv', 'w')sys.stdout = ftitle = ['id', 'value', 'sk_value']answer = np.dot(test_x, w)sk_answer = np.dot(test_x, sk_w)print(title)for i in range(test_x.shape[0]): content = ['id_'+str(i), answer[i][0], sk_answer[i][0]] print(content)f.close() Reference 待完成","link":"/2020/04/06/ml-lee-hw1/"},{"title":"「Python」：Module &amp; Method","text":"长期记录帖：关于遇到过的那些Python 的Packets &amp; Module &amp; Method &amp; Attribute。中英记录。 Trickylist comprehension List comprehension provides a concise way to create lists. e.g. : squares = [x**2 for x in range(10)] A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it. e.g. : Double loop: [(x,y) for x in [1,2,3] for y in [3,1,4] if x != y] 输出7个 e.g. : (Using zip() to loop together): [(x, y) for x,y in zip([1,2,3], [3,1,4]) if x!=y] 输出2个 Python-Build functionprint print(*objects, sep=’ ‘, end=’\\n’, file=sys.stdout) len Return the length(the number of items) of an object. str.format() 字符串格式化 eg: “{} {}”.format(“Hello”,”World) ‘Hello World’ zip(*iterables) Make an iterator that aggregates【聚集】 elements from each of the iterales. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. 【返回一个元组的迭代器】 使用zip可以同时对多个迭代器进行迭代 enumerate Enumerate is a built-in funciton of Python. It allows us to loop over something and have an automatic counter. e.g. for counter, value in enumerate(some_list): print(counter, value) e.g. : an optional argument: tell enumerate from where to start the index. for c, value in enumerate(my_list, 1): print(c, value) with open(path) as f: 由于读写文件都可能产生IOError，一旦出错，后面的f.close()就不会调用。 用try……finally来实现，比较麻烦。 try: ​ f = open(path, ‘r’) ​ print(f.read()) finally: ​ if f: ​ f.close() 用with as 简化 with open(path, ‘r’) as f: ​ print(f.read()) numpynumpy.argsort numpy.argsort(a, axis=-1, kind=None, order=None) Returns the indices that would sort an array. Perform an indirect sort along the given axis using the algorithm specified by the kind keyword. It returns an array of indices of the same shape as a that index data along the given axis in sorted order. Parameters: a :array_like. axis : int or None, optional Axis along which to sort. The default is -1(the last axis). 【默认按照最后一个维度】 2-D: axis = 0按列排序 2-D: axis = 1 按行排序 kind :{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, optional The default is ‘quicksort’ Return: index_array: ndarray, int.【返回的是降序排列的索引数组】 e.g.: x = np.array([5, 1, 2]) np.argsort(x) # 降序 array([1,2,0]) np.argsort(-x) # 升序 array([0,2,1]) Linear algebra(numpy.linalg)numpy.dot numpy.dot(a,b) Dot product of two arrays. If both a and b are 1-D arrays, it is inner porduct of vectors. If both a and b are 2-D arrays, it is matrix multiplication, but using matmul is preferred. Id either a or b is 0-D(scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a*b is preferred. …… numpy.matmul Matrix product of two arrays. numpy.matmul(x1, x2) numpy.linalg.inv(a) Compute the inverse of a matrix. Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])/ numpy.linalg.inv(a) Parameters: a :(…, M, M) array_like. Matrix to e inverted. Return: ainv. numpy.linalg.svd numpy.linalg.svd(a, full_matrices=True, compute_uv=True, hermitian=False) Singular Value Decomposition 矩阵的奇异值分解 A = u @ s @ vh u, vh是标准正交矩阵, inv(u) = uh s是对角矩阵 Parameters: a :array_like, a real or complex array with a.ndim &gt;=2 full_matrices :bool, optional. True(default) If True, u and vh have the shapes(…, M, M) and (…, N, N), respectively. Otherwise, the shapes are(…, M, K) and (…, K, N), respectively, where K = min(M,N) compute_uv : bool, optional.True(default) Whether or not to compute u and vh in addition to s.【注，vh就是v的转置】 Return： u: array s: array vh:array numpy.zeros numpy.zeros(shape, dtype=float, order=’C’) Return a new array of given shape and type, filled with zeros. parameters: shape: int or truple of ints. e.g.,(2,3) or 2 dtype: data-type, optional.(Defaults is numpy.float64) oder: optional Returns: out ndarray numpy.full numpy.full(shape, fill_value, dtype=None) Return a new array of given shape and type, filled with fill_value. parameters: shape :int or sequence of ints (2,3) or 2 fill_value :scalar dtype :data-type, optional numpy.arange numpy.arange([start, ]stop, [step, ]dtype=None) Return evenly spaced values within a given interval. parameters: start: number, optional. (Defaults is 0) stop :number. [start,stop) step :number, optional(Defaults is 1) dtype : Returns :ndarray differ with built-in range function numpy.arange returnan ndarray rathan than a list. numpy.arange’s step can be float. numpy.meshgrid numpy.meshgrid(x, y) 生成用x向量为行，y向量为列的矩阵（坐标系） 返回 X矩阵和Y矩阵 X矩阵：网格上所有点的x值 Y矩阵：网格上所有点的y值 e.g., X, Y = np.meshgrid(x, y) 【X,Y 都是网格点坐标矩阵】 numpy.genfromtext numpy.genfromtxt (fname, delimiter=None) Load data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments characters are discarded. Parameters: fname :file, str, list of str, generator. dtype :dtype, optional. delimiter :str, int, or sequence, optional. (default = whitespace) The strin used to separate values. Python的列表读取处理数据很慢，numpy.genfromtext就很棒。 numpy.isnan numpy.isnan(x) Test element-wise for NaN(Not a number) and return result as a boolean array. Parameters: x :array_like Returns: y:ndarray or bool. True where x is NaN, false otherwise. numpy.empty nmpy.empty(shape, dtype=float, order=’C’) Return a new arry of given shape and type, without initializing entries. Parameters: shape :int or tuple of int dtype :data-type,optional Default is numpy.float64. Returns: out: ndarray numpy.reshape numpy.reshape(a, newshape, order=’C’) Gives a new shape to an array without changing its data.【改变张量的shape，不改变张量的数据】 Parameters: a : array-like newshape : int or tuple of ints One shape dimension can be -1. The value is inferred from the length of the array and remaning dimensions. Returns: reshaped_array:ndarray numpy.mean numpy.mean(a, axis=None) Compute the arithmetic mean along the specifiied axis.(the average of the array elements) Parameters: a : array_like axis ：None or int or tuple of ints, optional Axis or axes along which the means are computed. axis=0 ：沿行的垂直往下（列） axis=1 ：沿列的方向水平向右（行） numpy.std numpy.std(a, axis=None,) Compute the standard deviation along the specified axis.【标准差】 Parameters: a :array_like axis :Axis or axes along which the means are computed. numpy.shape attribute Tuple of array dimensions. numpy.concatenate numpy.concatenate((a1, a2, …), axis=0) Join a sequence of arrays along an existing axis. Parameters: a1, a2, … :sequence of array_like The arrays must have the same shape, excepting in the dimension corresponding to axis.【除了axia方向，其他维度的shape要相同】 If axis is None, arrays are flattened before use.【值为None，就先将向量变成一维的】 Default=0 numpy.ndarray.astype method numpy.ndarray.astype(dtype) Copy of the array cast to a specified type.【强制转换数据类型】 Parameters: dtype : str or dtype numpy.ones numpy.ones(shape, dtype=None) Return a new array of given shape and type, filled with ones. Parameters: shape : int or sequence of ints. dtype : data-type, optional numpy.array numpy.array(object, dtype = none) Create an array Parameters: object :array_like An array, any object exposing the array interface, an object whose array method returns an array, or any(nested) sequence. numpy ndarray 运算 [[1]]*3 = [[1],[1],[1]] A * B 元素相乘 numpy.dot(A, B) 矩阵相乘 numpy.power numpy.power(x1, x2) First array elements raised to powers from second array. Parameters: x1 :array_like . The bases. x2 :array_like The exponents. numpy.sum numpy.sum(a, axis=None, dtype=None) Sum of arrays elements over a given axis. Parameters: a :array_like Elements to sum. axis :None or int or tuple of ints, optional Axis or axes along which a sum is perfomed. The default, None, will sum all of the elementsof the input array. numpy.transpose numpy.transpose(a, axes=None) Permute the dimensions of the array.【tensor的维度换位】 Parameters: a : array_like axes : list of ints, optinal Default, reverse the dimensions. Otherwise permute the axes according to the values given. Returns : ndarray 张量a的shape是(10,2,15), numpy.transport(2,0,1)的shape就是(15,10,2) 对于一维：行向量变成列向量 对于二维：矩阵的转置 numpy.save numpy.save(file, arr) Save an array to a binary file in Numpy .npy format. Parameters: file :file, str, or pathlib arr :array_like Array data to be saved. numpy.clip Clip(limit) the values in an array numpy.clip(a, a_min, a_max) Parameters: a :array_like a_min : scalar or array_like a_max :scalar or array_like numpy.around Evenly round to the given number of decimals(十进制) numpy.around(a) Parameters： a :array_like Notes: For values exactly halfway between rounded decimal values, Numpy rounds to the nearest even values. 【这什么意思呢？ 就是说对于0.5的这种，为了统计上平衡，不会全部向上取整或者向下取整，会向最近的偶数取整，around（2.5）=2】 numpy.log The natural logarithm log is the inverse of exponential functions, so that log(exp(x))=x. numpy.log(x) Parameters: x : array_like numpy.ndarray.T attribute, the transpose array. ndarray.T numpy.random.shuffle Modify a sequence in-space by shufflng its contents. This function only shuffles the array along the first axis of a multi-diensional array. The order of sub-arrays is changed but their contents remains the same. numpy.random.shuffle(x) Parameters: x : array_like e.g. shuffle two list, X and Y, together. 【以相同的顺序打乱两个array】 np.random.seed(0) randomize = np.arrange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize] sklearnskelearn.linear_model.LinearRegression class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None) Ordinary least squares Linear Regress(最小二乘法回归运算) Parameters: fit_intercept :bool, optional, defalut True【True：需要bias的截距项】 normalize :bool, optional, default False 【True：对样本做feature scaling】 Attributes： coef_ :array of shape(n_features) 【权重】 intercept_ :bias Methods： fit(self,X,y[,sample_weight]) :Fit linear model e.g. , LinearRegression().fit(x_data, y_data) matplotlib.pyplotmatplotlib.pyplot.contourf contour and contourf draw contour lines and filled contours, respectively.【一个画等高线，一个填充等高线/轮廓】 contour([X, Y, ] Z, [levels], **kwargs) Parameters X, Y: The coordinates of the values in Z. A and Y must both be 2-D with the sanme shape as Z(e.g. created via numpy.meshgrid), or they must both be 1-D such that len(X) == M is the number of columns in Z and len(Y) = N is the number of rows in Z. 【X，Y要么是由像numpy.mershgrid(x, y) 生成的网格点坐标矩阵，要么X，Y是（基）向量，X向量是x轴的，对应到Z矩阵，是Z矩阵的列数，Y向量同理】 Z ：array-like(N,M) levels : int or array-like, optional. Determines the number and positions of contour lines / religions.【划分多少块等高区域】 alpha :float, optional. Between 0(transparent) and 1(opaque).【透明度】 cmap :str or Colormap, optional. e.g., pyplot.contourf(x, y, Z, 50, alpha=0.5, cmap=pyplot.get_cmap(‘jet’))【‘jet’是常用的那种红橙黄绿青蓝紫】 matplotlib.pyplot.plot plot([x], y, [fmt], , data=None, *kwargs) The coordinates of the points or line nodes are given by x, y. Parameters: x, y :array-like or scalar. fmt :str, optional. A format string. e.g., ‘.’, point marker. ‘-‘, solid line style. ‘–’,dashed line style. ‘b’, blue. ms/markersize : float lw/linewidth :float color : matplotlib.pyplot.xlim xlim(args, *kwargs) Get or set the x limits of the current axes. e.g. left, right = xlim() :get xlim(left, right) :set matplotlib.pyplot.show show(args, *kwargs) display a figure. matplotlib.pyplot.vlines Plot vertical lines. vlines(x, ymin, ymax, color=’k’, linestyles=’solid’) Parameters: x :scalar or 1D array_like ymin, ymax :scalar or 1D array_like matplotlib.pyplot.hlines Plot horizontal lines. vlines(y, xmin, xmax, color=’k’, linestyles=’solid’) Parameters: y :scalar or 1D array_like xmin, xmax :scalar or 1D array_like matplotlib.pyplot.savefig Save the current figure. savefig(fname) Parameters: fname :str ot Pathlike matplotlib.pyplot.legend Place a lengend on the axes. e.g. : legend() Labeling exisiting plot elements plt.plot(train_loss) plt.plot(dev_loss) plt.legend([‘train’, ‘dev’]) e.g. : le syssys.argv[] python a.py data.csv sys.argv = [‘a.py’, ‘data.csv’] 重定向到文件 f = open(‘out.csv’, ‘w’) sys.stdout = f print(‘此时print掉用的就是文件对象的write方法’)","link":"/2020/03/07/python/"},{"title":"「机器学习-李宏毅」：HW2-Binary Income Predicting","text":"这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。包括对数据集的处理，训练模型，可视化，预测等。有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub Task introduction and Dataset Kaggle competition: link Task: Binary Classification Predict whether the income of an individual exceeds $50000 or not ? *Dataset: * Census-Income (KDD) Dataset (Remove unnecessary attributes and balance the ratio between positively and negatively labeled data) Feature Format train.csv, test_no_label.csv【都是没有处理过的数据，可作为数据参考和优化参考】 text-based raw data unnecessary attributes removed, positive/negative ratio balanced. X_train, Y_train, X_test【已经处理过的数据，可以直接使用】 discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial state…) continuous features in train.csv =&gt; remain the same in X_train (age, capital losses…). X_train, X_test : each row contains one 510-dim feature represents a sample. Y_train: label = 0 means “&lt;= 50K” 、 label = 1 means “ &gt;50K ” 注：数据集超大，用notepad查看比较舒服；调试时，也可以先调试小一点的数据集。 Logistic RegressionLogistic Regression 原理部分见这篇博客。 Prepare data本文直接使用X_train Y_train X_test 已经处理好的数据集。 12345678910111213141516# prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_Train'X_test_fpath = './data/X_test'output_fpath = './logistic_output/output_logistic.csv'fpath = './logistic_output/logistic'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:] 统计一下数据集： 123456789101112train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In logistic model:\\n') f.write('Size of Training set: {}\\n'.format(train_size)) f.write('Size of development set: {}\\n'.format(dev_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n'.format(data_dim)) 结果如下： 12345In logistic model:Size of Training set: 48830Size of development set: 5426Size of test set: 27622Dimension of data: 510 normalizenormalize data. 对于train data，计算出每个feature的mean和std，保存下来用来normalize test data。 代码如下： 12345678910111213141516def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X. # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std # Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std) Development set split在logistic regression中使用的gradient，没有closed-form解，所以在train set中划出一部分作为development set 优化参数。 12345678def _train_dev_split(X, Y, dev_ratio=0.25): # This function splits data into training set and development set. train_size = int(X.shape[0] * (1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]# Split data into train data and development datadev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio) Useful function_shuffle(X, Y)本文使用mini-batch gradient。 所以在每次epoch时，以相同顺序同时打乱X_train,Y_train数组，再mini-batch。 12345678np.random.seed(0)def _shuffle(X, Y): # This function shuffles two two list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize] _sigmod(z)计算 $\\frac{1}{1+e^{-z}}$ ，注意：防止溢出，给函数返回值规定上界和下界。 12345def _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8)) _f(X, w, b)是sigmod函数的输入，linear part。 输入： X：shape = [size, data_dimension] w：weight vector, shape = [data_dimension, 1] b: bias, scalar 输出： 属于Class 1的概率（Label=0，即收入小于$50k的概率） 12345678910def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b) _predict(X, w, b)预测Label=0？（0或者1，不是概率） 1234def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int) _accuracy(Y_pred, Y_label)计算预测出的结果（0或者1）和真实结果的正确率。 这里使用 $1-\\overline{error}$ 来表示正确率。 123456def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc _cross_entropy_loss(y_pred, Y_label)计算预测出的概率（是sigmod的函数输出）和真实结果的交叉熵。 计算公式为： $\\sum_n {C(y_{pred},Y_{label})}=-\\sum[Y_{label}\\ln{y_{pred}}+(1-Y_{label})\\ln(1-{y_{pred}})]$ 123456789def _cross_entropy_loss(y_pred, Y_label): # This function calculates the cross entropy of Y_pred and Y_label # # Argument: # y_pred: predictions, float vector # Y_label: truth labels, bool vector cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((1 - Y_label).T, np.log(1 - y_pred)) return cross_entropy[0][0] _gradient(X, Y_label, w, b)和Regression的最小二乘一样。（严谨的说，最多一个系数不同） 12345678def _gradient(X, Y_label, w, b): # This function calculates the gradient of cross entropy # X, Y_label, shape = [batch_size, ] y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = - np.dot(X.T, pred_error) b_grad = - np.sum(pred_error) return w_grad, float(b_grad) Training (Adagrad)初始化一些参数。 这里特别注意 : 由于adagrad的参数更新是 $w \\longleftarrow w-\\eta \\frac{gradient}{ \\sqrt{gradsum}}$ . 防止除0，初始化gradsum的值为一个较小值。 12345678910111213# training by logistic model# Initial weights and biasw = np.zeros((data_dim, 1))b = np.float(0.)w_grad_sum = np.full((data_dim, 1), 1e-8) # avoid divided by zerosb_grad_sum = np.float(1e-8)# Some parameters for trainingepoch = 20batch_size = 2**3learning_rate = 0.2 AdagradAagrad具体原理见这篇博客的1.2节。 迭代更新时，每次epoch计算一次loss和accuracy，以便可视化更新过程，调整参数。 1234567891011121314151617181920212223242526272829303132333435363738# Keep the loss and accuracy history at every epoch for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Iterative trainingfor it in range(epoch): # Random shuffle at every epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for id in range(int(np.floor(train_size / batch_size))): X = X_train[id*batch_size: (id+1)*batch_size] Y = Y_train[id*batch_size: (id+1)*batch_size] # calculate gradient w_grad, b_grad = _gradient(X, Y, w, b) # adagrad gradient update w_grad_sum = w_grad_sum + w_grad**2 b_grad_sum = b_grad_sum + b_grad**2 w_ada = np.sqrt(w_grad_sum) b_ada = np.sqrt(b_grad_sum) w = w - learning_rate * w_grad / np.sqrt(w_grad_sum) b = b - learning_rate * b_grad / np.sqrt(b_grad_sum) # compute loss and accuracy of training set and development set at every epoch y_train_pred = _f(X_train, w, b) Y_train_pred = np.around(y_train_pred) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size) train_acc.append(_accuracy(Y_train_pred, Y_train)) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.around(y_dev_pred) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size) dev_acc.append(_accuracy(y_dev_pred, Y_dev)) Loss &amp; accuracy输出最后一次迭代的loss和accuracy。 结果如下： 1234Training loss: 0.2933570286596322Training accuracy: 0.8839238173254147Development loss: 0.31029505347634456Development accuracy: 0.8336166253549906 画出loss 和 accuracy的更新过程： loss： accuracy： 由于Feature数量较大，将权重影响最大的feature输出看看： 12345678910Other Rel &lt;18 spouse of subfamily RP: [7.11323764] Grandchild &lt;18 ever marr not in subfamily: [6.8321061] Child &lt;18 ever marr RP of subfamily: [6.77322397] Other Rel &lt;18 ever marr RP of subfamily: [6.76688406] Other Rel &lt;18 never married RP of subfamily: [6.37488958] Child &lt;18 spouse of subfamily RP: [5.97717831] United-States: [5.53932651] Grandchild 18+ spouse of subfamily RP: [5.42948497] United-States: [5.41543809] Mexico: [4.79920763] Code完整数据集、代码等，欢迎光临小透明GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222################## Data:2020-04-05# Author: Fred Lau# ML-Lee: HW2 : Binary Classification###########################################################import numpy as npimport csvimport sysimport matplotlib.pyplot as plt########################################################### prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_Train'X_test_fpath = './data/X_test'output_fpath = './logistic_output/output_logistic.csv'fpath = './logistic_output/logistic'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X. # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): # This function splits data into training set and development set. train_size = int(X.shape[0] * (1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)# Split data into train data and development datadev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In logistic model:\\n') f.write('Size of Training set: {}\\n'.format(train_size)) f.write('Size of development set: {}\\n'.format(dev_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n'.format(data_dim))np.random.seed(0)################################################################ useful functiondef _shuffle(X, Y): # This function shuffles two two list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize]def _sigmod(z): # Sigmod function can be used to calculate probability # To avoid overflow return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic function, parameterized by w and b # # Arguments: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probability of each row of X being positively labeled, shape = [batch_size, 1] return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This fucntion returns a truth value prediction for each row of X by logistic regression return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return accdef _cross_entropy_loss(y_pred, Y_label): # This function calculates the cross entropy of Y_pred and Y_label # # Argument: # y_pred: predictions, float vector # Y_label: truth labels, bool vector cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((1 - Y_label).T, np.log(1 - y_pred)) return cross_entropy[0][0]def _gradient(X, Y_label, w, b): # This function calculates the gradient of cross entropy # X, Y_label, shape = [batch_size, ] y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = - np.dot(X.T, pred_error) b_grad = - np.sum(pred_error) return w_grad, float(b_grad)######################################## training by logistic model# Initial weights and biasw = np.zeros((data_dim, 1))b = np.float(0.)w_grad_sum = np.full((data_dim, 1), 1e-8) # avoid divided by zerosb_grad_sum = np.float(1e-8)# Some parameters for trainingepoch = 20batch_size = 2**3learning_rate = 0.2# Keep the loss and accuracy history at every epoch for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Iterative trainingfor it in range(epoch): # Random shuffle at every epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for id in range(int(np.floor(train_size / batch_size))): X = X_train[id*batch_size: (id+1)*batch_size] Y = Y_train[id*batch_size: (id+1)*batch_size] # calculate gradient w_grad, b_grad = _gradient(X, Y, w, b) # adagrad gradient update w_grad_sum = w_grad_sum + w_grad**2 b_grad_sum = b_grad_sum + b_grad**2 w_ada = np.sqrt(w_grad_sum) b_ada = np.sqrt(b_grad_sum) w = w - learning_rate * w_grad / np.sqrt(w_grad_sum) b = b - learning_rate * b_grad / np.sqrt(b_grad_sum) # compute loss and accuracy of training set and development set at every epoch y_train_pred = _f(X_train, w, b) Y_train_pred = np.around(y_train_pred) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size) train_acc.append(_accuracy(Y_train_pred, Y_train)) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.around(y_dev_pred) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size) dev_acc.append(_accuracy(y_dev_pred, Y_dev))with open(fpath, 'a') as f: f.write('Training loss: {}\\n'.format(train_loss[-1])) f.write('Training accuracy: {}\\n'.format(train_acc[-1])) f.write('Development loss: {}\\n'.format(dev_loss[-1])) f.write('Development accuracy: {}\\n'.format(dev_acc[-1]))#################### Plotting Loss and accuracy curve# Loss curveplt.plot(train_loss, label='train')plt.plot(dev_loss, label='dev')plt.title('Loss')plt.legend()plt.savefig('./logistic_output/loss.png')plt.show()plt.plot(train_acc, label='train')plt.plot(dev_acc, label='dev')plt.title('Accuracy')plt.legend()plt.savefig('./logistic_output/acc.png')plt.show()################################## Predictpredictions = _predict(X_test, w, b)with open(output_fpath, 'w') as f: f.write('id, label\\n') for id, label in enumerate(predictions): f.write('{}, {}\\n'.format(id, label[0]))################################ Output the weights and biasind = (np.argsort(np.abs(w), axis=0)[::-1]).reshape(1, -1)with open(X_test_fpath) as f: content = f.readline().strip('\\n').split(',')content = content[1:]with open(fpath, 'a') as f: for i in ind[0, 0: 10]: f.write('{}: {}\\n'.format(content[i], w[i])) Generative ModelGenerative Model 原理部分见 这篇博客 Prepare data这部分和Logistic regression一样。 只是，因为generative model有closed-form solution，不需要划分development set。 1234567891011121314151617181920212223242526272829303132333435363738394041# Prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './generative_output/output_{}.csv'fpath = './generative_output/generative'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)train_size = X_train.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In generative model:\\n') f.write('Size of training data: {}\\n'.format(train_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n\\n'.format(data_dim)) Useful functions1234567891011121314151617181920212223242526# Useful functionsdef _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8))def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function computes prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc Training公式再推导计算公式： $$ \\begin{equation}\\begin{aligned}P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\&=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z)\\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\end{aligned}\\end{equation} $$ 计算z的过程： 首先计算Prior Probability。 假设模型是Gaussian的，算出 $\\mu_1,\\mu_2 ,\\Sigma$ 的closed-form solution 。 根据 $\\mu_1,\\mu_2,\\Sigma$ 计算出 $w,b$ 。 计算Prior Probability。 程序中用list comprehension处理较简单。 1234# compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1]) 计算 $\\mu_1,\\mu_2 ,\\Sigma$ （Gaussian） $\\mu_0=\\frac{1}{C0} \\sum_{n=1}^{C0} x^{n} $ (Label=0) $\\mu_1=\\frac{1}{C1} \\sum_{n=1}^{C1} x^{n} $ (Label=0) $\\Sigma_0=\\frac{1}{C0} \\sum_{n=1}^{C0}\\left(x^{n}-\\mu^{}\\right)^{T}\\left(x^{n}-\\mu^{}\\right)$ (注意 ：这里的 $x^n,\\mu$ 都是行向量，注意转置的位置） $\\Sigma_1=\\frac{1}{C1} \\sum_{n=1}^{C1}\\left(x^{n}-\\mu^{}\\right)^{T}\\left(x^{n}-\\mu^{}\\right)$ $\\Sigma=(C0 \\times\\Sigma_0+C1\\times\\Sigma_1)/(C0+C1)$ (shared covariance) 12345678910111213141516mean_0 = np.mean(X_train_0, axis=0)mean_1 = np.mean(X_train_1, axis=0)# compute in-class covariancecov_0 = np.zeros(shape=(data_dim, data_dim))cov_1 = np.zeros(shape=(data_dim, data_dim))for x in X_train_0: # (D,1)@(1,D) np.matmul(np.transpose([x]), x) cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# shared covariancecov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train.shape[0]) 计算 $w,b$ 在 这篇博客中的第2小节中的公式推导中， $x^n,\\mu$ 都是列向量，公式如下： $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ 但是 ，一般我们在处理的数据集，$x^n,\\mu$ 都是行向量。推导过程相同，公式如下： （主要注意转置和矩阵乘积顺序） $$ z=x\\cdot \\Sigma^{-1}\\left(\\mu^{1}-\\mu^{2}\\right)^{T} -\\frac{1}{2} \\mu^{1}\\Sigma^{-1}\\left(\\mu^{1}\\right)^{T}+\\frac{1}{2}\\mu^{2}\\Sigma^{-1} \\left(\\mu^{2}\\right)^{T} +\\ln \\frac{N_{1}}{N_{2}} $$ $w=\\Sigma^{-1}\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\qquad b=-\\frac{1}{2} \\mu^{1}\\Sigma^{-1}\\left(\\mu^{1}\\right)^{T}+\\frac{1}{2}\\mu^{2}\\Sigma^{-1} \\left(\\mu^{2}\\right)^{T} +\\ln \\frac{N_{1}}{N_{2}}$ 但是，协方差矩阵的逆怎么求呢？ numpy中有直接求逆矩阵的方法(np.linalg.inv)，但当该矩阵是nearly singular，是奇异矩阵时，就会报错。 而我们的协方差矩阵（510*510）很大，很难保证他不是奇异矩阵。 于是，有一个 牛逼 强大的数学方法，叫SVD(singular value decomposition, 奇异值分解) 。 原理步骤我……还没有完全搞清楚QAQ（先挖个坑）[1] 利用SVD，可以将任何一个矩阵（即使是奇异矩阵），分界成 $A=u s v^T$ 的形式：其中u,v都是标准正交矩阵，s是对角矩阵。（numpy.linalg.svd方法实现了SVD） 可以利用SVD求矩阵的伪逆 $A=u s v^T$ u,v是标准正交矩阵，其逆矩阵等于其转置矩阵 s是对角矩阵，其”逆矩阵“（注意s矩阵的对角也可能有0元素） 将非0元素取倒数即可。 $A^{-1}=v s^{-1} u$ 计算 $w,b$ 的代码如下： 12345678910111213141516# compute weights and bias# Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.# cov = u@s@vh# cov_inv = dot(vh.T * 1 / s, u.T)u, s, vh = np.linalg.svd(cov, full_matrices=False)s_inv = s # s_inv avoid &lt;1e-8for i in range(s.shape[0]): if s[i] &lt; (1e-8): break s_inv[i] = 1./s[i]cov_inv = np.matmul(vh.T * s_inv, u.T)w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))b = (-0.5) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (0.5) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) Accuracy accuracy结果： 1Training accuracy: 0.8756450899439694 也将权重较大的feature输出看看： 12345678910age: [-0.51867291] Masters degree(MA MS MEng MEd MSW MBA): [-0.49912643] Spouse of householder: [0.49786805]weeks worked in year: [-0.44710924] Spouse of householder: [-0.43305697]capital gains: [-0.42608727]dividends from stocks: [-0.41994666] Doctorate degree(PhD EdD): [-0.39310961]num persons worked for employer: [-0.37345994] Prof school degree (MD DDS DVM LLB JD): [-0.35594107] Code具体数据集和代码，欢迎光临小透明GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import numpy as npnp.random.seed(0)############################################### Prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './generative_output/output_{}.csv'fpath = './generative_output/generative'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)train_size = X_train.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In generative model:\\n') f.write('Size of training data: {}\\n'.format(train_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n\\n'.format(data_dim))######################### Useful functionsdef _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8))def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function computes prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc######################## Generative Model: closed-form solution, can be computed directly# compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis=0)mean_1 = np.mean(X_train_1, axis=0)# compute in-class covariancecov_0 = np.zeros(shape=(data_dim, data_dim))cov_1 = np.zeros(shape=(data_dim, data_dim))for x in X_train_0: # (D,1)@(1,D) np.matmul(np.transpose([x]), x) cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# shared covariancecov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train.shape[0])# compute weights and bias# Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.# cov = u@s@vh# cov_inv = dot(vh.T * 1 / s, u.T)u, s, vh = np.linalg.svd(cov, full_matrices=False)s_inv = s # s_inv avoid &lt;1e-8for i in range(s.shape[0]): if s[i] &lt; (1e-8): break s_inv[i] = 1./s[i]cov_inv = np.matmul(vh.T * s_inv, u.T)w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))b = (-0.5) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (0.5) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0])# compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)with open(fpath, 'a') as f: f.write('\\nTraining accuracy: {}\\n'.format(_accuracy(Y_train_pred, Y_train)))# Predictpredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id, label\\n') for i, label in enumerate(predictions): f.write('{}, {}\\n'.format(i, label))# Output the most significant weightwith open(X_test_fpath) as f: content = f.readline().strip('\\n').split(',')content = content[1:]ind = np.argsort(np.abs(np.concatenate(w)))[::-1]with open(fpath, 'a')as f: for i in ind[0:10]: f.write('{}: {}\\n'.format(content[i], w[i])) Reference SVD原理，待补充","link":"/2020/04/15/ml-lee-hw2/"},{"title":"「PyTorch」：3-Data And Data Processing","text":"PyTorch框架学习。 这篇文章介绍了深度学习中的数据和数据处理的常用类DataSet和DataLoader。 colab笔记：Data And Data Processing Data-Fashion MNISTWhy Study A Dataset?Data is the primary ingredient of deep learning. 【Data是deep learning的原材料】 Data focused considerations: Who created the dataset?【谁收集的数据集】 How was the dataset created?【数据集是如何收集的】 What transformations were used?【数据运用了哪些变换】 What intent does the dataset have?【数据集的意图是什么】 Possible unintentional consequences?【还可能有什么其他结果吗】 Is the dataset biased?【数据集是否是biased】 Are there ethical issues with the dataset?【数据集会引起道德问题吗】 What Is The MNIST Dataset?The MNIST dataset, Modified National Institute of Standards and Technology database, is a famous dataset of handwritten digits that is commonly used for training image processing systems for machine learning. NIST stands for National Institute of Standards and Technology. 【MNIST, 全称Modified National Institute of Standards and Technology。著名的手写数据集，用于训练图像处理系统。】 MNIST is famous because of how often the dataset is used. It’s common for two reasons: Beginners use it because it’s easy Researchers use it to benchmark (compare) different models. 【MNIST简单；其次研究者常常用MNIST作为其他模型的基准】 The dataset consists of 70,000 images of hand written digits with the following split: 60,000 training images 10,000 testing images 【MNIST的组成，60000个training 图像，10000个testing图像】 MNIST has been so widely used, and image recognition tech has improved so much that the dataset is considered to be too easy. This is why the Fashion-MNIST dataset was created. 【因为MNIST数据集太过简单了，因此Fashion-MNIST数据集出现了】 What Is Fashion-MNIST?Fashion-MNIST as the name suggests is a dataset of fashion items. Specifically, the dataset has the following ten classes of fashion items: 【Fashion-MNIST数据集由许多fashion的物件组成，物件类别如下。】 Index Label 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot Fashion-MNIST is based on the assortment on Zalando’s website. Zalando is a German based multi-national fashion commerce company that was founded in 2008. 【Fashion-MNIST数据集中的数据都是来着Zalando网址上售卖的样图，Zalando创立于2008年，是一家德国跨国时尚公司】 We’ll see the specific ways that Fashion-MNIST mirrors the original dataset in the paper, but one thing we have already seen is the number of classes. MNIST – has 10 classes (one for each digit 0-9) Fashion-MNIST – has 10 classes (this is intentional) 【Fashion-MNIST和MNIST是镜像对应的，比如他们都有10个类别】 How Fashion-MNIST Was BuiltUnlike the MNIST dataset, the fashion set wasn’t hand-drawn, but the images in the dataset are actual images from Zalando’s website. 【Fashion-MNNST中所有的图像都来自Zalando的官网的图片，再通过多种变换，变成28*28的图像】 Extract, Transform, Load(ETL)There are four general steps that we’ll be following as we move through this project: 【一般分为4步：准备数据；构建模型；训练模型；分析结果】 Prepare the data Build the model Train the model Analyze the model’s results The ETL ProcessIn this post, we’ll kick things off by preparing the data. To prepare our data, we’ll be following what is loosely known as an ETL process. 【准备数据的过程一般又叫ETL过程：提取、转化、装载】 Extract data from a data source.【从数据源提取数据】 Transform data into a desirable format.【转化为便于处理的格式】 Load data into a suitable structure.【装载数据，便于读取】 PyTorch包的主要组成： Package Description torch PyTorch的顶层包和tensor库 torch.nn 包含构建NN的模型和扩展类 torch.autograd PyTorch中支持的Tensor操作 torch.nn.functional 包含构建NN的函数接口，像loss function, activation fucntion, convolution operation torch.optim 包含标准的优化，像SGD, Adam torch.utils 包含实用类，像数据集，数据装载器，方便数据预处理 torchvision 提供著名的数据集，模型架构和计算机视觉图像转换 torchvision.transforms: An interface that contains common transforms for image processing. 【一个包含图像转换（用于图像处理）的接口。】 常用包： pandas:https://www.pypandas.cn/ Pandas是一个强大的分析结构化数据的工具集；它的使用基础是Numpy（提供高性能的矩阵运算）；用于数据挖掘和数据分析，同时也提供数据清洗功能。 NumPy: https://www.numpy.org.cn/user/setting-up.html NumPy是Python中科学计算的基础包。它是一个Python库，提供多维数组对象，各种派生对象（如掩码数组和矩阵），以及用于数组快速操作的各种API，有包括数学、逻辑、形状操作、排序、选择、输入输出、离散傅立叶变换、基本线性代数，基本统计运算和随机模拟等等。 Matplotlib：https://www.matplotlib.org.cn/ Matplotlib 是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。Matplotlib可用于Python脚本，Python和IPython Shell、Jupyter笔记本，Web应用程序服务器和四个图形用户界面工具包。 为了简单绘图，该 pyplot 模块提供了类似于MATLAB的界面，尤其是与IPython结合使用时。 对于高级用户，您可以通过面向对象的界面或MATLAB用户熟悉的一组功能来完全控制线型，字体属性，轴属性等。 pdb 是Python的调试器。 Preparing Our Data Extract – Get the Fashion-MNIST image data from the source.【获得Fashion-MNIST数据集】 Transform – Put our data into tensor form.【转换：将我们的数据转换为tensor】 Load – Put our data into an object to make it easily accessible.【装载：聚合数据为一个对象，方便获取】 For these purposes, PyTorch provides us with two classes: 【PyTorch为处理数据所提供的两个类】 Class Description torch.utils.data.Dataset 数据集的抽象类 torch.utils.data.DataLoader 打包数据集，提供访问底层数据的接口。 To create a custom dataset using PyTorch, we extend the Dataset class by creating a subclass that implements these required methods. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object. 【创建数据集必须继承Dataset类，继承的子类作为参数传递给DataLoader对象】 All subclasses of the Dataset class must override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive. 【Dateset的子类必须重写__len__ 方法（表示数据集的大小），重写__getitem__ （按索引获得特定数据）】 PyTorch Torchvision PackageThe torchvision package, gives us access to the following resources: 【torchvision主要提供一些典型数据集、模型、转换、工具】 Datasets (like MNIST and Fashion-MNIST) Models (like VGG16) Transforms Utils The PyTorch FashionMNIST dataset simply extends the MNIST dataset and overrides the urls. 【FashionMNIST数据集继承了MNIST数据集，只重写了数据集的url】 12345678910111213141516171819202122class FashionMNIST(MNIST): &quot;&quot;&quot;`Fashion-MNIST &lt;https://github.com/zalandoresearch/fashion-mnist&gt;`_ Dataset. Args: root (string): Root directory of dataset where ``processed/training.pt`` and ``processed/test.pt`` exist. train (bool, optional): If True, creates dataset from ``training.pt``, otherwise from ``test.pt``. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. &quot;&quot;&quot; urls = [ 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz', 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz', 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz', 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz', ] Let’s see now how we can take advantage of torchvision. PyTorch Dataset ClassTo get an instance of the FashionMNIST dataset using torchvision, we just create one like so: 12345678train_set = torchvision.datasets.FashionMNIST( root='./data' ,train=True ,download=True ,transform=transforms.Compose([ transforms.ToTensor() ])) We specify the following arguments: Parameter Description root The location on disk where the data is located.【数据集的位置】 train If the dataset is the training set.【是否是训练数据集】 download If the data should be downloaded.【如果数据集不存在，是否下载】 transform A composition of transformations that should be performed on the dataset elements.【变换的组合】 PyTorch DataLoader ClassTo create a DataLoader wrapper for our training set, we do it like this: 1234train_loader = torch.utils.data.DataLoader(train_set ,batch_size=1000 ,shuffle=True) 【DataLoader，聚合数据集和取样器，提供数据集的迭代器】 We just pass train_set as an argument. Now, we can leverage the loader for tasks that would otherwise be pretty complicated to implement by hand: 【DataLoader能让一些手动实现复杂的简单化】 batch_size (1000 in our case) shuffle (True in our case) num_workers (Default is 0 which means the main process will be used) PyTorch Datasets And DataLoadersWorking With The Training SetIn this post, we are going to see how we can work with the dataset and the data loader objects that we created in the previous post. PyTorch Dataset:Suppose we want to see the labels for each image. This can be done like so: 123# Starting with torchvision 0.2.2&gt; train_set.targetstensor([9, 0, 0, ..., 3, 0, 5]) If we want to see how many of each label exists in the dataset, we can use the PyTorch bincount() function like so: Class Imbalance: Balanced And Unbalanced DatasetsThis shows us that the Fashion-MNIST dataset is uniform with respect to the number of samples in each class. This means we have 6000 samples for each class. As a result, this dataset is said to be balanced. If the classes had a varying number of samples, we would call the set an unbalanced dataset. 【Fashion-MNIST数据集是均匀分布，即每个class的samples数量相同。均匀分布的数据集被称为是balanced。】 To read more about the ways to mitigate unbalanced datasets in deep learning, see this paper: A systematic study of the class imbalance problem in convolutional neural networks. Accessing Data In The Training Set【获得数据集中的数据：先将train_set 传递给Python函数iter() 生成迭代器，再将迭代器传递给内置函数next 用来迭代】 iter(object) ： object：支持迭代的集合对象 返回值：迭代器对象 next(iterable[, default]) : 常和iter() 一同使用 返回迭代器中的下一个项目 To access an individual element from the training set, we first pass the train_set object to Python’s iter() built-in function, which returns an object representing a stream of data. With the stream of data, we can use Python built-in next() function to get the next data element in the stream of data. After passing the sample to the len() function, we can see that the sample contains two items, and this is because the dataset contains image-label pairs. Each sample we retrieve from the training set contains the image data as a tensor and the corresponding label as a tensor. 【train_set中的sample是一个image-label对，因此sample的len为2】 12345image,label = sampleprint(type(image))print(type(label))&lt;class 'torch.Tensor'&gt;&lt;class 'int'&gt; Working With Batches Of DataWe’ll start by creating a new data loader with a smaller batch size of 10 so it’s easy to demonstrate what’s going on: 123456display_loader = torch.utils.data.DataLoader( train_set, batch_size=10)batch = next(iter(display_loader))len(batch)2 We get a batch from the loader in the same way that we saw with the training set. We use the iter() and next() functions. 【相较于使用training set，从DataLoader中可以取出a batch of data】 Checking the length of the returned batch, we get 2 just like we did with the training set. Let’s unpack the batch and take a look at the two tensors and their shapes: 12345images, labels = batchprint('types:', type(images), type(labels))print('shapes:', images.shape, labels.shape)types: &lt;class 'torch.Tensor'&gt; &lt;class 'torch.Tensor'&gt;shapes: torch.Size([10, 1, 28, 28]) torch.Size([10]) The size of each dimension in the tensor that contains the image data is defined by each of the following values: (batch size, number of color channels, image height, image width) 1234567grid = torchvision.utils.make_grid(images, nrow=10)plt.figure(figsize=(15, 15))plt.imshow(np.transpose(grid, (1,2,0)))//grid = torchvision.utils.make_grid(images, nrow=10)plt.figure(figsize=(15,15))plt.imshow(grid.permute(1,2,0)) plt.imshow(X) : X=(M,N,3) RGB2D图 而grid返回的应该是（C-H-W），所以要置换一下axes。置换使用np.transpose(grid, (1,2,0))或grid.permute(1,2,0) Plot Images Using PyTorch DataLoaderHere is another was to plot the images using the PyTorch DataLoader.","link":"/2021/02/27/pytorch-data/"},{"title":"「Cryptography-MIT6875」: Lecture 7","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ It is indeed possible for $F(x)$ to leak a lot of information about $x$ even if $F$ is one-way. The hardcore predicate $B(x)$ represent the specific piece of information about $x$ which is hard to compute given $F(x)$. Topics Covered: Definition of one-way functions (OWF) Definition of hardcore bit/predicate (HCB) One-way permutations → PRG.(In fact, one-way functions → PRG, but that’s a much harder theorem.) Goldreich-Levin Theorem: every OWF has a HCB.(Proof for an important special case.) One-way FunctionsInformally, one-way function is easy to compute and hard to invert. DefinitionIn last blog, we introduced briefly the definition of One-way functions. Take 1 (Not a useful definition): A function (family) $\\{F_n\\}_{n\\in \\mathbb{N}}$ where $F_n:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{m(n)}$ is one-way if for every p.p.t. adversary $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F_n(x):A(1^n,y)=x]\\le \\mu(n) $$ Consider $F_n(x)=0$ for all $x$. This is one-way according to the above definition. But it’s impossible to find the inverse even if $A$ has unbounded time. The probability of guessing the inverse of $0$ is negligible since it is essentially random. But $f(x)=0$ is not one-way function obviously. Hence, it’s not a useful or meaningful definition. The right definition should be that it is impossible to find an inverse in p.p.t. rather than the exactly chosen $x$. One-way Functions Definition: A function (family) $\\{F_n\\}_{n\\in \\mathbb{N}}$ where $F_n:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{m(n)}$ is one-way if for every p.p.t. adversary $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F_n(x):A(1^n,y)=\\color{red}{x':y=F_n(x')}]\\le \\mu(n) $$ I’m not going to ask the adversary to come up with $x$ itself which may be impossible. Instead, I’m just going to ask the adversary to come up with an $x’$ such that $y$ is equal to $F(x’)$. It’s sort of the pre-image of $y$. Hence, the adversary can always find an inverse with unbounded time. But it should be hard with probabilistic polynomial time. Moreover, One-way Permutations are one-to-one one-way functions with $m(n)=n$. Hardcore BitsIf $F$ is a one-way function, it’s hard to compute a pre-image of $F(x)$ for a randomly chosen $x$. How about computing partial information about an inverse ? We saw in last blog that we can tell efficiently if $h$ is quadratic residue. So for the discrete log function $x=\\operatorname{dlog}_g(h)$, computing the least significant bit(LSB) of $x$ is easy. But MSB turns out to be hard. Moreover, there are one-way functions for which it is easy to compute the first half of the bits of the inverse. One-way function family easy to compute the first half of the bits: There is an obvious fact that if $f(x)$ is one-way, then $f’(r||x)=r||f(x)$ is one-way. ($|r|=|f(x)|$)It’s hard to compute the pre-image of $f’(r||x)$ because if you can break $f’$ then you can break $f$.But it’s easy to compute the first half of the bits since they are written in the output. Nevertheless, there has to be a hardcore set of hard to invert inputs. Concretely, dose there necessarily exist some bit of $x$ that is hard to compute ? Particularly, “hard to compute” means “hard to guess with probability non-negligibly better than 1/2” since any bit can be guessed correctly with probability 1/2. So dose there necessarily exist some bit of $x$ that is hard to guess with probability non-negligibly better than 1/2 ? Hardcore Bit DefHardcore Bit Definition (Take 1): For any function (family) $F:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ , a bit $i=i(n)$ is hardcore if for every p.p.t. adversary $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x):A(y)=x_i]\\le 1/2 +\\mu(n) $$ The definition says that it is hard to guess the $i$-th bit of the inverse given the $y$. I mentioned above that there are one-way functions for which it is easy to compute the first half of the bits of the inverse. Moreover, there are functions that are one-way, yet every bit is somewhat easy to predict with probability $\\frac{1}{2}+1/n$. This is actually an (hard) exercise in the lecture. Sadly, I haven’t figured out the construction that every bit is easy to compute w.p. $\\frac{1}{2}+1/n$.Hope to change the ideas with you. Although the entire inverse of $f(x)$ is hard to compute, it is indeed possible for $f(x)$ to leak a lot of information about $x$ even if $f$ is one-way. Hardcore Predicate DefSo, we generalize the notion of a hardcore “bit”. We define hardcore predicate to identify a specific piece of information about $x$ that is “hidden” by $f(x)$. Informally, a hardcore predicate of a one-way function $f(x)$ is hard to compute given $f(x)$. Hardcore Predicate Definition: For any function (family) $F:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ , a function $B:\\{0,1\\}^n\\rightarrow \\{0,1\\}$ is a hardcore predicate if for every p.p.t. adversary $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x):A(y)=B(x)]\\le 1/2 +\\mu(n) $$ The definition says that a hardcore predicate $B(x)$ of a one-way function $f(x)$ is hard to compute with probability non-negligibly better than $1/2$ given $f(x)$ since the predicate $B(x)$ is a boolean function that can be computed with probability $1/2$. Besides, this is perfectly consistent with the fact that the entire inverse is actually hard to compute. Otherwise, you can compute $B(x)$. Henceforth, for us, a hardcore bit will mean a hardcore predicate. We can represent the definition by the following picture. It’s easy to compute the one-way function $F(x)$ given $x$. It’s easy to compute the hardcore predicate $B(x)$ of $F(x)$ given $x$. But it’s hard to compute $B(x)$ given $F(x)$. We know that it is indeed possible for $F(x)$ to leak a lot of information about $x$ even if $F$ is one-way. Hence, the hardcore predicate $B(x)$ represent the specific piece of information about $x$ which is hard to compute given $F(x)$. One-way Permutations → PRGIn this section, we show that One-way Permutations imply PRG. The construction is as follows: PRG Construction from OWP: Let $F$ be a one-way permutation, and $B$ an associated hardcore predicate for $F$. Then, define $G(x)=F(x)||B(x)$. Note: $G$ stretches by one bit. We can turn this into a $G’$ that stretches to any poly. number of bits from PRG Length Extension. Theorem: $G$ is a PRG assuming $F$ is a one-way permutation. Proof (using NBU): The thing we want to prove is that if $F$ is a one-way permutation and $B$ is the hardcore predicate for $F$, then $G(x)=F(x)||B(x)$ is a PRG. We prove it using next-bit unpredictability. Assume for contradiction that $G$ is not a PRG. Therefore, there is a next-bit predictor $D$, and index $i$, and a polynomial function $p$ such that $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):D(y_1\\dots y_{i-1})=y_i]\\ge \\frac{1}{2}+1/p(n)$ If we want to get the contradiction to $B(x)$, the index $i$ has to be $n+1$.(Because the $G(x)=F(x)||B(x)$ where $F(x)$ is $n$-bit and $B(x)$ is $1$-bit.) $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):D(y_1\\dots y_{n})=y_{n+1}]\\ge \\frac{1}{2}+1/p(n)$ Then we can construct a hardcore bit (predicate) predictor from $D$.In fact, $D$ is a hardcore predictor. $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):D(F(x))=B(x)]\\ge \\frac{1}{2}+1/p(n)$ QED. Proof (using Indistinguishability): We can also prove it using indistinguishability. Assume for contradiction that $G$ is not a PRG.Therefore, there is a p.p.t. distinguisher $D$, and a polynomial function $p$ such that $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):D(y)=1] &- \\\\ \\operatorname{Pr}[y\\leftarrow \\{0,1\\}^{n+1}:D(y)] &\\ge 1/p(n)\\end{aligned} $$ We construct a hardcore predictor $A$ and show: $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n:A(F(x))=B(x)]\\ge \\frac{1}{2}+1/p'(n)\\end{aligned} $$ What information can we learn from the distinguisher $D$ ? Rewrite $y$ in the first term by definition. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=\\color{blue}{F(x)||B(x)}:D(y)=1] &- \\\\\\operatorname{Pr}[y\\leftarrow \\{0,1\\}^{n+1}:D(y)=1]&\\ge 1/p(n)\\end{aligned} $$ Rewrite the second term by a syntactic change. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x)||B(x):D(y)=1]&-\\\\ \\operatorname{Pr}[{\\color{blue}{y_0\\leftarrow \\{0,1\\}^{n},y_1\\leftarrow \\{0,1\\}},y=y_0||y_1}:D(y)=1] &\\ge 1/p(n)\\end{aligned} $$ Rewire the second term since $F$ is one-way permutation. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x)||B(x):D(y)=1]&-\\\\ \\operatorname{Pr}[{\\color{blue}{x\\leftarrow \\{0,1\\}^{n},y_1\\leftarrow \\{0,1\\},y=F(x)||y_1}}:D(y)=1] &\\ge 1/p(n)\\end{aligned} $$ ​ Rewrite the second term since $\\operatorname{Pr}[y_1=0]=\\operatorname{Pr}[y_1=0]=1/2$. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x)||B(x):D(y)=1] &-\\\\ \\frac{\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||0}}:D(y)=1]+\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||1}}:D(y)=1]}{2} &\\ge 1/p(n)\\end{aligned} $$ Rewrite the second term using $B(x)$. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x)||B(x):D(y)=1] &-\\\\ \\frac{\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||B(x)}}:D(y)=1]+\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||\\overline{B(x)}}}:D(y)=1]}{2} &\\ge 1/p(n)\\end{aligned} $$ Putting thins together. $$ \\begin{aligned}\\frac{1}{2}(\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||B(x)}}:D(y)=1] &- \\\\ \\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||\\overline{B(x)}}}:D(y)=1]) &\\ge 1/p(n)\\end{aligned} $$ Although the mathematical transformations seem complex, the main idea is to turn what we know to what we want. We want to know some information about $B(x)$. The takeaway is that $D$ says 1 more often when fed with the “right bit” than the “wrong bit”. It’s a familiar statement. In [Lecture 3] , we construct a predictor from a distinguisher to prove the next-bit unpredictability → the indistinguishability. In that proof, we got the takeaway from hybrid argument, that the distinguisher $D$ says 1 more often when fed with the “right bit” than the “wrong bit”. Construct a hardcore bit predictor $A$ from the distinguisher $D$ using the takeaway. The task of $A$ is get as input $z=F(x)$ and try to guess the hardcore predicate. The predictor works as follows: $$ \\begin{aligned}A(F(x))=\\begin{cases} b,& \\text{if }D(F(x)||b)=1\\\\ \\overline{b},& \\text{otherwise}\\end{cases},b\\leftarrow\\{0,1\\} \\end{aligned} $$ Pick a random bit $b$. Feed $D$ with input $z||b$ If $D$ says “1”, output $b$ as the prediction for the hardcore bit and if $D$ says “0”, output $\\overline{b}$. Analysis of the predictor $A$. $\\operatorname{Pr}[x\\leftarrow {0,1}^n:A(F(x))=B(x)]$ $A$ output $b=B(x)$ or $\\overline{b}=B(x)$. $\\begin{aligned} =\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||b)=1 \\color{blue}{\\wedge b=B(x)}] + \\\\ \\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||b)=0 \\color{blue}{\\wedge b\\ne B(x)}]& \\\\\\end{aligned}$ $\\begin{aligned}=\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||b)=1 \\mid b=B(x)]\\operatorname{Pr}[b=B(x)]& + \\\\ \\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||b)=0 \\mid b\\ne B(x)]\\operatorname{Pr}[b\\ne B(x)]& \\end{aligned}$ Since $b$ is random.$\\begin{aligned}=\\color{blue}{\\frac{1}{2}}(\\operatorname{Pr}[x\\leftarrow{0,1}^n:D(F(x)||b)=1 \\mid b=B(x)]&amp; + \\ \\operatorname{Pr}[x\\leftarrow{0,1}^n:D(F(x)||b)=0 \\mid b\\ne B(x)]&amp;) \\end{aligned}$ Put the prior condition in it.$\\begin{aligned}=\\frac{1}{2}(\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||\\color{blue}{B(x)})=1]& + \\\\ \\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||\\color{blue}{\\overline{B(x)}})=0 ) \\end{aligned}$ $\\begin{aligned}=\\frac{1}{2}(\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||\\color{blue}{B(x)})=1]& + \\\\ 1-\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||\\color{blue}{\\overline{B(x)}})=1 ) \\end{aligned}$ From the takeaway, we can get$=\\frac{1}{2}(1+(*))\\ge \\frac{1}{2} + 1/p(n)$ So the $A$ can predict the hardcore bit (predicate) with probability non-negligible better than $1/2$. Goldreich-Levin Theorem: every OWF has a HCB.A Universal Hardcore Predicate for all OWFWe have defined the hardcore predicate $B(x)$ for the particular one-way function $F(x)$. Let’s shoot for a universal hardcore predicate for every one-way function, i.e., a single predicate $B$ where it is hard to guess $B(x)$ given $F(x)$. Is this possible ? Turns out the answer is “no”. Pick a favorite amazing $B$. We can construct a one-way function $F$ which $B$ is not hardcore. There is a contradiction example. Claim 1: If $f(x)$ is a one-way function, then $f’(x)=f(x)||B(x)$ is one-way as well.Claim 2:But $B(x)$ is NOT a hardcore predicate for $f’(x)$. For claim 1, if you can compute the inverse of $f’(x)$, then you can compute the inverse of $f(x)$.For claim 2, $B(x)$ is actually written in $f’(x)$.So $f’(x)$ is one-way and dose not leak much information. Goldreich-Levin(GL) TheoremBut Goldreich-Levin(GL) Theorem changes the statement to a random hardcore and tells us every one-way function has a hardcore predicate/bit. Goldreich-Levin(GL) Theorem: Let $\\{B_r:\\{0,1\\}^n\\rightarrow \\{0,1\\}\\}$ where $$ B_r(x)=\\langle r, x \\rangle =\\sum_{i=1}^n r_ix_i \\mod 2 $$ be a collection of predicates (one for each $r$). Then a random $B_r$ is hardcore for every one-way function $F$. That is, for every one-way function $F$, every p.p.t. $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;r\\leftarrow \\{0,1\\}^n:A(F(x),r)=B_r(x)]\\le\\frac{1}{2} +\\mu(n) $$ It defines a collection of predicates ${B_r:{0,1}^n\\rightarrow {0,1}}$ indexed by $r$, a $n$-bit vector. So the evaluation of $B_r(x)$ is essentially an inner product of $r$ and $x$ (mod 2). The definition says that a random $B_r$ is hardcore for every one-way funciton $F$. So it picks a random $x$ and a random $r$. For every p.p.t. adversary $A$, it is hard to compute $B_r(x)$ given $F(x)$ and $r$. Alternative Interpretations to GL TheoremThere are some alternative interpretations to GL Theorem. Alternative Interpretation 1: For every one-way function $F$, there is a related one-way function $F’(x,r)=(F(x),r)$ which has a deterministic hardcore predicate. For every one-way function $F$, you can change it to a related one-way function $F’(x,r)=(F(x),r)$. Although $F’$ leak the second half of bits, $F’$ is one-way. Then $F’$has a deterministic hardcore predicate $B(x,r)=\\langle x,r \\rangle\\pmod 2$. $B(x,r)$ is a fixed function, which performs an inner-product mod 2 between the first half and the second half of the inputs. Hence, in this interpretation, GL Theorem says that $B$ is not a hardcore bit for every OWF, but it’s a hardcore bit for the related function, which we created it from the OWF. Moreover, if $F(x)$ is one-way permutation, then $F’(x,r)=(F(x),r)$ is one-way permutation. And we can get a PRG from $F’(x,r)$. Then $G(x,r)=F(x)||r||\\langle x,r\\rangle$ where the last bit is the hardcore bit. Alternative Interpretation 2: For every one-way function $F$, there exists (non-uniformly) a (possibly different) hardcore predicate $\\langle r_F,x\\rangle$. To be honest, I do not comprehend the advanced statement. The professor left a cool open problem: remove the non-uniformity. Proof of GL TheoremAssume for contradiction there is a predictor $P$ and a polynomial $p$ s.t. $\\operatorname{Pr}[x\\leftarrow {0,1}^n;r\\leftarrow {0,1}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge\\frac{1}{2} +p(n)$ We need to show an inverter $A$ for $F$ and a polynomial $p’$ such that $\\operatorname{Pr}[x\\leftarrow {0,1}^n:A(F(x))=x’:F(x’)=F(x)]\\ge 1/p’(n)$ But it is too hard to prove this contradiction. Let’s make our lives easier. A Perfect PredicatorFor simplicity, we assume a perfect predicator. Assume a perfect predictor: Assume a perfect predictor $P$, which can completely predicte the HCB correctly, s.t. $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]=1$ Now we can construct an inverter $A$ for $F$. The inverter $A$ works as follows: On input $y=F(x)$, $A$ runs the predictor $P$ $n$ times on input $(y, e_1),(y,e_2),\\dots,$and $(y,e_n)$ where $e_1=100..0,e_2=010..0,\\dots$ are the unit vectors. Since $A$ is perfect, it returns $\\langle e_i,x\\rangle=x_i$, the $i$-th bit of $x$ on the $i$-th invocation. A Pretty Good PredictorThen we assume less. Assume a pretty good predictor: Assume a pretty good predictor $P$, s.t. $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4} + 1/p(n)$ What can we learn from the predictor ? Claim: For at least a $1/2p(n)$ fraction of the $x$,$\\operatorname{Pr}[r\\leftarrow {0,1}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4} + 1/2p(n)$ How to derive the claim by averaging argument ? When I wrote this blog, I contemplated it for a long long time.I figured it out with the help of Wengjie Li. Thanks!I read the details about the proof of GL Theorem in some textbooks.Some proved the lower bound of fraction $1/2p(n)$, but how about the lower bound of probability, $\\frac{3}{4}+1/2p(n)$?In fact, they are related. Before that, let’s introduce averaging argument. In computational complexity theory and cryptography, averaging argument is a standard argument for proving theorems. It usually allows us to convert probabilistic polynomial-time algorithms into non-uniform polynomial-size circuits.——Wiki Averaging Argument: Let $f$ be some function. The averaging argument is the following claim: If we have a circuit $C$ such that $C(x,y)=f(x)$ with probability at least $\\rho$ where $x$ is chosen at random and $y$ is chosen independently from some distribution $Y$ over $\\{0,1\\}^m$ (which might not even be effciently sampleable), then there exists a single string $y_0\\in \\{0,1\\}^m$ such that $\\operatorname{Pr}_x[C(x,y_0)=f(x)]\\ge \\rho$. Indeed, for every $y$ define $p_y$ to be $\\operatorname{Pr}_x[C(x,y)=f(x)]$, then $$ \\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\\mathbb{E}_y[p_y] $$ and then this reduces to the claim that for every random variable $Z$, if $\\mathbb{E}[Z]\\ge \\rho$ then $\\operatorname{Pr}[Z\\ge \\rho]&gt;0$ (this holds since $\\mathbb{E}[Z]$ is the weighted average of Z and clearly if the average of some values is at least $\\rho$ then one of the values must be at least $\\rho$.) The claim tells us if $\\operatorname{Pr}_{x,y}[C(x,y)=f(x)]\\ge \\rho$, then there exist some $y_0$ such that $\\operatorname{Pr}_x[C(x,y_0)=f(x)]\\ge \\rho$ as well. So we can single out these $y_0$ using averaging argument. Actually, I think the equation, $\\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\\mathbb{E}_y[p_y]$, is the manifestation of averaging. The following analysis is mostly my own deduction and comprehension since the proof is omitted in the lecture. Corrections and advice are welcome. Some analysis to $\\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\\mathbb{E}_y[p_y]$: By definition of $p_y$ $y$ is a variable and $p_y$ is a variable. By definition, $p_y$ is up to $y$. So the distribution of $p_y$ is equal to the distribution of $y$. Expand the left term.$\\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\\sum _i \\operatorname{Pr}_x[C(x,y_i)=f(x)]\\operatorname{Pr}[y=y_i]$ The right term is the expected value of variable $p_y$. $\\mathbb{E}_y[p_y] = \\sum_i p_{y_i} \\cdot \\operatorname{Pr}[p_y=p_{y_i}]$ Because the distribution of $p_y$ is equal to the distribution of $y$.$\\operatorname{Pr}[p_y=p_{y_0}]=\\operatorname{Pr}[y={y_0}]$ So, $\\mathbb{E}_y[p_y] = \\sum_i p_{y_i} \\cdot \\operatorname{Pr}[y=y_i]$ = the left term. Now back to the claim. The following is mostly my own deduction and comprehension since the proof is omitted in the lecture. Corrections and advice are welcome. Proof of the Claim : What can we learn from the claim? What do we know from the predictor ? We know $\\operatorname{Pr}_{x,r}[P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4} + 1/p(n)$. For every $x$, define $p_x=\\operatorname{Pr}_r[P(F(x),r)=\\langle r,x\\rangle]$. By averaging argument, We know there exists some $x$ such that $p_x\\ge \\frac{3}{4} + 1/p(n)$. We know $\\operatorname{Pr}_{x,r}[P(F(x),r)=\\langle r,x\\rangle]=\\mathbb{E}[p_x]$ by averaging argument. So we know the lower bound of $\\mathbb{E}[p_x]$ (expected value of $p_x$) is $\\frac{3}{4} + 1/p(n)$. What do we really want ? We want to single out these $x$ such that $p_x$ is non-negligibly better than $\\frac{3}{4}$ in (probabilistic) polynomial time. So we want a set of $x$, which the fraction is polynomial , for every $x$ in the set, the $p_x$ is non-negligibly better than $\\frac{3}{4}$. We show the fraction and the (lower bound of ) probability are related. Notation $\\epsilon$ define the lower bound of $\\mathbb{E}[p_x]$, i.e. $\\epsilon =\\frac{3}{4} + 1/p(n)$. Define a good set of $x$ as $S$ and $s$ denotes the poly. fraction of the $x$. ($|S|=s\\cdot 2^n$). $S=\\{x\\mid p_x \\ge \\varepsilon' \\}$ where $\\varepsilon’$ denotes the lower bound of $p_x$ for every $x\\in S$, non-negligibly better than $\\frac{3}{4}$. Rewrite $\\mathbb{E}[p_x]$ using $s$ and $\\varepsilon$. $p_x= \\begin{cases} \\varepsilon' \\le p_x\\le 1,& x\\in S \\\\ 0\\le p_x \\le \\varepsilon',& x\\notin S \\end{cases}$ Similarly, $p_x$ is up to $x$.Hence, the distribution of $p_x$ is equal to the distribution of $x$, i.e.$\\operatorname{Pr}[p_x]=\\operatorname{Pr}[x]$ $\\mathbb{E}[p_x] = \\sum p_x \\cdot \\operatorname{Pr}[p_x]=\\sum p_x \\cdot \\operatorname{Pr}[x]$ $=\\sum p_x \\cdot \\operatorname{Pr}[x\\in S] + \\sum p_x \\cdot \\operatorname{Pr}[x\\notin S]$ $=\\sum p_x \\cdot s + \\sum p_x \\cdot (1-s)$ We do not know the actual $p_x$ but we know the boundary of $p_x$. Calculate the boundary of $\\mathbb{E}[p_x]$ using $s$ and $\\varepsilon$. $\\mathbb{E}[p_x] =\\sum p_x \\cdot s + \\sum p_x \\cdot (1-s)$ The lower bound is $\\varepsilon’ \\cdot s + 0\\cdot (1-s)=\\varepsilon’ \\cdot s$ (useless) The upper bound is $1\\cdot s + \\varepsilon’ \\cdot (1-s)=s+\\varepsilon’ (1-s)$ (useful) This upper bound should be greater than the known lower bound $\\epsilon$. We get the relation of $s$ and $\\varepsilon’$. $\\epsilon \\le s+\\varepsilon’ (1-s)$ $\\frac{\\epsilon-s}{1-s}\\le \\varepsilon’$ (since $s&lt;1$) Take $\\epsilon$ with $\\frac{3}{4} + 1/p(n)$. We want the fraction $s$ to be polynomial. Suppose the fraction $s=1/2p(n)$.We can get $\\varepsilon’ \\ge \\epsilon -s = \\frac{3}{4} +1/2p(n)$. (since $1-s$ is very small). We can also suppose the fraction $s=1/3p(n)$ and so on. The point I want to elucidate here is the fraction $s$ and the lower bound of $p_x$ for every $x\\in S$ can both be polynomial if the advantage of predicting is non-negligible. Hence, the takeaway is if the probability of predicting is non-negligibly better than $3/4$, then we can single out these “good $x$ ”in (probabilistic) polynomial time. Similarly, if the probability of predicting is non-negligibly better than $1/2$ (the predictor in general case), we can also single out these “good $x$ ” in (probabilistic) polynomial time. We get the takeaway that if the probability of predicting is non-negligibly better than $3/4$, then we can single out these “good $x$ ” in (probabilistic) polynomial time. Invert the i-th bit: Now we can compute the $i$-th bit of $x$ with the probability better than $1/2$.The key idea is linearity. Pick a random $r$ Ask $P$ to tell us $\\langle r,x\\rangle$ and $\\langle r+e_i,x \\rangle$. Subtract the two answers to get $\\langle e_i,x\\rangle =x_i$ Proof of invert $i$-th bit: $\\operatorname{Pr}[\\text{we compute }x_i \\text{ correctly}]$ $\\ge \\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ and }\\langle r+e_i,x \\rangle \\text{ correctly}]$ $=1-\\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ or }\\langle r+e_i,x \\rangle \\text{ wrong}]$ $\\ge1-\\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ wrong}]\\text{ + } \\operatorname{Pr}[P \\text{ predicts }\\langle r+e_i,x \\rangle \\text{ wrong}]$(by union bound) $\\ge 1 - 2\\cdot (\\frac{1}{4} -\\frac{1}{2p(n)})=\\frac{1}{2}+1/p(n)$ I think it takes the fraction as $1/2p(n)$ just to make the advantage equal to $1/p(n)$. It’s beautiful. Invert the entire x: Construct the Inverter $A$ Repeat for each bit $i\\in{1,2,\\dots, n}$: Repeat $p(n) \\cdot p(n)$ times:(one $p(n)$ is for singling out, and another is for computing correctly) Pick a random $r$ Ask $P$ to tell us $\\langle r,x\\rangle$ and $\\langle r+e_i,x \\rangle$. Subtract the two answers to get $\\langle e_i,x\\rangle =x_i$ Compute the majority of all such guesses and set the bit as $x_i$. Output the concatenation of all $x_i$ as $x$. Analysis of $A$ is ommited.Hint: Chernoff + Union Bound. A General PredictorLet’s proceed to a general predictor, which is in the foremost contradiction. Assume a general good predictor: Assume there is a predictor $P$ and a polynomial $p$ s.t. $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge\\frac{1}{2} +p(n)$ Likewise, there is a similar claim. Claim: For at least a $1/2p(n)$ fraction of the $x$, $\\operatorname{Pr}[r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{1}{2} + 1/2p(n)$ The takeaway is that probability of predicting is non-negligibly better than $1/2$ , we can single out these “good $x$ ” in (probabilistic) polynomial time. However, we cannot invert the $i$-th bit of $x$ with the probability better than $1/2$ using the above method. Analysis of Inverting $x_i$: (error doubling) (For at least $1/2p(n)$ fraction of the $x$) If the probability of predicting $\\langle r,x\\rangle$ correctly is non-negligibly better than $3/4 + 1/2p(n)$, i.e. $\\operatorname{Pr}[r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4} + 1/2p(n)$ $\\operatorname{Pr}[\\text{we compute }x_i \\text{ correctly}]$ $\\ge \\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ and }\\langle r+e_i,x \\rangle \\text{ correctly}]$ $=1-\\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ or }\\langle r+e_i,x \\rangle \\text{ wrong}]$ $\\ge1-\\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ wrong}]\\text{ + } \\operatorname{Pr}[P \\text{ predicts }\\langle r+e_i,x \\rangle \\text{ wrong}]$ $\\ge 1 - \\color {blue}{2\\cdot(\\frac{1}{4} -\\frac{1}{2p(n)})}=\\frac{1}{2}+1/p(n)$ So it can compute $x_i$ with advantage $1/p(n)$. If the probability of predicting $\\langle r,x\\rangle$ correctly is exactly $3/4$, i.e. $\\operatorname{Pr}[r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4}$ $\\operatorname{Pr}[\\text{we compute }x_i \\text{ correctly}]$ $\\ge 1 - \\color{blue} {2\\cdot(\\frac{1}{4})}=1/2$ It is unlikely to compute $x_i$ since it’s the same with random. If the probability of predicting $\\langle r,x\\rangle$ correctly is non-negligibly better than $1/2 + 1/2p(n)$, i.e.$\\operatorname{Pr}[r\\leftarrow {0,1}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{1}{2} + 1/2p(n)$ $\\operatorname{Pr}[\\text{we compute }x_i \\text{ correctly}]$ $\\ge 1 - \\color{blue} {2\\cdot(\\frac{1}{2} -\\frac{1}{2p(n)})}=1/p(n)$ It cannot compute $x_i$. The problem above is that it doubles the original error probability of predicting, i.e. $1-2\\cdot(*)$. When the probability of error is significantly smaller than $1/4$, the “error-doubling” phenomenon raises no problem. However, in general case (and even in special case where the error probability is exactly $1/4$), the procedure is unlikely to compute $x_i$. Hence, what is required is an alternative way of using the predictor $P$, which dose not double the original error probability of $P$. The complete proof is referred in Goldreich Book Part 1, Section 2.5.2. The key idea is pairwise independence. I read the reference. To be honest, I do not comprehend it thoroughly. Happy to exchange the ideas. The Coding-Theoretic View of GL To be honest, I just write it down so I could figure it out in the near future. Happy to exchange the ideas. $x\\rightarrow (\\langle x,r\\rangle )_{r\\in{0,1}^n}$ can be viewed as a highly redundant, exponentially long encoding of $x$ = the Hadamard code. $P(F(x),r)$ can be thought of as providing access to a noisy codeword. What we proved = unique decoding algorithm for Hadamard code with error rate $\\frac{1}{4}-1/p(n)$. The real proof = list-decoding algorithm for Hadamard code with error rate rate $\\frac{1}{2}-1/p(n)$.","link":"/2022/07/20/mit6875-lec7/"},{"title":"「PyTorch」：1-PyTorch Explained","text":"PyTorch框架学习。 本篇文章主要介绍PyTorch的相关背景知识。 PyTorch Explained: Python DNN API【PyTorch，是使用Python来深度学习的一个API。】 PyTorch torch.Tensor objects that are created from NumPy ndarray objects, share memory. This makes the transition between PyTorch and NumPy very cheap from a performance perspective. With PyTorch tensors, GPU support is built-in. It’s very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system. 【PyTorch原生支持GPU】 A Brief HistoryThe initial release of PyTorch was in October of 2016, and before PyTorch was created, there was and still is, another framework called Torch. Torch is a machine learning framework that’s been around for quite a while and is based on the Lua programming language. The connection between PyTorch and this Lua version, called Torch, exists because many of the developers who maintain the Lua version are the individuals who created PyTorch. PyTorch is that it was created and is maintained by Facebook. This is because Soumith Chintala worked at Facebook AI Research when PyTorch was created (still does at the time of this writing). However, there are many other companies with a vested interest in PyTorch. Deep Learning With PyTorchPyTorch的包和主要组成： Package Description torch PyTorch的顶层包和tensor库 torch.nn 包含构建NN的模型和扩展类 torch.autograd PyTorch中支持的Tensor操作 torch.nn.functional 包含构建NN的函数接口，像loss function, activation fucntion, convolution operation torch.optim 包含标准的优化，像SGD, Adam torch.utils 包含实用类，像数据集，数据装载器，方便数据预处理 torchvision 提供著名的数据集，模型架构和计算机视觉图像转换 Why use PyTorch for Deep Learning ? PyTorch is thin and stays out of the way! PyTorch is as close as it gets to the real thing! Investing In PyTorch As A Deep Learning Framework To optimize neural networks, we need to calculate derivatives, and to do this computationally, deep learning frameworks use what are called computational graphs. 计算图。 Computational graphs are used to graph the function operations that occur on tensors inside neural networks. 计算图用来绘制NN中内部张量发生的函数运算。 These graphs are then used to compute the derivatives needed to optimize the neural network. PyTorch uses a computational graph that is called a dynamic computational graph. This means that the graph is generated on the fly as the operations are created. 计算图用来计算偏微分优化NN。PyTorch使用的是动态计算图，图是即时创建的。 CUDA Explained - Why Deep Learning Uses GPUs In this post, we are going to introduce CUDA at a high-level. The goal of this post is to help beginners understand what CUDA is and how it fits in with PyTorch, and more importantly, why we even use GPUs in neural network programming anyway. Graphics Processing Unit(GPU) To understand CUDA, we need to have a working knowledge of graphics processing units (GPUs). A GPU is a processor that is good at handling specialized computations. 【GPU擅长处理某种特定的计算】 This is in contrast to a central processing unit (CPU), which is a processor that is good at handling general computations. 【CPU擅长处理general computations.】 Parallel Computing Parallel computing is a type of computation where by a particular computation is broken into independent smaller computations that can be carried out simultaneously. The resulting computations are then recombined, or synchronized, to form the result of the original larger computation. Neural Networks Are Embarrassingly ParallelIn parallel computing, an embarrassingly parallel task is one where little or no effort is needed to separate the overall task into a set of smaller tasks to be computed in parallel. Tasks that embarrassingly parallel are ones where it’s easy to see that the set of smaller tasks are independent with respect to each other. Nvidia Hardware(GPU) And Software(CUDA) Nvidia is a technology company that designs GPUs, and they have created CUDA as a software platform that pairs with their GPU hardware making it easier for developers to build software that accelerates computations using the parallel processing power of Nvidia GPUs. An Nvidia GPU is the hardware that enables parallel computations, while CUDA is a software layer that provides an API for developers. GPU是能平行运算的硬件，而CUDA是为开发者提供的上层API。 PyTorch Comes With CUDAOne of the benefits of using PyTorch, or any other neural network API is that parallelism comes baked into the API. This means that as neural network programmers, we can focus more on building neural networks and less on performance issues. 关注怎么构建NN，而不是一下performance issues. Now, if we wanted to work on the PyTorch core development team or write PyTorch extensions, it would probably be useful to know how to use CUDA directly. After all, PyTorch is written in all of these: Python C++ CUDA","link":"/2020/10/20/pytorch-introduction/"},{"title":"「PyTorch」：2-Tensors Explained And Operations","text":"PyTorch框架学习。 本篇文章主要介绍PyTorch中的Tensor及其基本操作，主要分为四个方面：Reshape, Element-wise, Reduction和Access。 Tensor的具体操作介绍，建议配合Colab笔记使用： PyTorch Tensors Explained Tensor Operations: Reshape Tensor Operations: Element-wise Tensor Operation: Reduction and Access 英文的表达解释都是比较清晰且精确的，所以以英语的形式作为主要记录，文中会夹带一些中文总结语句，方便阅读。 Introducing TensorsTensor Explained - Data Structures of Deep LearningWhat Is A Tensor?A tensor is the primary data structure used by neural networks. 【Tensor是NN中最主要的数据结构】 Indexes Required To Access An ElementThe relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure. 【以下pairs都是需要同等数量的indexes才能确定特定的元素。】 【而tensor是generalizations，是一种统一而普遍的定义。】 Indexes required Computer science Mathematics 0 number scalar 1 array vector 2 2d-array matrix Tensors Are GeneralizationsWhen more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language. MathematicsIn mathematics, we stop using words like scalar, vector, and matrix, and we start using the word tensor or nd-tensor. The n tells us the number of indexes required to access a specific element within the structure. 【数学中，当我们需要用大于两个的indexes才能确定特点元素时，我们使用tensor或者nd-tensor来表示该数据结构，说明需要n个index才能确定该数据结构中的特定元素。】 Computer ScienceIn computer science, we stop using words like, number, array, 2d-array, and start using the word multidimensional array or nd-array. The n tells us the number of indexes required to access a specific element within the structure. 【计算机科学中，我们使用nd-array来表示，因此，nd-array和tensor实则是一个东西。】 Indexes required Computer science Mathematics n nd-array nd-tensor Tensors and nd-arrays are the same thing! One thing to note about the dimension of a tensor is that it differs from what we mean when we refer to the dimension of a vector in a vector space. The dimension of a tensor does not tell us how many components exist within the tensor. 【需要注意的地方是，tensor中的维度和vector向量空间中的维度不是同一个东西，vector向量空间中的维度表示该vector有多少个元素组成的，而tensor中的维度是下文中rank的含义。】 Rank, Axes, And Shape Explained【下文会详细解释深度学习tensor的几个重要性质：Rank, Axes, Shape.】 The concepts of rank, axes, and shape are the tensor attributes that will concern us most in deep learning. Rank Axes Shape Rank And IndexesWe are introducing the word rank here because it is commonly used in deep learning when referring to the number of dimensions present within a given tensor. The rank of a tensor tells us how many indexes are required to access (refer to) a specific data element contained within the tensor data structure. A tensor’s rank tells us how many indexes are needed to refer to a specific element within the tensor. 【这里的rank实则就是tensor的维度。】 【tensor的rank值告诉我们需要多少个indexes才能确定该tensor中的特定元素。】 Axes Of A TensorIf we have a tensor, and we want to refer to a specific dimension, we use the word axis in deep learning. An axis of a tensor is a specific dimension of a tensor. Elements are said to exist or run along an axis. This running is constrained by the length of each axis. Let’s look at the length of an axis now. Length Of An AxisThe length of each axis tells us how many indexes are available along each axis. 【当我们关注tensor的某一具体维度时，在深度学习中我们使用axis来表达。】 【元素被认为是在某一axie上存在或延伸的，元素延伸的长度取决于axis的长度。】 【Axis的长度表示在每一维度（axis）上有多少个索引】 Shape Of A TensorThe shape of a tensor is determined by the length of each axis, so if we know the shape of a given tensor, then we know the length of each axis, and this tells us how many indexes are available along each axis. The shape of a tensor gives us the length of each axis of the tensor. 【tensor的shape由每一axis的长度决定，即每一axis的索引数目】 Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called reshaping. Reshaping changes the shape but not the underlying data elements. 【tensor的常见操作reshape只改变tensor的shape，而不改变底层的数据。】 CNN Tensors Shape ExplainedCNN的相关介绍，可见 这篇文章 What I want to do now is put the concepts of rank, axes, and shape to use with a practical example. To do this, we’ll consider an image input as a tensor to a CNN. Remember that the shape of a tensor encodes all the relevant information about a tensor’s axes, rank, and indexes, so we’ll consider the shape in our example, and this will enable us to work out the other values. 【tensor的shape能体现tensor的axes、rank、index所有信息】 【以CNN为例来说明rank, axes, shape.】 Shape Of A CNN InputThe shape of a CNN input typically has a length of four. This means that we have a rank-4 tensor with four axes. Each index in the tensor’s shape represents a specific axis, and the value at each index gives us the length of the corresponding axis. 【CNN的input 是一个rank4-tensor.】 Each axis of a tensor usually represents some type of real world or logical feature of the input data. If we understand each of these features and their axis location within the tensor, then we can have a pretty good understanding of the tensor data structure overall. 【tensor的每个axis往往代表着某一个逻辑feature，所以理解features和tensor中axis的位置的关系能帮助我们更好的理解tensor。】 Image Height And WidthTo represent two dimensions, we need two axes. The image height and width are represented on the last two axes. 【表示图像的height和width，需要2个axes，使用最后两个axes表示。】 Image Color ChannelsThe next axis represents the color channels. Typical values here are 3 for RGB images or 1 if we are working with grayscale images. This color channel interpretation only applies to the input tensor. 【下一个axis(从右至左)表示图像的color channels（颜色通道，如灰度图像就有1个颜色通道，RGB图像有三个）。】 【注意：color channel的说法只适用于input tensor。】 Image BatchesThis brings us to the first axis of the four which represents the batch size. In neural networks, we usually work with batches of samples opposed to single samples, so the length of this axis tells us how many samples are in our batch. Suppose we have the following shape [3, 1, 28, 28] for a given tensor. Using the shape, we can determine that we have a batch of three images. 【第一个axis表示batch属性，表明该batch的size。在深度学习中，我们通常使用一批样本，而不是一个单独的样本，所以这一维度表明了我们的batch中有多少样本。】 tensor：[Batch, Channels, Height, Width] Each image has a single color channel, and the image height and width are 28 x 28 respectively. Batch size Color channels Height Width NCHW vs NHWC vs CHWNIt’s common when reading API documentation and academic papers to see the B replaced by an N. The N standing for number of samples in a batch. 【在API文档或学术论文中，N经常会代替代替B，表示the number of samples in a batch。】 Furthermore, another difference we often encounter in the wild is a reordering of the dimensions. Common orderings are as follows: NCHW NHWC CHWN 【除此之外，也会经常遇到这些axes的其他顺序。】 As we have seen, PyTorch uses NCHW, and it is the case that TensorFlow and Keras use NHWC by default (it can be configured). Ultimately, the choice of which one to use depends mainly on performance. Some libraries and algorithms are more suited to one or the other of these orderings. 【PyTorch 默认使用NCHW，而TensorFlow和Keras使用NHWC】 Output Channels And Feature MapsLet’s look at how the interpretation of the color channel axis changes after the tensor is transformed by a convolutional layer. Suppose we have three convolutional filters, and lets just see what happens to the channel axis. Since we have three convolutional filters, we will have three channel outputs from the convolutional layer. These channels are outputs from the convolutional layer, hence the name output channels opposed to color channels. 【tensor送入convolutional layer（卷积层）后，color channel 这一axis的长度发生变化。 【在Post not found: % CNN CNN的介绍文章中解释到，有几个convolutional filters，卷积层输出的tensor就有几个channel（channel代替color channel的表达）。】 Feature MapsWith the output channels, we no longer have color channels, but modified channels that we call feature maps. These so-called feature maps are the outputs of the convolutions that take place using the input color channels and the convolutional filters. Feature maps are the output channels created from the convolutions. 【卷积层输出tensor的channel维度代替color channels的叫法。】 【卷积层的输出也叫叫feature maps】 PyTorch TensorsWhen programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form. 【数据预处理往往是编写NN的第一步，将原始数据转换为tensor form。】 Tensor的基本操作见Colab运行笔记链接：PyTorch Tensors Explained (不会用的也可以直接看github 上的) PyTorch Tensors Attributes torch.dtype：tensor包含数据类型。 常见数据类型： Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point torch.float16 torch.HalfTensor torch.cuda.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 torch.LongTensor torch.cuda.LongTensor torch.device: tensor数据所分配的设备，如CPU，cuda:0 torch.layout: tensor在内存中的存储方式。 As neural network programmers, we need to be aware of the following: Tensors contain data of a uniform type (dtype). Tensor computations between tensors depend on the dtype and the device. 【Tensors包含相同类型的数据】 【Tensors之间的计算取决于他的类型和他所分配的设备】 Creating TensorsThese are the primary ways of creating tensor objects (instances of the torch.Tensor class), with data (array-like) in PyTorch: Creating Tensors with data. 【四种用数据创建tensor的方式】 torch.Tensor(data) torch.tensor(data) torch.as_tensor(data) torch.from_numpy(data) torch.Tensor() Vs torch.tensor()The first option with the uppercase T is the constructor of the torch.Tensor class, and the second option is what we call a factory function that constructs torch.Tensor objects and returns them to the caller. However, the factory function torch.tensor() has better documentation and more configuration options, so it gets the winning spot at the moment. 【torch.Tensor(data) 是 torch.Tensor class的Constructor，而torch.tensor(data) 是生成/返回 torch.Tensor class的函数（factory functions)】 【因为torch.tensor() 有更多的选项设置，比如可以设置数据类型，所以一般用torch.tensor() 来生成。】 Default dtype Vs Inferred dtypeThe difference here arises in the fact that the torch.Tensor() constructor uses the default dtype when building the tensor. The other calls choose a dtype based on the incoming data. This is called type inference. The dtype is inferred based on the incoming data. 【torch.Tensor() 在生成tensor时，使用的是默认dtype=torch.float32 ，而其他三种是使用的引用dtype ，即生成tensor的数据类型和输入的数据类型一致。】 Sharing Memory For Performance: Copy Vs Sharetorch.Tensor() and torch.tensor() copy their input data while torch.as_tensor() and torch.from_numpy() share their input data in memory with the original input object. This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the torch.Tensor and the numpy.ndarray. Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory. 【torch.Tensor() 和 torch.tensor() 在根据data创建tensor时，在内存中额外复制数据】 【torch.as_tensor() 和 torch.from_numpy() 在根据data创建tensor时，是和原输入数据共享的内存，即原numpy.ndarry的数据改变，相应的tensor也会改变。】 Share Data Copy Data torch.as_tensor() torch.tensor() torch.from_numpy() torch.Tensor() Some things to keep in mind about memory sharing (it works where it can): Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used. 【在使用GPU时， as_tensor() 也会将ndarray数据从CPU复制到GPU上。】 The memory sharing of as_tensor() doesn’t work with built-in Python data structures like lists. 【as_tensor() 在Python内置数据结构时不会共享内存】 The as_tensor() performance improvement will be greater if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. 【as_tensor() 在ndarry和tensor之间大量连续操作时能有效提高性能】 torch.as_tensor() Vs torch.from_numpy()This establishes that torch.as_tensor() and torch.from_numpy() both share memory with their input data. However, which one should we use, and how are they different? The torch.from_numpy() function only accepts numpy.ndarrays, while the torch.as_tensor() function accepts a wide variety of array-like objects, including other PyTorch tensors. 【这两个都是和输入数据共享内存，但 torch.from_numpy() 只能接受numpy.ndarrays 类型的数据，而torch.as_tensor() 能接受array-like(像list, tuple)等类型，所以一般torch.as_tensor() 更常用。】 If we have a torch.Tensor and we want to convert it to a numpy.ndarray 【用torch.numpy() 把tensor转换为ndarray】 Creating Tensors without data. 【还有几种创建常见tensor的方式】 torch.eyes(n) : 创建2-D tensor，即n*n的单位向量。 torch.zeros(shape) : 创建shape=shape的全0tensor。 torch.ones(shape) : 创建全1tensor。 torch.rand(shape) : 创建随机值tensor。 Tensor Operation关于Tensor 操作的Colab运行笔记。对照使用最佳。如果打不开也可以看github Tensor Operations: Reshape Tensor Operations: Element-wise Tensor Operation: Reduction and Access We have the following high-level categories of operations: Reshaping operations Element-wise operations Reduction operations Access operations 【对tensor的操作主要分为4种：reshape, element-wise, reduction, access】 ReshapeAs neural network programmers, we have to do the same with our tensors, and usually shaping and reshaping our tensors is a frequent task. 【reshape在NN编程中是很常见的操作】 （具体操作见colab运行笔记本:Tensor Operations: Reshape ） 12345678import torcht = torch.tensor([ [1,1,1,1], [2,2,2,2], [3,3,3,3]], dtype=torch.float32)t.reshape([2,6])t.reshape(2,2,3) Reshaping changes the tensor’s shape but not the underlying data. Our tensor has 12 elements, so any reshaping must account for exactly 12 elements. 【reshape操作不改变底层的数据，只是改变tensor的shape】 In PyTorch, the -1 tells the reshape() function to figure out what the value should be based on the number of elements contained within the tensor. 【reshape中传入的-1参数，PyTorch可以自动计算该值，因为PyTorch要保证tensor的元素个数不变】 Squeezing And Unsqueezing Squeezing a tensor removes the dimensions or axes that have a length of one. 【Squeezing操作：移除tensor中axis长度为1的维度】 Unsqueezing a tensor adds a dimension with a length of one. 【Unsqueezing操作：增加一个axis长度为1的维度】 （具体操作见colab运行笔记本:Tensor Operations: Reshape ） 12t.squeeze()t.squeeze().unsqueeze(dim=0) Concatenation TensorsWe combine tensors using the cat() function, and the resulting tensor will have a shape that depends on the shape of the two input tensors. （具体操作见colab运行笔记本:Tensor Operations: Reshape ） 12torch.cat((t1,t2,t3), dim=0)torch.cat((t1,t2,t3), dim=1) Flatten这里从CNN的例子看Flatten，CNN的相关细节见：这篇文章 A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input. 【flatten在卷积层网络很常见，因为输入必须flatten后才能连接到一个全连接网络层】 对于MNIST数据集中18*18的手写数字，在前文说到CNN的输入是[Batch Size, Channels, Height, Width] ，怎么才能flatten tensor的部分axis，而不是全部维度。 CNN的输入，需要flatten的axes：(C,H,W) 从dim1维度开始flatten（具体操作见colab运行笔记本:Tensor Operations: Reshape ） 1t.flatten(start_dim=1, end_dim=-1) Broadcasting and Element-WiseAn element-wise operation operates on corresponding elements between tensors. 【element-wise操作两个tensor之间对应的元素。】 BroadcastingBroadcasting describes how tensors with different shapes are treated during element-wise operations. Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors. 【broadcast描述了不同shape之间的tensor如何进行element-wise操作】 【broadcast允许我们增加scalars到高维度】 Let’s think about the t1 + 2 operation. Here, the scaler valued tensor is being broadcasted to the shape of t1, and then, the element-wise operation is carried out. 【在t1+2时，scalar 2实际是先被broadcast到和t1相同的shape, 再执行element-wise操作】 We have two tensors with different shapes. The goal of broadcasting is to make the tensors have the same shape so we can perform element-wise operations on them. （具体操作见colab运行笔记本:Tensor Operations: Element-wise ） Broadcasting Details（具体操作见colab运行笔记本:Tensor Operations: Element-wise ） Same Shapes: 直接操作 Same Rank, Different Shape: Determine if tensors are compatible（兼容）. 【两个tensor兼容，才可以对tensor broadcast，再执行element-wise操作】 We compare the shapes of the two tensors, starting at their last dimensions and working backwards. Our goal is to determine whether each dimension between the two tensors’ shapes is compatible. 【从最后一个维度向前判断，每个维度是否兼容】 【判断该维度兼容的条件是满足下面两个条件其一：维度长度相同；或者其中一个为1】 The dimensions are compatible when either: They’re equal to each other. One of them is 1. Determine the shape of the resulting tensor. 【操作的结果是一个新的tensor，结果tensor的每个维度长度是原tensors在该维度的最大值】 Different Ranks: Determine if tensors are compatible.(同上) When we’re in a situation where the ranks of the two tensors aren’t the same, like what we have here, then we simply substitute a one in for the missing dimensions of the lower-ranked tensor. 【对低维度的tensor的缺失维度，用1来代替，比如shape为(1,3) 和 ()，低维度的shape变为(1,1)】 Determine the shape of the resulting tensor. ArgMax and ReductionA reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. 【reduction 操作是能减少tensor元素数量的操作。】 Reshaping operations gave us the ability to position our elements along particular axes. Element-wise operations allow us to perform operations on elements between two tensors, and reduction operations allow us to perform operations on elements within a single tensor. 【Reshape操作让我们能沿着某一axis操纵tensor 中的元素位置；Element-wise操作让我们能对tensors之间对应元素进行操作；Reduction操作能让我们对单个tensor间的元素操作。】 (具体操作见colab笔记本：Tensor Operation: Reduction and Access ) 1234t.sum()t.prod()t.mean()t.std() Reducing Tensors By Axes只需要对这些方法传一个维度对参数。 (具体操作见colab笔记本：Tensor Operation: Reduction and Access ) 12t.sum(dim=0)t.sum(dim=1) ArgmaxArgmax returns the index location of the maximum value inside a tensor. 【Argmax返回最大value的index】 (具体操作见colab笔记本：Tensor Operation: Reduction and Access ) 1t.argmax(dim=0) Aceessing Elements Inside TensorsThe last type of common operation that we need for tensors is the ability to access data from within the tensor. 【Access操作能获得tensor中的数据，即将tensor中的数据拿出来放在Python内置的数据结构中】 (具体操作见colab笔记本：Tensor Operation: Reduction and Access ) 123t.mean().item()t.mean(dim=0).tolist()t.mean(dim=0).numpy() Advanced Indexing And SlicingPyTorch Tensor支持大多数NumPy的index和slicing操作。 坑：https://numpy.org/doc/stable/reference/arrays.indexing.html Reference 挖坑：advanced indexing and slicing: https://numpy.org/doc/stable/reference/arrays.indexing.html","link":"/2020/10/21/pytorch-tensors/"},{"title":"「机器学习-李宏毅」：Recurrent Neural Network（RNN）","text":"这篇文章中首先从RNN能解决什么问题入手，分析了RNN与一般NN的区别。然后着重讲解了基于RNN的LSTM模型，包括LSTM的细节、和一般NN的区别，以及如何训练LSTM模型。具体阐述了在模型（RNN类模型）训练过程中为什么会遇到剧烈抖动问题和如何解决抖动的工程解决方法。 Example applicationSolt filling先从RNN的应用说起，RNN能做什么？ RNN可以做智慧系统： 如下图中，用户告诉订票系统：”I would like to arrive Taipei on November 2nd”. 订票系统能从这句话中得到Destination: Taipei，time of arrival: November 2nd. 这个过程也就是Solt Filling （槽位填充）。 如果用Feedforward network来解决solt filling问题，输入就是单词，输出是每个槽位（slot）的单词，如下图。 上图中，如何将word表示为一个vector？ EncodingHow to represent each word as a vector? 1-of-N encoding最简单的方式是1-of-N encoding方式（独热方式）。 向量维度大小是整个词汇表的大小，每一个维度代表词汇表中的一个单词，如果该维度置1，表示这个维度代表的单词。 Beyond 1-of-N encoding对1-of-N encoding方式改进。 第一种：Dimension for “Other” 在1-of-N的基础上增加一维度——‘other’维度，即当单词不在系统词汇表中，将other维度置1代表该单词。 第二种：Word hashing 即便是增加了”other”维度，编码vector的维度也很大，用word hashing的方式将大幅减少维度。 以apple为例，拆成app, ppl, ple三个部分，如上图所示，vector中表示这三个部分的维度置1。 用这样的word hashing方式，vector的维度只有 $26\\times 26\\times26$ ，大幅减少词向量的维度。 Example通过encoding的方式，单词用vector来表示，用前馈神经网络来解决solt filling问题。 如下图. input:一个单词（encoding为vector） output: input单词中属于该槽位(solts)的概率分布(vector)。 但用普通的前馈神经网络处理solt filling问题会出现下图问题： 上图中，arrive Taipei on November 2nd 和 leave Taipei on November 2nd，将这两句话的每个单词（vector）放入前馈神经网络，得出的dest槽位都应该是Taipei。 但，通过之前的语意，arrive Taipei的Taipei应该是终点，而leave Taipei的Taipei是起点。 因此，在处理这种问题时，我们的神经网络应该需要memory，对该输入的上下文有一定的记忆存储。 Recurrent Neural Network(RNN)Basic structure因此，我们对一般的前馈神经网络加入记忆元件a, a 存储hidden layer的输出，同时a也作为下一次计算的输入部分,下图就是最基础的RNN模型。 举一个例子来说明该过程： Input sequence: $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}2 \\ 2 \\end{bmatrix}$ … RNN模型如下图所示：所有的weight都是1，没有bias; 所有的神经元的activation function 都是线性的。 input : $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$, 记忆元件初值 a1=0 a2=0. 记忆元件也作为输入的一部分，hidden layer的输出为 2 2, 更新记忆元件的值. output: $\\begin{bmatrix}4 \\ 4 \\end{bmatrix}$ , 记忆元件存储值 a1=2 a2=2. input : $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ , 记忆元件存储值 a1=2 a2=2. 记忆元件也作为输入的一部分，hidden layer 的输出为6 6,更新记忆元件的值。 output: $\\begin{bmatrix}12 \\ 12 \\end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6. 这里可以发现，第一次和第二次的输入相同，但是由于有记忆元件的缘故，两次输出不同。 input : $\\begin{bmatrix}2 \\ 2 \\end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6. 记忆元件也作为输入的一部分，hidden layer 的输出为16 16,更新记忆元件的值。 output: $\\begin{bmatrix}32 \\ 32 \\end{bmatrix}$ , 记忆元件存储值 a1=16 a2=16. RNN中，由于有memory，会和一般前馈模型有两个不同的地方：一是输入相同的vector，输出可能是不同的；二是将一个sequence连续放进RNN模型中，如果sequence中改变顺序，输出也大多不同。 用这个RNN模型来解决之前的solt filling问题，就可以解决上下文语意不同影响solt的问题。 将arrive Taipei on November 2nd的每个单词都放入同样的模型中。 因此将RNN展开，如上图，像不同时间点的模型，但其实是不同时间点循环使用同一个模型。 由于左边的前文是arrive，右边的前文是leave，所以存储在memory中的值不同，Taipei作为input的输出（槽位的概率分布）也不同。 Elman Network &amp; Jordan Network上文中只是RNN模型中的一种，即Elman Network，记忆元件存储的是上一个时间点hidden layer的输出。 而Jordan Network模型中,他的记忆元件存储的是上一时间点的output。 （据说，记忆元件中存储output的值会有较好的performance，因为output是有target vector的，因此能具象的体现放进memory的是什么） Bidirectional RNN上文中的RNN模型，记忆元件中存储的都是上文的信息，如果要同时考虑上下文信息，即是bidirectional RNN(双向RNN)。 模型如下图。 双向RNN的好处是看的范围比较广，当计算输出 $y^t$ 时，上下文的内容都有考虑到。 Long Short-term Memory(LSTM)现在最常用的RNN模型是LSTM，Long Short-term Memory，这里的long是相当于上文中的RNN模型，因为上文提到的RNN模型都是short-term,即每一个时间点，都会把memory中的值洗掉，LSTM的long，就是会把memory的值保留的相对于久一些。 LSTM如下图，与一般NN不同的地方是，他有4个inputs,一个outputs。 LSTM主要有四部分组成： Input Gate：输入门，下方箭头是输入，左方箭头是输入信号控制输入门的打开程度，完全打开LSTM才能将输入值完全读入，打开的程度也是NN自己学。 Output Gate：输出门，上方箭头是输出，左方箭头是输入信号控制输出门的打开程度，同理，打开程度也是NN自己学习。 Memory Cell：记忆元件。 Forget Gate：遗忘门，右边的箭头是输入信号控制遗忘门的打开程度，控制将memory cell洗掉的程度。 LSTM更详细的阐述LSTM的内部机制： 注意： $z_o,z_i,z_f$ 是门的signal control,其实就等同于一般NN中neuron的输入z，是scalar。 gate其实就是一个neuron，通常gate neuron 的activation function f取 sigmod,因为值域在0到1之间，即对应门的打开程度。 input/forget/output gate的neuron的activation function是f(sigmod function), input neuron的activation function是g。 input gate控制输入:$g(z)f(z_i)$ input: z $\\rightarrow$ $g(z)$ input gate signal control: $z_i \\rightarrow f(z_i)$ multiply：$g(z)f(z_i)$ forget gate 控制memory：$cf(z_f)$ forget gate signal control: $z_f\\rightarrow f(z_f)$ 如果 $f(z_f)=1$ ,说明memory里的值保留；如果 $f(z_f)=0$ ,说明memory里的值洗掉。 更新当前时间点的memory(输入+旧的memory值) ：$c’=g(z)f(z_i)+cf(z_f)$ output gate 控制输出：$h(c’)f(z_o)$ output: $c’ \\rightarrow h(c’)$ output gare signal control: $z_o \\rightarrow f(z_o)$ multiply: $h(c’)f(z_o)$ LSTM模型（trained）如下图： 输入序列为： $\\begin{bmatrix}3 \\ 1 \\ 0 \\end{bmatrix}$$\\begin{bmatrix}4 \\ 1 \\ 0 \\end{bmatrix}$ $\\begin{bmatrix}2 \\ 0 \\ 0 \\end{bmatrix}$ $\\begin{bmatrix}1 \\ 0 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}3 \\ -1 \\ 0 \\end{bmatrix}$ 该LSTM activation function: g、h都为linear function（即输出等于输入），f为sigmod. 通过该LSTM的输出序列为： 0 0 0 7 0 0 （建议手算一遍） Compared with Original Networkoriginal network如下图： LSTM 的NN即用LSTM替换原来的neuron，这个neuron有四个inputs，相对于original network也有4倍的参数，如下图： 所以原来RNN的neuron换为LSTM，就是下图： 上图中： 这里的 $z^f,z^u,z,z^o$ 都是 $x^t \\begin{bmatrix} \\quad\\end{bmatrix}$ 矩阵运算得到的vector, 因为上图中有多个LSTM，因此 $z^i$ 的第k个元素，就是控制第k个LSTM的input signal control scalar。所以，$z^f,z^u,z,z^o$ 的维度等于下一层neuron/LSTM的个数。 所以这里memory（cell）$c^t$ 也是一个vector，第k个元素是第k个LSTM中cell存储的值。 向量运算和scalar一样，LSTM细节如下图： Extension：“peephole”上小节的LSTM是simplified，将LSTM hidden layer的输出 $h^t$ 和cell中存储的值 $c^t$ 和下一时间点的输入 $x^{t+1}$ 一同作为下一时间点的输入，就是LSTM的扩展版”peephole”。 如下图： Multi-layer LSTM多层的peephole LSTM如下图： （：wtf 我到底看到了什么 不要怕：Keras、PyTorch等套件都有 “LSTM”，“GUR，”SimpleRNN“ 已实现好的layers. Learning训练RNN时，输入与target如下所示： 估测模型的好坏，计算RNN的Loss时，需要看作一个整体，计算每个时间点RNN输出与target的crossentropy的和。 训练也可同样用Backpropagation，但考虑到时间点，有一个进阶版的”Backpropogation through time(BPTT)”[1]。 RNN一般就用BPTT训练。 How to train wellnot easy to trainRNN-based network is not always easy to learn. 但基于RNN的模型往往不太好训练，总是会出现下图中的绿色线情况（即抖动）。 error surface is rougherror surface，即total loss在参数变化时的函数图。 会发现基于RNN的模型的error surface会长下图这个样子：有时很平坦(flat)有时很陡峭(steep) 橙色点出发： 起初处在flat的位置。 随着一次次更新，gradient在变小，learning rate即会变大。 可能稍微不幸，就会出现跨过悬崖，即出现了剧烈震荡的问题。 如果刚好当前处在悬崖低，这时的gradient很大，learning rate也很大，step就会很大，飞出去，极可能出现segment fault(NaN). Thomas Mikolv 用工程师的角度来解决这个问题，即当此时的gradient大于某个阈值(threshold)时，就不要让当前的gradient超过这个阈值（通常取15）。 这样处在悬崖低的橙色点，（Clipping路线），更新就会到绿色的，继续更新。 Why为什么RNN模型会出现抖动的情况呢？ 用下图这个简单例子说明（一般activation function用sigmod,而ReLu的performance一般较差）： 上图中，输入序列是1 0 0 0 …，memory连接下一个时间点的权重是w，可以轻易得到最后一个时间点的输出 $y^{1000}=w^{999}$ 。 上图中，循环输出1000次，如果w变化 $\\Delta w$ ，看输出 $y^{1000}$ 的变化，来直观体现gradient 的变化： 上图中，可以看出： 绿色部分：当w从1变化为1.01时， $y^{1000}$ 的输出变化即大，既有较大的gradient，理应有小的learning rate。 黄色部分：当w从0.99变化为0.01时， $y^{1000}$ 的输出几乎不变化，即有较小的gradient，理应有大大learning rate. 在很小的地方（0.01 到 1.01），他的gradient就变化即大，即抖动的出现。 Reason：RNN，虽然可以看作不同时间点的展开计算，但始终是同一个NN的权重计算（cell连接到下一个时间点的权重），在不同时间中，反复叠乘，因此会出现这种情况。 Helpful Techniques LSTM几乎已经算RNN的一个标准了，为什么LSTM的performance比较好呢。 为什么用LSTM替换为RNN？ :Can deal with gradient vanishing(not gradient explode). 可以解决gradient vanish的问题（gradient vanish problem 具体见 这篇文章2.1.1） 为什么LSTM可以解决gradient vanish问题 ：memory and input are added.（LSTM的的输出与输入和memory有关） : The influence never disappears unless forget gate is closed.（memory的影响可以很持久） GRU[2]（Gated Recurrent Unit）：是只有两个Gate，比LSTM简单，参数更少，不容易overfitting 玄学了叭 More Applications【待更新】 Many to OneMany to ManyBeyond SequenceSeq2SeqAuto-encoder-TextAuto-encoder-SpeechChat-botReference BPTT GRU","link":"/2020/06/11/rnn/"},{"title":"「机器学习-李宏毅」:Semi-supervised Learning","text":"这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？ 再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。 对于Generative Model，文章重点讲述了如何用EM算法来训练模型。 对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。 对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。 对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。 Introduction什么是Semi-supervised learning(半监督学习)？和Supervised learning（监督式学习）的区别在哪？ Supervised learning（监督式学习）： 用来训练的数据集 $R$ 中的数据labeled data，即 ${(x^r,\\hat{y}^r)}_{r=1}^R$ . 比如在图像分类数据集中： $x^r$ 是image，对应的target output $y^r$ 是分类的label。 而Semi-supervised learning（半监督式学习）： 用来的训练的数据集由两部分组成 $\\{(x^r,\\hat{y}^r)\\}_{r=1}^R$ , $\\{x^u\\}_{u=R}^{R+U}$ ，即labeled data和unlabeled data，而且通常情况下，unlabeled data的数量远远高于labeled data的数量，即 $U&gt;&gt;R$ . 对于一般的机器学习，有训练集（labeled）和测试集，测试集是不会出现在训练集中的，这种情况就是inductive learning（归纳推理，即通过已有的labeled的data去推断没有见过的其他的数据的label）。 而Semi-supervised learning 又分为两种，Transductive learning （转导/推论推导）和 Inductive learning（归纳推理） Transductive learing: unlabeled data is the testing data. 即这里用来训练的 $\\{x^u\\}_{u=R}^{R+U}$ 就是来自测试数据集中的数据。（只使用他的feature，而不使用他的label！） Inductive learning: unlabeled data is not the testing data.即用来训练的 $\\{x^u\\}_{u=R}^{R+U}$ 不是来自测试数据集中的数据，是另外的unlabeled data。 这里的使用testing data是指使用testing data的feature，即unlabel而不是使用testing data的label。 为什么会有semi-supervised learning？ Collecting data is easy, but collecting “labelled” data is expensive. 【收集数据很简单，但收集有label的数据很难】 We do semi-supervised learning in our lives 【在生活中，更多的也是半监督式学习，我们能明白少量看到的事物，但看到了更多我们不懂的，即unlabeled data】 Why Semi-supervised learning helps为什么半监督学习能帮助解决一些问题？ 如上图所示，如果只有labeled data，分类所画的boundary可能是一条竖线。 但如果有一些unlabeled data（如灰色的点），分类所画的boundary可能是一条斜线。 The distribution of the unlabeled data tell us something. 半监督式学习之所以有用，是因为这些unlabeled data的分布能告诉我们一些东西。 通常这也伴随着一些假设，所以半监督式学习是否有用往往取决于这些假设是否合理。 Semi-supervised Learning for Generative ModelSupervised Generative Model在这篇文章中，有详细讲述分类问题中的generative model。 给定一个labelled training data $x^r\\in C_1,C_2$ 训练集。 prior probability（先验概率）有 $P(C_i)$ 和 $P(x|C_i)$ ，假设是Gaussian模型，则 $P(x|C_i)$ 由Gaussian模型中的 $\\mu^i,\\Sigma$ 参数决定。 根据已有的labeled data，计算出假设的Gaussian模型的参数（如下图），从而得出prior probability。 即可算出posterior probability $P\\left(C_{1} \\mid x\\right)=\\frac{P\\left(x \\mid C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x \\mid C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x \\mid C_{2}\\right) P\\left(C_{2}\\right)}$ Semi-supervised Generative Model在只有labeled data的图中，算出来的 $\\mu,\\Sigma$ 参数如下图所示： 但如果有unlabeled data（绿色点），会发现分布的模型参数更可能是是下图： The unlabeled data $x^u$ help re-estimate $P(C_1),P(C_2),\\mu^1,\\mu^2,\\Sigma$ . 因此，unlabeled data会影响分布，从而影响prior probability，posterior probability，最终影响 boundary。 EM所以有unlabeled data, 这个Semi-supervised 的算法怎么做呢？ 其实就是EM（Expected-maximization algorithm，期望最大化算法。） Initialization : $\\theta={P(C_1),P(C_2),\\mu^1,\\mu^2,\\Sigma}$ . 初始化Gaussian模型参数，可以随机初始，也可以通过labeled data得出。 虽然这个算法最终会收敛，但是初始化的参数影响收敛结果，就像gradient descent一样。 E：Step 1: compute the posterior probability of unlabeled data $P_\\theta(C_1|x^u)$ (depending on model $\\theta$ ) 根据当前model的参数，计算出unlabeled data的posterior probability $P(C_1|x^u)$ .(以$P(C_1|x^u)$ 为例) M：Step 2: update model. Back to step1 until the algorithm converges enventually. 用E步得到unlabeled data的posterior probability来最大化极大似然函数，更新得到新的模型参数，公式很直觉。(以 $C_1$ 为例) （$N$ ：data 的总数，包括unlabeled data; $N_1$ :label= $C_1$ 的data数） $P(C_1)=\\frac{N_1+\\Sigma_{x^u}P(C_1|x^u)}{N}$ 对比没有unlabeled data之前的式子， $P(C_1)=\\frac{N_1}{N}$ ，除了已有label= $C_1$ ，还多了一部分，即unlabeled data中属于 $C_1$ 的概率和。 $\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}+\\frac{1}{\\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right)} \\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right) x^{u}$ 对比没有unlabeled data的式子 ，$\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}$ ，除了已有的label= $C_1$ ，还多了一部分，即unlabeled data的 $x^u$ 的加权平均（权重为 $P(C_1\\mid x^u)$ ，即属于 $C_1$ 的概率）。 $\\Sigma$ 公式也包括了unlabeled data. 所以这个算法的Step 1就是EM算法的Expected期望部分，根据已有的labeled data得出极大似然函数的估计值； Step 2就是EM算法的Maximum部分，利用unlabeled data（通过已有模型的参数）最大化E步的极大似然函数，更新模型参数。 最后反复迭代Step 1和Step 2，直至收敛。 Why EM[1]挖坑EM详解。 为什么可以用EM算法来解决Semi-supervised? 只有labeled data 极大似然函数 $\\log{L(\\theta)}=\\sum_{x^r}\\log{P_\\theta(x^r,\\hat{y}^r)}$ , 其中 $P_\\theta(x^r,\\hat{y}^r)=P_\\theta(x^r\\mid \\hat{y}^r)P(\\hat{y}^r)$ . 对上式子求导是有closed-form solution的。 有labeled data和unlabeled data 极大似然函数增加了一部分 $\\log L(\\theta)=\\sum_{x^{r}} \\log P_{\\theta}\\left(x^{r}, \\hat{y}^{r}\\right)+\\sum_{x^{u}} \\log P_{\\theta}\\left(x^{u}\\right)$ . 将后部分用全概率展开， $P_{\\theta}\\left(x^{u}\\right)=P_{\\theta}\\left(x^{u} \\mid C_{1}\\right) P\\left(C_{1}\\right)+P_{\\theta}\\left(x^{u} \\mid C_{2}\\right) P\\left(C_{2}\\right)$ . 如果要求后部分，因为是unlabeled data, 所以模型 $\\theta$ 需要得知unlabeled data的label，即 $P(C_1\\mid x^u)$ ,而求这个式子，也需要得到 prior probability $P(x^u\\mid C_1)$ ,但这个式子需要事先得知模型 $\\theta$ ，因此陷入了死循环。 因此这个极大似然函数不是convex（凸），不能直接求解，因此用迭代的EM算法逐步maximum极大似然函数。 Low-density Separation Assumption另一种假设是Low-density Separation的假设，即这个世界是非黑即白的”Black-or-white”。 两种类别之间是low-density，交界处有明显的鸿沟，因此要么是类别1，要么是类别2，没有第三种情况。 Self-training对于Low-density Separation Assumption的假设，使用Self-training的方法。 Given：labeled data set $=\\{(x^r,\\hat{y}^r\\}_{r=1}^R$ ,unlabeled data set $ =\\{x^u\\}_{u=R}^{R+U}$ . Repeat： Train model $f^*$ from labeled data set. $f^*$ is independent to the model) 从labeled data set中训练出一个模型 Apply $f^*$ to the unlabeled data set. Obtain pseudo-label $\\{(x^u,y^u\\}_{u=l}^{R+U}\\}$ 用这个模型 $f^*$ 来预测unlabeled data set， 获得伪label Remove a set of data from unlabeled data set, and add them into the labeled data set. 拿出一些unlabeled data(pseudo-label)，放到labeled data set中，回到步骤1，再训练。 how to choose the data set remains open 如何选择unlabeled data 是自设计的 you can also provide a weight to each data. 训练中可以对unlabeled data(pseudo-label)和labeled data 赋予不同的权重. 注意： Regression模型是不能self-training的，因为unlabeled data和其pseudo-label放在模型中的loss为0，无法再minimize。 Hard LabelV.S. semi-supervised learning for generative model Semi-supervised learning for generative model和Low-density Separation的区别其实是soft label 和hard label的区别。 Generative Model是利用来unlabeled data的 $P(C_1|x^u)$ posterior probability来计算新的prior probability，迭代更新模型。 而low-density是计算出unlabeled data的pseudo-label，选择性扩大labeled data set(即加入部分由pseudo-label的unlabeled data)来迭代训练模型。 因此，如果考虑Neural Network： ($\\theta^*$ 是labeled data计算所得的network parameters) 如下图，unlabeled data $x^u$ 放入模型中预测，得到 $\\begin{bmatrix} 0.7 \\ 0.3\\end{bmatrix}$ . 如果是使用hard label，则 $x^u$ 的target是 $\\begin{bmatrix} 1 \\ 0\\end{bmatrix}$ . 如果是使用soft label，则 $x^u$ 的target是 $\\begin{bmatrix} 0.7 \\ 0.3\\end{bmatrix}$ . 如果是使用soft label，则self-training不会有效，因为新的data对原loss的改变为0，不会增大模型的loss，也就无法再对其minimize. 所以基于Low-density Separation的假设，是非黑即白的，需要使用hard label来self-training。 Entropy-based Regularization在训练模型中，我们需要尽量保证unlabeled data在模型中的分布是low-density separation。 即下图中，unlabeled data得到的pseudo-label的分布应该尽量集中，而不应该太分散。 所以，在训练中，如何评估 $y^u$ 的分布的集中度？ 根据信息学，使用 $y^u$ 的entropy，即 $E\\left(y^{u}\\right)=-\\sum_{m=1}^{5} y_{m}^{u} \\ln \\left(y_{m}^{u}\\right)$ (注：这里的 $y^u_m$ 是变量 $y^u=m$ 的概率) 当 $E(y^u)$ 越小，说明 $y^u$ 分布越集中，如下图。 因此，在self-training中： $L=\\sum_{y^r} C(x^r,\\hat{y}^r)+\\lambda\\sum_{x^u}E(y^u)$ Loss function的前一项（cross entropy）minimize保证分类的正确性，后一项（entropy of $y^u$ ) minimize保证 unlabeled data分布尽量集中，最大可能满足low-density separation的假设。 training：gradient decent. 因为这样的形式很像之前提到过的regularization(具体见这篇文章的3.2)，所以又叫entropy-based regularization. Outlook: Semi-supervised SVMSVM也是解决semi-supervised learning的方法. 上图中，在有unlabeled data的情况下，希望boundary 分的越开越好（largest margin）和有更小的error. 因此枚举unlabeled data所有可能的情况，但枚举在计算量上是巨大的，因此SVM（Support Vector Machines）可以实现枚举的目标，但不需要这么大的枚举量。 Smoothness AssumptionSmoothness Assumption的思想可以用以下话归纳： “You are known by the company you keep” 近朱者赤，近墨者黑。 蓬生麻中，不扶而直。白沙在涅，与之俱黑。 Assumption：“similar” $x$ has the same $\\hat{y}$ . 【意思就是说：相近的 $x$ 有相同的label $\\hat{y}$ .】 More precise assumption： x is not uniform if $x^1$ and $x^2$ are close in a hign density region, $\\hat{y}^1$ and $\\hat{y}^2$ are the same. Smoothness Assumption假设更准确的表述是： x不是均匀分布，如果 $x^1$ 和 $x^2$ 通过一个high density region的区域连在一起，且离得很近，则 $\\hat{y}^1$ 和 $\\hat{y}^2$ 相同。 如下图， $x^1$ 和 $x^2$ 通过high density region连接在一起，有相同的label，而 $x^2$ 和 $x^3$ 有不同的label. Smoothness Assumption通过观察大量unlabeled data，可以得到一些信息。 比如下图中的两张人的左脸和右脸图片，都是unlabeled，但如果给大量的过渡形态（左脸转向右脸）unlabeled data，可以得出这两张图片是相似的结论. Smoothness Assumption还可以用在文章分类中，比如分类天文学和旅游学的文章。 如下图， 文章 d1和d3有overlap word（重叠单词），所以d1和d3是同一类，同理 d4和d2是一类。 如果，下图中，d1和d3没有overlap word，就无法说明d1和d3是同一类。 但是，如果我们收集到足够多但unlabeled data，如下图，通过high density region的连接和传递，也可以得出d1和d3一类，d2和d4一类。 Cluster and then Label在Smoothness Assumption假设下，直观的可以用cluster and then label，先用所有的data训练一个classifier。 直接聚类标记(比较难训练）。 Graph-based Approach另一种方法是利用图的结构（Graph structure）来得知 $x^1$ and $x^2$ are close in a high density region (connected by a high density path). Represent the data points as a graph. 【把这些数据点看作一个图】 建图有些时候是很直观的，比如网页中的超链接，论文中的引用。 但有的时候也需要自己建图。 注意： 如果是影像类，base on pixel，performance就不太好，一般会base on autoencoder，将feature抽象出来，效果更好。 Graph Construction建图过程如下： Define the similarity $s(x^i, x^j)$ between $x^i$ and $x^j$ . 【定义data $x^i$ 和 $x^j$ 的相似度】 Add edge【定义数据点中加边（连通）的条件】 K Nearest Neighbor【和该点最近的k个点相连接】 e-Neighborhood【与离该点距离小于等于e的点相连接】 Edge weight is proportional to $s(x^i, x^j)$ 【边点权重就是步骤1定义的连接两点的相似度】 Gaussian Radial Basis Function： $s\\left(x^{i}, x^{j}\\right)=\\exp \\left(-\\gamma\\left\\|x^{i}-x^{j}\\right\\|^{2}\\right)$ 一般采用如上公式（经验上取得较好的performance）。 因为利用指数化后（指数内是两点的Euclidean distance），函数下降的很快，只有当两点离的很近时，该相似度 $s(x^i,x^j)$ 才大，其他时候都趋近于0. Graph-based Approach图建好后： The labeled data influence their neighbors. Propagate through the graph. 【label data 不仅会影响他们的邻居，还会一直传播下去】 如果data points够多，图建的好，就会像下图这样： 但是，如果data较少，就可能出现下图这种label传不到unlabeled data的情况： Smoothness Definition因为是基于Smoothness Assumption，所以最后训练出的模型应让得到的图尽可能满足smoothness的假设。 注意： 这里的因果关系是，unlabeled data作为NN的输入，得到label $y$ ，该label $y$ 和labeled data的 label $\\hat{y}$ 一起得到的图是尽最大可能满足Smoothness Assumption的。 （而不是建好图，然后unlabeled data的label $y$ 是labeled data原有的 $\\hat{y}$ 直接传播过来的，不然训练NN干嘛） 把unlabeled data作为NN的输入，得到label ，对labeled data和”unlabeled data” 建图。 为了在训练中使得最后的图尽可能满足假设，定义smoothness of the labels on the graph. $S=\\frac{1}{2} \\sum_{i,j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}$ （对于所有的labeled data 和 “unlabeled data”（作为NN输入后，有label）） 按照上式计算，得到的Smoothness如下图所示： Smaller means smoother. 【Smoothness $S$ 越小，表示图越满足这个假设】 计算smoothness $S$ 有一种简便的方法： $S=\\frac{1}{2} \\sum_{i, j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}=y^{T} L y$ (这里的1/2只是为了计算方便) $y$ : (R+U)-dim vector，是所有label data和”unlabeled data” 的label，所以是R+U维。 $y=\\begin{bmatrix}…y^i…y^j…\\end{bmatrix}^T$ $L$ :(R+U) $\\times$ (R+U) matrix，也叫Graph Laplacian（调和矩阵，拉普拉斯矩阵） $L$ 的计算方法：$L=D-W$ 其中 $W$ 矩阵算是图的邻接矩阵（区别是无直接可达边的值是0） $D$ 矩阵是一个对角矩阵，对角元素的值等于 $W$ 矩阵对应行的元素和 矩阵表示如下图所示： （证明据说很枯燥，暂时略[2]) Smoothness Regularization$S=\\frac{1}{2} \\sum_{i, j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}=y^{T} L y$ $S$ 中的 $y$ 其实是和network parameters有关的（unlabeled data的label），所以把 $S$ 也放进损失函数中minimize，以求尽可能满足smoothness assumption. 以满足smoothness assumption的损失函数： $L=\\sum_{x^r} C\\left(y^{r}, \\hat{y}^{r}\\right)+\\lambda S$ 损失函数的前部分使labeled data的输出更贴近其label，后部分 $\\lambda S$ 作为regularization term，使得labeled data和unlabeled data尽可能满足smoothness assumption. 除了让NN的output满足smoothness的假设，还可以让NN的任何一层的输出满足smoothness assumption，或者让某层外接一层embedding layer，使其满足smoothness assumption，如下图： Better RepresentationBetter Presentation的思想就是：去芜存菁，化繁为简。 Find the latent(潜在的) factors behind the observation. The latent factors (usually simpler) are better representation. 【找到所观察事物的潜在特征，即该事物的better representation】 该部分后续见这篇博客。 Reference 挖坑：EM算法详解 挖坑：Graph Laplacian in smoothness. Olivier Chapelle：Semi-Supervised Learning","link":"/2020/07/03/semi-supervised/"},{"title":"「区块链」：Solidity-basic","text":"Solidity的官方教程笔记：basic。 Part 1第2章: 合约从最基本的开始入手: Solidity 的代码都包裹在合约里面. 一份合约就是以太应币应用的基本模块， 所有的变量和函数都属于一份合约, 它是你所有应用的起点. 一份名为 HelloWorld 的空合约如下: 123contract HelloWorld {} 版本指令所有的 Solidity 源码都必须冠以 “version pragma” — 标明 Solidity 编译器的版本. 以避免将来新的编译器可能破坏你的代码。 例如: pragma solidity ^0.4.19; (当前 Solidity 的最新版本是 0.4.19). 要有分号！ 1234pragma solidity ^0.4.19;contract HelloWorld {} 第3章: 状态变量和整数状态变量是被永久地保存在合约中。也就是说它们被写入以太币区块链中. 想象成写入一个数据库。 无符号整数: uintuint 无符号数据类型， 指其值不能是负数，对于有符号的整数存在名为 int 的数据类型。 注: Solidity中， uint 实际上是 uint256代名词， 一个256位的无符号整数。你也可以定义位数少的uints — uint8， uint16， uint32， 等…… 但一般来讲你愿意使用简单的 uint， 除非在某些特殊情况下，这我们后面会讲。 第4章: 数学运算在 Solidity 中，数学运算很直观明了，与其它程序设计语言相同: 加法: x + y 减法: x - y, 乘法: x * y 除法: x / y 取模 / 求余: x % y (例如, 13 % 5 余 3, 因为13除以5，余3) Solidity 还支持 乘方操作\\ (如：x 的 y次方） 如： 5 ** 2 = 25 1uint x = 5 ** 2; // equal to 5^2 = 25 第5章: 结构体1234struct Person { uint age; string name;} 第6章: 数组如果你想建立一个集合，可以用 数组这样的数据类型. Solidity 支持两种数组: 静态 数组和动态 数组: 123456// 固定长度为2的静态数组:uint[2] fixedArray;// 固定长度为5的string类型的静态数组:string[5] stringArray;// 动态数组，长度不固定，可以动态添加元素:uint[] dynamicArray; 记住：状态变量被永久保存在区块链中。所以在你的合约中创建动态数组来保存成结构的数据是非常有意义的。 公共数组你可以定义 public 数组, Solidity 会自动创建 getter 方法. 语法如下: 1Person[] public people; 其它的合约可以从这个数组读取数据（但不能写入数据），所以这在合约中是一个有用的保存公共数据的模式。 第7章: 定义函数在 Solidity 中函数定义的句法如下: 123function eatHamburgers(string _name, uint _amount) {} 注：习惯上函数里的变量都是以(*_)开头 (但不是硬性规定) 以区别全局变量。(整个教程都会沿用这个习惯。) 第8章: 使用结构体和数组现在我们学习创建新的 Person 结构，然后把它加入到名为 people 的数组中. 12345// 创建一个新的Person:Person satoshi = Person(172, &quot;Satoshi&quot;);// 将新创建的satoshi添加进people数组:people.push(satoshi); 你也可以两步并一步，用一行代码更简洁: 1people.push(Person(16, &quot;Vitalik&quot;)); 注：array.push() 在数组的 尾部 加入新元素 ，所以元素在数组中的顺序就是我们添加的顺序， 如: 第9章: 私有 / 公共函数Solidity 定义的函数的属性默认为公共。 这就意味着任何一方 (或其它合约) 都可以调用你合约里的函数。 显然，不是什么时候都需要这样，而且这样的合约易于受到攻击。 所以将自己的函数定义为私有是一个好的编程习惯，只有当你需要外部世界调用它时才将它设置为公共。 如何定义一个私有的函数呢？ 12345uint[] numbers;function _addToArray(uint _number) private { numbers.push(_number);} 这意味着只有我们合约中的其它函数才能够调用这个函数，给 numbers 数组添加新成员。 可以看到，在函数名字后面使用关键字 private 即可。和函数的参数类似，私有函数的名字用(_)起始。 第10章: 函数的更多属性本章中我们将学习函数的返回值和修饰符。 返回值要想函数返回一个数值，按如下定义： 12345string greeting = &quot;What's up dog&quot;;function sayHello() public returns (string) { return greeting;} Solidity 里，函数的定义里可包含返回值的数据类型(如本例中 string)。 函数的修饰符上面的函数实际上没有改变 Solidity 里的状态，即，它没有改变任何值或者写任何东西。 这种情况下我们可以把函数定义为 view, 意味着它只能读取数据不能更改数据: 1function sayHello() public view returns (string) { Solidity 还支持 pure 函数, 表明这个函数甚至都不访问应用里的数据，例如： 123function _multiply(uint a, uint b) private pure returns (uint) { return a * b;} 这个函数甚至都不读取应用里的状态 — 它的返回值完全取决于它的输入参数，在这种情况下我们把函数定义为 pure. 注：可能很难记住何时把函数标记为 pure/view。 幸运的是， Solidity 编辑器会给出提示，提醒你使用这些修饰符。 第11章: Keccak256 和 类型转换如何让 _generateRandomDna 函数返回一个全(半) 随机的 uint? Ethereum 内部有一个散列函数keccak256，它用了SHA3版本。一个散列函数基本上就是把一个字符串转换为一个256位的16进制数字。字符串的一个微小变化会引起散列数据极大变化。 例子: 1234//6e91ec6b618bb462a4a6ee5aa2cb0e9cf30f7a052bb467b0ba58b8748c00d2e5keccak256(&quot;aaaab&quot;);//b1f078126895a1424524de5321b339ab00408010b7cf0e6ed451514981e58aa9keccak256(&quot;aaaac&quot;); 注: 在区块链中安全地产生一个随机数是一个很难的问题， 本例的方法不安全，但是在我们的Zombie DNA算法里不是那么重要，已经很好地满足我们的需要了。* 类型转换有时你需要变换数据类型。例如: 123456uint8 a = 5;uint b = 6;// 将会抛出错误，因为 a * b 返回 uint, 而不是 uint8:uint8 c = a * b;// 我们需要将 b 转换为 uint8:uint8 c = a * uint8(b); 上面, a * b 返回类型是 uint, 但是当我们尝试用 uint8 类型接收时, 就会造成潜在的错误。如果把它的数据类型转换为 uint8, 就可以了，编译器也不会出错。 第13章: 事件事件 是合约和区块链通讯的一种机制。你的前端应用“监听”某些事件，并做出反应。 例子: 123456789// 这里建立事件event IntegersAdded(uint x, uint y, uint result);function add(uint _x, uint _y) public { uint result = _x + _y; //触发事件，通知app IntegersAdded(_x, _y, result); return result;} 你的 app 前端可以监听这个事件。JavaScript 实现如下: 123YourContract.IntegersAdded(function(error, result) { // 干些事} array.push() 返回数组的长度类型是uint - 因为数组的第一个元素的索引是 0， array.push() - 1 将是我们加入的僵尸的索引。 zombies.push() - 1 就是 id，数据类型是 uint。 第14章: Web3.js以太坊有一个 JavaScript 库，名为Web3.js。 在后面的课程里，我们会进一步地教你如何安装一个合约，如何设置Web3.js。 但是现在我们通过一段代码来了解 Web3.js 是如何和我们发布的合约交互的吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 下面是调用合约的方式:var abi = /* abi是由编译器生成的 */var ZombieFactoryContract = web3.eth.contract(abi)var contractAddress = /* 发布之后在以太坊上生成的合约地址 */var ZombieFactory = ZombieFactoryContract.at(contractAddress)// `ZombieFactory` 能访问公共的函数以及事件// 某个监听文本输入的监听器:$(&quot;#ourButton&quot;).click(function(e) { var name = $(&quot;#nameInput&quot;).val() //调用合约的 `createRandomZombie` 函数: ZombieFactory.createRandomZombie(name)})// 监听 `NewZombie` 事件, 并且更新UIvar event = ZombieFactory.NewZombie(function(error, result) { if (error) return generateZombie(result.zombieId, result.name, result.dna)})// 获取 Zombie 的 dna, 更新图像function generateZombie(id, name, dna) { let dnaStr = String(dna) // 如果dna少于16位,在它前面用0补上 while (dnaStr.length &lt; 16) dnaStr = &quot;0&quot; + dnaStr let zombieDetails = { // 前两位数构成头部.我们可能有7种头部, 所以 % 7 // 得到的数在0-6,再加上1,数的范围变成1-7 // 通过这样计算： headChoice: dnaStr.substring(0, 2) % 7 + 1， // 我们得到的图片名称从head1.png 到 head7.png // 接下来的两位数构成眼睛, 眼睛变化就对11取模: eyeChoice: dnaStr.substring(2, 4) % 11 + 1, // 再接下来的两位数构成衣服，衣服变化就对6取模: shirtChoice: dnaStr.substring(4, 6) % 6 + 1, //最后6位控制颜色. 用css选择器: hue-rotate来更新 // 360度: skinColorChoice: parseInt(dnaStr.substring(6, 8) / 100 * 360), eyeColorChoice: parseInt(dnaStr.substring(8, 10) / 100 * 360), clothesColorChoice: parseInt(dnaStr.substring(10, 12) / 100 * 360), zombieName: name, zombieDescription: &quot;A Level 1 CryptoZombie&quot;, } return zombieDetails} 我们的 JavaScript 所做的就是获取由zombieDetails 产生的数据, 并且利用浏览器里的 JavaScript 神奇功能 (我们用 Vue.js)，置换出图像以及使用CSS过滤器。在后面的课程中，你可以看到全部的代码。 Part 2第2章: 映射（Mapping）和地址（Address）如此一来，我们需要引入2个新的数据类型：mapping（映射） 和 address（地址）。 Addresses （地址）以太坊区块链由 account ** (账户)组成，你可以把它想象成银行账户。一个帐户的余额是 **以太 （在以太坊区块链上使用的币种），你可以和其他帐户之间支付和接受以太币，就像你的银行帐户可以电汇资金到其他银行帐户一样。 每个帐户都有一个“地址”，你可以把它想象成银行账号。这是账户唯一的标识符，它看起来长这样： 10x0cE446255506E92DF41614C46F1d6df9Cc969183 现在你只需要了解地址属于特定用户（或智能合约）的。 Mapping（映射）在第1课中，我们看到了 结构体 ** 和 **数组 。 映射 是另一种在 Solidity 中存储有组织数据的方法。 映射是这样定义的： 1234//对于金融应用程序，将用户的余额保存在一个 uint类型的变量中：mapping (address =&gt; uint) public accountBalance;//或者可以用来通过userId 存储/查找的用户名mapping (uint =&gt; string) userIdToName; 映射本质上是存储和查找数据所用的键-值对。 第3章: Msg.sendermsg.sender在 Solidity 中，有一些全局变量可以被所有函数调用。 其中一个就是 msg.sender，它指的是当前调用者（或智能合约）的 address。 注意：在 Solidity 中，功能执行始终需要从外部调用者开始。 一个合约只会在区块链上什么也不做，除非有人调用其中的函数。所以 msg.sender总是存在的。 12345678910111213mapping (address =&gt; uint) favoriteNumber;function setMyNumber(uint _myNumber) public { // 更新我们的 `favoriteNumber` 映射来将 `_myNumber`存储在 `msg.sender`名下 favoriteNumber[msg.sender] = _myNumber; // 存储数据至映射的方法和将数据存储在数组相似}function whatIsMyNumber() public view returns (uint) { // 拿到存储在调用者地址名下的值 // 若调用者还没调用 setMyNumber， 则值为 `0` return favoriteNumber[msg.sender];} 在这个小小的例子中，任何人都可以调用 setMyNumber 在我们的合约中存下一个 uint 并且与他们的地址相绑定。 然后，他们调用 whatIsMyNumber 就会返回他们存储的 uint。 使用 msg.sender 很安全，因为它具有以太坊区块链的安全保障 —— 除非窃取与以太坊地址相关联的私钥，否则是没有办法修改其他人的数据的。 跟在 JavaScript 中一样， 在 Solidity 中你也可以用 ++ 使 uint 递增。 123uint number = 0;number++;// `number` 现在是 `1`了 第4章: Require require使得函数在执行过程中，当不满足某些条件时抛出错误，并停止执行： 12345678function sayHiToVitalik(string _name) public returns (string) { // 比较 _name 是否等于 &quot;Vitalik&quot;. 如果不成立，抛出异常并终止程序 // (敲黑板: Solidity 并不支持原生的字符串比较, 我们只能通过比较 // 两字符串的 keccak256 哈希值来进行判断) require(keccak256(_name) == keccak256(&quot;Vitalik&quot;)); // 如果返回 true, 运行如下语句 return &quot;Hi!&quot;;} 如果你这样调用函数 sayHiToVitalik（“Vitalik”） ,它会返回“Hi！”。而如果调用的时候使用了其他参数，它则会抛出错误并停止执行。 因此，在调用一个函数之前，用 require 验证前置条件是非常有必要的。 第5章: 继承（Inheritance） 当代码过于冗长的时候，最好将代码和逻辑分拆到多个不同的合约中，以便于管理。 有个让 Solidity 的代码易于管理的功能，就是合约 inheritance (继承)： 1234567891011contract Doge { function catchphrase() public returns (string) { return &quot;So Wow CryptoDoge&quot;; }}contract BabyDoge is Doge { function anotherCatchphrase() public returns (string) { return &quot;Such Moon BabyDoge&quot;; }} 由于 BabyDoge 是从 Doge 那里 inherits （继承)过来的。 这意味着当你编译和部署了 BabyDoge，它将可以访问 catchphrase() 和 anotherCatchphrase()和其他我们在 Doge 中定义的其他公共函数。 这可以用于逻辑继承（比如表达子类的时候，Cat 是一种 Animal）。 但也可以简单地将类似的逻辑组合到不同的合约中以组织代码。 第6章: 引入（Import）在 Solidity 中，当你有多个文件并且想把一个文件导入另一个文件时，可以使用 import 语句： 12345import &quot;./someothercontract.sol&quot;;contract newContract is SomeOtherContract {} 这样当我们在合约（contract）目录下有一个名为 someothercontract.sol 的文件（ ./ 就是同一目录的意思），它就会被编译器导入。 第7章: Storage与Memory在 Solidity 中，有两个地方可以存储变量 —— storage 或 memory。 Storage 变量是指永久存储在区块链中的变量。 Memory 变量则是临时的，当外部函数对某合约调用完成时，内存型变量即被移除。 你可以把它想象成存储在你电脑的硬盘或是RAM中数据的关系。 大多数时候你都用不到这些关键字，默认情况下 Solidity 会自动处理它们。 状态变量（在函数之外声明的变量）默认为“存储”形式，并永久写入区块链；而在函数内部声明的变量是“内存”型的，它们函数调用结束后消失。 然而也有一些情况下，你需要手动声明存储类型，主要用于处理函数内的 结构体 和 数组 时： 1234567891011121314151617181920212223242526272829303132contract SandwichFactory { struct Sandwich { string name; string status; } Sandwich[] sandwiches; function eatSandwich(uint _index) public { // Sandwich mySandwich = sandwiches[_index]; // ^ 看上去很直接，不过 Solidity 将会给出警告 // 告诉你应该明确在这里定义 `storage` 或者 `memory`。 // 所以你应该明确定义 `storage`:把他当指针 Sandwich storage mySandwich = sandwiches[_index]; // ...这样 `mySandwich` 是指向 `sandwiches[_index]`的指针 // 在存储里，另外... mySandwich.status = &quot;Eaten!&quot;; // ...这将永久把 `sandwiches[_index]` 变为区块链上的存储 // 如果你只想要一个副本，可以使用`memory`:在内存的临时副本 Sandwich memory anotherSandwich = sandwiches[_index + 1]; // ...这样 `anotherSandwich` 就仅仅是一个内存里的副本了 // 另外 anotherSandwich.status = &quot;Eaten!&quot;; // ...将仅仅修改临时变量，对 `sandwiches[_index + 1]` 没有任何影响 // 不过你可以这样做: sandwiches[_index + 1] = anotherSandwich; // ...如果你想把副本的改动保存回区块链存储 }} 当你不得不使用到这些关键字的时候，Solidity 编译器会发警示提醒你的。 第9章: 更多关于函数可见性错误在于，我们尝试从 ZombieFeeding 中调用 _createZombie 函数，但 _createZombie 却是 ZombieFactory 的 private （私有）函数。这意味着任何继承自 ZombieFactory 的子合约都不能访问它。 internal 和 external除 public 和 private 属性之外，Solidity 还使用了另外两个描述函数可见性的修饰词：internal（内部） 和 external（外部）。 internal 和 private 类似，不过， 如果某个合约继承自其父合约，这个合约即可以访问父合约中定义的“内部”函数。而private只允许合约内部函数访问。 external 与public 类似，只不过这些函数只能在合约之外调用，它们不能被合约内的其他函数调用。稍后我们将讨论什么时候使用 external 和 public。而public允许合约内和合约外的函数调用。 声明函数 internal 或 external 类型的语法，与声明 private 和 public类 型相同： 1234567891011121314151617contract Sandwich { uint private sandwichesEaten = 0; function eat() internal { sandwichesEaten++; }}contract BLT is Sandwich { uint private baconSandwichesEaten = 0; function eatWithBacon() public returns (string) { baconSandwichesEaten++; // 因为eat() 是internal 的，所以我们能在这里调用 eat(); }} 与其他合约的交互如果我们的合约需要和区块链上的其他的合约会话，则需先定义一个 interface (接口)。 先举一个简单的栗子。 假设在区块链上有这么一个合约： 1234567891011contract LuckyNumber { mapping(address =&gt; uint) numbers; function setNum(uint _num) public { numbers[msg.sender] = _num; } function getNum(address _myAddress) public view returns (uint) { return numbers[_myAddress]; }} 这是个很简单的合约，您可以用它存储自己的幸运号码，并将其与您的以太坊地址关联。 这样其他人就可以通过您的地址查找您的幸运号码了。 现在假设我们有一个外部合约，使用 getNum 函数可读取其中的数据。 首先，我们定义 LuckyNumber 合约的 interface ： 123contract NumberInterface { function getNum(address _myAddress) public view returns (uint);} 请注意，这个过程虽然看起来像在定义一个合约，但其实内里不同： 首先，我们只声明了要与之交互的函数 （在本例中为 getNum ），在其中我们没有使用到任何其他的函数或状态变量。 其次，我们并没有使用大括号（{ 和 }）定义函数体，我们单单用分号（;）结束了函数声明。这使它看起来像一个合约框架。 编译器就是靠这些特征认出它是一个接口的。 在我们的 app 代码中使用这个接口，合约就知道其他合约的函数是怎样的，应该如何调用，以及可期待什么类型的返回值。 我们已经为你查看过了 CryptoKitties 的源代码，并且找到了一个名为 getKitty的函数，它返回所有的加密猫的数据，包括它的“基因”（我们的僵尸游戏要用它生成新的僵尸）。 该函数如下所示： 1234567891011121314151617181920212223242526function getKitty(uint256 _id) external view returns ( bool isGestating, bool isReady, uint256 cooldownIndex, uint256 nextActionAt, uint256 siringWithId, uint256 birthTime, uint256 matronId, uint256 sireId, uint256 generation, uint256 genes) { Kitty storage kit = kitties[_id]; // if this variable is 0 then it's not gestating isGestating = (kit.siringWithId != 0); isReady = (kit.cooldownEndBlock &lt;= block.number); cooldownIndex = uint256(kit.cooldownIndex); nextActionAt = uint256(kit.cooldownEndBlock); siringWithId = uint256(kit.siringWithId); birthTime = uint256(kit.birthTime); matronId = uint256(kit.matronId); sireId = uint256(kit.sireId); generation = uint256(kit.generation); genes = kit.genes;} 这个函数看起来跟我们习惯的函数不太一样。 它竟然返回了…一堆不同的值！ 在 Solidity中，可以让一个函数返回多个值。 第11章: 使用接口继续前面 NumberInterface 的例子，我们既然将接口定义为： 123contract NumberInterface { function getNum(address _myAddress) public view returns (uint);} 我们可以在合约中这样使用： 获得合约地址（该合约必须为external或者public） 定义一个变量：指向该合约地址的合约对象（之前定义的与之交互的接口对象） 调用该合约对象中的函数 123456789101112contract MyContract { address NumberInterfaceAddress = 0xab38...; // ^ 这是FavoriteNumber合约在以太坊上的地址 NumberInterface numberContract = NumberInterface(NumberInterfaceAddress); // 现在变量 `numberContract` 指向另一个合约对象 function someFunction() public { // 现在我们可以调用在那个合约中声明的 `getNum`函数: uint num = numberContract.getNum(msg.sender); // ...在这儿使用 `num`变量做些什么 }} 通过这种方式，只要将您合约的可见性设置为public(公共)或external(外部)，它们就可以与以太坊区块链上的任何其他合约进行交互。 第12章: 处理多返回值getKitty 是我们所看到的第一个返回多个值的函数。我们来看看是如何处理的： 123456789101112131415161718function multipleReturns() internal returns(uint a, uint b, uint c) { return (1, 2, 3);}function processMultipleReturns() external { uint a; uint b; uint c; // 这样来做批量赋值: (a, b, c) = multipleReturns();}// 或者如果我们只想返回其中一个变量:function getLastReturnValue() external { uint c; // 可以对其他字段留空: (,,c) = multipleReturns();} 第13章: 奖励: Kitty 基因if 语句if语句的语法在 Solidity 中，与在 JavaScript 中差不多： 123456function eatBLT(string sandwich) public { // 看清楚了，当我们比较字符串的时候，需要比较他们的 keccak256 哈希码 if (keccak256(sandwich) == keccak256(&quot;BLT&quot;)) { eat(); }} 第14章: 放在一起JavaScript 实现我们只用编译和部署 ZombieFeeding，就可以将这个合约部署到以太坊了。我们最终完成的这个合约继承自 ZombieFactory，因此它可以访问自己和父辈合约中的所有 public 方法。 我们来看一个与我们的刚部署的合约进行交互的例子， 这个例子使用了 JavaScript 和 web3.js： 123456789101112131415161718192021222324252627282930var abi = /* abi generated by the compiler */var ZombieFeedingContract = web3.eth.contract(abi)var contractAddress = /* our contract address on Ethereum after deploying */var ZombieFeeding = ZombieFeedingContract.at(contractAddress)// 假设我们有我们的僵尸ID和要攻击的猫咪IDlet zombieId = 1;let kittyId = 1;// 要拿到猫咪的DNA，我们需要调用它的API。这些数据保存在它们的服务器上而不是区块链上。// 如果一切都在区块链上，我们就不用担心它们的服务器挂了，或者它们修改了API，// 或者因为不喜欢我们的僵尸游戏而封杀了我们let apiUrl = &quot;https://api.cryptokitties.co/kitties/&quot; + kittyId$.get(apiUrl, function(data) { let imgUrl = data.image_url // 一些显示图片的代码})// 当用户点击一只猫咪的时候:$(&quot;.kittyImage&quot;).click(function(e) { // 调用我们合约的 `feedOnKitty` 函数 ZombieFeeding.feedOnKitty(zombieId, kittyId)})// 侦听来自我们合约的新僵尸事件好来处理ZombieFactory.NewZombie(function(error, result) { if (error) return // 这个函数用来显示僵尸: generateZombie(result.zombieId, result.name, result.dna)})","link":"/2020/11/03/solidity-basic/"},{"title":"「PyTorch」：4-Neural Network Design","text":"PyTorch框架学习。 这篇文章主要介绍如何用PyTorch设计实现一个NN。 colab笔记： Neural Network Design 1: The Layers Neural Network Design 2: Callable Neural Networks Neural Network Design 3: CNN Forward Method Neural Network Design 4: Pass A Batch of Images 以CNN为例，讲解PyTorch中的layer、weight、 Overview： Build PyTorch CNN - Object Oriented Neural Networks CNN Layers - Deep Neural Network Architecture CNN Weights - Learnable Parameters in Neural Networks Callable Neural Networks - Linear Layers in Depth CNN Forward Method - Deep Learning Implementation Forward Propagation Explained - Pass Image to PyTorch Neural Network Neural Network Batch Processing - Pass Image Batch to PyTorch CNN CNN Output Size Formula - Bonus Neural Network Debugging Session Building Neural Networks With PyTorchFrom a high-level perspective or bird’s eye view of our deep learning project, we prepared our data, and now, we are ready to build our model. 【从高层次看，这一部分主要讲解如何用PyTorch设计model】 Prepare the data Build the model Train the model Analyze the model’s results We’ll do a quick OOP review in this post to cover the details needed for working with PyTorch neural networks, but if you find that you need more, the Python docs have an overview tutorial here. 【OOP的细节】 PyTorch’s torch.nn PackageTo build neural networks in PyTorch, we use the torch.nn package, which is PyTorch’s neural network (nn) library. We typically import the package like so: 1import torch.nn as nn PyTorch’s neural network library contains all of the typical components needed to build neural networks. 【nn库包含所有构建NN的典型组件】 PyTorch’s nn.Module ClassAs we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components: 【NN中的每一layer都由代码（input tensor到output tensor 的转换）和权重（weights）组成，因此可以用OOP的思想来抽象表示。】 A transformation (code) A collection of weights (data) In fact, this is the case with PyTorch. Within the nn package, there is a class called Module, and it is the base class for all of neural network modules which includes layers. 【nn库的Module类是所有NN模型中Layers的父类，即所有networks都要继承nn.Modules类】 PyTorch nn.Modules Have A forward() MethodWhen we pass a tensor to our network as input, the tensor flows forward though each layer transformation until the tensor reaches the output layer. This process of a tensor flowing forward though the network is known as a forward pass. 【forward pass：tensor向前流，直至输出层】 Every PyTorch nn.Module has a forward() method, and so when we are building layers and networks, we must provide an implementation of the forward() method. The forward method is the actual transformation. 【所有layers 和 networks在继承nn.Module时，都要实现forward()接口，这个forward方法就是实际的输入到输出的转换】 PyTorch’s nn.functional PackageWhen we implement the forward() method of our nn.Module subclass, we will typically use functions from the nn.functional package. This package provides us with many neural network operations that we can use for building layers. In fact, many of the nn.Module layer classes use nn.functional functions to perform their operations. 【nn.functional包有很多实用的函数操作，可以帮助我们构建layers。事实上，nn.Module的许多子类就使用了nn.functional的方法来完成他们的操作。】 Building A Neural Network In PyTorchWe now have enough information to provide an outline for building neural networks in PyTorch. The steps are as follows: Short version: Extend the nn.Module base class.【继承nn.Module类】 Define layers as class attributes.【定义layers作为该类的属性】 Implement the forward() method.【实现forward()接口】 More detailed version: Create a neural network class that extends the nn.Module base class. In the class constructor, define the network’s layers as class attributes using pre-built layers from torch.nn. Use the network’s layer attributes as well as operations from the nn.functional API to define the network’s forward pass. Define The Network’s Layers As Class AttributesWe’re building a CNN, so the two types of layers we’ll use are linear layers and convolutional layers. 12345678910111213class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # implement the forward pass return t Inside of our Network class, we have five layers that are defined as attributes. We have two convolutional layers, self.conv1 and self.conv2, and three linear layers, self.fc1, self.fc2, self.out. 【在Network中，有5个layers作为该类的attributes】 We used the abbreviation fc in fc1 and fc2 because linear layers are also called fully connected layers. They also have a third name that we may hear sometimes called dense. So linear, dense, and fully connected are all ways to refer to the same type of layer. PyTorch uses the word linear, hence the nn.Linear class name. We used the name out for the last linear layer because the last layer in the network is the output layer. 【fc是fully connected layers的缩写，全连接层，nn.Linear】 【out是输出层。】 Our CNN LayersEach of our layers extends PyTorch’s neural network Module class. For each layer, there are two primary items encapsulated inside, a forward function definition and a weight tensor. 【每一layer都会继承PyTorch的Module类。对于每一layer，都会封装两个组件：forward函数和权重tensor】 The weight tensor inside each layer contains the weight values that are updated as the network learns during the training process. 【每一层的weitght tensor都包含在NN训练中更新的权重参数。】 PyTorch’s neural network Module class keeps track of the weight tensors inside each layer. The code that does this tracking lives inside the nn.Module class, and since we are extending the neural network module class, we inherit this functionality automatically. 【其中，权重tensor就是在训练NN中会更新的参数，Module类会自动跟踪其每一层的weight tensor】 CNN Layer ParametersParameter Vs ArgumentWe’ll parameters are used in function definitions as place-holders while arguments are the actual values that are passed to the function. The parameters can be thought of as local variables that live inside a function. 【Parameters作为占位符用于函数定义，而Arguments是传递给函数的实际的值。】 Two Types Of ParametersTo better understand the argument values for these parameters, let’s consider two categories or types of parameters that we used when constructing our layers. 【在构建layers时，有两种参数】 Hyperparameters【超参数】 Data dependent hyperparameters【数据依赖超参数】 When we construct a layer, we pass values for each parameter to the layer’s constructor. With our convolutional layers have three parameters and the linear layers have two parameters. 【在构建layer时，我们向layer constructor传递参数。】 Convolutional layers in_channels out_channels kernel_size Linear layers in_features out_features HyperparametersIn general, hyperparameters are parameters whose values are chosen manually and arbitrarily. 【hyperparameters是手动主观确定的参数。】 As neural network programmers, we choose hyperparameter values mainly based on trial and error and increasingly by utilizing values that have proven to work well in the past. For building our CNN layers, these are the parameters we choose manually. 【超参数往往是基于经验trial和误差error确定的.】 【在CNN中，我们需要确定这些参数。】 Parameter Description kernel_size Sets the filter size. The words kernel and filter are interchangeable. out_channels Sets the number of filters. One filter produces one output channel. out_features Sets the size of the output tensor. Data Dependent HyperparametersData dependent hyperparameters are parameters whose values are dependent on data. The first two data dependent hyperparameters that stick out are the in_channels of the first convolutional layer, and the out_features of the output layer. 【依赖于数据的超参数。比如在第一个卷积层的in_channles和输出层的out_features参数的确定都依赖于数据。】 In general, the input to one layer is the output from the previous layer, and so all of the in_channels in the conv layers and in_features in the linear layers depend on the data coming from the previous layer. 【一个layer的输入依赖于前一层的输出。】 When we switch from a conv layer to a linear layer, we have to flatten our tensor. This is why we have 12*4*4. Summary Of Layer Parameters123456self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)self.fc2 = nn.Linear(in_features=120, out_features=60)self.out = nn.Linear(in_features=60, out_features=10) Layer Param name Param value The param value is conv1 in_channels 1 the number of color channels in the input image. conv1 kernel_size 5 a hyperparameter. conv1 out_channels 6 a hyperparameter. conv2 in_channels 6 the number of out_channels in previous layer. conv2 kernel_size 5 a hyperparameter. conv2 out_channels 12 a hyperparameter (higher than previous conv layer). fc1 in_features 12 * 4 * 4 the length of the flattened output from previous layer. fc1 out_features 120 a hyperparameter. fc2 in_features 120 the number of out_features of previous layer. fc2 out_features 60 a hyperparameter (lower than previous linear layer). out in_features 60 the number of out_channels in previous layer. out out_features 10 the number of prediction classes. CNN Weights - Learnable Parameters In Neural NetworksColab: Neural Network Design: The Layers Learnable ParametersLearnable parameters are parameters whose values are learned during the training process. 【Learnable parameters是在训练中可以学习的参数。】 With learnable parameters, we typically start out with a set of arbitrary values, and these values then get updated in an iterative fashion as the network learns. 【从一个主观确定的值开始，在网络学习中迭代更新。】 Where are the learnable parameters? We’ll the learnable parameters are the weights inside our network, and they live inside each layer. 【Learnable parameters存在在网络中的每一层，是在我们网络中的权重参数。】 Getting An Instance The Network Let’s grab an instance of our network class and see this. 1network = Network() After the object is initialized, we can then access our object using the network variable. 【获得一个网络的实例，即会自动运行__init__ 对其初始化。】 How Overriding WorksAll Python classes automatically extend the object class. If we want to provide a custom string representation for our object, we can do it, but we need to introduce another object oriented concept called overriding. 【所有Python的类都会继承oobject class，可以重写该类的字符表达（string representation）。】 We can override Python’s default string representation using the __repr__ function. This name is short for representation. 【重写__repr__ 函数】 12345678910network = Network()print (network)Network( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=192, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=60, bias=True) (out): Linear(in_features=60, out_features=10, bias=True)) What’s In The String Representation?Convolutional LayersFor the convolutional layers, the kernel_size argument is a Python tuple (5,5) even though we only passed the number 5 in the constructor. 【kernel_size的值，确定filter的大小。当传递一个值时，默认为square filter。】 The stride is an additional parameter that we could have set, but we left it out. When the stride is not specified in the layer constructor the layer automatically sets it. 【kernel移动的stride如果没有设置会自动设置。】 Linear LayersFor the linear layers, we have an additional parameter called bias which has a default parameter value of true. It is possible to turn this off by setting it to false. 【linear layers还有一个自动设置为True的bias参数。】 Accessing The Network’s LayersWell, now that we’ve got an instance of our network and we’ve reviewed our layers, let’s see how we can access them in code. 【如何访问NN中的layers：当一般属性访问，每一层都会返回一个字符表达】 1234567891011121314&gt; network.conv1Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))&gt; network.conv2Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))&gt; network.fc1Linear(in_features=192, out_features=120, bias=True)&gt; network.fc2 Linear(in_features=120, out_features=60, bias=True)&gt; network.outLinear(in_features=60, out_features=10, bias=True) Accessing The Layer WeightsNow that we have access to each of our layers, we can access the weights inside each layer. 【访问每一层的权重参数。】 Colab: Neural Network Design: The Layers 1&gt; network.conv1.weight PyTorch Parameter ClassPyTorch has a special class called Parameter. The Parameter class extends the tensor class, and so the weight tensor inside every layer is an instance of this Parameter class. 【PyTorch还有一个特殊的类：Parameter类。这个类继承了tensor类，所以每一层中的weight tensor实则都是Parameter类的实例。】 Weight Tensor ShapeFor the convolutional layers, the weight values live inside the filters, and in code, the filters are actually the weight tensors themselves. 【对卷积层来说，weight是在filter中的，而filter在代码中的体现就是weight tensor。】 The convolution operation inside a layer is an operation between the input channels to the layer and the filter inside the layer. This means that what we really have is an operation between two tensors. 【卷积操作实则就是input tensor 和filter 的weight tensor之间的操作。】 For the first conv layer, we have 1 color channel that should be convolved by 6 filters of size 5x5 to produce 6 output channels. This is how we interpret the values inside our layer constructor. 【对于第一个卷积层来说，有6个 5 * 5的filer，会生成6个输出channel。】 12&gt; network.conv1Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) Inside our layer though, we don’t explicitly have 6 weight tensors for each of the 6 filters. We actually represent all 6 filters using a single weight tensor whose shape reflects or accounts for the 6 filters. 【但我们不会使用6个weight tensor来表示该卷积层的6个filters，而是使用一个单独的weight tensor来表示该层的所有filers。】 12&gt; network.conv1.weight.shapetorch.Size([6, 1, 5, 5]) The second axis has a length of 1 which accounts for the single input channel, and the last two axes account for the height and width of the filter. 【weight tensor的第一维度表示filters的数量，该tensor把所有filters都打包在一起。第二维度认为是filter的depth，和输入tensor的channel相同，最后两维为height 和width】 The two main takeaways about these convolutional layers is that our filters are represented using a single tensor and that each filter inside the tensor also has a depth that accounts for the input channels that are being convolved. 【卷积层的两个要点（takeways）】 All filters are represented using a single tensor.【用一个tensor表示该层的所有filters】 Filters have depth that accounts for the input channels.【其中每一个filter有depth，值等于输入的channels】 卷积层Weight Tensor的shape：(Number of filters, Depth, Height, Width) Weight MatrixWith linear layers or fully connected layers, we have flattened rank-1 tensors as input and as output. The way we transform the in_features to the out_features in a linear layer is by using a rank-2 tensor that is commonly called a weight matrix. 【对于全连接层，我们需要拉直（flatten）输入/输出为rank-1的tensor】 【这种在全连接层中in_features到out_features的转换，使用weight matrix来实现，所以该层的参数就是一个rank-2的tensor】 Linear Function Represented Using A MatrixThe important thing about matrix multiplications like this is that they represent linear functions that we can use to build up our neural network. Specifically, the weight matrix is a linear function also called a linear map that maps a vector space of 4 dimensions to a vector space of 3 dimensions. 【矩阵乘法实则是线性函数。具体来说，矩阵乘法也称为一个线性映射，将一个4D vector映射为一个3D vector。】 Accessing The Networks Parameters【访问NN的所有参数】 The first example is the most common way, and we’ll use this to iterate over our weights when we update them during the training process. 【遍历network.parameters() 】 12345678910111213for param in network.parameters(): print(param.shape)torch.Size([6, 1, 5, 5])torch.Size([6])torch.Size([12, 6, 5, 5])torch.Size([12])torch.Size([120, 192])torch.Size([120])torch.Size([60, 120])torch.Size([60])torch.Size([10, 60])torch.Size([10]) The second way is just to show how we can see the name as well. This reveals something that we won’t cover in detail, the bias is also a learnable parameter. Each layer has a bias by default, so for each layer we have a weight tensor and a bias tensor. 【每一层中的bias也是一个可学习的参数。】 【遍历network.named_parameters() 】 12345678910111213for name, param in network.named_parameters(): print(name, '\\t\\t', param.shape)conv1.weight torch.Size([6, 1, 5, 5])conv1.bias torch.Size([6])conv2.weight torch.Size([12, 6, 5, 5])conv2.bias torch.Size([12])fc1.weight torch.Size([120, 192])fc1.bias torch.Size([120])fc2.weight torch.Size([60, 120])fc2.bias torch.Size([60])out.weight torch.Size([10, 60])out.bias torch.Size([10]) Callable Neural Networks - Linear Layers In DepthColab: Neural Network Design 2: Callable Neural Networks In this one, we’ll learn about how PyTorch neural network modules are callable, what this means, and how it informs us about how our network and layer forward methods are called. 【在这一节中，我们能知道在network和layer中forward方法是如何调用的？】 How Linear Layers WorkTransform Using A Matrix【使用矩阵乘法来转换】 12345678910in_features = torch.tensor([1,2,3,4], dtype=torch.float32)weight_matrix = torch.tensor([ [1,2,3,4], [2,3,4,5], [3,4,5,6]], dtype=torch.float32)&gt; weight_matrix.matmul(in_features)tensor([30., 40., 50.]) Transform Using A PyTorch Linear Layer【使用PyTorch Linear Layer来转换。】 1fc = nn.Linear(in_features=4, out_features=3, bias=False) 【根据源码，在LinearLayer中会有一个3 * 4 的weight matrix】 123456789101112# torch/nn/modules/linear.py (version 1.0.1)def __init__(self, in_features, out_features, bias=True): super(Linear, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = Parameter(torch.Tensor(out_features, in_features)) if bias: self.bias = Parameter(torch.Tensor(out_features)) else: self.register_parameter('bias', None) self.reset_parameters() Let’s see how we can call our layer now by passing the in_features tensor. 【直接传tensor来调用该layer】 12&gt; fc(in_features)tensor([-0.8877, 1.4250, 0.8370], grad_fn=&lt;SqueezeBackward3&gt;) We can call the object instance like this because PyTorch neural network modules are callable Python objects. 【因为PyTorch中的module是可以调用的类，即类中有__call__ 方法。 Let’s explicitly set the weight matrix of the linear layer to be the same as the one we used in our other example. 【可以单独设置linear layer中weight matrix的值】 1fc.weight = nn.Parameter(weight_matrix) Callable Layers And Neural Networks【可调用的Layers和NN】 We pointed out before how it was kind of strange that we called the layer object instance as if it were a function. 【为什么可以将实例作为函数调用？】 12&gt; fc(in_features)tensor([30.0261, 40.1404, 49.7643], grad_fn=&lt;AddBackward0&gt;) What makes this possible is that PyTorch module classes implement another special Python function called __call__(). If a class implements the __call__() method, the special call method will be invoked anytime the object instance is called. 【如果该类实现了__call()__ 方法，那么该类的实例就可以作为函数调用】 This fact is an important PyTorch concept because of the way the __call__() method interacts with the forward() method for our layers and networks. 【而PyTorch中该类的__call__ 方法是和forward() 方法交互的】 123456789101112131415161718192021222324252627def __call__(self, *input, **kwargs): for hook in self._forward_pre_hooks.values(): hook(self, input) if torch._C._get_tracing_state(): result = self._slow_forward(*input, **kwargs) else: result = self.forward(*input, **kwargs) for hook in self._forward_hooks.values(): hook_result = hook(self, input, result) if hook_result is not None: raise RuntimeError( &quot;forward hooks should never return any values, but '{}'&quot; &quot;didn't return None&quot;.format(hook)) if len(self._backward_hooks) &gt; 0: var = result while not isinstance(var, torch.Tensor): if isinstance(var, dict): var = next((v for v in var.values() if isinstance(v, torch.Tensor))) else: var = var[0] grad_fn = var.grad_fn if grad_fn is not None: for hook in self._backward_hooks.values(): wrapper = functools.partial(hook, self) functools.update_wrapper(wrapper, hook) grad_fn.register_hook(wrapper) return result The extra code that PyTorch runs inside the __call__() method is why we never invoke the forward() method directly. If we did, the additional PyTorch code would not be executed. As a result, any time we want to invoke our forward() method, we call the object instance. This applies to both layers, and networks because they are both PyTorch neural network modules. 【因为有__call__() ，所以不需要直接调用forward() 方法。所以，如果任何时候我们想要调用forward() 方法时，我们都调用对象实例。】 CNN Forward Method - PyTorch Deep Learning ImplementationColab: CNN Forward Method We created our network by extending the nn.Module PyTorch base class, and then, in the class constructor, we defined the network’s layers as class attributes. Now, we need to implement our network’s forward() method, and then, finally, we’ll be ready to train our model. 【前面通过继承nn.Module 来构建model，在model的构造器中，定义网络的layer作为model的属性。而构建model的最后一步是实现model中的forward() 方法】 【步骤】 Prepare the data Build the model Create a neural network class that extends the nn.Module base class. In the class constructor, define the network’s layers as class attributes. Use the network’s layer attributes as well nn.functional API operations to define the network’s forward pass. 【用网络的layer属性和nn.functional 库的激活函数等来定义网络的前向传播】 Train the model Analyze the model’s results Implementing The forward() MethodColab: Neural Network Design 3: CNN Forward Method 【实现forward()方法】 Input Layer #1The input layer of any neural network is determined by the input data. 【input layer 依赖于输入的数据】 For this reason, we can think of the input layer as the identity transformation. Mathematically, this is the function, $f(x)=x$ . 【可以把input layer看作identification transformation】 Hidden Convolutional Layers: Layers #2 And #3123456789# (2) hidden conv layert = self.conv1(t)t = F.relu(t)t = F.max_pool2d(t, kernel_size=2, stride=2)# (3) hidden conv layert = self.conv2(t)t = F.relu(t)t = F.max_pool2d(t, kernel_size=2, stride=2) Each of these layers is comprised of a collection of weights (data) and a collection operations (code). The weights are encapsulated inside the nn.Conv2d() class instance. The relu() and the max_pool2d() calls are just pure operations. 【每一层都是weights和operations的组合，weights封装在nn.Conv2d实例中，而relu()和max_pool2d()都是单纯的operations】 For example, we’ll say that the second layer in our network is a convolutional layer that contains a collection of weights, and preforms three operations, a convolution operation, the relu activation operation, and the max pooling operation. 【只是其中的一种表示：认为卷积层有一组weights（layer中包含的weights），三组操作：卷积操作、relu操作和max pooling 操作。】 Mathematically, the entire network is just a composition of functions, and a composition of functions is a function itself. So a network is just a function. All the terms like layers, activation functions, and weights, are just used to help describe the different parts. 【整个网络，其实就是functions的组合。因此，network本身就是一个function。layers, activation functions, weights只是来帮助描述这个function】 Hidden Linear Layers: Layers #4 And #5Before we pass our input to the first hidden linear layer, we must reshape() or flatten our tensor. This will be the case any time we are passing output from a convolutional layer as input to a linear layer. 【在将卷积层的输出传递给全连接层之前，需要将他flatten】 12345678# (4) hidden linear layert = t.reshape(-1, 12 * 4 * 4)t = self.fc1(t)t = F.relu(t)# (5) hidden linear layert = self.fc2(t)t = F.relu(t) Output Layer #6The sixth and last layer of our network is a linear layer we call the output layer. When we pass our tensor to the output layer, the result will be the prediction tensor. 【第六层是输出层，该层的输出是一个有10个元素的tensor】 Inside the network we usually use relu() as our non-linear activation function, but for the output layer, whenever we have a single category that we are trying to predict, we use softmax(). The softmax function returns a positive probability for each of the prediction classes, and the probabilities sum to 1. 【前面的层我们都是用relu()来作为非线性函数，但输出层需要得到每一类的预测值，因此使用softmax()】 【softmax能返回每一类的预测概率，所有类的概率和为1】 1234567891011121314151617181920212223242526272829def forward(self, t): # (1) input layer t = t # (2) hidden conv layer t = self.conv1(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2) # (3) hidden conv layer t = self.conv2(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2) # (4) hidden linear layer t = t.reshape(-1, 12 * 4 * 4) t = self.fc1(t) t = F.relu(t) # (5) hidden linear layer t = self.fc2(t) t = F.relu(t) # (6) output layer t = self.out(t) #t = F.softmax(t, dim=1) return t Forward Propagation ExplainedForward Propagation ExplainedForward propagation is the process of transforming an input tensor to an output tensor. 【前馈传播是将输入tensor转换为输出tensor的过程。】 Predicting With The Network: Forward PassBefore we being, we are going to turn off PyTorch’s gradient calculation feature. This will stop PyTorch from automatically building a computation graph as our tensor flows through the network. 【在开始之前，我们需要关闭PyTorch的gradient计算。当tensor流过网络图时，这会阻止PyTorch自动构建计算图。】 The computation graph keeps track of the network’s mapping by tracking each computation that happens. The graph is used during the training process to calculate the derivative (gradient) of the loss function with respect to the network’s weights. 【计算图通过跟踪计算来跟踪网络图，该计算图在训练过程中用于计算损失函数对权重参数的梯度。】 Since we are not training the network yet, we aren’t planning on updating the weights, and so we don’t require gradient calculations. We will turn this back on when training begins. 【因为我们还没有训练网络，所以我们并不打算更新参数，也就不需要梯度计算。】 Passing A Single Image To The NetworkLet’s continue by creating an instance of our Network class: 【创建NN实例】 1&gt; network = Network() Next, we’ll procure a single sample from our training set, unpack the image and the label, and verify the image’s shape: 【从training set中生成一个单独的例子。】 1234&gt; sample = next(iter(train_set)) &gt; image, label = sample &gt; image.shape torch.Size([1, 28, 28]) Now, there’s a second step we must preform before simply passing this tensor to our network. When we pass a tensor to our network, the network is expecting a batch, so even if we want to pass a single image, we still need a batch. 【第二步，网络期望传递的tensor是一批，因此需要将单独的例子也打包。】 12345678910111213&gt; pred = network(image.unsqueeze(0)) # image shape needs to be (batch_size × in_channels × H × W)&gt; predtensor([[0.0991, 0.0916, 0.0907, 0.0949, 0.1013, 0.0922, 0.0990, 0.1130, 0.1107, 0.1074]])&gt; pred.shapetorch.Size([1, 10])&gt; label9&gt; pred.argmax(dim=1)tensor([7]) For each input in the batch, and for each prediction class, we have a prediction value. If we wanted these values to be probabilities, we could just the softmax() function from the nn.functional package. 【用F.softmax()将预测值转换为概率。】 12345&gt; F.softmax(pred, dim=1)tensor([[0.1096, 0.1018, 0.0867, 0.0936, 0.1102, 0.0929, 0.1083, 0.0998, 0.0943, 0.1030]])&gt; F.softmax(pred, dim=1).sum()tensor(1.) Neural Network Batch Processing With PyTorch Prepare the data Build the model Understand how batches are passed to the network Train the model Analyze the model’s results Colab: Pass A Batch of Images Using Argmax: Prediction Vs LabelColab: Pass A Batch of Images CNN Output Size FormulaCNN Output Size FormulaCNN Output Size Formula (Square) Suppose we have an $n\\times n$ input.【输入尺寸】 Suppose we have an $f\\times f$ filter.【filter尺寸】 Suppose we have a padding of $p$ and a stride of $s$ .【padding和stride】 The output size $O$ is given by this formula: $O = \\frac{n-f+2p}{s}+1$ 【输出】 CNN Output Size Formula (Non-Square) Suppose we have an $n_h×n_w$ input. Suppose we have an $f_h×f_w$ filter. Suppose we have a padding of $p$ and a stride of $s$. The height of the output size $O_h$ is given by this formula:$O_h = \\frac{n_h-f_h+2p}{s}+1$ The width of the output size $O_w$ is given by this formula: $O_w = \\frac{n_w-f_w+2p}{s}+1$","link":"/2021/02/28/pytorch-nn-design/"},{"title":"「算法导论」:排序-总结","text":"本篇文章review了算法中的排序算法，包括冒泡排序、插入排序、归并排序、堆排序（以及用堆实现优先队列）、快速排序和计数排序。 分别从算法思路、算法伪代码实现、算法流程、算法时间复杂度四个方面阐述每个算法。 排序排序问题： 输入：一个 $n$个数的序列 $&lt;a_1,a_1,…,a_n&gt;$ 输出：输入序列的一个重拍 $&lt;a_1’,a_2’,…,a_n’&gt;$ ，使得 $a_1’\\leq a_2’ \\leq…\\leq a_n’$ . 在实际中，待排序的数很少是单独的数值，它们通常是一组数据，称为记录(record)。每个记录中包含一个关键字(key)，这就是需要排序的值。记录剩下的数据部分称为卫星数据(satellite data)，通常和关键字一同存取。 原址排序：输入数组中仅有常数个元素需要在排序过程中存储在数组之外。 典型的原址排序有：插入排序、堆排序、快速排序。 符号说明： $\\Theta$ 记号： $\\Theta$ 记号渐进给出一个函数的上界和下界。 $\\Theta(g(n))=\\left\\{f(n): \\text { there exist positive constants } c_{1}, c_{2}, \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq c_{1} g(n) \\leq f(n) \\leq c_{2} g(n) \\text { for all } n \\geq n_{0}\\right\\}$ $g(n)$ 称为 $f(n)$ 的一个渐进紧确界(asymptotically tight bound) $O$ 记号 $O$ 记号只给出了函数的渐进上界。 $O(g(n))=\\left\\{f(n): \\text { there exist positive constants } c \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq f(n) \\leq c g(n) \\text { for all } n \\geq n_{0}\\right\\}$ $\\Omega$ 记号 $\\Omega$ 记号给出了函数的渐进下界。 $\\Omega(g(n))=\\left\\{f(n): \\text { there exist positive constants } c \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq c g(n) \\leq f(n) \\text { for all } n \\geq n_{0}\\right\\}$ 符号比较如下图： 排序算法运行时间一览 ： 算法 最坏情况运行时间 平均情况/期望运行时间 插入排序 $\\Theta (n^2)$ $\\Theta(n^2)$ 归并排序 $\\Theta(n\\lg{n})$ $\\Theta(n\\lg{n})$ 堆排序 $O(n\\lg{n})$ - 快速排序 $\\Theta(n^2)$ $\\Theta(n\\lg{n})$ (expected) 计数排序 $\\Theta(k+n)$ $\\Theta(k+n)$ 基数排序 $\\Theta(d(n+k))$ $\\Theta(d(n+k))$ 桶排序 $\\Theta(n^2)$ $\\Theta(n)$ (average-case) 冒泡排序反复交互相邻未按次序排列的元素。 BUBBLESORT(A) 参数：A待排序数组 1234for i = 1 to A.lengh-1 for j = A.length downto i+1 //每次迭代找出A[i..j]中最小的元素放在A[i]位置 if A[j] &lt; A[j-1] exchange A[j] with A[j-1] 冒泡排序是原址排序，流行但低效。 插入排序如下图所示，插入排序就像打牌时排序一手扑克牌。 开始时，我们的左手为空，桌子上的牌面向下。 然后，我们每次从桌子上拿走一张牌，想把它放在左手中的正确位置。 为了找到这张牌的正确位置，我们从右到左将这张牌和左手里的牌依次比较，放入正确的位置。 左手都是已排序好的牌。 INSERTION-SORT(A) A：待排序数组 12345678for j = 2 to A.length key = A[j] //将key插入到已排序好的A[1..j-1] i = j - 1 //pointer for sorted sequence A[1..j-1] while i &gt; 0 and A[i] &gt; key A[i+1] = A[i] i-- A[i+1] = key 插入排序是原址排序，对于少量元素是一个有效的算法。 最坏情况的运行时间： $\\Theta(n^2)$ . 归并排序分治分治： 将原问题分解为几个规模较小但类似于原问题的子问题，递归地求解这些子问题，然后再合并这些子问题的解来建立原问题的解。 分治的步骤： 分解：分解原问题为若干规模较小的子问题。 解决：递归地求解各子问题，规模较小，可直接求解。 合并：合并这些子问题的解成原问题的解。 算法核心归并排序中的分治 ： 分解：分解待排序的n个元素序列成各n/2个元素序列的两个子序列。 解决：使用归并排序递归地排序两个子序列，当序列长度为1时，递归到达尽头。 合并：合并两个已经排序好的子序列以产生排序好的原序列。 核心 ：合并两个已经排序好的子序列——MERGE(A, p, q, r) A: 待排序原数组。 p, q, r: 数组下标，满足 $p\\leq q&lt;r$ 。 假设子数组 A[p..q] 和A[q+1..r]都已经排好序，合并这两个数组代替原来的A[p..r]子数组。 MERGE算法理解： 牌桌上有两堆牌面朝上，每堆都已排好序，最小的牌在顶上。希望将两堆牌合并成排序好的输出牌堆，且牌面朝下。 比较两堆牌顶顶牌，选取较小的那张，牌面朝下的放在输出牌堆。 重复步骤2直至某一牌堆为空。 将剩下的另一堆牌面朝下放在输出堆。 MERGE合并的过程如下图所示： MERGE算法分析： 在上述过程中，n个元素，我们最多执行n个步骤，所以MERGE合并需要 $\\Theta(n)$ 的时间。 伪代码MERGE(A, p, q, r) 功能：合并已排序好的子数组A[p..q]和A[q+1..r] 参数：A为待排序数组，p, q, r为数组下标，且满足 $p\\leq q&lt;r$ 12345678910Let S[p..r] be new arraysk = p //pointer for S[]i = p, j = q+1 //pointer for subarraywhile k &lt;= r while ( i &lt;= q and j &lt;= r and A[i] &lt;= A[j] ) or j &gt; r // 取A[p..q]牌堆 S[k++] = A[i++] while ( i &lt;= q and j &lt;= r and A[i] &gt;= A[j] ) or i &gt; q //取A[q+1..r]牌堆 A[p..r] = S[p..r] MERGE-SORT(A, p, r) 功能：排序子数组A[p..r] 12345if p &lt; r q = (p+r)/2 MERGE-SORT(A, p, q) MERGE-SORT(A, q+1, r) MERGE(A, p, q, r) 算法分析分治算法运行时间分析分治算法运行时间递归式来自三个部分。 假设 $T(n)$ 是规模为 $n$ 的一个问题的运行时间。若规模问题足够小，则直接求解需要常量时间，将其写作 $\\Theta(1)$ 。 假设把原问题分解成 $a$ 个子问题，每个子问题的规模是原问题的 $1/b$ (在归并排序中， $a$ 和 $b$ 都为2，但很多分治算法中 $a\\neq b$ )。为了求解一个规模为 $n/b$ 规模的子问题，需要 $T(n/b)$ 的时间，所以需要 $aT(n/b)$ 的时间求解 $a$ 个子问题。 如果分解子问题需要 $D(n)$ 时间，合并子问题需要 $C(n)$ 时间。 递归式： $$ T(n)=\\left\\{\\begin{array}{ll}\\Theta(1) & \\text { if } n \\leq c \\\\ a T(n / b)+D(n)+C(n) & \\text { otherwise }\\end{array}\\right. $$ 归并排序分析前文分析了MERGE(A, p, q, r) 合并两个子数组的时间复杂度是 $\\Theta(n)$ ，即 $C(n)=\\Theta(n)$ ，且 $D(n)=\\Theta(n)$ . 归并排序的最坏情况运行时间 $T(n)$ : $$ T(n)=\\left\\{\\begin{array}{ll}\\Theta(1) & \\text { if } n=1 \\\\ 2 T(n / 2)+\\Theta(n) & \\text { if } n>1\\end{array}\\right. $$ 用递归树的思想求解递归式： 即递归树每层的代价为 $\\Theta(n)=cn$ ，共有 $\\lg{n}+1$ 层，所以归并排序的运行时间结果是 $\\Theta(n\\lg{n})$ . 堆排序堆自由树 ：连通的、无环的无向图。 有根数 ：是一棵自由树，其顶点存在一个与其他顶点不同的顶点，称为树的根。 度 ：有根树中一个结点 $x$ 孩子的数目称为 $x$ 的度。 深度 :从根 $r$ 到结点 $x$ 的简单路径。 二叉树 ：不包括任何结点，或者包括三个不相交的结点集合：一个根结点，一棵称为左子树的二叉树和一棵称为右子树的二叉树。 完全k叉树 ：所有叶结点深度相同，且所有内部结点度为k的k叉树。 （二叉）堆 ：是一个数组，它可以被看成一个近似的完全二叉树，树上的每个结点对应数组中的一个元素。除了最底层外，该树被完全填满，并且是从左到右填充。如下图所示。 堆的数组$A$ 有两个属性： $A.length$ ：数组元素的个数，A[1..A.length]中都存有值。 $A.heap-size$ ：有多少个堆元素在数组，A[1..heap-size]中存放的是堆的有效元素。 （$0\\leq A.heap-size\\leq A.lengh$ ) 堆的性质： $A[1]$ :存放的是树的根结点。 对于给定的一个结点 $i$ ，很容易计算他的父结点、左孩子和右孩子的下标。 PARENT(i) 1return i/2 //i&gt;&gt;&gt;1 LEFT(i) 1return 2*i //i&lt;&lt;&lt;1 RIGHT(i) 1return 2*i+1 //i&lt;&lt;&lt;1 | 1 包含$n$ 个元素的堆的高度为 $\\Theta(\\lg{n})$ 堆结构上的基本操作的运行时间至多和堆的高度成正比，即时间复杂度为 $O(\\lg{n})$ . 叶子结点：n/2+1 , n/2+2 , … , n 堆的分类： 最大堆： 除了根以外的结点 $i$ 都满足 $A[\\text{PARENT}(i)]\\geq A[i]$ . 某个结点最多和其父结点一样大。 堆的最大元素存放在根结点中。 最小堆： 除了根以外的结点 $i$ 都满足 $A[\\text{PARENT}(i)]\\leq A[i]$ . 堆的最小元素存放在根结点中。 堆的基本过程 : MAX-HEAPIFY：维护最大堆的过程，时间复杂度为 $O(\\lg{n})$ BUILD-MAX-HEAP：将无序的输入数据数组构造一个最大堆，具有线性时间复杂度 $O(n\\lg{n})$ 。 HEAPSORT：对一个数组进行原址排序，时间复杂度为 $O(n\\lg{n})$ MAX-HEAP-INSERT、HEAP-EXTRACT-MAX、HEAP-INCREASE-KEY和HEAP-MAXIMUM：利用堆实现一个优先队列，时间复杂度为 $O(\\lg{n})$ . 维护：MAX-HEAPIFY调用MAX-HEAPIFY的时候，假定根结点LEFT(i)和RIGHT(i)的二叉树都是最大堆，但A[i]可能小于其左右孩子，因此违背了堆的性质。 MAX-HEAPIFY通过让 A[i]“逐级下降”，从而使下标为i的根结点的子树满足最大堆的性质。 MAX-HEAPIFY(A, i) 功能：维护下标为i的根结点的子树，使其满足最大堆的性质。 参数：i 是该子树的根结点，其左子树右子树均满足最大堆的性质。 12345678910l = LEFT(i)r = RIGHT(i)if l &lt;= A.heap-size and A[l] &gt; A[i] largest = lelse largest = iif r &lt;= A.heap-size and A[r] &gt; A[i] largest = rif largest != i exchange A[i] with A[largest] MAX-HEAPIFY(A, largest) 下图是执行 MAX-HEAPIFY(A, 2)的执行过程。A.heap-size=10, 图(a)(b)(c)依次体现了值为4的结点依次下降的过程。 时间复杂度分析 ： MAX-HEAPIFY的时间复杂度为 $O(lg{n})$. 建堆：BUILD-MAX-HEAP堆的性质： 子数组A[n/2+1..n]中的元素都是树的叶子结点。因为下标最大的父结点是n/2，所以n/2以后的结点都没有孩子。 建堆 ：每个叶结点都可以看成只包含一个元素的堆，利用自底向上的方法，对树中其他结点都调用一次MAX-HEAPIFY，把一个大小为n = A.length的数组A[1..n]转换为最大堆。 BUILD-MAX-HEAP(A) 功能：把A[1..n]数组转换为最大堆 123A.heap-size = A.lengthfor i = A.length/2 downto 1 MAX-HEAPIFY(A, i) 下图是把A数组构造成最大堆的过程： 时间复杂度分析 ： BUILD-MAX-HEAP需要 $O(n)$ 次调用MAX-HEAPIFY，因此构造最大堆的时间复杂度是 $O(n\\lg{n})$ . 排序：HEAPSORT算法思路： 初始化时，调用BUILD-MAX-HEAP将输入数组A[1..n]建成最大堆，其中 n = A.length。 调用后，最大的元素在A[1]，将A[1]和A[n]互换，可以把元素放在正确的位置。 将n结点从堆中去掉(通过减少A.heap-size实现)，剩余结点中，原来根的孩子仍是最大堆，但根结点可能会违背堆的性质，调用MAX-HEAPIFY(A, 1)，从而构造一个新的最大堆。 重复步骤3，直到堆的大小从n-1降为2. HEAPSORT(A) 功能：利用堆对数组排序 12345BUILD-MAX-HEAP(A)for i = A.length downto 2 exchange A[1] with A[i] A.heap-size = A.heap-size - 1 MAX-HEAPIFY(A, 1) 下图为调用HEAPSORT的过程图： 时间复杂度分析 ： 建堆BUILD-MAX-HEAP的时间复杂度为 $O(n\\lg{n})$ ，n-1次调用MAX-HEAPIFY的时间复杂度为 $O(n\\lg{n})$ ，所以堆排序的时间复杂度为 $O(n\\lg{n})$ . 堆的应用：优先队列这里关注如何用最大堆实现最大优先队列。 优先队列(priority queue)： 一种用来维护由一组元素构成的集合S的数据结构，其中每一个元素都有一个相关的值，称为关键字(key)。 （最大）优先队列支持的操作 ： INSERT(S, x)：把元素 $x$ 插入集合S中，时间复杂度为 $O(\\lg{n})$ 。 MAXIMUM(S)：返回S中具有最大关键字的元素，时间复杂度为 $O(1)$ 。 EXTRACT-MAX(S)：去掉并返回S中的具有最大关键字的元素，时间复杂度为 $O(\\lg{n})$ 。 INCREASE-KEY(S, x, k)：将元素 $x$ 的关键字值增加到k，这里假设k的大小不小于元素 $x$ 的原关键字值，时间复杂度为 $O(\\lg{n})$ 。 MAXIMUM将集合S已建立最大堆的前提下，调用HEAP-MAXIMUM在 $\\Theta(1)$ 实现MAXIMUM的操作。 HEAP-MAXIMUM(A) 功能：实现最大优先队列MAXIMUM的操作，即返回集合中最大关键字的元素。 1return A[1] 时间复杂度分析 ：$\\Theta(1)$ EXTRACT-MAX类似于HEAPSORT的过程。 A[1]为最大的元素，A[1]的孩子都是最大堆。 将A[1]和A[heap-size]交换，减少堆的大小(heap-size)。 此时根结点的孩子满足最大堆，而根不一定满足最大堆性质，维护一下当前堆。 HEAP-EXTRACT-MAX(A) 功能：实现最大优先队列EXTRACT-MAX的操作，即去掉并返回集合中最大关键字的元素。 1234567if A.heap-size &lt; 1 error &quot;heap underflow&quot;max = A[1]A[1] = A[A.heap-size]A.heap-size = A.heap-size - 1MAX-HEAPIFY(A, 1)return max 时间复杂度分析 ：$O(\\lg{n})$ . INCREASE-KEY如果增加A[i]的关键词，可能会违反最大堆的性质，所以实现HEAP-INCREASE-KEY的过程类似插入排序：从当前i结点到根结点的路径上为新增的关键词寻找恰当的插入位置。 当前元素不断和其父结点比较，如果当前元素的关键字更大，则和父结点进行交换。 步骤1不断重复，直至当前元素的关键字比父结点小。 HEAP-INCREASE-KEY(A, i, key) 功能：实现最大优先队列INCREASE-KEY的功能，即将A[i]的关键字值增加为key. 参数：i为待增加元素的下标，key为新关键字值。 123456if key &lt; A[i] error &quot;new key is smaller than current key&quot;A[i] = keywhile i &gt; 1 and A[PARENT(i)] &lt; A[i] exchange A[i] with A[PARENT(i)] i = PARENT(i) 下图展示了HEAP-INCREASE-KEY的过程： 时间复杂度分析 ：$O(\\lg{n})$ INSERT如何插入一个元素扩展最大堆？ 先通过增加一个关键字值为 $-\\infin$ 的叶子结点扩展最大堆。 再调用HEAP-INCREASE-KEY过程为新的结点设置对应的关键字值。 MAX-HEAP-INSERT(A, key) 功能：实现最大优先队列的INSERT功能，即将关键字值为key的新元素插入到最大堆中。 参数：key是待插入元素的关键字值。 123A.heap-size = A.heap-size + 1A[A.heap-size] = -∞HEAP-INCREASE-KEY(A, A.heap-size, key) 时间复杂度分析 ：$O(\\lg{n})$ . 快速排序对于包含 $n$个数的输入数组来说，快速排序是一个最坏情况时间复杂度为 $\\Theta(n^2)$ 的排序算法。 虽然最坏情况时间复杂度很差，但是快速排序通常是实际排序应用中最好的选择，因为他的平均性能非常好：他的期望时间复杂度为 $\\Theta(n\\lg{n})$ ，而且 $\\Theta(n\\lg{n})$ 中隐含的常数因子非常小。 另外，它还能进行原址排序。 分治对A[p..r]子数组进行快速排序的分治过程： 分解： 数组A[p..r]被划分为两个（可能为空）的子数组A[p..q-1]和A[q+1..r]。 使得A[p..q-1]中的每个元素都小于等于A[q]，A[q+1..r]中的每个元素都大于等于A[q]。 其中计算下标q也是分解过程的一部分。 解决：通过递归调用快速排序，对子数组A[p..q-1]和A[q+1..r]进行排序。 合并：因为子数组都是原址排序的，所以不需要合并操作，A[p..r]已经排好序。 快速排序：QUICKSORT按照分治的过程。 QUICKSORT(A, p, r) 功能：快速排序子数组A[p..r] 1234if p &lt; r q = PARTITION(A, p, r) QUICKSORT(A, p, q-1) QUICKSORT(A, q+1, r) 数组的划分：PARTITION快速排序的关键部分就在于如何对数组A[p..r]进行划分，即找到位置q。 PARTITION(A, p, r) 功能：对子数组A[p..r] 划分为两个子数组A[p..q-1]和子数组A[q+1..r]，其中A[p..q-1] 小于等于A[q]小于等于A[q+1..r] 返回：数组的划分下标q 12345678x = A[r]i = p - 1for j = p to r - 1 // j is pointer for comparation if A[j] &lt;= x i = i+1 exchange A[i] with A[j]exchange A[i+1] with A[r]return i+1 PARTITION总是选择一个 $x=A[r]$ 作为主元(pivot element)，并围绕它来划分子数组A[p..r]。 在循环中，数组被划分为下图四个（可能为空的）区域： $p\\leq k\\leq i$ ，则 $A[k]\\leq x$ . $i+1\\leq k \\leq j-1$ ，则 $A[k]&gt;x$. $k = r$ ，则 $A[k]=x$ . $j\\leq k\\leq r-1$ ，则 $A[k]$ 与 $x$ 无关。 下图是将样例数组PARTITION的过程： 快速排序的性能[*]待补充 快速排序的随机化版本与始终采用 $A[r]$ 作为主元的方法不同，随机抽样是从子数组A[p..r]随机选择一个元素作为主元。 加入随机抽样，在平均情况下，对子数组A[p..r]的划分是比较均匀的。 RANDOMIZED-PEARTITION(A, p, r) 功能：数组划分PARTITION的随机化主元版本 123i = RANDOM(p, r)exchange A[r] with A[i]return PARTITION(A, p, r) RANDOMIZED-QUICKSORT(A, p, r) 功能：使用随机化主元的快速排序 1234if p &lt; r q = RANDOMIZED-PARTITION(A, p, r) RANDOMIZED-QUICKSORT(A, p, q-1) RANDOMIZED-QUICKSORT(A, q+1, r) 计数排序计数排序 ： 假设 $n$ 个输入元素中的每一个都是在 0到 $k$ 区间内到一个整数，其中 $k$ 为某个整数。当 $k = O(n)$ 时，排序的运行时间为 $\\Theta(n)$ . 计数排序的思想 ： 对每一个输入元素 $x$，确定小于 $x$ 的元素个数。利用这个信息，就可以直接把 $x$ 放在输出数组正确的位置了。 COUNTING-SORT(A, B, k) 功能：计数排序 参数： A[1..n]输入的待排序数组，A.length = n B[1..n] 存放排序后的输出数组； 临时存储空间 C[0..k] ，A[1..n]中的元素大小不大于k. 123456789101112let C[0..k] be a new arrayfor i = 0 to k C[i] = 0for j = 1 to A.length C[A[j]] = C[A[j]] + 1//C[i] now contains the number of elements equal to i.for i = 1 to k C[i] = C[i] + C[i-1]//C[i] now contains the number of elements less than or equal to i.for j = A.length downto 1 B[C[A[j]]] = A[j] C[A[j]] = C[A[j]] - 1 下图是计数排序的过程： Reference Introduction to Algorithms. 算法导论","link":"/2020/06/29/sort-preview/"},{"title":"「Cryptography-Boneh」:Block Cipher 1","text":"这篇文章介绍了块密码。 文中主要分为四个部分。 第一个部分解释了块密码的基础概念，包括抽象概念PRF（伪随机函数）和PRP（伪随机置换）的定义及其安全定义。 第二个部分介绍了经典块密码DES，包括DES的Feistel网络，支撑Feistel网络的Luby-Rackoff定理，triple-DES和对DES的一些攻击方法。特别是有效的中间相遇攻击。 第三个部分介绍了目前流行的块密码AES，包括AES的结构和一些攻击方法等。 最后一小部分介绍了用PRG构造PRF，再利用Feistel网络变成块密码。 Overview块密码，简单来说，就是在给定密钥 (k bits)下，将输入块(n bits)映射到输出块(n bits)。 比如，在3DES中，n = 64 bits, k = 168 bits；在AES中，n = 128 bits, k = 128, 192, 256 bits。 块密码算法一般是通过迭代（iteration）实现的，如下图的结构： 密钥k通过密钥扩展(key expansion)得到一系列轮密钥。 明文块m通过迭代运算，得到最终的密文c，其中每一次运算称为轮函数R(k, m)。 对于3DES来说，轮数为n = 48，对于AES-128来说，轮数为n = 10。 虽然迭代运算会让块密码比流密码慢很多，但块密码有很多应用。 PRPs and PRFs这里介绍两个抽象的概念，伪随机置换（PRP）和伪随机函数（PRF）。 Pseudo Random Function(PRF): defined over(K, X, Y): $F:K\\times X \\rightarrow Y$ such that exists “efficient” algorithm to evaluate F(k, x) 【伪随机函数定义在(K, X, Y)上，只要求是可计算的，没有要求是可逆的】 Pseudo Random Permutations(PRP): defined over(K, X): $E:K\\times X\\rightarrow X$ such that: Exists “efficient” deterministic algorithm to evaluate E(k, x) The function E(k, ·) is one to one Exists “efficient” inversion algorithm D(k, y) 【而伪随机置换定义在(K, X)上，除了要求是可计算的，还要求是一对一映射（单射），即存在可逆运算】 块密码都是PRPs（不然怎么解密），常见的PRP有3DES，AES等： AES: $K \\times X \\rightarrow X$ where K = X = $\\{0, 1\\}^{128}$ 3DES: $K\\times X \\rightarrow X$ where X = $\\{0, 1\\}^{64}$ , K = $\\{0, 1\\}^{168}$ 从定义上来说，任何PRP都是PRF；但一个PRF是PRP的充分条件是X=Y，且存在可逆运算(efficiently invertible)。 Secure PRFs在介绍PRF的安全性之前，先定义一些符号： $F: K \\times X \\rightarrow Y$ is a PRF. Funs[X, Y]: the set of all functions from X to Y 【X到Y的所有映射关系】 $S_F={ F(k, \\cdot)}$ s.t. $k\\in K$ $\\subseteq$ Funs[X, Y] 【确定k就确定了一种X到Y的映射关系】 直觉上来说，当从Funs[X, Y]中随机采样的函数与从 $S_F$ 中随机采样的函数不可区分时，PRF是安全的。 Intuition: a PRF is secure if a random function in Funs[X, Y] is indistinguishable from a random function in $S_F$ . 定义下图的游戏： 云从Funs[X, Y]中随机选取一个函数f，并随机选择一个k，即确定了一个在 $S_F$ 中函数。 攻击者向云多次询问x的计算结果，对云来说，如果是b = 0，他返回f(x)，如果b = 1，他返回F(k, x)。 当攻击者输入任意输入x时，都无法区分计算结果是f(x)计算的还是F(k, x)计算的，PRF是安全的。 Secure PRPs/Block Ciphers块密码都是PRP，所以这里也在定义什么是安全的块密码。 同样定义一些符号： $E: K\\times X\\rightarrow Y$ is a PRP ( X = Y). Perms[X, Y]: the set of all one-to-one functions from X to Y 【X到Y的所有的一对一映射关系】 $S_F={ E(k, \\cdot)}$ s.t. $k\\in K$ $\\subseteq$ Perms[X, Y] 【确定k就确定了一种X到Y的一对一映射关系】 同样，当从Perms[X, Y]中随机采样的函数与从 $S_F$ 中随机采样的函数不可区分时，PRP是安全的。 Intuition: a PRP is secure if a random function in Perms[X, Y] is indistinguishable from a random function in $S_F$ . 定义下图的游戏： 云从Perms[X, Y]中随机选取一个函数 $\\pi$ ，并随机选择一个k，即确定了一个在 $S_F$ 中函数。 攻击者向云多次询问x的计算结果，对云来说，如果是b = 0，他返回$\\pi(x)$，如果b = 1，他返回F(k, x)。 当攻击者输入任意输入x时，都无法区分计算结果是$\\pi(x)$计算的还是F(k, x)计算的，PRP是安全的。 注意：这里是对任意输入，如果对某一个输入，攻击者能区分，这个PRP都是不安全的。 PRF $\\rightarrow$ PRGPRF的一个简单应用是把它变成一个PRG。 Let $F: K\\times \\{0, 1\\}^n\\rightarrow \\{0, 1\\}^n$ be a secure PRF. The following $G: K\\rightarrow \\{0,1\\}^{nt}$ is a secure PRF: $$ G(k) = F(k, 0)\\mid\\mid F(k, 1) \\mid\\mid \\dots \\mid\\mid F(k, t-1) $$ 这是一种计数器模式，计数器模式最显著的优点是可并行计算。 此外该PRG的安全性来自于PRF的安全性，即F(k, ·)与f(·)不可区分。 DESData Encryption Standard（DES） DES的核心就是Feistel网络结构： $R_i = f_i(R_{i-1})\\oplus L_{i-1}$ $L_i = R_{i-1}$ 对于给定的函数的轮函数 f1, …, fd: $ \\{0,1\\}^n\\rightarrow \\{0,1\\}$ ，Feistel网络结构都可以构造出2n bits的可逆函数: $ F: \\{0,1\\}^{2n}\\rightarrow \\{0,1\\}^{2n}$ Claim: for all f1, …, fd: $ \\{0,1\\}^n\\rightarrow \\{0,1\\}$ , Feistel network $ F: \\{0,1\\}^{2n}\\rightarrow \\{0,1\\}^{2n}$ is invertible. 证明：构造出可逆函数即可 $R_{i-1} = L_i$ $L_{i-1}=R_i\\oplus f_i(L_i)$ 需要注意的是，在解密电路中，轮函数的使用如下图是逆序使用的。 Luby-Rackoff’85 Thm支撑Feistel Network安全性的一个理论是Luby-Rackoff的一个定理，这将安全的PRF变成安全的PRP，这也是块密码（使用Feistel结构的块密码）安全性的支撑。 Thm: $f:K\\times \\{0,1\\}^n\\rightarrow \\{0,1\\}^n$ a secure PRF then, 3-round Feistel $F: K^3\\times{0,1}^{2n}\\rightarrow {0,1}^{2n}$ a secure PRP. DES StructureDES的核心是一个16轮的Feistel的网络。 DES的输入是64 bits块，通过一个初始置换（IP），该置换并不是为安全性服务的，只是DES标准里的要求。 然后再通过16轮的Feistel网络，在Feistel网络中依次调用轮函数，轮函数中的轮密钥是通过初试密钥扩展得到的16个不同密钥。 最后再通过逆置换得到64 bits的输出块。 The function F(ki, x)DES的轮函数结构如下图： 轮函数的输入是32 bits x和48 bits的密钥ki： 32 bits x通过expansion box，扩展为48 bits。扩展方式只是复制一些位。 扩展后的x与轮密钥ki进行异或运算。 异或运算后的结果为48 bits，再被划分为8组6 bits，每一组都通过一个S-box，每一个S-box实则是一个非线形映射 ${0,1}^6\\rightarrow {0,1}^4$ ，在实现中往往使用查表的形式使用。 通过S-box后，又得到了32 bits，最后通过一个置换（Permutation），得到该轮的最终输出。 S-boxS-box也是DES中最重要的一部分，因为他是DES中唯一的非线性部分，它保证了整个DES的非线形映射关系。 来看一个糟糕的S-box的例子： $\\mathrm{S}_{\\mathrm{i}}\\left(\\mathrm{x}_{1}, \\mathrm{x}_{2}, \\ldots, \\mathrm{x}_{6}\\right)=\\left(\\mathrm{x}_{2} \\oplus \\mathrm{x}_{3}, \\mathrm{x}_{1} \\oplus \\mathrm{x}_{4} \\oplus \\mathrm{x}_{5}, \\quad \\mathrm{x}_{1} \\oplus \\mathrm{x}_{6}, \\quad \\mathrm{x}_{2} \\oplus \\mathrm{x}_{3} \\oplus \\mathrm{x}_{6}\\right)$ 上式也可以写做： $\\mathrm{S}{\\mathrm{i}}(\\mathbf{x})=\\mathrm{A}{\\mathrm{i}} \\cdot x(\\bmod 2)$ 我们称上面的S盒是一个线性函数。 这会带来什么影响呢？如果S盒是线性的，那么整个DES都会变成线性的，即存在一个确定的二元矩阵 $\\mathbf{B}$ 满足： 而该线性关系还有一个简单的检验方法： Exhaustive Search AttacksDES的密钥空间比较小，因此可以使用穷举密钥的方法攻击。 对于已知的一些明文-密文对，我们希望穷举攻击能得到唯一的一个密钥，而不是说对于一个明文-密文对，有多个密钥（映射关系）都符合。 因此支撑穷举攻击DES的一个定理是： 假设DES是一个理想的密码，即 $2^{56}$ 个密钥对应 $2^{56}$ 个不同的随机可逆函数，那么对于任意的明文-密文对（$c = \\mathrm{DES}(k,m)$） ，最多只有一个密钥的概率大于 99.5%。 证明： 假设存在一个密钥k’，不是真正的密钥k，但满足 $c = \\mathrm{DES}(k’, m)=\\mathrm{DES}(k,m)$ 。 这样的概率是小于 所有 $2^{56}$ 个密钥都满足该明文-密文对的概率。 因此，如果已知两个明文-密文对，那么通过这两个DES pairs就可以以 $1-1/2^{71}$ 的概率确定出正确的密钥。 同样，对于AES-128来说，通过两个AES pairs就可以以 $1-1/2^{128}$ 的概率确定出正确的密钥。 Strength DES against ex. search因为DES的密钥空间实在是太小了，所以有一个DES challenge的挑战，随着计算能力的发展，破解DES的速度越来越快。 1: Triple-DES一个抵御DES穷举攻击的算法是Triple- DES。 Let $\\mathrm{E}:\\mathrm{K}\\times \\mathrm{M}\\rightarrow \\mathrm{M}$ be a block cipher. Define $\\mathrm{3E}: \\mathrm{K}^3\\times \\mathrm{M}\\rightarrow \\mathrm{M}$ as $\\mathrm{3E}((\\mathrm{k_1},\\mathrm{k_2},\\mathrm{k_3}),\\mathrm{m})=\\mathrm{E}(\\mathrm{k_1},\\mathrm{D}(\\mathrm{k_2},\\mathrm{E}(\\mathrm{k_3},\\mathrm{m})))$ triple- DES为什么是E、D、E的运算顺序，而不是E、E、E的运算顺序？ 因为对于Triple- DES来说，如果 $k_1=k_2=k_3$ ，就变成了single DES（虽然在运算速度上，没有什么必要） Triple- DES的密钥空间大小是 $3\\times 56=168$ bits，在运算上会比DES慢三倍，但攻击triple- DES只需要 $O(2^{118})$ 的时间复杂度。（下文介绍的中间相遇攻击） Meet in the middle attack为什么不用double DES? 因为有一种中间相遇攻击(meet in the middle attack)，让攻击double DES的时间和穷举攻击single DES的时间差不多。 如果我们使用double DES，$\\mathrm{2E}((\\mathrm{k_1},\\mathrm{k_2}),\\mathrm{m})=\\mathrm{E}(\\mathrm{k_1},\\mathrm{E}(\\mathrm{k_2},\\mathrm{m}))$ double DES的密钥空间为112 bits，但在中间相遇攻击下，穷举范围可以少一半比特。 中间相遇攻击的核心思想是，我们希望找到 $(k_1, k_2)$ 满足 $E(k_1, E(k_2, M))=C$ ，即满足：$E(k_2,M)=D(k_1, C)$ 。 Attack: M = (m1, …, m10), C = (c1, …, c10) build table and sort on 2nd column. 从double DES 的左边的明文开始，枚举 $k_2$ ，计算 $E(k^i, M)$ 的值，最后按照 $E(k^i, M)$ 的值排序。 for all $k\\in {0,1}^{56} do:$ test if $D(k,C)$ is in 2nd column. 从double DES的右边的密文开始检测，是否 $D(k,C)$ 存在在上述表中，如果 $E(k^i, M)=D(k,C)$ ，那么就得到了 $(k^i, k)=(k_2, k_1)$ 分析一下攻击时间，第一部分是建表排序，第二部分是计算二分查表的时间。 Time = $2^{56} \\log \\left(2^{56}\\right)+2^{56} \\log \\left(2^{56}\\right)&lt;2^{63}&lt;&lt;2^{112}, \\quad \\text { space } \\approx 2^{56}$ 同样的，中间相遇攻击同样可以应用于3DES中：$\\text{Time}=2^{118},\\quad \\text{space}\\approx 2^{56}$ 2: DESX$\\mathrm{E}: \\mathrm{K} \\times{0,1}^{n} \\rightarrow{0,1}^{\\mathrm{n}} \\text { a block cipher }$ Define EX as $\\operatorname{EX}\\left(\\left(\\mathrm{k}{1}, \\mathrm{k}{2}, \\mathrm{k}{3}\\right), \\mathrm{m}\\right)=\\mathrm{k}{1} \\oplus \\mathrm{E}\\left(\\mathrm{k}{2}, \\mathrm{~m} \\oplus \\mathrm{k}{3}\\right)$ 对于DESX来说，密钥空间长度为: 64+56+64 = 184 bits，但同样的通过中间相遇攻击，time = $2^{64+56}=2^{120}$ . 需要注意的一点是，在实际使用中，有人使用 $\\mathrm{k}{1} \\oplus \\mathrm{E}\\left(\\mathrm{k}{2}, \\mathrm{m}\\right) \\text { and } \\mathrm{E}\\left(\\mathrm{k}_{2}, \\mathrm{m} \\oplus \\mathrm{k}_{1}\\right)$ 的设计，这样的设计和原始DES一样易收ex. search的攻击。（通过中间相遇攻击） More attacks on block ciphersAttacks on the implementation对于DES，还有一些针对实现上的攻击。 第一种是侧信道攻击（Side channel attacks），通过测量加密解密时的时间差异、能源消耗差异来得到一些额外的信息。 另一种是Fault attacks，通过最后一轮的计算错误可能会暴露密钥k。 对于实现上的攻击的经验教训是：除了不要自己设计密码学算法，甚至都不要自己去实现。 Linear and differential attacks该攻击的目标是，给定许多inp/out pairs，希望能在比 $2^{56}$ 更少的时间内攻击成功。 Linear cryptanalysis (overview): let c = DES(k, m), suppose for random k, m: 第一部分是msg bits的子集合，第二部分是密文bits 的子集合，第三部分是key bits的子集合。 如果这些都是相互独立的，那么这个概率应该为1/2，但因为有slightly 线性的关系，所以这个概率会有一点bias。对于DES来说，因为第五个S-box的一点bug，$\\varepsilon=1/2^{21}\\approx 0.0000000477$ . Thm: 基于上述线性的关系，该定理表示如果给了 $1/ \\varepsilon^2$ 个随机明文密文对，大部分都满足上式比特集合的线性关系。 所以，对于DES来说，$\\varepsilon=1/2^{21}$ ，通过 $2^{42}$ 个inp/out pairs能在 $O(2^{42})$ 时间内得到 $k[l_1,…,k_u]$ 比特集异或的结果。 但其实不止是xor of key bits，实际上: can find 14个key bits in time $2^{42}$ 。（todo) 这样就将穷举攻击的复杂度降低到56-14=42 bits in time $2^{42}$ Total attack time $\\approx 2^{43}&lt;&lt;2^{56}$ with $2^{42}$ random inp/out pairs. Lesson: A tiny bit of linearly in $S_5$ lead to a $2^{42}$ time attack. So don’t design ciphers yourself. Quantum attacks另一个针对穷举攻击的解决方法是量子攻击。 对于一个一般的搜索问题： 一般计算机解决该类问题的最优时间是 $O(|X|)$ ，而量子计算机解决该类问题的最优时间是 $O(|X|^{1/2})$ 。 不过，量子计算机是否被实现还是一个疑问。 用量子计算机来穷举密钥空间，只需要 $O(|K|^{1/2})$ 的时间即可找到密钥。 AESAES的发展过程如图： AES不是Feistel网络结构的密码，他是一个替换-置换类网络(Subs-Perm network): AES-128 schematicAES-128的结构如下图： 输入块是16 bytes的4*4矩阵，通过10轮运算得到最后的输出块。 每一轮都包括同样的操作（除了最后一轮不太一样），密钥加，字节替换(ByteSub)，行移位(ShiftRow)，列混淆(MixColumn)。其中每一轮的轮密钥是初始16 bytes的密钥通过密钥扩展得到的。 轮密钥中的字节替换可以通过预计算查表的形式，也可以通过直接运算的形式。因此在实际应用中可以根据需求做出权衡。 AES in hardware为了加快AES的运算速度，各公司有设计相关指令。 AES instructions in Intel Westmere: aesenc, aesenclast: 执行AES中的一轮运算 128-bit registers: xmm1=state, xmm2=round key aesenc xmm1, xmm2 ：执行一轮运算，并把计算结果放在xmm1中 aeskeygenassist: 执行AES的密钥扩展。 同样的硬件上，相对于OpenSSL的实现，可以快14倍。 同样，在 AMD Bulldozer上也有类似的指令设计。 AttacksAES也存在比暴力搜索密钥空间更好的算法。 Best key recovery attack: four times better than ex. search [BKR’11] Related key apack on AES-256: Given $2^{99}$ inp/out pairs from four related keys in AES-256 can recover keys in time ≈299[BK’09] Block ciphers from PRGs我们能否从PRG中构建PRF？ 答案是：是的。 Let G: $K\\rightarrow K^2$ be a secure PRG. Define 1-bit PRF F: $K\\times {0,1} \\rightarrow K$ as$$F(k,x\\in {0,1})=G(k)[x]$$根据定理，如果G是一个secure PRG,那么F就是一个secure PRF。 Thm: If G is a secure PRG then F is a secure PRF. 同样，我们可以通过这样的方式构建更大域的PRF。 对得到的结果再做一次G运算，得到的G1就是一个2-bit PRF： Proof: G1 is a secure PRG 证明思路就是，因为G是安全的PRG，所以k通过G运算得到的结果与随机串不可区分，同样的，最后扩展出的在 $K^4$ 中的字符串与随机串是不可区分的。 同样的，可以扩展出3-bit 的PRF，对于给定的比特输入，也是易于计算的： 而扩展出更多bit时，就得到了GGM PRF。 GGM PRF就是通过上述的扩展方式得到 ${0,1}^n$ 域上的PRF： 这种方式，PRF的安全性依赖于PRG的安全性。 但由于这样的方式运行效率较低，所以在实际应用中很少。 说到现在，我们好像还没有提到如何从PRG中构建块密码，因为块密码时PRPs。 如果记性比较好的读者应该还记得我们刚刚提到的Luby-Rackoff定理，将一个安全的PRF作为Feistel网络中的轮函数，3-round Feistel就可以构建一个安全的PRP，即安全的块密码。 Thm: $f:K\\times \\{0,1\\}^n\\rightarrow \\{0,1\\}^n$ a secure PRF then, 3-round Feistel $F: K^3\\times{0,1}^{2n}\\rightarrow {0,1}^{2n}$ a secure PRP.","link":"/2021/09/09/stanford-crypto-blockcipher1/"},{"title":"「Cryptography-Boneh」：Introduction","text":"本系列是学习Dan Boneh教授的Online Cryptography Course。 这是Dan教授的第一讲：对密码学的一些Introduction。 What is cryptography?Crypto core：安全通信 Secret key establishment (密钥的建立)： Alice 和 Bob 会得到一个shared secret key，而且Alice 知道她是在和Bob通信，Bob也知道他是在和Alice通信。而attacker不能从通信中获取key。 Secure communication （安全通信）： 在通信中，Alice、Bob用key将信息加密，保证了通信的confidentiality（机密性）；同时attacker也无法篡改通信的信息，保证了通信的integrity（完整性）。 Crypto can do much more密码学除了能保证安全通信，密码学还能做很多其他的事。 Digital signature &amp; Anonymous Digital signatures（数字签名）： 现实中，人们对不同的文档进行签名，虽然是不同的文档，但是签名的字是相同的。 如果这应用在网络的文档签名中，这会很危险的。攻击者只需要将签名进行复制、粘贴，就可以将你的签名签在你并不想签的文档中。 数字签名的主要思想：数字签名其实是代签内容的函数值，所以如果攻击者只是复制数字签名（原签名的函数值），那么攻击者得到的数字签名也是无效的（函数值不同）。 Anonymous communication（匿名通信）： 匿名通信的实现，有Mix network （wiki详细介绍）协议，这是一种路由协议，通过使用混合的代理服务器链来实现难以追踪的通信。 通过这些代理的不断加密解密可以实现： Bob不知道与之通信的是Alice。 代理也不知道是Alice和Bob在通信。 双向通信：虽然Bob不知与之通信的是Alice，但也能respond。 Anonymous digital cash（匿名数字现金）： 现实中，我们可以去超市花掉一元钱，而超市不知道我是谁。 在网络中，如果Alice想去网上商店花掉数字现金一元钱，网上商店可以不知道是谁花掉的这一元钱吗？ 这就是匿名数字现金需要解决的问题： 可以在匿名的情况下花掉数字现金吗？ 如果可以，当Alice将这一元钱复制多次（数字现金都是数据串），得到了三元钱，再去把它花掉，由于匿名的原因，没人知道是谁花掉的这三元钱，商店找不到责任人。 这是匿名数字现金需要解决的第二个问题： 如何防止 double spending情况的发生？ 可以用这样的机制去实现匿名数字现金：当Alice花费这一块 once时，系统保证Alice的匿名性；但当Alice花费这一块 more than once ,系统立即揭露Alice的全部信息。 Protocols在介绍什么是Protocols之前，先介绍两种应用场景。 Elections 有5个人要进行投票选举0和1号候选人，但是需要保证：每个人除了知道自己的投票结果，互相不知道其他人的投票情况。在这种情况下怎么知道最后的winner是谁吗？ 如上图，可以引入一个第三方——election center，第三方验证每一个人只能投一次，最后统计票数决策出最后的winner。 Private auctions 介绍一种拍卖机制，Vickery auction：对一个拍卖品，每个投标者在不知道其他人投标价格的情况下进行投标，最后的acution winner： highest bidder &amp; pays 2nd highers bid。即是标价最高者得标，但他只需要付第二高的标价。 所以public知道的信息只有：中标者和第二高投标者的标价。 需要实现这种机制，也可以引入一个第三方——auction center。 但是引入第三方真的安全吗？安全第三方也不安全。 再看上面那个Election的例子，如果把上面四个人的投票情况作为输入，第三方的任务其实是输出一个函数 $f(x_1,x_2,x_3,x_4)$ 而不公开其他信息。 因为安全第三方也许并不安全，所以如果去掉第三方，上面四个人遵从某种协议，相互通信，最后能否得出这个 $f(x_1,x_2,x_3,x_4)$ 这个结果函数，而不透露其投票信息？ 答案是 “Yes”。 有一个惊人的定理：任何能通过第三方做到的事，也能不通过第三方做到。 Thm: anythong that can done with trusted auth. can also be done without. 怎么做到？答案是 Secure multi-party computation（安全多方计算）。 在MPC专栏 有相关介绍。 Crypto magic Privately outsourcing computation (安全外包计算) Alice想要在Google服务器查询信息，为了不让别人知道她查询的是什么，她把search query进行加密。 Google服务器接收到加密的查询请求，虽然Google不知道她实际想查询什么信息，但是服务器能根据E[query]返回E[results]。 最后Alice将收到的E[results]解密，得到真正的results。 这就是安全外包计算的简单过程：Encryption、Search、Decryption。 Zero knowledge（proof of knowledge) (零知识证明)： Alice 知道p、q(两个1000位的质数)相乘等于N。 Bob只知道N的值，不知道具体的p、q值。 Alice 给 Bob说她能够分解数N，但她不用告诉Bob N的具体因子是什么，只需要证明我能分解N，证明这是我的知识。 最后Bob知道Alice能够分解N，但他不知道怎么分解（不知道N的因子到底是什么）。 A rigorous science在密码学的研究中，通常是这样的步骤： Precisely specify threat model. 准确描述其威胁模型或为达到的目的。比如签名的目的：unforgeable（不可伪造）。 Propose a construction. Prove that breaking construction under threat mode will solve an underlying hard problem. 证明攻击者攻击这个系统必须解决一个很难的问题（大整数分解问题之类的NP问题）。 这样也就证明了这个系统是安全的。 HistorySubstitution cipher（替换）what is it 替换密码很好理解，如上图的这种替换表（key）。 比较historic的替换密码——Caesar Cipher（凯撒密码），凯撒密码是一种替换规则：向后移三位，因此也可以说凯撒密码没有key。 the size of key space用$\\mathcal{K}$ （花体的K）来表示密钥空间。 英语字母的替换密码，易得密钥空间的大小是 $|\\mathcal{K}|=26!\\approx2^{88}$ （即26个字母的全排列）。 这是一个就现在而言也就比较perfect的密钥空间。 但替换密码也很容易被破解。 how to break it问：英语文本中最commom的字母是什么？ 答：“E” 在英语文本（大量）中，每个字母出现的频率并不是均匀分布，我们可以利用一些最common的字母和字母组合来破解替换密码。 Use frequency of English letters. Dan教授统计了标准文献中字母频率： “e”: 12.7% , “t”: 9.1% , “a” : 8.1%. 统计密文中（大量）出现频率最高、次高、第三高的字母，他们的明文也就是e、t、a。 Use frequency of pairs of letters (diagrams).（二合字母） 频率出现较高的二合字母：”he”, “an”, “in” , “th” 也能将h, n,i等破解出。 trigrams（继续使用三合字母） ……直至全部破解 因此substitution cipher是CT only attack！（唯密文攻击：仅凭密文就可以还原出原文） Vigener cipherEncryption 加密过程如上图所示： 密钥是 “CRYPTO”, 长度为6，将密钥重复书写直至覆盖整个明文长度。 将密钥的字母和对应的明文相加模26，得到密文。 Decryption解密只需要将密文减去密钥字母，再模26即可。 How to break it破解方法和替换密码类似，思想也是使用字母频率来破解。 这里分两种情况讨论： 第一种：已知密钥长度 破解过程： 将密文按照密钥长度分组，按照图中的话，6个一组。 统计每组的的第一个位置的字母出现频率。 假设密文中第一个位置最common的是”H” 密钥的第一个字母是：”H”-“E”=”C” 统计剩下位置的字母频率，直至完全破解密钥。 第二种：未知密钥长度 未知密钥长度，只需要依次假设密钥长度是1、2、3…，再按照第一种情况破解，直至破解情况合理。 Rotor MachinesRotor: 轴轮。 所以这种密码的加密核心是：输入明文字母，轴轮旋转一定角度，映射为另一字母。 single rotor 早期的是单轴轮，rotor machine的密钥其实是图右中间那个圆圆的可以旋转的柱子。 图左是变动的密钥映射表。 变动过程： 第一次输入A，密文是K。 轴轮旋转一个字母位：看图中E，从最下到最上（一个圈，只相隔一位）。 所以第二次再输入A，密文是E。 …… Most famous ：the Enigma Enigma machine是二战时期纳粹德国使用的加密机器，因此完全破解了Enigma是盟军提前胜利的关键。 左图中可以看出Enigma机器中是有4个轴轮，每个轴轮都有自己的旋转字母位大小，因此密钥空间大小是 $|\\mathcal{K}|=26^4\\approx2^{18}$ (在plugboard中，实际是 $2^{36}$)。 密钥空间很小，放在现在很容易被暴力破解。 plugboard 允许操作员重新配置可变接线，实现两个字母的交换。plugboard比额外的rotor提供了更多的加密强度。 对于Enigma machine的更多的具体介绍可以戳Enigma machine 的wiki链接。 Data Encryption StandardDES：#keys = $2^{56}$ ,block siez = 64bits，一次可以加密8个字母。 Today：AES（2001）、Salsa20（2008）…… 这里只是简单介绍。 Discrete Probability Background on discrete probability: [html] Randomized algorithms随机算法有两种，一种是Deterministic algorithm（也就是伪随机），另一种是Randomized algorithm。 Deterministic algorithm $ y\\longleftarrow A(m)$ ，这是一个确定的函数，输入映射到唯一输出。 Randomized algorithm $y\\longleftarrow A(m ; r) \\quad \\text { where } r \\stackrel{R}{\\longleftarrow}{0,1}^{n}$ output： $y \\stackrel{R}{\\longleftarrow} A(m)$ ，y is a random variable. $ r \\stackrel{R}\\longleftarrow {0,1}^n $ :意思是r是n位01序列中的任意一个取值。R，random。变量r服从在 ${0,1}^n$ 取值的均匀分布。 由于随机变量r，对于给定m，$A(m;r)$ 是 ${0,1}^n$ 中的一个子集。 所以，对m的加密结果y，也是一个的随机变量，而且，y在 $A(m,r)$ 也是服从均匀分布。 因此，由于r的影响，对于给定m，加密结果不会映射到同一个值。（如上图所示） XORXOR有两种理解：（ $x \\oplus y $ ） 一种是：x,y的bit位相比较，相同则为0，相异为1. 另一种是：x,y的bit位相加 mod2. 异或在密码学中被频繁使用，主要是因为异或有一个重要的性质。 异或的重要性质：有两个在 ${0,1}^n$ （n位01串）取值的随机变量X、Y。X、Y相互独立，X服从任意某种分布，随机变量Y服从均匀分布。那么 $Z=Y\\oplus X$ ，Z在 ${0,1}^n$ 取值，且Z服从均匀分布。 Thm: Y a rand. var. over ${0,1}^n$ , X an index. uniform var. on ${0,1}^n$ ​ Then Z := Y $\\oplus$ X is uniform var. on ${0,1}^n$ . Proof： 当n=1 画出联合分布 Pr[ Z=0 ]=Pr[ (x,y)=(0,0)] + Pr[(x,y)=(1,1)]=1/2 每一bit位都服从均匀分布，可以容易得出 Z是服从均匀分布。 The birthday paradox（生日悖论）更具体的分析见 Birthday problem 。 问题前提：一个人的生日在365天的任意一天是均匀分布的（实际当然不是，貌似更多集中在9月）。 根据信鸽理论（有N个鸽子，M个隔间，如果N&gt;M，那么一定有一个隔间有两只鸽子），所以367个人中，以100%的概率有两个人的生日相同。但是，当只有70个人时，就有99.9%的概率，其中两人生日相同；当只有23人，这个概率可以达到50%。 其实这并不是一个悖论，只是直觉误导，理性和感性认识的矛盾。当只有一个人，概率为0，当有367人时，为100%，所以我们直觉认为，这是线性增长的，其实不然。 概率论知识： 设事件A：23个人中，有两个人生日相等。 $P\\left(A^{\\prime}\\right)=\\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\frac{362}{365} \\times \\cdots \\times \\frac{343}{365}$ $P\\left(A^{\\prime}\\right)=\\left(\\frac{1}{365}\\right)^{23} \\times(365 \\times 364 \\times 363 \\times \\cdots \\times 343)$ $P\\left(A^{\\prime}\\right) \\approx 0.492703$ $P(A) \\approx 1-0.492703=0.507297 \\quad(50.7297 \\%)$ 推广到一般情况，n个人(n","link":"/2020/03/04/stanford-crypto-intro/"},{"title":"「Cryptography-Boneh」:Block Cipher 2","text":"作为BlockCipher的第二篇文章。第一部分介绍了块密码中的抽象概念PRF和PRP的安全定义。第二部分介绍了两个概念，一个是抵抗one-time key的语义安全，另一个是抵抗many-time key(CPA)的语义安全。 在one-time key中，每条消息都使用新的密钥，类似于流密码中的OTP。介绍了不能抵抗CPA的ECB模式，还阐述了能抵抗CPA的det. CTR模式。 在many-time key中，同一条密钥可以用于加密多条消息，攻击者可以轻易具备CPA能力，文中说明了如果确定性的加密算法，则不能抵抗CPA，而random IV或者unique nonce的方式则可以抵抗CPA。 PRPs and PRFs在前文中，有提到PRP和PRF的直觉性的定义。现在我们来看他们的安全定义。 PRF：如下图定义这样一个游戏，有挑战者和攻击者，挑战者有两个实验EXP(b)，$b=0,1$。 在EXP(0)中，挑战者随机选择一个密钥K，即确定了一个伪随机函数 $f \\leftarrow \\mathrm{F(k, ·)}$ ；而在EXP(1)中，挑战者随机选择了一个函数 $f\\leftarrow \\mathrm{Funs[X, Y]} $ 。 攻击者会向挑战者发出许多输入询问 $x_1,x_2,…,x_q$ ，挑战者对于攻击者发出的询问，会随机选择 $b=0,1$ ，用 $f(x_1),f(x_2), …,f(x_q)$ 响应。 攻击者对于收到的回复，他会输出 $b’$ ，表示攻击者视角下，他认为该输出是用伪随机函数计算的(b=0)，还是用随机函数计算的(b=1)；定义实验的输出为EXP(b)=b’​ 。 因此安全定义如下，攻击者区分输出是伪随机函数计算的还是随机函数计算的优势是neg。 Def(PRF): F is a secure PRF if for all “efficient” A:$$\\operatorname{Adv}_{\\mathrm{PRF}}[\\mathrm{A}, \\mathrm{F}]:=\\mid \\operatorname{Pr}[\\operatorname{EXP}(0)=1]-\\operatorname{Pr}[\\operatorname{EXP}(1)=1] \\quad \\text{is neg.}$$ PRP:同理，对PRP来说，定义的游戏如下： 同理，PRP的安全定义为，攻击者区分是伪随机置换计算的还是随机置换计算的优势为neg。 Def(PRP): E is a secure PRP if for all “efficient” A:$$\\operatorname{Adv}_{\\mathrm{PRP}}[\\mathrm{A}, \\mathrm{E}]:=\\mid \\operatorname{Pr}[\\operatorname{EXP}(0)=1]-\\operatorname{Pr}[\\operatorname{EXP}(1)=1] \\quad \\text{is neg.}$$ 例题： Let X = {0,1}. Perms[X] contains two functions. Consider the following PRP: key space K={0,1}, PRP defined as:$\\mathrm{E(k,x)=x\\oplus k}$ Is this a secure PRP? 答案：Yes 当X = {0,1}下，随机置换如下： 而选择K的分布和选择随机置换的分布相同，所以攻击者不能区分。 而如果问是否是secure PRF，答案则是：No。 攻击者可以定义这样一个统计算法，如果f(0)=f(1)则输出1，否则输出0。 则攻击者有 $\\operatorname{Adv}_{\\mathrm{PRF}}[\\mathrm{A}, \\mathrm{E}]:=｜0-1/2｜=1/2$ 的优势攻击成功。 现实中常见安全PRPs，有3DES, AES等，比如AES-128的置换是：$\\mathrm{K\\times X\\rightarrow X}$ where $\\mathrm{K=X={0,1}^{128}}$ 。 如果对AES的安全性定义的更具体，则是所有 $2^{80}$ 的算法A对AES攻击的优势 $\\mathrm{Adv_{PRP}[A,AES]}&lt;2^{-40}$ . PRF Switching LemmaAny Secure PRP is also a secure PRF, if |X| is sufficiently large. 【对于任意的安全PRP，当定义域 $X$ 足够大时，它也是安全PRF，所以AES也是安全的PRF。】 证明如下： Lemma: Let E be a PRP over (K, X). Then for any q-query adversay A:$$|\\mathrm{Adv_{PRF}[A,E]-\\mathrm{Adv_{PRP}[A, E]}}|&lt;\\mathrm{q^2/2|X|}$$该引理的结果是，当 ｜X｜很大时，$\\mathrm{q^2/2|X|}$ 是neg.，因此左部分也是neg.。 而当E是安全PRP时，$\\mathrm{Adv_{PRP}[A, E]}$ 是neg.，因此 $\\mathrm{Adv_{PRF}[A,E]}$ 也是neg.，所以E也是安全的PRF。 证毕。 One Time Key使用块密码有很多种方式，而我们的目的就是希望能从安全的PRP构建安全的加密方式。 本小节主要阐述one-time keys的方式，即每条消息都使用新的密钥，所以攻击者只能得到这一条有关该密钥的密文，即一次一密的唯密文攻击能力，但攻击者的攻击目标是希望能从密文中得到关于明文的信息，即破坏语义安全。 one-time key: Adversary’s power: Adv sees only one ciphertext (one-time key) Adversary’s goal: Learn info about PT from CT (semantic security) 在使用AES的ECB(Electronic Code Book)方式，当消息中的不同块 $\\mathrm{m_1=m_2}$ 时，$\\mathrm{c_1=c_2}$ ，攻击者能从密文中得到一些他不应该得到的信息。 比如用ECB的方式加密图片，可以看到加密后的图片也是可见的： Semantic Security(one-time key)这里的one-time key方式对于攻击者来说，就是攻击者只能看到一条与密钥相关的密文。 定义如下两个实验，挑战者随机选择一个密钥，以此确定用于加密的伪随机置换，攻击者发送任意两个消息 $\\mathrm{m_0, m_1}$ ，长度相同。 在EXP(0)中，挑战者对 $\\mathrm{m_0}$ 加密；而在EXP(1)中，挑战者对 $\\mathrm{m_1}$ 加密。 攻击者收到挑战者响应的密文，通过一定的算法来判断该密文是对 $\\mathrm{m_0}$ 加密的，还是对 $\\mathrm{m_1}$ 加密的。 如果one-time key方式是语义安全的，则：$$\\mathrm{Adv_{SS}[A,OTP]=|Pr[EXP(0)=1]-Pr[EXP(1)=1]} \\quad \\text{should be “neg”.}$$ 而我们上文提到的ECB，就不是语义安全的。 因为一条消息实则有许多块，而这些块中如果有相同的明文，则对应的密文相同。 攻击者可以利用这一点来区分 $\\mathrm{m_0,m_1}$ ： 攻击者可以用算法判断密文块是否相等来判断是对哪一条消息加密的。 该场景下， 攻击者攻击成功的优势为： $\\mathrm{Adv_{SS}[A,ECB]}=|0-1|=1$ 。 Det. CTR这里介绍一种从PRF构造的安全流密码：确定的计数器模式（Deterministic counter mode） 证明该种模式的安全性： Theorem: For any L&gt;0, if F is a secure PRF over (K,X,X) then $\\mathrm{E_{DETCTR}}$ is sem. sec. cipher over $\\mathrm{(K,X^L,X^L)}$ . In particular, for any eff. adversary A attacking $\\mathrm{E_{DETCTR}}$ there exists an eff. PRF adversary B s.t.:$$\\mathrm{Adv_{SS}[A,E_{DETCTR}]=2\\cdot Adv_{PRF}[B,F]}$$该定理告诉我们，如果F是一个(K,X,X)上安全的PRF，那么 $\\mathrm{E_{DETCTR}}$ 就是 $\\mathrm{(K,X^L,X^L)}$ 上满足语义安全的密码（L是消息块数）。 此外，对于任意的有效攻击者A攻击 $\\mathrm{E_{DETCTR}}$ 密码，都存在一个有效的PRF攻击者B攻击 $\\mathrm{E_{DETCTR}}$ 底层的PRF，他们各自攻击的优势满足： $\\mathrm{Adv_{SS}[A,E_{DETCTR}]=2\\cdot Adv_{PRF}[B,F]}$ 。 当PRF是安全的PRF时，即 $\\mathrm{Adv_{PRF}[B,F]}$ is negligible。因此，$\\mathrm{Adv_{SS}[A,E_{DETCTR}]}$ 也必须是negligible的。 也可以通过下图直观的证明： Many-time Key块密码的另一种使用方式是many-time key方式，常用于文件系统，比如用同样的AES密钥加密许多文件；他也是IPsec的加密方式，相同的AES密钥用于加密许多IP包。 many-time key的方式是说，同一条密钥用于加密多条消息，在这种情况下，攻击者有选择明文攻击的能力，可以获得他任意选择明文消息对应的密文。而攻击者的目标同样是破坏语义安全。 many-time key: Adversary’s power: chosen-palintext attack(CPA): can obtain the encryption of arbitrary message of his choice. Adversary’s goal: break semantic security. Semantic Security(many-time key)挑战者随机选择密钥，以此确定加密解密密码。因为是many-time key的加密方式，攻击者可以发送多组消息：$\\mathrm{m_{i,0},m_{i,1}}$ 。定义如下两个实验，对于攻击者发出的每一个询问中，在EXP(0)中，挑战者加密 $\\mathrm{m_{i,0}}$ ，在EXP(1)中，挑战者加密 $\\mathrm{m_{i,1}}$ 。攻击者对于收到的密文，来判断该密文是对 $\\mathrm{m_{i,0},m_{i,1}}$ 那条消息加密的。 当攻击者想要指定明文 $\\mathrm{m}$ 的密文时，攻击者可以向挑战者发出询问，询问中 $\\mathrm{m_{j,0}=m_{j,1}=m}$ 。因此在many-time key的加密方式下，攻击者拥有选择明文攻击的能力。 定义many-time key 的语义安全： Def: E is sem. sec. under CPA if for all “efficient” A:$$\\mathrm{Adv_{CPA}[A,E]=|Pr[EXP(0)=1]-EXP(1)=1|}\\quad \\text{is neg.}$$ CPA Security如果 $\\mathrm{E(k,m)}$ 对消息 $\\mathrm{m}$ 总是输出相同的密文 $\\mathrm{m}$ ，那该密码就不是语义安全的。 攻击过程如下图，攻击者在第一个询问发送 $\\mathrm{m_0,m_0}$ ，得到 $\\mathrm{m_0}$ 的密文，在第二询问中发送 $\\mathrm{m_0,m_1}$ 。攻击者可以通过算法 A(output 0 if $\\mathrm{c=c_0}$) 破坏其语义安全。 在现实生活中，这样确定的加密算法(deterministic encryption)都不能达到语义安全，比如一个攻击者可以通过两个加密文件的内容是相同的，来判断这两份文件时相同的。 所以，对于many-time key的加密方式来说，如果给定相同的密文消息，加密算法必须输出不同的密文，才能保证语义安全。 Solution 1: randomized encryption第一种解决方案是 $\\mathrm{E(k,m)}$ 是一个随机算法(randomized algorithm)： 随机算法的好处是，每次加密相同的消息都能得到不同的密文。 但这样的性质需要满足，密文消息的长度比明文消息的长度长，即 CT-size = PT-size + #random bits 例如这样的加密算法：$\\mathrm{F:K\\times R \\rightarrow M}$ 是一个安全的PRF，$\\mathrm{E(k,m)=[r\\stackrel{R}\\longrightarrow, \\text{output}(r, F(k,r)\\oplus m)]}$ 就是一个在CPA攻击下语义安全的加密算法。（不过需要保证R的空间足够大，以此保证r不会重复） Solution 2: nonce-based Encryption第二种解决方案是基于nonce的加密算法。对于每一条消息nonce n都应该是不同的，即 $\\mathrm{(k,n)}$ 不会重复。 nonce可以是一个计数器(counter)，也可以是一个随机值(random nonce)。 当nonce是攻击者选择时（每个询问的nonce都是不同的），系统保证语义安全的定义： Def : nonce-based E is sem. sec. under CPA if for all “efficient” A:$$\\mathrm{Adv_{nCPA}=|Pr[EXP(0)=1]-Pr[EXP(1)=1]|}\\quad \\text{is neg.}$$ CBC一种many-time key方式的块密码是CBC（Cipher Block Chaining）。 CBC with random IVLet $\\mathrm{(E,D)}$ be a PRP. $\\mathrm{E_{CBC}(k,m)}$ : choose random $\\mathrm{IV\\in X}$ and do: IV是随机的，因此要把IV的值放在密文中一起发送给解密方。 解密的电路如下： CBC: CPA Analysis定理：对于任意块 L&gt;0（L就是每个消息能具备的最大块数），如果E在(K,X)上是一个安全的PRP，那么在CPA攻击下 $\\mathrm{E_{CBC}} $ $\\mathrm{=(K,X^L,X^{L+1})}$ 就是一个具备语义安全的密码。如果有一个攻击 $\\mathrm{E_{CBC}}$ 的攻击者A，发起q个询问（q就是最大消息数），那么存在一个攻击其 PRP的攻击者B，满足：$$\\mathrm{Adv_{CPA}[A,E_{CBC}]\\le2\\cdot Adv_{PRP}[B,E] +2q^2L^2/|X|}$$ CBC Theorem: For any L&gt;0, if E is a secure PRP over (K,X) then $\\mathrm{E_{CBC}} $ is a sem. sec. under CPA over $\\mathrm{(K,X^L,X^{L+1})}$ . In particular, for a q-query adversary A attacking $\\mathrm{E_{CBC}} $ , there exists a PRP adversary B s.t.:$$\\mathrm{Adv_{CPA}[A,E_{CBC}]\\le2\\cdot Adv_{PRP}[B,E] +2q^2L^2/|X|}$$q = # messages encrypted with k, L = length of max message PRP是安全的，$\\mathrm{ Adv_{PRP}[B,E]}$ 是neg.，因此只有当 $\\mathrm{q^2L^2&lt;&lt;|X|}$ 时，CBC才是安全的。 Example： 假设我们希望 $\\mathrm{Adv_{CPA}[A,E_{CBC}]}\\le1/2^{32}$ ，即需要满足 $\\mathrm{2q^2L^2/|X|\\le1/2^{32}}$ ： 对于AES来说，$\\mathrm{|X|=2^{128}}$ ，所以 $\\mathrm{qL\\lt 2^{48}}$ 。 因此，在 $2^{48}$ 个AES块后，必须改变密钥。 而对于DES来说，$\\mathrm{|X|=2^{64}}$ ，所以 $\\mathrm{qL\\lt 2^{16}}$ 。 Warning: an attack on CBC with random IV当攻击者可以预测IV时，CBC就不再具备CPA-secure了。 假设攻击者对于给定的密文$\\mathrm{c\\leftarrow E_{CBC}(k,m)}$可以预测下一条消息的IV。 比如在SSL/TLS1.1中的一个bug：第i-1条报文的最后一个密文块就是第i条报文的IV。 CPA attack: 第一次询问 攻击者发送两条消息 $\\mathrm{m_0=m_1=0}\\in \\mathrm{X}$ 挑战者用 $\\mathrm{c_1=[IV_1, E(k,0\\oplus IV_1)]}$ 响应 攻击者通过密文预测下一条消息的IV 第二次询问 攻击者发送两条消息 $\\mathrm{m_0=IV\\oplus IV_1,m_1\\ne m_0}$ 挑战者的密文 $\\mathrm{c=[IV,E(k,IV_1)]\\quad or \\quad c=[IV,E(k, m_1\\oplus IV)]}$ 攻击者的统计算法 A: output 0 if $\\mathrm{c[1]=c_1[1]}$ 所以IV必须是随机的。 nonce-based CBCCBC的另一种使用方式是基于unique nonce。 在这种方式中，密钥由一组密钥对组成 key = (k, k1)，k1是用来加密nonce的。 unique nonce的含义是，nonce可以不是随机的，但对于每一条消息，(key, nonce)对都必须是不同的。 所以，对于用户提供的非随机nonce，必须经过加密。 在这种方式中，一个非常重要但步骤就是使用 k1来加密nonce。 如果把nonce当作IV使用（忘掉k1加密nonce的步骤），CBC就没有CPA-secure。 如果k1=k，CBC也没有CPA-secure（IV=0的模式）。 A CBC technicality: padding对于消息长度未能达到块的整数倍，需要padding技术进行填充消息。 在TLS中的填充方法是，对于需要填充n(n&gt;0)个bytes的消息块，n-byte pad中的每个字节都是n。 如果不需要填充，就需要添加一个dummy block，该块中的每个字节都是16。 CTR另一种方式是CTR。 rand ctr-modeLet F be a secure PRF. $\\mathrm{E_{CBC}(k,m)}$ : choose random $\\mathrm{IV\\in X}$ and do: CTR模式的最大优点是可以并行计算。 其次，因为CTR的加密其实是流密码的形式，所以只需要要求F是一个安全的PRF，不需要其可逆。 nonce ctr-mode为例保证F(k,x)只会使用一次，对每条消息选择一个64 bits的nonce，同时使用一个64 bits的counter。 CTR: CPA analysisCTR Theorem: For any L&gt;0, if F is a secure PRF over (K, X, X) then $\\mathrm{E_{CTR}} $ is a sem. sec. under CPA over $\\mathrm{(K,X^L,X^{L+1})}$ . In particular, for a q-query adversary A attacking $\\mathrm{E_{CTR}} $ , there exists a PRF adversary B s.t.:$$\\mathrm{Adv_{CPA}[A,E_{CTR}]\\le2\\cdot Adv_{PRF}[B,F] +2q^2L/|X|}$$q = # messages encrypted with k, L = length of max message CTR-mode中，只有当 $\\mathrm{q^2L}&lt;&lt;|X|$ ，才具备CPA-secure，比CBC好。 Example： 假设我们希望 $\\mathrm{Adv_{CPA}[A,E_{CTR}]}\\le1/2^{32}$ ，即需要满足 $\\mathrm{2q^2L/|X|\\le1/2^{32}}$ ： 对于AES来说，$\\mathrm{|X|=2^{128}}$ ，所以 $\\mathrm{qL^{1/2}\\lt 2^{48}}$ 。 因此，在 $2^{32}$ 条消息后（每条消息有 $2^{32}$个 AES块），必须改变密钥。 CTR v.s. CBC","link":"/2021/09/13/stanford-crypto-blockcipher2/"},{"title":"「区块链」：Solidity-advanced","text":"Solidity的官方教程笔记：advanced。 Part 1第1章: 智能协议的永固性到现在为止，我们讲的 Solidity 和其他语言没有质的区别，它长得也很像 JavaScript。 但是，在有几点以太坊上的 DApp 跟普通的应用程序有着天壤之别。 第一个例子，在你把智能协议传上以太坊之后，它就变得不可更改, 这种永固性意味着你的代码永远不能被调整或更新。 你编译的程序会一直，永久的，不可更改的，存在以太坊上。这就是 Solidity 代码的安全性如此重要的一个原因。如果你的智能协议有任何漏洞，即使你发现了也无法补救。你只能让你的用户们放弃这个智能协议，然后转移到一个新的修复后的合约上。 但这恰好也是智能合约的一大优势。代码说明一切。如果你去读智能合约的代码，并验证它，你会发现，一旦函数被定义下来，每一次的运行，程序都会严格遵照函数中原有的代码逻辑一丝不苟地执行，完全不用担心函数被人篡改而得到意外的结果。 外部依赖关系在第2课中，我们将加密小猫（CryptoKitties）合约的地址硬编码到 DApp 中去了。有没有想过，如果加密小猫出了点问题，比方说，集体消失了会怎么样？ 虽然这种事情几乎不可能发生，但是，如果小猫没了，我们的 DApp 也会随之失效 – 因为我们在 DApp 的代码中用“硬编码”的方式指定了加密小猫的地址，如果这个根据地址找不到小猫，我们的僵尸也就吃不到小猫了，而按照前面的描述，我们却没法修改合约去应付这个变化！ 我们不能硬编码，而要采用“函数”，以便于 DApp 的关键部分可以以参数形式修改。 我们不再一开始就把猎物地址给写入代码，而是写个函数 setKittyContractAddress, 运行时再设定猎物的地址，这样我们就可以随时去锁定新的猎物，也不用担心加密小猫集体消失了。 第2章: Ownable ContractsOpenZeppelin库的Ownable 合约OpenZeppelin 是主打安保和社区审查的智能合约库，您可以在自己的 DApps中引用。等把这一课学完，您不要催我们发布下一课，最好利用这个时间把 OpenZeppelin 的网站看看 1234567891011121314151617181920212223242526272829303132333435/** * @title Ownable * @dev The Ownable contract has an owner address, and provides basic authorization control * functions, this simplifies the implementation of &quot;user permissions&quot;. */contract Ownable { address public owner; event OwnershipTransferred(address indexed previousOwner, address indexed newOwner); /** * @dev The Ownable constructor sets the original `owner` of the contract to the sender * account. */ function Ownable() public { owner = msg.sender; } /** * @dev Throws if called by any account other than the owner. */ modifier onlyOwner() { require(msg.sender == owner); _; } /** * @dev Allows the current owner to transfer control of the contract to a newOwner. * @param newOwner The address to transfer ownership to. */ function transferOwnership(address newOwner) public onlyOwner { require(newOwner != address(0)); OwnershipTransferred(owner, newOwner); owner = newOwner; }} 下面有没有您没学过的东东？ 构造函数：function Ownable()是一个 constructor (构造函数)，构造函数不是必须的，它与合约同名，构造函数一生中唯一的一次执行，就是在合约最初被创建的时候。 函数修饰符：modifier onlyOwner()。 修饰符跟函数很类似，不过是用来修饰其他已有函数用的， 在其他语句执行前，为它检查下先验条件。 在这个例子中，我们就可以写个修饰符 onlyOwner 检查下调用者，确保只有合约的主人才能运行本函数。我们下一章中会详细讲述修饰符，以及那个奇怪的_;。 indexed 关键字：别担心，我们还用不到它。 所以Ownable 合约基本都会这么干： 合约创建，构造函数先行，将其 owner 设置为msg.sender（其部署者） 为它加上一个修饰符 onlyOwner，它会限制陌生人的访问，将访问某些函数的权限锁定在 owner 上。 允许将合约所有权转让给他人。 onlyOwner 简直人见人爱，大多数人开发自己的 Solidity DApps，都是从复制/粘贴 Ownable 开始的，从它再继承出的子类，并在之上进行功能开发。 既然我们想把 setKittyContractAddress 限制为 onlyOwner ，我们也要做同样的事情。 第3章: onlyOwner 函数修饰符现在我们有了个基本版的合约 ZombieFactory 了，它继承自 Ownable 接口，我们也可以给 ZombieFeeding 加上 onlyOwner 函数修饰符。 ZombieFeeding 是个 ZombieFactory ZombieFactory 是个 Ownable 因此 ZombieFeeding 也是个 Ownable, 并可以通过 Ownable 接口访问父类中的函数/事件/修饰符。往后，ZombieFeeding 的继承者合约们同样也可以这么延续下去。 函数修饰符函数修饰符看起来跟函数没什么不同，不过关键字modifier 告诉编译器，这是个modifier(修饰符)，而不是个function(函数)。它不能像函数那样被直接调用，只能被添加到函数定义的末尾，用以改变函数的行为。 咱们仔细读读 onlyOwner: 1234567/** * @dev 调用者不是‘主人’，就会抛出异常 */modifier onlyOwner() { require(msg.sender == owner); _;} onlyOwner 函数修饰符是这么用的： 12345678contract MyContract is Ownable { event LaughManiacally(string laughter); //注意！ `onlyOwner`上场 : function likeABoss() external onlyOwner { LaughManiacally(&quot;Muahahahaha&quot;); }} 注意 likeABoss 函数上的 onlyOwner 修饰符。 当你调用 likeABoss 时： 首先执行 onlyOwner 中的代码。 执行到 onlyOwner 中的 _; 语句时，程序再返回并执行 likeABoss 中的代码。 可见，尽管函数修饰符也可以应用到各种场合，但最常见的还是放在函数执行之前添加快速的 require检查。 因为给函数添加了修饰符 onlyOwner，使得唯有合约的主人（也就是部署者）才能调用它。 注意：主人对合约享有的特权当然是正当的，不过也可能被恶意使用。比如，万一，主人添加了个后门，允许他偷走别人的僵尸呢？ 所以非常重要的是，部署在以太坊上的 DApp，并不能保证它真正做到去中心，你需要阅读并理解它的源代码，才能防止其中没有被部署者恶意植入后门；作为开发人员，如何做到既要给自己留下修复 bug 的余地，又要尽量地放权给使用者，以便让他们放心你，从而愿意把数据放在你的 DApp 中，这确实需要个微妙的平衡。 第4章: Gas现在我们懂了如何在禁止第三方修改我们的合约的同时，留个后门给咱们自己去修改。 让我们来看另一种使得 Solidity 编程语言与众不同的特征： Gas - 驱动以太坊DApps的能源在 Solidity 中，你的用户想要每次执行你的 DApp 都需要支付一定的 gas，gas 可以用以太币购买，因此，用户每次跑 DApp 都得花费以太币。 一个 DApp 收取多少 gas 取决于功能逻辑的复杂程度。每个操作背后，都在计算完成这个操作所需要的计算资源，（比如，存储数据就比做个加法运算贵得多）， 一次操作所需要花费的 gas 等于这个操作背后的所有运算花销的总和。 由于运行你的程序需要花费用户的真金白银，在以太坊中代码的编程语言，比其他任何编程语言都更强调优化。 为什么要用 gas 来驱动？以太坊就像一个巨大、缓慢、但非常安全的电脑。当你运行一个程序的时候，网络上的每一个节点都在进行相同的运算，以验证它的输出 —— 这就是所谓的“去中心化” 由于数以千计的节点同时在验证着每个功能的运行，这可以确保它的数据不会被被监控，或者被刻意修改。 可能会有用户用无限循环堵塞网络，抑或用密集运算来占用大量的网络资源，为了防止这种事情的发生，以太坊的创建者为以太坊上的资源制定了价格，想要在以太坊上运算或者存储，你需要先付费。 注意：如果你使用侧链，倒是不一定需要付费，比如咱们在 Loom Network 上构建的 CryptoZombies 就免费。你不会想要在以太坊主网上玩儿“魔兽世界”吧？ - 所需要的 gas 可能会买到你破产。但是你可以找个算法理念不同的侧链来玩它。我们将在以后的课程中咱们会讨论到，什么样的 DApp 应该部署在太坊主链上，什么又最好放在侧链。 省 gas 的招数：结构封装 （Struct packing）在第1课中，我们提到除了基本版的 uint 外，还有其他变种 uint：uint8，uint16，uint32等。 通常情况下我们不会考虑使用 uint 变种，因为无论如何定义 uint的大小，Solidity 为它保留256位的存储空间。例如，使用 uint8 而不是uint（uint256）不会为你节省任何 gas。 除非，把 uint 绑定到 struct 里面。 如果一个 struct 中有多个 uint，则尽可能使用较小的 uint, Solidity 会将这些 uint 打包在一起，从而占用较少的存储空间。例如： 123456789101112131415struct NormalStruct { uint a; uint b; uint c;}struct MiniMe { uint32 a; uint32 b; uint c;}// 因为使用了结构打包，`mini` 比 `normal` 占用的空间更少NormalStruct normal = NormalStruct(10, 20, 30);MiniMe mini = MiniMe(10, 20, 30); 所以，当 uint 定义在一个 struct 中的时候，尽量使用最小的整数子类型以节约空间。 并且把同样类型的变量放一起（即在 struct 中将把变量按照类型依次放置），这样 Solidity 可以将存储空间最小化。例如，有两个 struct： 1uint c; uint32 a; uint32 b;` 和 `uint32 a; uint c; uint32 b; 前者比后者需要的gas更少，因为前者把uint32放一起了。 第5章: 时间单位level 属性表示僵尸的级别。以后，在我们创建的战斗系统中，打胜仗的僵尸会逐渐升级并获得更多的能力。 readyTime 稍微复杂点。我们希望增加一个“冷却周期”，表示僵尸在两次猎食或攻击之之间必须等待的时间。如果没有它，僵尸每天可能会攻击和繁殖1,000次，这样游戏就太简单了。 为了记录僵尸在下一次进击前需要等待的时间，我们使用了 Solidity 的时间单位。 时间单位Solidity 使用自己的本地时间单位。 变量now: 变量 now 将返回当前的unix时间戳（自1970年1月1日以来经过的秒数）。我写这句话时 unix 时间是 1604319686。 注意：Unix时间传统用一个32位的整数进行存储。这会导致“2038年”问题，当这个32位的unix时间戳不够用，产生溢出，使用这个时间的遗留系统就麻烦了。所以，如果我们想让我们的 DApp 跑够20年，我们可以使用64位整数表示时间，但为此我们的用户又得支付更多的 gas。真是个两难的设计啊！ 时间单位：seconds minutes hours days weeks years Solidity 还包含秒(seconds)，分钟(minutes)，小时(hours)，天(days)，周(weeks) 和 年(years) 等时间单位。它们都会转换成对应的秒数放入 uint 中。所以 1分钟 就是 60，1小时是 3600（60秒×60分钟），1天是86400（24小时×60分钟×60秒） 123456789101112uint lastUpdated;// 将‘上次更新时间’ 设置为 ‘现在’function updateTimestamp() public { lastUpdated = now;}// 如果到上次`updateTimestamp` 超过5分钟，返回 'true'// 不到5分钟返回 'false'function fiveMinutesHavePassed() public view returns (bool) { return (now &gt;= (lastUpdated + 5 minutes));} 有了这些工具，我们可以为僵尸设定“冷静时间”功能。 第6章: 僵尸冷却首先，我们要定义一些辅助函数，设置并检查僵尸的 readyTime。 将结构体作为参数传入由于结构体的存储指针可以以参数的方式传递给一个 private 或 internal 的函数，因此结构体可以在多个函数之间相互传递。 把结构体作为指针，因此传递参数前需要加storage 遵循这样的语法： 123function _doStuff(Zombie storage _zombie) internal { // do stuff with _zombie} 这样我们可以将某僵尸的引用直接传递给一个函数，而不用是通过参数传入僵尸ID后，函数再依据ID去查找。 第7章: 公有函数和安全性你必须仔细地检查所有声明为 public 和 external的函数，一个个排除用户滥用它们的可能，谨防安全漏洞。请记住，如果这些函数没有类似 onlyOwner 这样的函数修饰符，用户能利用各种可能的参数去调用它们。 第8章: 进一步了解函数修饰符我们打算让僵尸在达到一定水平后，获得特殊能力。但是达到这个小目标，我们还需要学一学什么是“函数修饰符”。 带参数的函数修饰符之前我们已经读过一个简单的函数修饰符了：onlyOwner。函数修饰符也可以带参数。例如： 1234567891011121314// 存储用户年龄的映射mapping (uint =&gt; uint) public age;// 限定用户年龄的修饰符modifier olderThan(uint _age, uint _userId) { require(age[_userId] &gt;= _age); _;}// 必须年满16周岁才允许开车 (至少在美国是这样的).// 我们可以用如下参数调用`olderThan` 修饰符:function driveCar(uint _userId) public olderThan(16, _userId) { // 其余的程序逻辑} 看到了吧， olderThan 修饰符可以像函数一样接收参数，是“宿主”函数 driveCar 把参数传递给它的修饰符的。 来，我们自己生产一个修饰符，通过传入的level参数来限制僵尸使用某些特殊功能。 带参数的函数修饰符之前我们已经读过一个简单的函数修饰符了：onlyOwner。函数修饰符也可以带参数。例如： 1234567891011121314// 存储用户年龄的映射mapping (uint =&gt; uint) public age;// 限定用户年龄的修饰符modifier olderThan(uint _age, uint _userId) { require(age[_userId] &gt;= _age); _;}// 必须年满16周岁才允许开车 (至少在美国是这样的).// 我们可以用如下参数调用`olderThan` 修饰符:function driveCar(uint _userId) public olderThan(16, _userId) { // 其余的程序逻辑} 看到了吧， olderThan 修饰符可以像函数一样接收参数，是“宿主”函数 driveCar 把参数传递给它的修饰符的。 来，我们自己生产一个修饰符，通过传入的level参数来限制僵尸使用某些特殊功能。 记住，修饰符的最后一行为 _;，表示修饰符调用结束后返回，并执行调用函数余下的部分。 第10章: 利用 ‘View’ 函数节省 Gas现在需要添加的一个功能是：我们的 DApp 需要一个方法来查看某玩家的整个僵尸军团 - 我们称之为 getZombiesByOwner。 实现这个功能只需从区块链中读取数据，所以它可以是一个 view 函数。这让我们不得不回顾一下“gas优化”这个重要话题。 “view” 函数不花 “gas”当玩家从外部调用一个view函数，是不需要支付一分 gas 的。 这是因为 view 函数不会真正改变区块链上的任何数据 - 它们只是读取。因此用 view 标记一个函数，意味着告诉 web3.js，运行这个函数只需要查询你的本地以太坊节点，而不需要在区块链上创建一个事务（事务需要运行在每个节点上，因此花费 gas）。 稍后我们将介绍如何在自己的节点上设置 web3.js。但现在，你关键是要记住，在所能只读的函数上标记上表示“只读”的“external view 声明，就能为你的玩家减少在 DApp 中 gas 用量。 注意：如果一个 view 函数在另一个函数的内部被调用，而调用函数与 view 函数的不属于同一个合约，也会产生调用成本。这是因为如果主调函数在以太坊创建了一个事务，它仍然需要逐个节点去验证。所以标记为 view 的函数只有在外部调用时才是免费的。 第11章: 存储非常昂贵Solidity 使用storage(存储)是相当昂贵的，”写入“操作尤其贵。 这是因为，无论是写入还是更改一段数据， 这都将永久性地写入区块链。”永久性“啊！需要在全球数千个节点的硬盘上存入这些数据，随着区块链的增长，拷贝份数更多，存储量也就越大。这是需要成本的！ 为了降低成本，不到万不得已，避免将数据写入存储。这也会导致效率低下的编程逻辑 - 比如每次调用一个函数，都需要在 memory(内存) 中重建一个数组，而不是简单地将上次计算的数组给存储下来以便快速查找。 遍历大数据集合都是昂贵的。但是在 Solidity 中，使用一个标记了external view的函数，遍历比 storage 要便宜太多，因为 view 函数不会产生任何花销。 （gas可是真金白银啊！）。 我们将在下一章讨论for循环，现在我们来看一下看如何如何在内存中声明数组。 在内存中声明数组在数组后面加上 memory关键字， 表明这个数组是仅仅在内存中创建，不需要写入外部存储，并且在函数调用结束时它就解散了。与在程序结束时把数据保存进 storage 的做法相比，内存运算可以大大节省gas开销 – 把这数组放在view里用，完全不用花钱。 12345678910function getArray() external pure returns(uint[]) { // 初始化一个长度为3的内存数组 uint[] memory values = new uint[](3); // 赋值 values.push(1); values.push(2); values.push(3); // 返回数组 return values;} 这个小例子展示了一些语法规则，下一章中，我们将通过一个实际用例，展示它和 for 循环结合的做法。 注意：内存数组 必须 用长度参数（在本例中为3）创建。目前不支持 array.push()之类的方法调整数组大小，在未来的版本可能会支持长度修改。 第12章: For 循环我们提到过，函数中使用的数组是运行时在内存中通过 for 循环实时构建，而不是预先建立在存储中的。 为什么要这样做呢？ 为了实现 getZombiesByOwner 函数，一种“无脑式”的解决方案是在 ZombieFactory 中存入”主人“和”僵尸军团“的映射。 1mapping (address =&gt; uint[]) public ownerToZombies 然后我们每次创建新僵尸时，执行 ownerToZombies [owner] .push（zombieId） 将其添加到主人的僵尸数组中。而 getZombiesByOwner 函数也非常简单： 123function getZombiesByOwner(address _owner) external view returns (uint[]) { return ownerToZombies[_owner];} 这个做法有问题做法倒是简单。可是如果我们需要一个函数来把一头僵尸转移到另一个主人名下（我们一定会在后面的课程中实现的），又会发生什么？ 这个“换主”函数要做到： 1.将僵尸push到新主人的 ownerToZombies 数组中， 2.从旧主的 ownerToZombies 数组中移除僵尸， 3.将旧主僵尸数组中“换主僵尸”之后的的每头僵尸都往前挪一位，把挪走“换主僵尸”后留下的“空槽”填上， 4.将数组长度减1。 但是第三步实在是太贵了！因为每挪动一头僵尸，我们都要执行一次写操作。如果一个主人有20头僵尸，而第一头被挪走了，那为了保持数组的顺序，我们得做19个写操作。 由于写入存储是 Solidity 中最费 gas 的操作之一，使得换主函数的每次调用都非常昂贵。更糟糕的是，每次调用的时候花费的 gas 都不同！具体还取决于用户在原主军团中的僵尸头数，以及移走的僵尸所在的位置。以至于用户都不知道应该支付多少 gas。 注意：当然，我们也可以把数组中最后一个僵尸往前挪来填补空槽，并将数组长度减少一。但这样每做一笔交易，都会改变僵尸军团的秩序。 由于从外部调用一个 view 函数是免费的，我们也可以在 getZombiesByOwner 函数中用一个for循环遍历整个僵尸数组，把属于某个主人的僵尸挑出来构建出僵尸数组。那么我们的 transfer 函数将会便宜得多，因为我们不需要挪动存储里的僵尸数组重新排序，总体上这个方法会更便宜，虽然有点反直觉。 使用 for 循环for循环的语法在 Solidity 和 JavaScript 中类似。 来看一个创建偶数数组的例子： 12345678910111213141516function getEvens() pure external returns(uint[]) { uint[] memory evens = new uint[](5); // 在新数组中记录序列号 uint counter = 0; // 在循环从1迭代到10： for (uint i = 1; i &lt;= 10; i++) { // 如果 `i` 是偶数... if (i % 2 == 0) { // 把它加入偶数数组 evens[counter] = i; //索引加一， 指向下一个空的‘even’ counter++; } } return evens;} 这个函数将返回一个形为 [2,4,6,8,10] 的数组。 part 2第1章: 可支付截至目前，我们只接触到很少的 函数修饰符。 要记住所有的东西很难，所以我们来个概览： 我们有决定函数何时和被谁调用的可见性修饰符: private 意味着它只能被合约内部调用； internal 就像 private 但是也能被继承的合约调用； external 只能从合约外部调用；最后 public 可以在任何地方调用，不管是内部还是外部。 我们也有状态修饰符， 告诉我们函数如何和区块链交互: view 告诉我们运行这个函数不会更改和保存任何数据； pure 告诉我们这个函数不但不会往区块链写数据，它甚至不从区块链读取数据。这两种在被从合约外部调用的时候都不花费任何gas（但是它们在被内部其他函数调用的时候将会耗费gas）。 然后我们有了自定义的 modifiers，例如在第三课学习的: onlyOwner 和 aboveLevel。 对于这些修饰符我们可以自定义其对函数的约束逻辑。 这些修饰符可以同时作用于一个函数定义上： 1function test() external view onlyOwner anotherModifier { /* ... */ } 在这一章，我们来学习一个新的修饰符 payable. payable 修饰符payable 方法是让 Solidity 和以太坊变得如此酷的一部分 —— 它们是一种可以接收以太的特殊函数。 当你在调用一个普通网站服务器上的API函数的时候，你无法用你的函数传送美元——你也不能传送比特币。 但是在以太坊中， 因为钱 (以太), 数据 (事务负载)， 以及合约代码本身都存在于以太坊。你可以在同时调用函数 并付钱给另外一个合约。 这就允许出现很多有趣的逻辑， 比如向一个合约要求支付一定的钱来运行一个函数。 12345678contract OnlineStore { function buySomething() external payable { // 检查以确定0.001以太发送出去来运行函数: require(msg.value == 0.001 ether); // 如果为真，一些用来向函数调用者发送数字内容的逻辑 transferThing(msg.sender); }} msg.value 可以查看msg.sender 向合约发送了多少以太的方法，另外 ether 是一个内置单位。 这里发生的事是，一些人会从 web3.js 调用这个函数 (从DApp的前端)， 像这样 : 12// 假设 `OnlineStore` 在以太坊上指向你的合约:OnlineStore.buySomething().send(from: web3.eth.defaultAccount, value: web3.utils.toWei(0.001)) 注意这个 value 字段， JavaScript 调用来指定发送多少(0.001)以太。如果把事务想象成一个信封，你发送到函数的参数就是信的内容。 添加一个 value 很像在信封里面放钱 —— 信件内容和钱同时发送给了接收者。 注意： 如果一个函数没标记为payable， 而你尝试利用上面的方法发送以太，函数将拒绝你的事务。 第2章: 提现在你发送以太之后，它将被存储进该合约的以太坊账户中， 并冻结在哪里 —— 除非你添加一个函数来从合约中把以太提现。 你可以写一个函数来从合约中提现以太，类似这样： 12345contract GetPaid is Ownable { function withdraw() external onlyOwner { owner.transfer(this.balance); }} 注意我们使用 Ownable 合约中的 owner 和 onlyOwner，假定它已经被引入了。 你可以通过 transfer 函数向一个地址发送以太， 然后 this.balance 将返回当前合约存储了多少以太。 所以如果100个用户每人向我们支付1以太， this.balance 将是100以太。 你可以通过 transfer 向任何以太坊地址付钱。 比如，你可以有一个函数在 msg.sender 超额付款的时候给他们退钱： 12uint itemFee = 0.001 ether;msg.sender.transfer(msg.value - itemFee); 或者在一个有买家和卖家的合约中， 你可以把卖家的地址存储起来， 当有人买了它的东西的时候，把买家支付的钱发送给它 seller.transfer(msg.value)。 有很多例子来展示什么让以太坊编程如此之酷 —— 你可以拥有一个不被任何人控制的去中心化市场。 第3章: 僵尸战斗第4章: 随机数优秀的游戏都需要一些随机元素，那么我们在 Solidity 里如何生成随机数呢？ 真正的答案是你不能，或者最起码，你无法安全地做到这一点。 用 keccak256 来制造随机数。SHA-3第三代安全散列算法(Secure Hash Algorithm 3)，之前名为Keccak（念作/ˈkɛtʃæk/或/kɛtʃɑːk/)）算法. Solidity 中最好的随机数生成器是 keccak256 哈希函数. 我们可以这样来生成一些随机数 12345// 生成一个0到100的随机数:uint randNonce = 0;uint random = uint(keccak256(now, msg.sender, randNonce)) % 100;randNonce++;uint random2 = uint(keccak256(now, msg.sender, randNonce)) % 100; 这个方法首先拿到 now 的时间戳、 msg.sender、 以及一个自增数 nonce （一个仅会被使用一次的数，这样我们就不会对相同的输入值调用一次以上哈希函数了）。 然后利用 keccak 把输入的值转变为一个哈希值, 再将哈希值转换为 uint, 然后利用 % 100 来取最后两位, 就生成了一个0到100之间随机数了。 这个方法很容易被不诚实的节点攻击在以太坊上, 当你在和一个合约上调用函数的时候, 你会把它广播给一个节点或者在网络上的 transaction\\ 节点们。 网络上的节点将收集很多事务, 试着成为第一个解决计算密集型数学问题的人，作为“工作证明”，然后将“工作证明”(Proof of Work, PoW)和事务一起作为一个 block\\ 发布在网络上。 一旦一个节点解决了一个PoW, 其他节点就会停止尝试解决这个 PoW, 并验证其他节点的事务列表是有效的，然后接受这个节点转而尝试解决下一个节点。 这就让我们的随机数函数变得可利用了 我们假设我们有一个硬币翻转合约——正面你赢双倍钱，反面你输掉所有的钱。假如它使用上面的方法来决定是正面还是反面 (random &gt;= 50 算正面, random &lt; 50 算反面)。 如果我正运行一个节点，我可以 只对我自己的节点 发布一个事务，且不分享它。 我可以运行硬币翻转方法来偷窥我的输赢 — 如果我输了，我就不把这个事务包含进我要解决的下一个区块中去。我可以一直运行这个方法，直到我赢得了硬币翻转并解决了下一个区块，然后获利。 所以我们该如何在以太坊上安全地生成随机数呢因为区块链的全部内容对所有参与者来说是透明的， 这就让这个问题变得很难，它的解决方法不在本课程讨论范围，你可以阅读 这个 StackOverflow 上的讨论 来获得一些主意。 一个方法是利用 oracle\\ 来访问以太坊区块链之外的随机数函数。 因为网络上成千上万的以太坊节点都在竞争解决下一个区块，我能成功解决下一个区块的几率非常之低。 这将花费我们巨大的计算资源来开发这个获利方法 — 但是如果奖励异常地高(比如我可以在硬币翻转函数中赢得 1个亿)， 那就很值得去攻击了。 所以尽管这个方法在以太坊上不安全，在实际中，除非我们的随机函数有一大笔钱在上面，你游戏的用户一般是没有足够的资源去攻击的。 第5章: 僵尸对战第6章: 重构通用逻辑第7章: 更多重构第8章: 回到攻击！第9章: 僵尸的输赢对我们的僵尸游戏来说，我们将要追踪我们的僵尸输赢了多少场。有了这个我们可以在游戏里维护一个 “僵尸排行榜”。 有多种方法在我们的DApp里面保存一个数值 — 作为一个单独的映射，作为一个“排行榜”结构体，或者保存在 Zombie 结构体内。 第10章: 僵尸胜利了 😄第11章: 僵尸失败 😞在我们的游戏中，僵尸输了后并不会降级 —— 只是简单地给 lossCount 加一，并触发冷却，等待一天后才能再次参战。 实现这个逻辑，我们需要一个 else 语句。 else 语句和 JavaScript 以及很多其他语言的 else 语句一样。 12345if (zombieCoins[msg.sender] &gt; 100000000) { // 你好有钱!!!} else { // 我们需要更多的僵尸币...} Part 3第1章: 以太坊上的代币让我们来聊聊 代币.(tokens) 如果你对以太坊的世界有一些了解，你很可能听过人们聊到代币——尤其是 ERC20 代币\\. A token\\ on Ethereum is basically just a smart contract that follows some common rules — namely it implements a standard set of functions that all other token contracts share, such as transferFrom(address _from, address _to, uint256 _tokenId) and balanceOf(address _owner). 一个代币就是一个智能合约。 一个 代币 在以太坊基本上就是一个遵循一些共同规则的智能合约——即它实现了所有其他代币合约共享的一组标准函数，例如 transfer(address _to, uint256 _value) 和 balanceOf(address _owner). 在智能合约内部，通常有一个映射， mapping(address =&gt; uint256) balances，用于追踪每个地址还有多少余额。 所以基本上一个代币只是一个追踪谁拥有多少该代币的合约，和一些可以让那些用户将他们的代币转移到其他地址的函数。 它为什么重要呢？由于所有 ERC20 代币共享具有相同名称的同一组函数，它们都可以以相同的方式进行交互。 这意味着如果你构建的应用程序能够与一个 ERC20 代币进行交互，那么它就也能够与任何 ERC20 代币进行交互。 这样一来，将来你就可以轻松地将更多的代币添加到你的应用中，而无需进行自定义编码。 你可以简单地插入新的代币合约地址，然后哗啦，你的应用程序有另一个它可以使用的代币了。 其中一个例子就是交易所。 当交易所添加一个新的 ERC20 代币时，实际上它只需要添加与之对话的另一个智能合约。 用户可以让那个合约将代币发送到交易所的钱包地址，然后交易所可以让合约在用户要求取款时将代币发送回给他们。 交易所只需要实现这种转移逻辑一次，然后当它想要添加一个新的 ERC20 代币时，只需将新的合约地址添加到它的数据库即可。 其他代币标准对于像货币一样的代币来说，ERC20 代币非常酷。 但是要在我们僵尸游戏中代表僵尸就并不是特别有用。 首先，僵尸不像货币可以分割 —— 我可以发给你 0.237 以太，但是转移给你 0.237 的僵尸听起来就有些搞笑。 其次，并不是所有僵尸都是平等的。 你的2级僵尸”Steve“完全不能等同于我732级的僵尸”H4XF13LD MORRIS 💯💯😎💯💯“。（你差得远呢，Steve）。 有另一个代币标准更适合如 CryptoZombies 这样的加密收藏品——它们被称为ERC721 代币.\\ ERC721 代币\\是不能互换的，因为每个代币都被认为是唯一且不可分割的。 你只能以整个单位交易它们，并且每个单位都有唯一的 ID。 这些特性正好让我们的僵尸可以用来交易。 请注意，使用像 ERC721 这样的标准的优势就是，我们不必在我们的合约中实现拍卖或托管逻辑，这决定了玩家能够如何交易／出售我们的僵尸。 如果我们符合规范，其他人可以为加密可交易的 ERC721 资产搭建一个交易所平台，我们的 ERC721 僵尸将可以在该平台上使用。 所以使用代币标准相较于使用你自己的交易逻辑有明显的好处。 第2章: ERC721 标准, 多重继承让我们来看一看 ERC721 标准： 12345678910contract ERC721 { event Transfer(address indexed _from, address indexed _to, uint256 _tokenId); event Approval(address indexed _owner, address indexed _approved, uint256 _tokenId); function balanceOf(address _owner) public view returns (uint256 _balance); function ownerOf(uint256 _tokenId) public view returns (address _owner); function transfer(address _to, uint256 _tokenId) public; function approve(address _to, uint256 _tokenId) public; function takeOwnership(uint256 _tokenId) public;} 这是我们需要实现的方法列表，我们将在接下来的章节中逐个学习。 注意： ERC721目前是一个 草稿**，还没有正式商定的实现。在本教程中，我们使用的是 OpenZeppelin 库中的当前版本，但在未来正式发布之前它可能会有更改。 所以把这 一个 可能的实现当作考虑，但不要把它作为 ERC721 代币的官方标准。 实现一个代币合约在实现一个代币合约的时候，我们首先要做的是将接口复制到它自己的 Solidity 文件并导入它，import &quot;./erc721.sol&quot;;。 接着，让我们的合约继承它，然后我们用一个函数定义来重写每个方法。 幸运的是在Solidity，你的合约可以继承自多个合约，参考如下： 123contract SatoshiNakamoto is NickSzabo, HalFinney { // 啧啧啧，宇宙的奥秘泄露了} 正如你所见，当使用多重继承的时候，你只需要用逗号 , 来隔开几个你想要继承的合约。在上面的例子中，我们的合约继承自 NickSzabo 和 HalFinney。 第3章: balanceOf 和 ownerOfbalanceOf1function balanceOf(address _owner) public view returns (uint256 _balance); 这个函数只需要一个传入 address 参数，然后返回这个 address 拥有多少代币。 在我们的例子中，我们的“代币”是僵尸。你还记得在我们 DApp 的哪里存储了一个主人拥有多少只僵尸吗？ ownerOf1function ownerOf(uint256 _tokenId) public view returns (address _owner); 这个函数需要传入一个代币 ID 作为参数 (我们的情况就是一个僵尸 ID)，然后返回该代币拥有者的 address。 同样的，因为在我们的 DApp 里已经有一个 mapping (映射) 存储了这个信息，所以对我们来说这个实现非常直接清晰。我们可以只用一行 return 语句来实现这个函数。 注意：要记得， uint256 等同于uint。我们从课程的开始一直在代码中使用 uint，但从现在开始我们将在这里用 uint256，因为我们直接从规范中复制粘贴。 第4章: 重构如果你尝试编译这段代码，编译器会给你一个错误说你不能有相同名称的修饰符和函数。 所以我们应该把在 ZombieOwnership 里的函数名称改成别的吗？ 不，我们不能那样做！！！要记得，我们正在用 ERC721 代币标准，意味着其他合约将期望我们的合约以这些确切的名称来定义函数。这就是这些标准实用的原因——如果另一个合约知道我们的合约符合 ERC721 标准，它可以直接与我们交互，而无需了解任何关于我们内部如何实现的细节。 所以，那意味着我们将必须重构我们第4课中的代码，将 modifier 的名称换成别的。 第5章: ERC721: Transfer Logic注意 ERC721 规范有两种不同的方法来转移代币： 1234function transfer(address _to, uint256 _tokenId) public;function approve(address _to, uint256 _tokenId) public;function takeOwnership(uint256 _tokenId) public; 第一种方法是代币的拥有者调用transfer 方法，传入他想转移到的 address 和他想转移的代币的 _tokenId。 第二种方法是代币拥有者首先调用 approve，然后传入与以上相同的参数。接着，该合约会存储谁被允许提取代币，通常存储到一个 mapping (uint256 =&gt; address) 里。然后，当有人调用 takeOwnership 时，合约会检查 msg.sender 是否得到拥有者的批准来提取代币，如果是，则将代币转移给他。 你注意到了吗，transfer 和 takeOwnership 都将包含相同的转移逻辑，只是以相反的顺序。 （一种情况是代币的发送者调用函数；另一种情况是代币的接收者调用它）。 第7章: ERC721: Approval记住，使用 approve 或者 takeOwnership 的时候，转移有2个步骤： 你，作为所有者，用新主人的 address 和你希望他获取的 _tokenId 来调用 approve 新主人用 _tokenId 来调用 takeOwnership，合约会检查确保他获得了批准，然后把代币转移给他。 因为这发生在2个函数的调用中，所以在函数调用之间，我们需要一个数据结构来存储什么人被批准获取什么。 第8章: ERC721: takeOwnership最后一个函数 takeOwnership， 应该只是简单地检查以确保 msg.sender 已经被批准来提取这个代币或者僵尸。若确认，就调用 _transfer； 第9章: 预防溢出不过要记住那只是最简单的实现。还有很多的特性我们也许想加入到我们的实现中来，比如一些额外的检查，来确保用户不会不小心把他们的僵尸转移给0 地址（这被称作 “烧币”, 基本上就是把代币转移到一个谁也没有私钥的地址，让这个代币永远也无法恢复）。 或者在 DApp 中加入一些基本的拍卖逻辑。（你能想出一些实现的方法么？） 但是为了让我们的课程不至于离题太远，所以我们只专注于一些基础实现。如果你想学习一些更深层次的实现，可以在这个教程结束后，去看看 OpenZeppelin 的 ERC721 合约。 合约安全增强: 溢出和下溢我们将来学习你在编写智能合约的时候需要注意的一个主要的安全特性：防止溢出和下溢。 什么是 溢出 (overflow\\)? 假设我们有一个 uint8, 只能存储8 bit数据。这意味着我们能存储的最大数字就是二进制 11111111 (或者说十进制的 2^8 - 1 = 255). 来看看下面的代码。最后 number 将会是什么值？ 12uint8 number = 255;number++; 在这个例子中，我们导致了溢出 — 虽然我们加了1， 但是 number 出乎意料地等于 0了。 (如果你给二进制 11111111 加1, 它将被重置为 00000000，就像钟表从 23:59 走向 00:00)。 下溢(underflow)也类似，如果你从一个等于 0 的 uint8 减去 1, 它将变成 255 (因为 uint 是无符号的，其不能等于负数)。 虽然我们在这里不使用 uint8，而且每次给一个 uint256 加 1 也不太可能溢出 (2^256 真的是一个很大的数了)，在我们的合约中添加一些保护机制依然是非常有必要的，以防我们的 DApp 以后出现什么异常情况。 使用 SafeMath为了防止这些情况，OpenZeppelin 建立了一个叫做 SafeMath 的 库(library\\)，默认情况下可以防止这些问题。 不过在我们使用之前…… 什么叫做库? 一个库 是 Solidity 中一种特殊的合约。其中一个有用的功能是给原始数据类型增加一些方法。 比如，使用 SafeMath 库的时候，我们将使用 using SafeMath for uint256 这样的语法。 SafeMath 库有四个方法 — add， sub， mul， 以及 div。现在我们可以这样来让 uint256 调用这些方法： 12345using SafeMath for uint256;uint256 a = 5;uint256 b = a.add(3); // 5 + 3 = 8uint256 c = a.mul(2); // 5 * 2 = 10","link":"/2020/11/05/solidity-advanced/"},{"title":"「Cryptography-Boneh」:Stream Cipher 1","text":"Stream Cipher的第一部分：介绍了One Time Pad和Stream Cipher中的PRG。其中OTP部分叙述了什么是Perfect Secrecy？为什么OTP很难在实践中应用？Stream Cipher部分中，本文主要阐述了什么是PRG？Stream Cipher的另一种安全的定义（依靠PRG的unpredictable)。本文后半部分，详细阐述了一种weak PRG——线性同余生成器，它是如何工作的？它为什么不安全？如何攻击它? The One Time PadSymmetric Ciphers: difinitionDef :a cipher difined over $\\mathcal{(K,M,C)}$ is a pair of “efiicient “ algorithms $(E,D)$ where $$ E :\\mathcal{K \\times M \\longrightarrow \\mathcal{C}} \\quad ,\\quad D:\\mathcal{K\\times\\mathcal{C}\\longrightarrow\\mathcal{M}} \\\\ s.t. \\quad \\forall m\\in \\mathcal{M},k\\in \\mathcal{K}:D(k,E(k,m))=m $$ $\\mathcal{(K,M,C)}$ 分别是密钥空间、明文空间、密文空间。 对称加密其实是定义在$\\mathcal{(K,M,C)}$ 的两个有效算法 $(E,D)$ ，这两个算法满足consistence equation(一致性方程)：$D(k,E(k,m))=m$ 。 一些说明： $E$ is ofen randomized. 即加密算法E总是随机生成一些bits，用来加密明文。 $D$ is always deterministic. 即当确定密钥和明文时，解密算法的输出总是唯一的。 “efficient” 的含义 对于理论派：efficient表示 in polynomial time（多项式时间） 对于实践派：efficient表示 in a certain time One Time Pad(OTP)Definition of OTPThe one time pad(OTP) 又叫一次一密。 用对称加密的定义来表示OTP： $\\mathcal{M=C=}{0,1}^n\\quad \\mathcal{K}={0,1}^n$ $E：\\quad c = E(k,m)=k\\oplus m \\quad$ $D:\\quad m = D(k,c)=k\\oplus c$ 明文空间和密文空间相同，密钥空间也是n位01串集合。 而且，在OTP中，密钥key的长度和明文message长度一样长。 加密过程如上图所示。 证明其一致性方程 Indeed ： $D(k,E(k,m))=D(k,k\\oplus m)=k\\oplus (k\\oplus m)=0\\oplus m=m$ 但是OTP加密安全吗？ 如果已知明文(m)和他的OTP密文(c)，可以算出用来加密m的OTP key吗？ ：当然，根据异或的性质，key $k=m\\oplus c$ 所以什么是安全呢？ Information Theoretic Security根据Shannon 1949发表的论文，Shannon’s basic idea: CT(Ciphertext) should reveal no “info” about PT(Plaintext)，即密文不应该泄露明文的任何信息。 Perfect Security Def:A cipher $(E,D)$ over $\\mathcal{(K,M,C)}$ has perfect security if $\\forall m_0,m_1 \\in \\mathcal{M}\\ (|m_0|=|m_1|) \\quad \\text{and} \\quad \\forall c\\in \\mathcal{C} $$$Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$$ $k \\overset{R}\\longleftarrow \\mathcal{K}$ 的意思是 $k$ 是 从$\\mathcal{K}$ 中随机取的，即随机变量 $k$ 的取值是均匀分布。 对任意 $m_0,m_1$ （并且message长度相同），那么在密钥空间任意取 $k$ , $k$ 将 $m_0,m_1$ 加密为相同密文的概率相同。 对attacker来说 ：攻击者截取一段密文c，那么c是由 $m_0,m_1$ 加密而来的概率是相同的，即攻击者也不知道明文到底是 $m_0$ 还是 $m_1$ （因为概率相同）。 $\\Rightarrow$ Given CT can’t tell if msg is $m_0 \\ \\text{or}\\ m_1 $ (for all $m_i$ ) . 【攻击者不能区分明文到底是 $m_?$ 】 $\\Rightarrow$ most powerful adv.(adversary) learns nothing about PT from CT. 【不管攻击者多聪明，都不能从密文中得到密文的信息】 $\\Rightarrow$ no CT only attack!! (but other attackers possible). 【唯密文攻击对OTP无效】 OTP has perfect secrecyLemma : OTP has perfect secrecy. 用上一小节的perfect securecy的定义来证明这个引理。 Proof： 要证明： $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$ 表达式： $\\forall m, c: \\quad \\operatorname{Pr}_{k}[E(k,m)=c]=\\frac{\\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c}{|\\mathcal{K}|}$ 对于任意m,c, $\\operatorname{Pr}_{k}[E(k,m)=c]$ 等于能将m加密为c的密钥个数除以密钥空间的大小。 $\\because |\\mathcal{K}|$ 是相同的，所以即证 ： $\\{ \\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c \\}=\\text{const}$ 对于任意 m,c，能将m加密为c的OTP key只有一个： $k=m\\oplus c$ $\\therefore$ OTP has perfect secrecy. key-len $\\geq$ msg-len Perfect Secrecy的性质带来了一个bad news。 Thm: perfect secrecy $\\Rightarrow$ $|\\mathcal{K}|\\geq|\\mathcal{M}|$ 如果一个cipher满足perfect secrecy,那么其密钥的长度必须大于等于明文长度。这也是perfect secrecy的必要条件。 所以OTP是perfect secrecy的最优情况，$|\\mathcal{K}|=|\\mathcal{M}|$ ，密钥长度等于明文长度。 为什么说是一个bad news呢？ 如果Alice用OTP给Bob发一段msg，在她发之前，她需要先发一个和msg等长的key，这个key只有Alice和Bob知道。 所以如果Alice有能保密传输key的方法，那她何不直接用这个方法传输msg呢？ 所以OTP : hard to use in practice! (long key-len) 因此，我们需要key-len短的cipher。 Pseudorandom Generators（伪随机数生成器）Stream Ciphers: making OTP practicalStream Ciphers（流密码）的思想就是：用PRG（pseudorandom Generators） key 代替 “random” key。 PRG其实就是一个function G：${ 0,1 }^s\\longrightarrow { 0,1 }^n \\quad, n&gt;&gt;s$ 。 通过函数将较小的seed space映射到大得多的output space。 注意： function G is eff. computable by a deterministic algorithm. 函数G是确定的，随机的只有s，s也是G的输入。 PRG的输出应该是 “look random”（下文会提到的PRG必须是unpredictable） Stream Ciphers的过程如上图所示：通过PRG，将长度较短的k映射为足够长的G(k)，G(k)异或m得到密文。 有两个问题？ 第一，Stream Cipher安全吗？为什么安全？ 第二，Stream Cipher have perfect secrecy? 现在，只能回答第二个问题。 ：流密码没有perfect secrecy。因为它不满足key-len $\\geq$ msg-len，流密码的密钥长度远小于明文长度。 流密码没有perfect secrecy，所以我们还需要引入另一种安全，这种安全和PRG有关。 PRG must be unpredictablePRG如果predictable，流密码安全吗？ Suppose predictable假设PRG是可预测的，即： $ \\exists:\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1,...,n} $ 已知G(k)输出的前i bis，存在一种算法，能计算G(k)的后面剩余的bits。 攻击如上图所示： 如果attacker has prior knowledge：已知一段密文前缀的对应明文（m斜线字段）（比如在SMTP协议中，报文的开头总是”from”） attacker将该密文字段与已知明文字段异或，得到G(k)的前缀。 因为PRG是可预测的，所以可以通过G(k)的前缀计算出G(k)的剩下部分。 得到的G(K)就可以恢复m。 即使，G(k)只能预测后一位，即 $\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1}$ ，也不安全，当预测出下一位时，又得到了新的前缀，最终得到完整的G(k)。 所以当PRG可预测时，流密码就不安全了。 所以用Stream Cipher时，PRG必须unpredictable! Predictable: difinitionPredictable Def : $ \\exists $ \"eff\" alg. A and $\\exists$ $0\\leq i\\leq n-1$ ， s.t. $Pr_{k \\overset{R}\\leftarrow \\mathcal{K} } {[A(G(k)|_{1,2,...,i})=G(k)|_{i+1}]}>1/2 +\\epsilon$ for non-negligible $\\epsilon$ (e.g. $\\epsilon=1/2^{30}$) 可预测：即存在算法，通过G(k)的前i位可以计算出第i+1位的概率大于1/2 + $\\epsilon$ (不可忽略的值) Unpredictable Def : 即predictable的反面， $\\forall i$ : no “eff.” adv. can predict bit(i+1) for “non-neg” $\\epsilon$ . Q：假设 $\\mathrm{G}: \\mathrm{K} \\rightarrow{0,1}^{\\mathrm{n}} $ ，满足XOR(G(k))=1，G可预测吗？ W：G可预测，存在i = n-1,因为当已知前n-1位,可以预测第n位。 Weak PRGsLinear Congruential Generators一种应该永远不在安全应用中使用PRG——LCG（linear congruential generators）(线性同余随机生成器)。 虽然他们在应用中使用很快，而且其输出还有良好的统计性质（比如0的个数和1的个数基本相等等），但他们应该never be used for cryptographic。 因为在实践中，给出LCG输出的一些连续序列，很容易计算出输出的剩余序列。 Basic LCGDefinitionBasic LCG has four public system parameters: an integer q, two constants a,b $\\in { 0,…,q-1}$ , and a positive integer $w\\leq q$ . The constant a is taken to be relatively prime to q. 【有四个公开参数：整数q，两个q剩余系下的常数a,b，（a与q互素）和一个小于等于q的正整数w。】 We use $\\mathcal{S}_q$ and $\\mathcal{R}$ to denote the sets: $\\mathcal{S}_{q}:=\\{0, \\ldots, q-1\\} ; \\quad \\mathcal{R}:=\\{0, \\ldots,\\lfloor(q-1) / w\\rfloor\\}$ Now, the generators $G_{\\mathrm{lcg}}: \\mathcal{S}_{q} \\rightarrow \\mathcal{R} \\times \\mathcal{S}_{q}$ with seed $s\\in\\mathcal{S}_{q}$ defined as follows: $G_{\\operatorname{lcg}}(s):=(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ 【LCG的输出是一对数，$(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ 】 当 $w=2^t$ 时，$\\lfloor s / w\\rfloor$ simply erases the t least significant bits of s【右移t位】。 Insecure当已知 $s^{\\prime}:=a s+b \\bmod q$ ，即可直接求出s，也就求出了所谓的随机数 $\\lfloor s/w\\rfloor$ . Variant: Blum-Micali constructionDefinition 如上图所示，变体的LCG是一个迭代，输出不包括 $s_i$ ，把 $r_1,…,r_n$ 作为一次迭代的输出。 不同的应用系统使用不同的 $q,a,b,w$ 参数，在Java 8 Development Kit（JDKv8）中，$q=2^{48}$ , $w=2^{22}$ ,constant $a=\\text{0x5DEECE66D}$ , $b=\\text{0x0B}$ 。 所以在JDKv8中, LCG的输出其实是 $s_i$（48bits） 的前48-22=26 bits 。 显然JDKv8中的参数大小应用在安全系统中，还是太不安全了。 how to attack in JDKv8 在迭代的第一次输出中，LCG就 reveal 26bits of the seed s。 对于s剩下的后22个bits，attacker can easily recover them by exhausitive search(穷举)： 对于每个可能的取值，attacker都能得到一个候选seed $\\hat{s}$ 用 $\\hat{s}$ 来验证我们所直接得到的LCG的输出。 如果 $\\hat{s}$ 验证失败，则到第三步继续穷举。直至验证成功。 当穷举至正确的s时，就可以直接预测LCG的剩余输出。 在现代处理器中，穷举 $2^{22}$ (4 million) 只需要1秒。所以LCG的参数较小时，是很容易attack。 当 $q=2^{512}$ 时，这种穷举的攻击方法就失效了。但是有一种对于LCG的著名攻击方法[1]，即使每次迭代，LCG只输出较少的bits，也能从这些较少的但连续的输出序列中预测出整个LCG输出序列。 Cryptanalysis ：elegant attackWarning of MathSupposeSuppose : q is large (e.g. $q=2^{512}$ ), and $G_{lcg}^{(n)}$ outputs about half the bits of the state s per iteration. 【q很大， $G_{lcg}^{(n)}$ 每次输出s的一半左右的bits】 More precisely, suppose: $w&lt;\\sqrt{q}/c$ for fixed c（e.g. $c=32$ ） 【保证输出s前一半左右bits的这个条件】 Suppose the attacker is given two consecutive outputs of the gnerator $r_i,r_{i+1}\\in \\mathcal{R}$ . 【已知两个连续输出 $r_i,r_{i+1}\\in \\mathcal{R}$ 】 Attacker Knows $r_{i}=\\left\\lfloor s_{i} / w\\right\\rfloor \\quad \\text { and } \\quad r_{i+1}=\\left\\lfloor s_{i+1} / w\\right\\rfloor=\\left\\lfloor\\left(a s_{i}+b \\bmod q\\right) / w\\right\\rfloor$ 【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i$ 】 $r_{i} \\cdot w+e_{0}=s \\quad \\text { and } \\quad r_{i+1} \\cdot w+e_{1}=a s+b+q x \\qquad (0\\leq e_0,e_1&lt;w&lt;\\sqrt{q}/c)$ 【 去掉floor符号和mod：$e_0,e_1$ 是 $s_i,s_{i+1}$ 除 $w$ 的余数】 【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i,e_0,e_1,x$ 】 re-arranging: put $x$ and $s$ on the left $s=r_{i} \\cdot w+e_{0} \\quad \\text { and } \\quad a s+q x=r_{i+1} w-b+e_{1}$ 【把未知参数s，x放在等式左边，方便写成矩阵形式】 $s \\cdot\\left(\\begin{array}{l}1 \\ a\\end{array}\\right)+x \\cdot\\left(\\begin{array}{l}0 \\ q\\end{array}\\right)=\\boldsymbol{g}+\\boldsymbol{e} \\quad \\text { where } \\quad \\boldsymbol{g}:=\\left(\\begin{array}{c}r_{i} w \\ r_{i+1} w-b\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{e}:=\\left(\\begin{array}{c}e_{0} \\ e_{1}\\end{array}\\right)$ 【已知： $\\boldsymbol{g},a,q$ ，未知：$\\boldsymbol{e},s,x$ 】 to break the generator it suffices to find the vector $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}$ . 【令 $u\\in {Z}^2$ , $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}=s \\cdot(1, a)^{\\mathrm{T}}+x \\cdot(0, q)^{\\mathrm{T}}$ 】 【如果我们求出了 $\\boldsymbol{u}$ ，那可以用线性代数的知识解出 $s$ 和 $x$ ,再用 $s$ 来预测PRG的剩下输出】 konws $\\boldsymbol{g}$ , knows $\\boldsymbol{e}$ is shorter, and $|\\boldsymbol{e} |_{\\infty}$ is at most $\\sqrt{q}/c$ , knows that $\\boldsymbol{u}$ is “close” to $\\boldsymbol{g}$ . 【e向量很小，$|\\boldsymbol{e} |_{\\infty}$ 上界是$\\sqrt{q}/c$ ，u离g很近】 Taxicab norm or Manhattan(1-norm) ${\\|}A{\\|}_1=\\max \\{ \\sum|a_{i1}|,\\sum|a_{i2}|,...,\\sum|a_{in}| \\}$ （列和范数，A矩阵每一列元素绝对值之和的最大值） Euclidean norm(2-norm) $\\|\\mathbf{x}\\|=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}\\right)^{1 / 2}$ $\\infty$-范数 $\\|A\\|_{\\infty}=\\max \\{ \\sum|a_{1j}|,\\sum|a_{2j}|,...,\\sum|a_{mj}| \\}$ （行和范数，A矩阵每一行元素绝对值之和的最大值） attack can figure the lattice with attacking LCG. the lattice is generated by the vectors $(1,5)^T$ and $(0,29)^T$ , the attacker has a vector $\\boldsymbol{g}=(9,7)^T$ and wishes to find the closest lattice vector $\\boldsymbol{u}$ . 【上图是 $(1,5)^T$ 和 $(0,29)^T$ 两个向量生成的的格点，希望能从以上格点找到离已知 $\\boldsymbol{g}$ 向量最近的格点】 $\\mathcal{L}_a$ :由 $(1, a)^{\\mathrm{T}},(0, q)^{\\mathrm{T}}$ 作为基向量生成的点集合。 The problem is a special case of a general problem call the closest vector problem: given a lattice $\\mathcal{L}$ and a vector $\\boldsymbol{g}$ ,find a vector in $\\mathcal{L}$ that is closest to $\\mathcal{g}$ . There is an efficient polynomial time algorithm for this problem.[2] 【问题归结于 closest vector problem问题，在已知栅格点集合中找离某一向量最近的点，此问题已有多项式时间算法】 step 8 aboveLemma : * For at least $(1-16/c^2)\\cdot q $ of the a in $\\mathcal{S}_q$ , the lattice $\\mathcal{L}_a\\sub Z_2$ has the following property: for every $\\boldsymbol{g} \\in Z^2$ there is at most one vector $\\boldsymbol{u}\\in \\mathcal{L}_a$ such that $\\|\\boldsymbol{g}-\\boldsymbol{u}\\|_{\\infty}","link":"/2020/03/15/stanford-crypto-streamcipher1/"},{"title":"「Cryptography-Boneh」:Stream Cipher 2","text":"作为Stream Cipher的第二篇文章。第一部分分析了基于Stream Cipher的两种攻击：第一种是Two time pad,第二种是对与其完整性的攻击，即流密码是可被篡改的。第二部分具体说明了一些使用流密码加密的例子。包括分析基于软件的RC4流密码、基于硬件的CSS流密码和现代的安全流密码:eStream中的Salsa20。 Attack on OTP and stream ciphersAttack1: two time pad is insecureNever use strame cipher key more than once!! why insecure使用相同的PRG(k)加密不同明文时：$$C_1 \\leftarrow m_1 \\oplus \\text{PRG(k)}\\C_2 \\leftarrow m_2 \\oplus \\text{PRG(k)}$$Eavesdropper（窃听者）截获这两段密文 $C_1\\ C_2$ ，对密文进行疑惑操作，可得： $C_1 \\oplus C_2\\rightarrow m_1\\oplus m_2$ 。 在传输中，英语字母是用ASCII编码后再传输，所以这样的编码会带来很多redundancy（冗余），即根据 $m_1\\oplus m_2$ ，可以得到 $m_1\\ m_2$ 。 因此，当一个密钥会被使用多次时，就不应该直接用stream cipher，后面的章节会介绍multi-use ciphers。 Examples: Project Venona(1941-1946)我们已经知道：加密应该用OTP，即一次性密钥。 但是，当时是通过人工掷骰子并记录得到密钥，工作费时费力。因此不得不用生成的密钥加密多条消息。 最后仅凭截获密文，就破译了3000多条消息。 Examples: MS-PPTP(Windows NT)微软在Windows NT的PPTP协议（point to point transfer protocol）中使用的流密码是：RC4。 在这个协议中允许一个端系统向另一个端系统发送加密后的信息。 过程如下： 在一次对话连接中：主机想发送$m_1\\ m_1\\ m_3$ 三条消息进行查询，服务器想发送 $s_1\\ s_1\\ s_3$ 三条消息进行响应。 主机和服务器hava a shared key:k。 知道密钥不能加密多次，于是主机将三条消息进行concatenation（联结）： $m_1||m_2||m_3$ 。 主机用k作为密钥，得到G(k)，进行加密 $[m_1||m_2||m_3]\\oplus\\text{G(k)}$ 。 同样，服务器也将响应消息进行联结： $s_1||s_2||s_3$ 。 服务器也用k作为密钥，得到相同的G(k)，对响应消息进行加密 $[s_1||s_2||s_3]\\oplus\\text{G(k)}$ 。 因此，在一次对话中，主机和服务器都使用了相同的 G(k)进行加密，也就是 two time pad。 如何改进主机和服务器have a shared pair of key, 即主机和服务器都使用不同的key进行加密。 Examples: 802.11b WEPHow it worksWEP(Wired Equivalent Privacy)，有效等效加密，是一种用于IEEE 802.11b的一种安全算法。这个算法设计的很糟糕，现已被WPA所淘汰。 WEP用于Wi-Fi通信，是他的的加密层。 WEP的算法过程如下： 主机和路由器进行通信： 主机和路由 have a shared key。 主机想要发送一段消息，包括明文m和其校验码CRC(m)。 PRG’s seed： IV||k, k is a long term key，IV is a counter. Length of IV: 24 bits. IV的作用：每一次传送数据包时，用IV来改变每次的密钥。 用(IV||k作为密钥，得到PRG(IV||k),使用流密码进行加密传输。 主机直接发送IV和密文。 路由器用收到的IV和k连接，用PRG(IV||k)，对密文解密。 Problems of IV IV 导致的问题1: two time pad Length of IV: 24 bits Related IV after $2^{24}$ (16M frames) 【当发送16百万的帧后，PRG又会重复】 On some 802.11 cards: IV rests to 0 after power cycle. 【在有些单片机上，重启后IV会变成0：每次重启都会使用PRG(0||k)加密】 IV 导致的问题2: related keys 在PRG中，key for frame is related。(IV||k)，k是104 bits, IV 是24 bits，所以key的后104位总是相同的，不同密钥之间的差异也很小。 对RC4 PRG的攻击： Fluhrer, Mantin, and Shamir在2001年:只需要监听 $10^6$ 帧即可破译密钥[1]。 Recent attacks：只需要监听4000帧，即可破译WEP网络中的密钥。 所以，密钥关联是灾难性的。 Avoid related keys！ A better construction对于WEP，一种更好的做法是：将多个要发送的帧联结起来，得到 $m_1||m_2||…||m_n$ 长流，再用PRG对这个长流加密。 如上图所示，k扩展后，被分成很多段，用第一段加密第一帧，第二段加密第二帧……。 这样，加密每一帧的密钥都是一个伪随机密钥。(no relationship, and looks like random)。 当然，更好的解决方法是使用更强的加密算法（WPA2）。 Examples: disk encryption另一个例子是硬盘加密，在这个例子中，你会发现：使用流密码对硬盘加密不是一个好的举措。 如果使用流密码： Alice 想要给Bob写一封邮件，如上图所示。 邮件经过硬盘加密（流密码）后，存入内存块。 Alice 想要对存在这个硬盘中的邮件进行修改。 Alice只把Bob改成了Eve，其他部分都没有变，如上图所示。 保存后，邮件再次经过硬盘加密（流密码）后，存入内存块。 Attacker：他得到了硬盘上最初的密文和修改后的密文。 通过分析，他发现两段密文只有前小部分不同。（用相同的流密码密钥加密，修改后，密文很容易看出变化） 虽然Attacker不知道Alice是怎么修改的，但是他知道了Alice修改的具体位置。 $\\Rightarrow$ attacker得到了他不应该知道的信息，即修改的具体位置。 在硬盘加密中，对于文本内容的修改前后，也使用了相同的密钥段加密不同的消息，即two time pad。 因此在硬盘加密中，不推荐使用流密码。 Two time pad: SummaryNever use stream cipher key more than once!! Network traffic: negotiate new key for every session. Disk encryption: typically do not use a stream cipher. Attack2: no integrity(OTP is malleable)OPTP和Stream Cipher一样，不提供完整性的保证，只提供机密性的保证。 如上图所示： 如果attacke截获：密文 $m\\oplus k$ 。 并用sub-permutation key（子置换密钥）来对密文进行修改，得到新的密文：$(m\\oplus k)\\oplus p$ 新的密文最后解密得到的明文是 $m\\oplus p$ 。 所以对于有修改密文能力的攻击者来说，攻击者很容易修改密文，并且修改后的密文，对原本解密后的明文也有很大的影响。 具体攻击如下： Bob想要发送一封邮件：消息开头是From: Bob，使用OTP加密后，发送密文。 Attacker：截获了这段密文 假设：attacker知道这封邮件是来自Bob。 attacker想修改这封密文邮件，使得它来自others。 于是它用一个子置换密钥对原密文的特定位置（即Bob密文位置）进行操作，得到新的密文：From： Eve。 这个子置换密钥是什么呢？ 如上图所示，Bob的ASCII码是 42 6F 62，Eve的ASCII码是 45 76 65。 那么Bob $\\oplus$ Eve的ASCII码是 07 19 07。 因此这个子置换密钥是07 19 07。 最后收件人进行解密，得到的是明文：From：Eve。 对attacker来说，虽然他不能创建来自Eve的密文，但是他可以通过修改原本的密文，达到相同的目的。 Conclusion: Modifications to ciphertext are undertected and have predictable impact on plaintext. Real-world Stream CiphersOld example(SW): RC4RC4流密码，是Ron RivestRC4在1987年设计的。曾用于secure Web traffic(in the SSL/TLS protocol) 和wireless traffic (in the 802.11b WEP protocol). It is designed to operate on 8-bit processors with little internal memory. At the heart of the RC4 cipher is a PRG, called the RC4 PRG. The PRG maintains an internal state consisting of an array S of 256 bytes plus two additional bytes i,j used as pointers into S. 【RC4 cipher的核心是一个PRG，called the RC4 PRG。这个PRG的内部状态是一个256字节的数组S和两个指向S数组的指针】 RC4 stream cipher generator setup algorithms: 对S数组进行初始化，0-255都只出现一次入下图所示： 伪代码 stream generator: Once tha array S is initialized, the PRG generates pseudo-random output one byte at a time, using the following stream generator: The procedure runs for as long as necessary. Again, the index i runs linearly through the array while the index j jumps around. Security of RC4具体参见「A Graduate Course in Applied Cryptography」的p76-78 挖坑，有空填坑 cryptanalysis of RC4[2] Weakness of RC4 Bias in initial output: Pr[2^nd^ byte=0]=2/256. 如果PRG是随机的，其概率应该是1/256。 而Pr[2^nd^ byte=0]=2/256的结果是：消息的第二个字节不被加密的概率比正常情况多一倍。（0异或不变） 统计的真实情况是，不止第二个字节，前面很多字节的概率很不都均匀。 因此，如果要使用RC4 PRG，从其输出的257个字节开始使用。 Prob. of (0,0) is 1/256^2^ +1/256^3^ . 如果PRG是随机的，00串出现的概率应该是(1/256)^2^ . Related key attackes. 在上小节中「Examples: 802.11b WEP」，WEP使用的RC4流密码，related key对安全通信也是灾难性的。 Old example(HW): CSS (badly broken)The Content Scrambling System (CSS) is a system used for protecting movies on DVD disks. It uses a stream cipher, called the CSS stream cipher, to encrypt movie contents. CSS was designed in the 1980’s when exportable encryption was restricted to 40-bits keys. As a result, CSS encrypts movies using a 40-bits key. 【1980的美国出口法，限制出口的加密算法只能是40位，于是CSS的密钥是40位】[amazing.jpg] While ciphers using 40-bit keys are woefully insecure, we show that the CSS stream cipher is particularly weak and can be broken in far less time than an exhaustive search over all 2^40^ keys. 【虽然40位的密钥本来就不够安全，但是我们能用远小于穷举时间的方法破解CSS】 因为博主也是第一次学，很多东西也不了解。 所以概述性语言，我用教科书的原文记录，附注一些中文。望海涵～ Linear feedback shift register(LFSR)CSS 流密码是由两个LFSR（线性反馈移位寄存器）组成的，除了CSS，还有很多硬件系统是基于LFSR进行加密操作，但无一例外，都被破解了。 DVD encryption (CSS)：2 LFSRs GSM encryption (A5/,2): 3 LFSRs 【全球通信系统】 Bluetooth(E0): 4LFSRs LFSR can be implemented in hardware with few transistors. And a result, stream ciphers built from LFSR are attractive for low-cost consumer electronics such as DVD players, cell phones, and Bluetooth devices. 【LFSR在硬件上运行很快，也很省电，所以虽然基于LFSR的算法都被破解了，但是改硬件有点困难，所以还是有很多系统在使用】 上图是一个8位LFSR{4,3,2,0}。 LFSR是由一组寄存器组成，LFSR每个周期输出一位（即0位）。 有一些位（如上图的4，3，2，0）称为tap positions(出头)，通过这些位计算出feedback bit(反馈位)。 反馈位和寄存器组的未输出位组成新的寄存器组。 伪代码如下： 所以基于LFSR的PGR的seed是寄存器组的初始状态。 how CSS worksCSS的seed=5 bytes=40 bits。 CSS由两个LFSR组成，如下图所示，一个17-bit LFSR，一个25-bit LFSR。 seed of LFSR: 17-bit LFSR: 1||first 2 bytes ，即17位。 25-bit LFSR: 1||last 3 bytes，即25位。 CSS过程： 两个LFSR分别运行8轮，输出8 bits。 两个8bits 相加mod 256（还有加上前一块的进位）即是CSS一轮的输出：one byte at a time. Cryptanalysis of CSS (2^16 time attack)当已知CSS PRG的输出时，我们通过穷举（2^40^ time）破解得到CSS的seed。 但还有一种更快的破解算法，只需要最多2^16^ 的尝试。 破解过程如下： 影片文件一般是MPEG文件，假设我们已知明文MPEG文件的前20个字节。（已知明文的prefix） CSS是流密码，可以通过已知prefix还原出CSS的prefix，即CSS PRG的前20个字节的输出。 For all possible initial settings of 17-bit LFSR do: run 17-it LFSR to get 20 bytes of output. 【先让17-bit输出20个字节】 subtract from CSS prefix $\\Rightarrow$ candidate 20 bytes output of 25-bit LFSR. 【通过还原的CSS PRG的输出，得到25-bit输出的前20个字节】 If consistent with 25-bit LFSR, found correct initial settings of both!! 【具体是如何判别这20个字节是否是一个25-bit LFSR的输出呢？】 假设前三个字节是y1, y2, y3. 那么25-bit LFSR的initial :{1, y1 , y2, y3},其中y都是8 bits. 用这个初始状态生成20个字节，如果和得到的20个字节相同，则正确，否则再次枚举。 当得到了两个LFSR的seed, 就可以得到CSS PRG的全部输出，即可破解。 Modern stream ciphers: eStreammain idea eStream PRG ： $\\{0,1\\}^s\\times R \\rightarrow \\{0,1\\}^n$ (n&gt;&gt;s) seed: ${0,1}^s$ nonce: a non-repeating value for a given key.【就对于确定的seed,nonce绝不重复】 Encryption: $\\text{E(k, m; r)}=\\text{m}\\oplus \\text{PRG(k; r)} $ The pair (k,r) is never used more than once. 【PRG的输入是(k,r), 确保OTP】 eStram: Salsa 20(SW+HW)Salsa20/12 and Salsa20/20 are fast stream ciphers designed by Dan Bernstein in 2005. Salsa 20/12 is one of four Profile 1 stream cipher selected for the eStream portfolio of stream ciphers. eStream is a project that identifies fast and secure stream ciphers that are appropriate for practicle use. Variants of Salsa20/12 and Salsa20/20, called ChaCha12 and ChaCha20 respectively, were proposed by Bernstein in 2008. These stream ciphers have been incorporated into several widely deployed protocols such as TLS and SSH. Salsa20 PRG: $\\{0,1\\}^{128 \\text { or } 256} \\times\\{0,1\\}^{64} \\longrightarrow\\{0,1\\}^{n}$ (max n = 2^73^ bits) Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 通过计数器，使得输出联结，可以输出as long as you want pseudorandom segment. 算法过程如上图所示：Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 32 bytes的{k,r,i}通过扩展得到64 bytes的{ $\\tau_0,k,\\tau_1,r,i,\\tau_2,k,\\tau_3$ }. k :16 bytes的seed. i: 8 bytes的index，计数器。 r: 8 bytes的nonce. $\\tau_{0,1,2,3}$ 都是4 bytes的常数，Salsa20算法说明书上公开确定的值。 64 bytes 通过h函数映射，十轮。 h : invertible function designed to be fast on x86(SEE2). 在x86上有SEE2指令可以直接运行h函数，所以很快。 h是逆函数（也是公开的函数），输出可以通过逆运算得到其输入。 h是一个一一映射的map，每一个64bytes的输入都有唯一对应的64 bytes的输出。 将第十轮H函数的输出和最开始的输入做加法运算，word by word(32位)，即每4 bytes相加。 为什么要有这一步？ h是可逆运算，如果直接将函数的输出作为PRG的输出，那可以通过h的逆运算得到原64 bytes，也就得到了(k; r). Is Salsa20 secure(unpredictable)前文我们通过unpredictable来定义PRG的安全（下一篇文章会给出安全PRG更好的定义），所以Salsa20 安全吗？是否是不可预测的？ Unknown：no known provably secure PRGs. 【不能严格证明是一个安全PRG（后文会继续讲解什么是安全的PRG），如果严格证明了，也就证明了P=NP】 In reality： no known attacks bertter than exhaustive search. 【现实中还没有比穷举更快的算法】 Performance通过下图的比较，如果在系统中需要使用流密码，建议使用eStream。 speed ：该算法每秒加密多少MB的数据。 Reference S. Fluhrer, I. Mantin, and A. Shamir. Weaknesses in the key scheduling algorithm of RC4. In proceedings of selected areas of cryptography (SAC), pages 1-24, 2001. 「A Graduate Course in Applied Cryptography」p76-78:挖坑 待补充","link":"/2020/03/19/stanford-crypto-streamcipher2/"},{"title":"「Algebraic ECCs」: Lec1 Basics of ECCs","text":"In this series, I will be learning Algebraic Error Correcting Codes, lectured by Mary Wootters. The lecture videos are available here. Feedback and sugguestions are always welcome! ^ - ^ Topics Covered: Basic problem in coding theory Code and codeword Hamming distance and minimum distance Rate Hamming bound on trade-off of the rate and distance IntroductionThis course is named “Algebraic Error Correcting Codes”, containing two parts: Error Correcting Codes (ECC): ECC are a fundamental tool for communication, storage, complexity theory, algorithm design, cryptography, pseudorandomness and etc. Algebraic: algebraic techniques are a fundamental tool for designing ECCs. Basic Problem in Coding TheoryLet’s start with the basic problem in coding theory: We encode a message $x$ of length $k$ into a codeword $c$ of length $n$, which adds some redundancy when $n&gt;k$. When we transmit or store this codeword, something bad might happen, i.e. some bits might be corrupted. Thus, we are left with a corrupted codeword $\\tilde{c}$. The goal is to find (something about) the message $x$ given the corrupted codeword $\\tilde{c}$. This has many applications, including communication and storage: Application1: Communication Suppose Alice has a message $x\\in \\{0,1\\}^{k}$ and wants to sends it to Bob. In order to add some redundancy, she encodes the message into a codeword $c$ and sends the codeword through a noisy channel. After Alice passes $c$ through the noisy channel, Bob receives $\\tilde{c}$ and tries to figure out the original message $x$ that Alice intended to send. Application2: Storage Suppose $x$ is a file we want to store. Instead of storing $x$ directly, we encode it as a codeword $c$, introducing redundancy. If $c$ is stored on a CD or in a RAID array, something bad, like a fire, might happen. However, the owner still wants to recover the original file $x$. Based on the above two applications, we summarize four things we care about in the coding schemes: Things We Care About (Take 1): We should be able to handle SOMETHING BAD. We should be able to recover WHAT WE WANT TO KNOW about $x$. We want to MINIMIZE OVERHEAD. Jumping ahead, the overhead is defined as the quantity $k/n\\in (0, 1]$, which should be as big as possible. We want to do all this EFFICIENTLY. The question about these things are: what is the best trade-off between 1-4? The trade-off depends on how we model things: What is something bad? What exactly do we want to know about $x$?All of $x$ or the first bit of $x$? What counts as efficient? This lecture will gives one way of answering these questions and there are many legit ways. Now, let’s give some formal definitions for a code. CodeLet $\\Sigma$ be any finite set and let $n&gt;0$ be a positive integer. Code: A code $\\mathcal{C}$ of block length $n$ over an alphabet $\\Sigma$ is a subset $\\mathcal{C}\\subseteq\\Sigma^n$. An element $c\\in \\mathcal{C}$ is called a codeword. Sometimes, block length is referred to simply as length. So far, this is not a very interesting definition. A descriptive example for this definition is provided below. Example 1: Another example is slightly more interesting. Example 2: The following is a set of 7 vectors, which forms a binary code of length 4 over $\\Sigma=\\{0, 1\\}$. We can relate this code to the communication between Alice and Bob mentioned earlier. Thus, the code $\\mathcal{C}$ is the image of this map, i.e. $\\mathcal{C}=\\mathrm{Im(ENC)}$. In other words, $\\mathcal{C}$ is the set of all codewords that can be obtained using this encoding map. Example 2 can actually be used to correct one erasure (something bad). An erasure means we know which bit got erased, but we don’t know its original value. Based on the map definition, the missing bit must be 1. Furthermore, it can also be used to detect one error (something bad). An error means we know one bit my have been changed, but we don’t know which one. To sum up, we say that the code in Example 2 can correct one erasure or detect one error. But it cannot correct one error. Let’s see a code that can correct one error. Example 3: Given this encoding map, we can define a code $\\mathcal{C}=\\mathrm{Im(ENC)}$ . Hence, $\\mathcal{C}\\subseteq\\{0,1\\}^7$ is a binary code of length 7. This code has a nice way to visualize as three overlapping circles. We place the messages $x_1, x_2, x_3, x_4$ in the middle and the remaining spaces in the circles correspond to the parity bits, which are the sum of the other bits in each circle. This ensures that the sum of the bits in each circle equals 0. With these parity bits, this code can correct one error. With these circles in mind, we can solve the following puzzle more easily. In a legit codeword, all three circles should sum to 0. Here, both the green and red circles are messed up while the blue circle is correct. Therefore, $\\tilde{c}_3$ must be flipped. But this solution with the help of circles seems pretty ad hoc. In order to make the solution less ad hoc, we’ll introduce more definitions to formalize the solution and flesh out the four things we care about for ECCs mentioned before. Hamming DistanceWe first give the definition of hamming distance, which equals to the number of positions on which two vectors $x, y\\in \\Sigma^n$ differ. Hamming Distance: The Hamming Distance between $x,y\\in \\Sigma^n$ is $$ \\Delta(x,y)=\\sum_{i=1}^n \\mathbb{1}(x_i\\ne y_i) $$ Then we can define the relative hamming distance, which is the hamming distance normalized by $n$. It’s the fraction of positions on which two vectors differ. Relative Hamming Distance: The Relative Hamming Distance between $x,y\\in \\Sigma^n$ is $$ \\delta(x,y)=\\frac1 n \\sum_{i=1}^n \\mathbb{1}(x_i\\ne y_i)=\\frac{\\Delta(x,y)}{n} $$ Another useful definition is the minimum distance, which is the minimum hamming distance over all distinct pairs. Jumping ahead, we will see that the code with large minimum distance can be used to correct errors. Minimum Distance: The Minimum Distance of a code $\\mathcal{C}\\subseteq \\Sigma^n$ is $$ \\min_{c\\ne c'\\in \\mathcal{C}}\\Delta(c,c') $$ Sometimes, the minimum distance is referred to simply as distance. Aside: The hamming distance is a metric, which obeys the triangle inequality $\\Delta(x, z)\\le \\Delta(x, y) + \\Delta(y, z)$. Claim: The code in Example 3 has minimum distance 3. Now we can convince ourselves the claim is true with the circles in mind. But we’ll see a much less ad hoc way to prove the distance in Lecture 2. If this claim is true, it explains why that code can correct one error using the triangle inequality for hamming distance. The minimum distance equal to 3 means that the hamming distance $\\Delta(c,c’)\\ge 3$ for every distinct pair $c, c’\\in \\mathcal{C}$. As depicted in the following figure, suppose the corrupted codeword we received is $\\tilde{c}$ and $c$ is the correct code should be $c$. There is another codeword $c’\\in \\mathcal{C}$. We know that the codeword $\\tilde{c}$ has exactly one error so the hamming distance $\\Delta(\\tilde{c}, c)=1$. The triangle inequality tells us: $$3\\le \\Delta(c,c’)\\le \\Delta(c, \\tilde{c}) + \\Delta(\\tilde{c}, c’)$$ that the hamming distance $\\Delta(\\tilde{c}, c’)\\ge 2$ for every codeword $c’$ other than the correct codeword $c$. Thus, the correct codeword $c\\in \\mathcal{C}$ is uniquely defined by “the one that is closest to $\\tilde{c}$”. Minimum Distance: Proxy for RobustnessThe point of this discussion is that the minimum distance is a reasonable proxy for robustness. To be more specific, in example 2, the code had minimum distance 2 and could correct 1 erasure and detect 1 error. In example 3, the code had minimum distance 3 and could correct 1 error. More generally, a code with minimum distance $d$ can: correct $\\le d-1$ erasures detect $\\le d-1$ errors correct $\\lfloor \\frac{d-1}2\\rfloor$ errors For point 1 and point 3, the (inefficient) algorithm is “if you see $\\tilde{c}$, return $c\\in \\mathcal{C}$ that’s closest to $\\tilde{c}$. For point 2, the (inefficient) algorithm is “if $\\tilde{c}\\notin \\mathcal{C}$, say that something wrong.” To understand why the algorithm works, we look at the following picture: Red Points represent the codewords $c, c’\\in \\mathcal{C}$. Orange Circle correspond to the hamming balls of radius $\\lfloor \\frac{d-1}{2} \\rfloor$ centered at codewords. These hamming balls are disjoint because the minimum distance is $d$. This hamming ball around a codeword $c$ is indeed a set of points $\\{x\\in \\Sigma^n:\\Delta(c, x)\\le \\lfloor\\frac{d-1}{2} \\rfloor\\}$. Green Circles correspond to the hamming balls of radius $d-1$ centered at codewords. These hamming balls are not disjoint, but they each contain exactly one codeword. The hamming balls help explain the (inefficient) algorithm we mention before. Correct $\\lfloor \\frac{d-1}2\\rfloor$ errors:If $c$ is the “correct” codeword (the left red point) and $\\le \\lfloor \\frac{d-1}{2}\\rfloor$ errors are introduced, we may end up with $\\tilde{c}_1$ (the purple point) within its orange circle. Since the orange circle are disjoint, the algorithm can correct codeword $c$ from $\\tilde{c}_1$. Detect $d-1$ errors: If $\\le d-1$ errors are introduced, we may end up with $\\tilde{c}_2$ (the pink point) within its green circles. Now it’s possible that $\\tilde{c}_2$ came from $c$ or that it came from $c’$; we can’t tell. However, since each green circles only contain exactly one codeword, meaning other codewords has to live outside of this ball, we can tell that something wrong. Correct $d-1$ erasures: If $\\le d-1$ erasures are introduced in $\\tilde{c}$, suppose the candidate codewords are the correct codeword $c$ and some other $c’$. We erase the corresponding positions in both candidate codewords $c$ and $c’$. Since the hamming distance between $c$ and $c’$ is at least $d$,there must be at least one position where $c$ and $c’$ differ. This differing position allows us to resolve the ambiguity the identify the correct codeword $c$. Returning to the things we care about, we can now clarify the firs two things: Things We Care About (Take 2): We should be able to handle $\\lfloor \\frac{d-1}{2}\\rfloor$ worst-case errors or $d-1$ worst-case erasures. We should be able to recover all of $x$ (aka correct the errors or erasures) We want to MINIMIZE OVERHEAD. Jumping ahead, the overhead is defined as the quantity $k/n\\in (0, 1]$, which should be as big as possible. We want to do all this EFFICIENTLY. Moreover, the first two things can be combined to the minimum distance $d$ and we want it as large as possible. Aside: In this class, we only focus on the worst-case model (also called the “Hamming model” or “adversarial model”. We will discuss a little bit about the random-error model (also called the “Shannon model” or “Stochastic model”. RateMoving on to the point 3, what do we mean by “overhead”? Dimension: The Message Length (sometimes called Dimension) of a code $\\mathcal{C}$ over an alphabet $\\Sigma$ is defined to be $$ k=\\log_{|\\Sigma|}|\\mathcal{C}| $$ This definition is in line of the the encoding operation which encodes a message $x$ of length $k$ over $\\Sigma$ into a codeword $c\\in \\mathcal{C}$. This map assigns each possible message to a single codeword so we have $|\\Sigma^k|=|\\mathcal{C}|$ aka $k=\\log_{|\\Sigma|}|\\mathcal{C}|$. Rate: The Rate of a code $\\mathcal{C}\\subseteq \\Sigma^n$ with block length $n$ over an alphabet $\\Sigma$ is $$ \\mathcal{R}=\\frac{\\log_{|\\Sigma|}|\\mathcal{C}|}{n}=\\frac{\\text{message length }k}{\\text{block length }n} $$ The rate captures the notion of overhead so minimizing the overhead means to maximize the rate. So if $\\mathcal{R}$ is close to $1$, it means not too much overhead. And if $\\mathcal{R}$ is close to 0, it means lots of overhead. We want the rate to be as close to 1 as possible. Now we can denote a code by these parameters Code: A code with distance $d$, message length $k$, block length $n$, and alphabet $\\Sigma$ is called a $(n, k, d)_{\\Sigma}$ code. After clarifying the things we care about, a question arises: What is the best trade-off between rate and distance? This question is still open for binary codes! Hamming Bound: Rate vs. DistanceThe Hamming bound gives us one bound on the trade of rate and distance. It establishes some limitations on how good this trade-off can be. Let’s return to the picture we had before, with $|C|$ disjoint Hamming balls of radius $\\lfloor \\frac{d-1}{2}\\rfloor$ in the space $\\Sigma^n$. The idea of the Hamming bound is: there can’t be too much of them or they wouldn’t all fit in space $\\Sigma^n$. To be more precise, we define the Hamming ball as follows. Hamming Ball: The Hamming Ball in $\\Sigma^n$ of radius $e$ about some point $x\\in \\Sigma^n$ is $$ B_{\\Sigma^n}(x, e)=\\{y\\in \\Sigma^n: \\Delta(x, y)\\le e\\} $$ The Volume of $B_{\\Sigma^n}(x, e)$ is denoted by $\\mathrm{Vol_{|\\Sigma|}}(e, n)=|B_{\\Sigma^n}(x, e)|$, representing the number of points in the Hamming ball. Notice that the right hand side, $|B_{\\Sigma^n}(x, e)|$, includes $x$ while the volume notation only refers to the radius $e$ and block length $n$. This is because the volume does not depend on $x$. Note: the $\\Sigma$ is sometimes dropped from the notation $B_{\\Sigma^n}(x, e)$. Note: The notation $B_{\\Sigma^n}(x, e/n)$ refers to the Hamming ball with relative distance. Say that $|\\Sigma|=q$, then we can express the volume of a $q$-array Hamming ball as $$\\mathrm{Vol}_q(e, n) = 1 + \\binom{n}{1}\\cdot (q-1) + \\binom{n}{2}\\cdot (q-1)^2 + \\dots + \\binom{n}{e}(q-1)^e$$ where term $1$ refers to the $\\vec{0}$, term $\\binom{n}{1}\\cdot (q-1)$ refers to all the elements of $\\Sigma^n$ of weight 1. Armed with these definitions, we can formalize the bound. If a code $\\mathcal{C}\\subseteq \\Sigma^n$ has distance $d$ and message length $k$, where $|\\Sigma|=q$, we have $$|\\mathcal{C}|\\cdot \\mathrm{Vol}_q\\left(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n\\right) \\le q^n$$ The LHS corresponds to the total volume taken up by all disjoint Hamming balls while the RHS corresponds to the total volume in the whole space $\\Sigma^n$. Taking logs base $q$ of both sides, it can be written as $$\\log_q |\\mathcal{C}|+\\log_q \\left(\\mathrm{Vol}_q\\left(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n\\right)\\right)\\le n$$ It gives the bound of the rate in terms of the distance. Hamming Bound: $$ \\text{Rate}=\\frac{k}{n} \\le 1-\\frac{\\log_q \\left(\\mathrm{Vol}_q\\left(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n\\right)\\right)}{n} $$ Back to Example 3, which was a $(n=7, k=4, d=3)_{q=2}$ code. We have $\\lfloor\\frac{d-1}{2}\\rfloor=1$, $\\mathrm{Vol}_2(1, 7) = 1 + \\binom{7}{1}\\cdot 1 = 8$. So we have rate bounded by $\\frac k n \\le 1 - \\frac{\\log_2(8)}{n} = 1 - \\frac 3 7 = \\frac 4 7$. In fact, the rate $\\frac k n= \\frac 4 7$, so in this case the Hamming bound is tight! It means this code achieves the best trade-off between the minimum distance and rate. When the Hamming bound is tight, we say the code is perfect.Example 3 (which is perfect) is a special case of something called a Hamming Code.","link":"/2024/12/11/stanford-cs250-ecc-lec1/"},{"title":"「Cryptography-Boneh」:Stream Cipher 3","text":"Stream Cipher的第三篇文章。 文章主要分为两部分，前部分逐步定义Secure PRG的定义，通过引入statistical test（统计测试）和Advantage（优势）得出当且仅当PRG is unpredictable,PRG is secure的结论。 后部分介绍了密码学中的一个重要概念Semantic Security的定义，通过引入 computationally indistinguishable(计算上不可区分)的概念给出定义，并证明了OTP的语意安全和在安全PRG条件下的流密码的语意安全，得出如果流密码中使用的PRG is secure,那么流密码就具备semantic security。 文章开头，也简单介绍了密码学中negligible和non-negligible的含义。 Negligible and non-negligibleIn practice: $\\epsilon$ is a scalar and $\\epsilon$ non-neg: $\\epsilon \\geq 1/2^{30}$ (likely to happen over 1GB of data) $\\epsilon$ negligible: $\\epsilon \\leq 1/2^{80}$ (won’t happen over life of key) 在实践中，$\\epsilon$ 是一个数值，如果是non-neg不可忽略的话，大约在1GB的数据下就会发生，如果是可忽略的值，在密钥的生存周期内基本不会发生。 In theory: $\\epsilon$ is a function $\\varepsilon: Z^{\\geq 0} \\rightarrow R^{\\geq 0}$ and $\\epsilon$ non-neg: $\\exists d: \\epsilon(\\lambda)\\geq 1/\\lambda^d$ ($\\epsilon \\geq 1/\\text{poly} $, for many $\\lambda$ ) $\\epsilon$ negligible: $\\forall d, \\lambda \\geq \\lambda_{d}: \\varepsilon(\\lambda) \\leq 1 / \\lambda^{d}$ ( $\\epsilon \\leq 1/\\text{poly}$, for large $\\lambda$ ) [0]（理论中，这个还不太理解，待补充。） PRG Security DefsLet $G:K\\longrightarrow \\{0,1\\}^n$ be a PRG. Goal: define what it means that [ $k\\stackrel{R}{\\longleftarrow} \\mathcal{K}$ , output G(k) ] is “indistinguishable” from [ $r\\stackrel{R}{\\longleftarrow} \\{0,1\\}^n$ , output r] . 【使得PGR的输出和真随机是不可区分的】（ $\\stackrel{R}{\\longleftarrow}$ 的意思是在均匀分布中随机取） 下图中，红色的圈是全部的 ${0,1}^n$ 串，按照定义是均匀分布。而粉色G()是PRG的输出，由于seed很小，相对于全部的 ${0,1}^n$ ，所以G()的输出范围也很小。 因此，attacker观测G(k)的输出，是不能和uniform distribution（均匀分布）的输出区分开。 这也就是我们所想构造的安全PGR的目标。 Statistical TestsStatistical test on ${0,1}^n$ ：有一个算法A，$x\\in{0,1}^n$ ,A(x) 根据算法定义输出”0”或”1”. 统计测试是自定义的。 Example： A(x)=1 if $| \\#0(x)-\\#1(x)|\\leq 10\\cdot\\sqrt{n}$ 【期望串中0、1的数目差不多，这样look random】 A(x)=1 if $|\\#00(x)-\\frac{n}{4}\\leq10\\cdot\\sqrt{n}$ 【期望Pr(00)=1/4 ,串中00出现的概率为1/4，认为是look random】 A(x)=1if max-run-of-0(x) &lt; 10·log(n) 【期望串中0的最大游程不要超过规定值】 上面的第三个例子，如果输出为全1，满足上述的统计条件输出1，但全1串看起来并不random。 统计测试也由于是自定义的，所以通过统计测试的也不一定是random，其PRG也不一定是安全的。 所以，如何评估一个统计测试的好坏？ 下面引入一个重要的定义advantage，优势。 Advantage引入Advantage（优势）来评估一个统计测试的好坏。 Let G: $k \\rightarrow \\{0,1\\}^n$ be a PRG and A a stat. test on ${0,1}^n$ 【G是一个PRG，A是一个对01串的统计测试】 Define: the advantage of statisticaltest A relative to PRG G Adv$_\\text{PRG}[A,G]$ $\\text{Adv}_\\text{PRG}[A,G]=|Pr[A(G(k))=1]-Pr[A(r)=1]|\\in[0,1]$ , $k\\stackrel{R}{\\longleftarrow} \\mathcal{K}, r\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ 【定义：Adv$_\\text{PRG}[A,G]$ 为统计测试A对于PRG G的优势为统计测试以PRG作为输入输出为1的概率 减去 统计测试以truly random string 作为输入输出为1的概率】 Adv close to 1 : 统计测试能区分PRG的输出和truly random string，即adversary可以利用区分PRG的输出和random的这一点从而破解系统。 Adv close to 0 : 统计测试不能区分PRG的输出和truly random string, 即adversary认为PRG的输出和random别无二致。 Advantage 优势[1] In cryptography, an adversary’s advantage is a measure of how successfully it can attack a cryptographic algorithm, by distinguishing it from an idealized version of that type of algorithm.Note that in this context, the “adversary” is itself an algorithm and not a person. A cryptographic algorithm is considered secure if no adversary has a non-negligible advantage, subject to specified bounds on the adversary’s computational resources (see concrete security). 在密码学中，adversary的优势是评估它通过某种理想算法破解一个加密算法的成功尺度。 这里的adversary是一种破解算法而不是指攻击者这个人。 当所有 adversary对该加密算法只有可忽略的优势时，该加密算法被认为是安全的，因为adversary只有有限的计算资源。 e.g.1 : A(x) = 0，统计测试A对PRG的任何输出都输出0，则Adv[A,G] = 0. e.g.2 : G: $k \\rightarrow {0,1}^n$ satisfies msb(G(k))=1 for 2/3 of keys in K. Define statistical test A(x) as : if[ msb(x)=1 ] output “1” else output “0” 【PRG G(k)的2/3的输出的最高有效位是1，定义统计测试A(x),输入的最高有效位为1输出1，否则输出0】 msb: most significant bit,最高有效位。 则 Adv[A,G] = | Pr[A(G(k))] - Pr[A(r)] | = 2/3 - 1/2 = 1/6 即 A can break the generator G with advantage 1/6, PRG G is not good. Secure PRGs: crypto definitionDef: We say that G: $k \\rightarrow {0,1}^n$ is secure PRG if $\\forall$ “eff” stat. tests A : Adv$_\\text{PRG}$ [A,G] is “negligible”. 这里的”eff”,指多项式时间内。 这个定义，“所有的统计测试”，这一点难以达到，因此没有provably secure PRGs。 但我们有heuristic candidates.（有限的stat. test 能满足） Easy fact: a secure PRG is unpredictable证明命题： a secure PRG is unpredictable. 即证明其逆否命题： PRG is predictable is insecure。 suppose A is an efficient algorithm s.t. $\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\\frac{1}{2} + \\epsilon$ for non-negligible $\\epsilon$ . 【 算法A是一个有效的预测算法， $\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\\frac{1}{2} + \\epsilon$ , $\\epsilon$ 是不可忽略的值，即A能以大于1/2的概率推测下一位。】 Adversary能利用算法A来区分这个PRG和random依次来破解系统。 Define statistical test B as: B(x)=1 if $A(x|_{1,...,i})=x_{i+1}$ , else B(x)=0. 【定义一个统计测试B，如果预测算法A预测下一位正确输出1，否则输出0】 计算Adv$_\\text{PRG}$ : $r\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ : Pr[B(r) = 1] = 1/2 $k\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ : Pr[B(G(k)) = 1] = 1/2+ $\\epsilon$ Adv$_\\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | &gt; $\\epsilon$ Adversary 能以 $\\epsilon$ 的优势区分PRG和random，因此PRG 不安全。 所以，如果A是一个好的预测算法，那么B就是一个好的统计算法，Adversary就能通过B以 $\\epsilon$ 的优势破解系统。 在此，证明了 if A can predict PRG, PRG is insecure $\\Rightarrow$ A secure PRG is unpredictable. Thm(Yao’82): an unpredictable PRG is secure上节证明了A secure PRG is unpredictable. 在1982 Yao 的论文[2]中证明了其逆命题： an unpredictable PRG is secure. G: $k \\rightarrow {0,1}^n$ be PRG “Thm“ : if $\\forall i \\in$ {0,…, n-1} PRG G is unpredictable at pos. i then G is a secure PRG. 【定理：如果在任何位置PRG都是不可预测的，那么PRG就是安全的】 Proof： A: 预测算法： $\\forall i \\quad\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]=\\frac{1}{2} $ B:统计测试： B(x)=1 if $A(x|_{1,...,i})=x_{i+1}$ , else B(x)=0. Adv$_\\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | = 0 If next-bit predictors cannot distinguish G from random then no statistial test can! 【next-bit predictor指用预测算法的统计测试】 e.g. Let G: $k \\rightarrow {0,1}^n$ be PRG such that from the last n/2 bits of G(k) it is easy to compute the first n/2 bits. Is G predictable for som i $\\in$ {0, …, n-1} ? : Yes.当n=2时，可以预测出第一位。 在此，可以得出结论： Adversary不可区分PRG的输出和truly random string时被认为是安全的。 当且仅当PRG在任意位置不可预测时，PRG是安全的。 More generally: computationally indistinguishableLet P1 and P2 be two distributions over ${0,1}^n$ Def : We say that P1 and P2 are computationally indistinguishable (denoted $P_1\\approx_p P_2$) If $\\forall$ “eff” stat. tests A $\\left|\\text{Pr}_{x\\leftarrow_{P_1}}[A(x)=1]-\\text{Pr}_{x\\leftarrow_{P_2}}[A(x)=1]\\right|","link":"/2020/06/26/stanford-crypto-streamcipher3/"},{"title":"「Algebraic ECCs」: Lec2 Linear Codes and Finite Fields","text":"In this series, I will be learning Algebraic Error Correcting Codes, lectured by Mary Wootters. The lecture videos are available here. Feedback and sugguestions are always welcome! ^ - ^ Topics Covered: Linear Algebra over $\\{0, 1\\}$ Generator Matrices Parity-Check Matrices Linear Algebra not Working over $\\{0, 1, 2, 3\\}$ Finite Fields and Linear Codes RecapIn last lecture, we are left with the open question: What is the best trade-off between rate and distance? We called the following example the Hamming Code of length 7 and we saw that it was optimal in that it met the Hamming Bound. We also came up with a decoding algorithm for this code. Viewing the code as overlapping circles, we can identify which circles don’t sum to 0 (mod 2) and flip the unique bit that ameliorates the situation. However, this circle view seems a bit ad hoc. In this Lecture, we are going to generalize this construction and generalize the decoding algorithm as well as the distance argument. Who was Hamming? Richard Hamming (1915-1998) was working at Bell labs starting in the late 1940’s, where he was colleagues with Claude Shannan (of the “Shannon model” which we also mentioned). Hamming was working on old-school computers (calculating machines), and they would return an error if even on bit was entered in error. This was extremely frustrating, and inspired Hamming to study this rate-vs-distance question, and to come up with Hamming codes. Linear Algebra over {0, 1}The Hamming code in the previous Example 3 has a really nice form of encoding: Generator MatricesWe can see this encoding map as the multiplication by a matrix modulo 2, written as $x\\mapsto Gx\\pmod 2$, where $G$ is some matrix called the Generator Matrix. Viewing codes generated by a generator matrix is a very useful view to look at things. Suppose for now “linear algebra works mod 2”. For example, the codes in Example 3has a very nice property: it is closed under addition modulo 2, which means that if $c\\in \\mathcal{C}$, $c’\\in \\mathcal{C}$, then $c+c’\\in \\mathcal{C}$. Writing it as the multiplication by a matrix makes it very clear: Moreover, we can view any codeword $c\\in \\mathcal{C}$ as a linear combination of the column vectors of the generator matrix $G$. In other words, the code $\\mathcal{C}$ is the column span of the generator matrix $G$, aka $\\mathcal{C}=\\text{span}(\\text{cols}(G))$ is a linear subspace of dimension 4. dist (C) = min wt (C)A key observation is that the minimum Hamming distance of the code is the same as the minimum weight of the non-zero codewords. Observation: If $\\mathcal{C}$ is linear, then $\\text{distance}(\\mathcal{C})=\\min \\text{wt}(C)$. Proof: The proof is straightforward: $$\\Delta(Gx, Gx’) = \\Delta(G(x-x’), 0)=\\text{wt}(G(x-x’))$$ where the first equality holds follows from linearity and $G(x-x’)$ is itself a codeword by the definition. $\\blacksquare$ Parity-check MatricesThe other way we looked at Example 3 was placing bits into circles as shown: Each circle constitutes a parity check, meaning the sum of the circle must equal 0 mod 2. This constrain is indeed the linear relation, which we can express as: where each of the three rows corresponds to the linear constrains given by a circle. For example, the first row corresponds to the green circle, indicating that $c_2+c_3+c_4+c_5=0 \\pmod 2$. This implies that multiplying any potential codeword by the matrix $H$ should results in 0. In other words, for any $c\\in \\mathcal{C}$, we have $Hc=0\\pmod2$, which means that $\\mathcal{C}$ is contained in the kernel of the matrix $H$: $$\\mathcal{C}\\subseteq \\text{Ker}(H)$$ A raised question is: Does $\\mathcal{C}=\\text{Ker}(H)?$ The answer is YES! Lemma: $\\mathcal{C}=\\text{Ker}(H)$ We prove it by counting dimension. Proof: $\\text{dim}(\\mathcal{C})=\\text{dim}(\\text{colspan}(G))=4$ It’s easy to see $\\text{dim}(\\text{colspan}(G))=4$ since the identity matrix is just sitting up there. $\\text{dim}(\\text{Ker}(H))=n - \\text{rank}(H) = 7 - \\text{dim}(\\text{rowspan}(H))=4$Again, it’s easy to see $\\text{dim}(\\text{rowspan}(H))=3$ because of the identity matrix. Having $\\mathcal{C}\\subseteq \\text{Ker}(H)$ with the same dimension, $\\text{dim}(C)=\\text{dim}(\\text{Ker}(H))$, it follows that $\\mathcal{C}=\\text{Ker}(H)$. $\\blacksquare$ Informally, the matrix $H$ here is called a Parity-Check matrix of $\\mathcal{C}$ so that the code $\\mathcal{C}$ is the kernel of this matrix. Easy to Capture DistanceOne benefit of the parity-check matrix is we can read off the distance easily. As mentioned in the previous lecture, we supposed that the distance of the code in Example 3 is 3. Now, we can provide a proof using parity-check matrices. Claim: $\\text{dist}(\\mathcal{C}) = 3$ Proof: As before, it suffices to show $\\min_{c\\in \\mathcal{C}\\backslash\\{0, 1\\} } \\text{wt}(c) = 3$ that the minimum weight of all non-zero codewords is 3. Firstly, we prove $\\text{wt}(c)\\ge 3$ for $\\forall c\\in \\mathcal{C}$, meaning that all non-zero codewords has weight of at least 3. By contradiction, suppose there is some codeword vector $c\\in \\mathcal{C}$ with has weight 1 or 2 so that $Hc = 0 \\pmod 2$. $\\text{wt}(c)=1$It implies one column of $H$ is 0 mod 2. (contradiction) $\\text{wt}(c)=2$ It implies the sum of some two columns of $H$ is 0 mod 2, aka there is a repeated column. (contradiction) Now, the codeword 0101010 has weight exactly 3, so this bound is tight. Hence, the minimum weight is 3. $\\blacksquare$ Easy to DecodeFurthermore, the parity-check matrix gives us a slick decoding algorithm. Recall the puzzle in Example 3: Puzzle: Given $\\tilde{c}=0111010$, which bit has suffered one bit flip, and what is the original codeword $c$? The goal of the solution is to find a codeword $c\\in \\mathcal{C}$ such that the Hamming distance $\\Delta(c, \\tilde{c})\\le 1$. Solution: We write $\\tilde{c}=c+z\\pmod 2$ where $z$ is an error vector with weight 1. Next, we consider what the product of $H\\cdot \\tilde{c}$ actually is? One the one hand, compute $H\\cdot \\tilde{c} \\mod 2$ : On the other hand, write the product as $H\\cdot (c+z) = Hc + Hz =Hz$ since $Hc=0$ for all $c\\in \\mathcal{C}$. This shows that $z$ is just picking one column of $H$. From the computation, we see $H\\cdot \\tilde{c}$ corresponds to the 3rd column of $H$, indicating the error occurred in position 3. This leads to an efficient decoding algorithm for $\\mathcal{C}$: Compute $H\\cdot \\tilde{c}$ and identify which column of $H$ matches the result Determine $z$ recover $\\tilde{c}=c + z\\mod 2$. This view with parity-check matrix gives us a much nicer way of seeing our circle-based algorithm. “Which circles fail to sum 1” is the same as “which bits of $H\\cdot \\tilde{c}$ are 1”, and it picks out which bit we need to flip. Now, let’s summarize the moral of the story so far: The Hamming code of length 7 $\\mathcal{C}$, is a subspace over $\\{0, 1\\}^7$, which means we can view it in two different liner-algebraic ways: $\\mathcal{C}=\\{G\\cdot x: x\\in \\{0, 1\\}^4\\}=\\text{span(cols}(G))$, as the column-span of the generator matrix. $\\mathcal{C}=\\{x\\in \\{0, 1\\}^7:H\\cdot x=0\\} = \\text{Ker}(H)$ , as the kernel of the parity-check matrix. However, all of these are predicated on the assumption that the linear algebra works over $\\{0, 1\\}$ . Linear Algebra not Working over {0, 1, 2, 3}A natural question arises: Does linear algebra “make sense” over $\\{0,1\\}$? To explore this, let’s consider what happens for $\\{0, 1,2,3\\}$. The statements below contains falsehoods. Non-example: Let $G = \\left[ \\begin{array}{cc} 2 & 0 \\\\ 0 & 2 \\\\ 2 & 2 \\end{array} \\right]$be a generator matrix, mod 4. Using this generator matrix, we can define a code $\\mathcal{C} = \\{G\\cdot x: x\\in \\{0, 1, 2,3\\}^2\\} = \\text{colspan}(G)$, with message length $k = 2$, block length $n = 3$, over the alphabet $\\Sigma = \\{0, 1, 2,3\\}$. Thus, $\\text{dim}(\\mathcal{C})=2$, since the columns of $G$ are not scalar multiples of each other, aka, they are linearly independent. Now, consider the parity-check matrix $H = G^{T} = \\left[\\begin{array}{cc} 2 & 0 & 2 \\\\ 0 & 2 & 2\\\\ \\end{array} \\right]$. We observe $H\\cdot G = 0$, which means $H$ is a legit parity-check matrix for the code $\\mathcal{C}$. Specifically, for any $c\\in \\mathcal{C}$, we have: $H\\cdot c = H\\cdot G\\cdot x = 0 \\mod 4$. However, when we calculate $\\text{dim(Ker}(H))$, we find: $\\text{dim}(\\text{Ker}(H)) = 3 - \\text{rank}(H) = 1$. This leads to a contradiction for the code $\\mathcal{C}$ is a liner subspace such that $\\mathcal{C} = \\text{colspan}(G) = \\text{Ker}(H)$. Additionally, when we look at the distance of code using the parity-check matrix, we find that $\\text{dist}(\\mathcal{C})\\ge 3$ since no two columns of $H$ are linearly dependent. Yet, there exists a codeword $c = G\\cdot \\left(\\begin{array}{c} 1\\\\1 \\end{array} \\right) = \\left(\\begin{array}{c} 2\\\\2 \\\\0 \\end{array} \\right)$, which has weight $2$, leading to another contradiction. The main reason for contradiction is that the linear algebra does not “work” over $\\{0, 1, 2, 3\\} \\mod 4$. In particular, the assertions that two vectors are linearly independent since they are not scalar multiples of each other are not working over $\\{0, 1, 2, 3\\}$. The following definitions are both for the linearly independent. Non-zero vectors $v$ and $w$ are linearly independent iff there is no non-zero $\\lambda$ s.t. $v = \\lambda \\cdot w$. Non-zero vectors $v$ and $w$ are linearly independent iff there is no non-zero $\\lambda_1, \\lambda_2$ s.t. $\\lambda_1 \\cdot v + \\lambda_2 \\cdot w = 0$. They are the same over the real field $\\mathbb{R}$. The proof is straightforward Proof: Suppose $\\exists \\lambda_1, \\lambda_2 \\ne 0$ s.t. $\\lambda_1 \\cdot v + \\lambda_2 \\cdot w = 0$. Then $v = (\\frac{-\\lambda_2}{\\lambda_1})\\cdot w$.Conversely, if $\\exists \\lambda$ s.t. $v = \\lambda w$, then choose $\\lambda_2 = \\lambda, \\lambda_1 = -1$ and $\\lambda_1 v + \\lambda_2 w = 0$. $\\blacksquare$ However, they not the same over $\\{0, 1,2, 3\\}$. There exists $\\lambda_1 =\\lambda_2 = 2$ such that $$ 2\\cdot \\left(\\begin{array}{c} 2 \\\\ 0 \\\\2 \\end{array}\\right ) + 2\\cdot \\left(\\begin{array}{c} 0 \\\\ 2 \\\\2 \\end{array}\\right ) = \\left(\\begin{array}{c} 4 \\\\ 4 \\\\8 \\end{array}\\right ) = \\left(\\begin{array}{c} 0 \\\\ 0 \\\\0 \\end{array}\\right ) \\mod 4 $$ even though the two columns of $G$ are not scalar multiples of each other. If you have the background of the finite field, the reason here is that $\\{0, 1, 2, 3\\}$ is not a finite field and we cannot divide by 2 mod 4. This does not bode well for algebraic coding theory if even linear algebra doesn’t work. Finite Field and Linear Codes The second part of the lecture covers finite fields. I omit it here since we have already covered it in this blog. The key takeaway is all the definitions we know for the linear algebra over $\\mathbb{R}$ also make sense over finite fields. Here, I list some essential points relevant to this lecture. Let $\\mathbb{F}$ be a finite field. Then: $\\mathbb{F}^n = \\{(x_1, \\dots, x_n): x_i \\in \\mathbb{F}\\}$ A subspace $V\\subseteq \\mathbb{F}^n$ is a subset that is closed unde r addition and scalar multiplication. Specifically: $\\forall v, w\\in V, \\forall \\lambda\\in \\mathbb{F}, v+ \\lambda w\\in V$. Vectors $v_1, \\dots, v_t \\in \\mathbb{F}^n$ are linearly independent if $\\forall \\lambda_1, \\dots, \\lambda_t\\in \\mathbb{F}$ that are not all 0, $\\sum_{i} \\lambda_i \\cdot v_i \\ne 0$. For $v_1, \\dots, v_t \\in \\mathbb{F}^n$, their span is defined as $\\text{span}(v_1, \\dots, v_t)=\\{\\sum_i \\lambda_i v_i : \\lambda_i \\in \\mathbb{F}\\}$. A basis for a subspace $V\\subseteq \\mathbb{F}$ is a collection of vectors $v_1, \\dots, v_t\\in V$ s.t. $v_1, \\dots, v_t$ are linearly independent $V = \\text{span}(v_1, \\dots, v_t)$ The dimension of a subspace $V$ is the number of elements in any basis of $V$. Now, we can define a linear code over a finite field. Definition of Linear Codes: A Linear Code $\\mathcal{C}$ of length $n$ and dimension $k$ over a finite field $\\mathbb{F}$ is a $k$-dimension linear subspace of $\\mathbb{F}^n$. (The alphabet of $\\mathcal{C}$ is $\\Sigma = \\mathbb{F}$) This definition implies that a linear code is essentially a linear space. This definition aligns with the one we introduced in the previous lecture. (Recall) Definition of Code : A code with distance $d$, message length $k$, block length $n$, and alphabet $\\Sigma$ is called a $(n, k, d)_{\\Sigma}$ code. Here, we use overloaded $k$ for both message length and dimension. This is intentional, as it makes sense in this context. If $\\mathcal{C}$ is a $k$-dimensional subspace over $\\mathbb{F}$, then $|\\mathcal{C}|=|\\mathbb{F}^k|$, hence $k = \\log _{|\\mathbb{F}|}|\\mathcal{C}| = \\log_{|\\Sigma|}|\\mathcal{C}|=$ message length. Moreover, if $\\mathcal{C}$ is a code of block length $n$, message length $k$, over alphabet $\\mathcal{\\Sigma = \\mathbb{F}}$, then $\\mathcal{C}$ is a linear code and also a $k$-dimensional subspace over $\\mathbb{F}$. Definition of Generator Matrix: Let $\\mathcal{C}$ be a linear code of length $n$ and dimension $k$ over a finite field $\\mathbb{F}$. A matrix $G\\in \\mathbb{F}^{n\\times k}$ is a generator matrix for $\\mathcal{C}$ if $\\mathcal{C} = \\text{colspan}(G)=\\{G\\cdot x: x\\in \\mathbb{F}^k\\}$. Observation: Any linear code has a generator matrix. Proof: Choose the columns of $G$ to be a basis of $\\mathcal{C}$. $\\blacksquare$ Note that the generator matrix for a code is not unique. There can be many generator matrices for the same code. They all describe the same code, but they implicitly describe different encoding maps. For example, the following $G$ and $G’$ are both generator matrices for the same Hamming code. In particular, we can always permute on rows since it does not change the space of column-span. Some generator matrices may be more useful than others. For example, $G$ above corresponds to a systematic encoding map. This means that that $\\text{Enc}_G(x_1, x_2, x_3, x_4) \\mapsto (x_1, x_2, x_3, x_4, \\text{stuff})$. In particular, for linear codes, there is always a systematic encoding map defined by a generator matrix look like the above $G$, having an identity matrix sit up there and some other stuff down there. Definition of Dual Code: If $\\mathcal{C}\\subseteq \\mathbb{F}^n$ is a linear code over $\\mathbb{F}$, then $\\mathcal{C}^\\perp=\\{v\\in \\mathbb{F}^n: \\langle v, c\\rangle =0 \\text{ for }\\forall c\\in \\mathcal{C}\\}$ Dual code is the same definition as the dual subspace in linear algebra. Note: Let $\\mathcal{C}\\subseteq \\mathbb{F}^n$ be a linear code of length $n$ over $\\mathbb{F}$ with $\\text{dim}(\\mathcal{C})=k$, then $\\text{dim}(\\mathcal{C}^\\perp) = n - k$. (Just like over $\\mathbb{R}$). Definition of Parity Check Matrix: Let $\\mathcal{C}$ be a linear code of length $n$ and dimension $k$ over a finite field $\\mathbb{F}$. A matrix $H\\in \\mathbb{F}^{(n-k)\\times n}$ so that $\\mathcal{C} = \\text{Ker}(H)=\\{c\\in \\mathbb{F}^n: H\\cdot c = 0\\}$ is a parity-check matrix for $\\mathcal{C}$. The rows of $H$ (or any vector $v$ s.t. $\\langle v, c\\rangle =0 \\;\\forall c\\in \\mathcal{C}$ ) are called parity checks. Observation: Any linear code has a parity-check matrix. Proof: Choose the rows of $H$ to be a basis of $\\mathcal{C}^\\perp$. $\\blacksquare$ Similarly, the parity-check matrix for a code is not unique—there can be many parity-check matrices for the same code. With these definitions under our belts, we can move to some facts about linear codes. Facts About Linear Codes: If $\\mathcal{C}\\subseteq \\mathbb{F}^n$ is a linear code over $\\mathbb{F}$ of dimension $k$ with the generator matrix $G$ and parity-check matrix $H$, then: $H\\cdot G = 0$ $\\mathcal{C}^\\perp$ is a linear code of dimension $n-k$ with generator matrix $H^T$ and parity-check matrix $G^T$.Note that the parity-check matrix $H$ for code $\\mathcal{C}$ is taken as the generator matrix for $\\mathcal{C}^\\perp$. The distance of $\\mathcal{C}$ is the minimum weight of any nonzero codeword in $\\mathcal{C}$: $\\text{dist}(\\mathcal{C})=\\min_{c\\in \\mathcal{C}\\backslash\\{0\\}}\\sum_{i=0}1\\{c_i \\ne 0\\}$. The distance of $\\mathcal{C}$ is the smallest number $d$ so that $H$ has $d$ linearly dependent columns.Recall how we proved the distance of the code in Example 3: we argue that it suffices to show that no pairs of columns in $H$ are linearly dependent.As shown below, the distance of the code corresponds to the smallest number $d$ so that there exists a codeword $c$ of weight $d$, which is picking up $d$ columns of $H$ that is linearly dependent.","link":"/2024/12/16/stanford-cs250-ecc-lec2/"},{"title":"「Algebraic ECCs」: Lec3 GV Bound and q-ARY Entropy","text":"In this series, I will be learning Algebraic Error Correcting Codes, lectured by Mary Wootters. The lecture videos are available here. Feedback and sugguestions are always welcome! ^ - ^ Topics Covered: The GV Bound: Efficiency and Maximum-Likelihood Decoding Application: McEliece Cryptosystem Off to Asymptopia Family of Codes q-ary Entropy Trade-off Between Rate and Distance RecapIn the last lecture, we saw linear codes over a finite field in two linear-algebraic ways. As figured below, we can view a code as the column-span of a generator matrix and as the kernel of the parity-check matrix. There can be many different generator matrices and parity-check matrices for the same code. Today, we are going to learn some useful things one can do with linear codes. The GV BoundIn Lecture 2, we explored the Hamming bound: $$R\\le 1 - \\frac{\\log_q\\text{Vol}_q(\\lfloor \\frac{d-1}{2}\\rfloor, n)}{n}$$ which provides an upper bound on the rate of code. This bound highlights an impossible result, meaning that we cannot construct a code with a large rate. In contrast, the Gilbert-Varshamov (GV) bound offers a possible result. It demonstrates that there exist codes with a decent rate. Gilber-Varshamov (GV) Bound: For any prime power $q$, and for any $d\\le n$, there exists a linear code $\\mathcal{C}$ of length $n$, alphabet size $q$, distance $d$, and rate $$ R\\ge 1 - \\frac{\\log_q {(\\text{Vol}_q(d-1, n))-1}}{n} $$ Note: you can remove the words “prime power” and “linear” and the statements is till true. Aside from the fact that the GV bound provides a possibility result, another difference lies in the quantity difference in the $q$-ary volume of the Hamming ball. We’ll explore the relationship between these two bounds later, but for now, keep in mind that the rate given by the GV bound must be less than that of the Hamming bound: $$R_{\\text{GV}} &lt; R_{\\text{Hamming}}$$ otherwise, the math is broken. Now, we’ll prove the GV bound — it’s pretty easy! Proof of the GV Bound: The idea of the proof is to choose a random linear code $\\mathcal{C}$ of specific dimension $k$, and show that it has a distance at least $d$ with non-zero probability—that is: $$\\text{Pr}[\\mathcal{C} \\text{ has distance}\\ge d]&gt;0$$ ​ This implies that there exists a linear code with a decent rate. This approach is known as the Probabilisitic Method. Let $\\mathcal{C}$ be a random subspace of $\\mathbb{F}_q^n$ with dimension $k = n -\\log_q(\\text{Vol}_q(d-1, n))-1$.If this code achieves a good distance, the $k/n$ basically corresponds to the rate given in the GV bound.This construction works because a linear code is essentially a linear space. Since there are finitely many subspaces of dimension $k$ in $\\mathbb{F}_q^n$, we can uniformly sample a random linear subspace. Let $G\\in \\mathbb{F}_q^{n\\times k}$ be a random generator matrix for $\\mathcal{C}$.As discussed before, each code can have many generator matrices. We can uniformly sample one by selecting a random basis for the subspace and using it as the columns of the generator matrix. Now, since the distance is the minimum weight of all non-zero codewords in the linear code, we have $\\text{dist}(\\mathcal{C})=\\min_{c\\in \\mathcal{C}\\backslash{0}}\\text{wt}(c)=\\min_{x\\in \\mathbb{F}_q^k\\backslash{0}}\\text{wt}(G\\cdot x)$. Useful Fact:For any fixed $x\\ne 0$, $G\\cdot x$ is uniformly random in $\\mathbb{F}_q^n\\backslash{0}$. For any given $x\\ne 0$, using the useful fact, the probability that the weight of $G\\cdot x$ is less than $d$ is equal to the probability of a random non-zero codeword lying within the Hamming ball centered at 0 with radius $d-1$. It is basically the volume of the Hamming ball divided by the volume of the whole space as indicated in the following second equality. $$\\begin{aligned}\\text{Pr}_G{\\text{wt}(G\\cdot x)&lt;d} &amp;= \\text{Pr}_G{G\\cdot x\\in B_q(0, d-1)}\\&amp;= \\frac{\\text{Vol}_q(d-1, n)-1}{q^n-1}\\&amp;\\le \\frac{\\text{Vol}_q(d-1, n)}{q^n}\\end{aligned}$$ By the union bound, we have $$\\text{Pr}{\\exists x\\in \\mathbb{F}_q^k:\\text{wt}(G\\cdot x)&lt;d}\\le q^k\\cdot \\frac{\\text{Vol}_q(d-1, n)}{q^n}$$ The complement of this event is that $\\forall x\\in \\mathbb{F}_q^k$ , $\\text{wt}(G\\cdot x)\\ge d$, which implies that the distance of the code is at least $d$. Thus, we win as long as this probability is strictly less than $1$, which guarantees the existence of a code with a good distance with non-zero probability. Taking logs of both sides, we win if $$k-n+\\log_q(\\text{Vol}_q(d-1, n))&lt;0$$ This is true since we precisely choose $k = n-\\log_q(\\text{Vol}_q(d-1, n))-1$ before. $\\blacksquare$ Efficiency &amp; Maximum-Likelihood DecodingThe GV bound tells us there exists good codes with decent rates. Next, we are going to discuss the extent to which linear codes admit efficient algorithms. We have the following efficient algorithms for linear codes to encode, detect errors and correct erasures: Efficient Encoding:If $\\mathcal{C}$ is linear, we have an efficient encoding map $x\\mapsto G\\cdot x$.The computational cost is one matrix-vector multiplication. Efficient Error Detection:If $\\mathcal{C}$ is linear with distance $d$, we can detect $\\le d - 1$ errors efficiently:If $0&lt; \\text{wt}(e)\\le d-1$ and $c\\in \\mathcal{C}$, then $H(c+e)=H\\cdot e \\ne 0$. Thus, we can just simply check if $H\\tilde{c}=0$. Efficient Erasure Correction:If $\\mathcal{C}$ is linear with distance $d$, we can correct $\\le d-1$ erasures efficiently:Erasing bits in the codeword $c$ corresponds to removing the corresponding rows of the generator matrix $G$. The remaining $n-(d-1)$ rows form a new linear system $G’\\cdot x = c’$. Since we know a code with distance $d$ can handle up to $d-1$ erasures (albeit with a non-efficient algorithm), there must be exactly one $x$ that is consistent with this linear system, and hence $G’$is full rank. The remaining task is to solve this linear system, which can be done with Gaussian elimination. The above is leaving out one important thing—correcting errors. We know how to correct errors in the $(7, 4, 3)_2$-Hamming code, but what about in general? If $\\mathcal{C}$ is linear with distance $d$, can we correct up to $\\lfloor \\frac{d-1}{2} \\rfloor$ errors efficiently? The bad news is no. Consider the following problem. If we would solve this problem, we can correct up to $\\lfloor \\frac{d-1}{2} \\rfloor$ errors. Maximum-Likelihood Decoding for Linear Codes: Given $\\tilde{c}\\in \\mathbb{F}_q^n$, and $G\\in \\mathbb{F}_q^{n\\times k}$, find $x\\in \\mathbb{F}_q^k$ such that $\\Delta(G\\cdot x, \\tilde{c})$ is minimized. Aka, find the codeword closest to a received word $\\tilde{c}$. This problem (called Maximum-likelihood decoding for linear codes) is NP-hard in general [Berlekamp-McEliece-Van Tilborg 1978], even if the code is known in advance and you have an arbitrary amount of preprocessing time [Bruck-Noar 1990, Lobstein 1990]. It is even NP-hard to approximate (within a constant factor)! [Arora-Babai-Stern-Sweedyk 1993]. Even computing the minimum distance of linear codes is NP-hard given the generator matrix. The take-away here is that we are unlikely to find a polynomial-time algorithm for this task. This may sounds discouraging, but remember that NP-hardness is a worst-case condition. While there exist linear codes that are probably hard to decode, but this does not imply that that all of them are. Going forward, we will focus on designing codes that admit efficiently-decodable algorithms. Before that, let’s look at a cryptography application that leverages this decoding hardness. Application: McEliece CryptosystemMcEliece cryptosystem is a public-key scheme based on the decoding hardness of binary linear codes. Suppose that Alice and Bob want to talk securely. Now there is no noise, just an Eavesdropper Eve. In public key cryptography, everyone has a public key and a private key. To send a message to Bob, Alice encrypts it using Bob’s public key. Bob decodes it with his private key. We hope this process is secure as long as Bob’s private key stays private. The McEliece Cryptosystem consists of three main algorithms: Generate Private and Public Keys Bob chooses $G\\in \\mathbb{F}_2^{n\\times k}$, the generator matrix for an (appropriate) binaray linear code $\\mathcal{C}$ that is efficiently decodable from $t$ errors. Not all codes work for McEliece crytosystem; the chosen code at least must be efficiently decodable. In particular, McEliece cryptosystem uses a binary “Goppa Code”. Bob chooses a random invertible $S\\in \\mathbb{F}_2^{k\\times k}$ and a random permutation matrix $P\\in \\mathbb{F}_2^{n\\times n}$. The permutation matrix $P$ has exactly one 1 in each column such that $Px$ permutes the coordinates of the vector $x$. Bob’s private key is $(S, G, P)$. Bob’s public key consists of $\\hat{G}=PGS$ and the parameter $t$. Encrypt with Bob’s Public keyTo send a message $x\\in \\mathbb{F}_2^k$ to Bob: Alice chooses a random vector $e\\in \\mathbb{F}_2^n$ with $\\text{wt}(e)=t$. Alice sends $\\hat{G}x+e$ to Bob. Decrypt with Bob’s Private KeyTo decrypt the message $G’x+e$: Bob computes $P^{-1}(\\hat{G}x+e)=GSx + P^{-1} e =G(Sx) +e’$, where $\\text{wt}(e’)=t$. At this point, we write it as a corrupted codeword $G(Sx)+e’$ with exactly $t$ errors since the permutation matrix $P^{-1}$ only permutes the coordinates of $e$. Bob can use the fact that $G$ is the generator matrix for a code that is efficiently able to correct up to $t$ errors. Bob uses the efficient decoding algorithm to recover $Sx$. Bob can compute $x = S^{-1}\\cdot Sx$. Why might this be secure? Suppose Eve sees $\\hat{G}x+e$ and she knows $G’$ and $t$. Hence, this problem is the same as decoding the code $\\hat{C}={\\hat{G}x : x\\in \\mathbb{F}_2^k}$ from $t$ errors. The security of the McEliece crytosystem relies the following assumptions: The public key $\\hat{G}$ looks random: By scrambling $G$ with $S$ and $P$, it is difficult for Eve to distinguish $\\hat{G}$ from a random generator matrix. Decoding a random linear code is computationally hard: While decoding the worst-case code is NP-hard, it is not too much of stretch that decoding a random linear code is also hard on average. If these assumptions hold true, decoding the code $\\hat{C}={\\hat{G}x : x\\in \\mathbb{F}_2^k}$ from $t$ errors is computationally hard for Eve. This assumption that “Decoding $\\hat{G}x+e$ is hard” (for an appropriate choice of $G$) is called the McEliece Assumption. Some people believe it and some don’t. “Decoding $\\hat{G}x+e$ is hard for Eve” is NOT the same as “Maximum likelihood decoding of linear codes is NP-hard”. There are two main differences: First, we have some promise that there were $\\le t$ errors in McEliece assumption. Second, NP-harness is a worst-case assumption. For cryptography, we need an average-case assumption. Worst-case vs. Average-case: Worst-case: The problem is considered hard on worst-case if it is difficult to solve for the most difficult case. If a solution is found for the worst-case instance, the problem is solvable for all instances, e.g. $P\\ne NP$. Average-case: The problem is considered hard if it is difficult to solve for a randomly chosen instance. This assumes that solving the problem is computationally hard for the “average” case rather than just the hardest instance. In cryptography, average-case hardness is more relevant because security relies on the assumption that attackers cannot efficiently solve a typical random instance of the underlying problem (e.g., decoding a random linear code or factoring a randomly chosen large integer). Off to AsymptopiaSo far, we’ve seen the optimal rate for a code with distance $d$ and $|\\Sigma|=q$ is bounded above by the Hamming bound and bounded below by the GV bound. $$1-\\frac{1}{n}\\cdot \\log_q (\\text{Vol}_q(d-1, n))\\le k/n \\le 1 - \\frac{1}{n} \\cdot \\log_q(\\text{Vol}_q(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n)$$ Recall the combinational question we posed in Lecture 1: What is the best trade-off between rate and distance? To address this in the asymptotical setting, we are going to think about the following limiting parameter regime: $n, k, d\\rightarrow \\infty$ so that the rate $k/n$ and the relative distance $\\delta$ approaches constants. The motivations for this parameter regime: It will allow us to better understand what’s possible and what’s not. In many applications, $n, k, d$ are pretty large and $R, \\delta$ are the things we want to be thinking about. It will let us talk meaningfully about computational complexity. Family of CodesBefore that, let’s define a family of codes. A Family of Codes A family of codes is a collection $\\mathcal{C}=\\{\\mathcal{C}_i\\}_{i=1}^{\\infty}$, where $\\mathcal{C}_i$ is an $(n_i, k_i, d_i)_{q_i}$ code. Given such a family of codes, we can define the rate and the relative distance: The rate of $\\mathcal{C}$ is $R(\\mathcal{C})=\\lim_{i\\to \\infty} k_i/n_i$. The relative distance of $\\mathcal{C}$ is $\\delta{(\\mathcal{C}})=\\lim_{i\\to \\infty} d_i /n_i$. We will frequently abuse notation and refer to $\\mathcal{C}$ as a “code”, and we’ll drop the subscript $i$ and just think about $n, k, d\\to \\infty$ Now, let’s look at an example of a family of codes—Hamming codes. The $i$-th code $\\mathcal{C}_i$ is a $(2^i-1, 2^i-i-1, 3)_2$ code so $\\mathcal{C}=\\{\\mathcal{C}_i\\}_{i=1}^\\infty$ represents a family of codes. $\\mathcal{C}_i$ is defined by its parity-check matrix, where the columns corresponds to the binary vector representations of all non-zero elements of $\\mathbb{F}_{2^i}$. The rate of this family is: $$\\lim_{i\\to \\infty }\\frac{2^i-i-1}{2^i-1}=1$$ This rate approaches to 1, which is a very good rate. However, the relative distance is: $$\\lim_{i\\to \\infty}\\frac{3}{2^i-1}=0$$ Hence, in the asymptotic setting, our question is: for any family of a code, What is the best trade-off between $R(\\mathcal{C})$ and $\\delta{(\\mathcal{C})}$? As we see in the family of Hamming codes, we cannot achieve good trade-off between rate and distance. While it has a phenomenal rate, its distance is poor. An easier question to ask is: Can we obtain codes with $R(\\mathcal{C})&gt;0$ and $\\delta{(\\mathcal{C})}&gt;0$? We define a family of codes with the rate $R(\\mathcal{C})&gt;0$ and relative distance $\\delta(\\mathcal{C})&gt;0$ (both strictly greater than 0) as asymptotically good. q-ary EntropyNow we have an asymptotic parameter regime, how should we parse the GV and Hamming bounds? In particular, what do these bounds look like in terms of $\\delta$? $$1-\\frac{1}{n}\\cdot \\log_q (\\text{Vol}_q(d-1, n))\\le R(\\mathcal{C}) \\le 1 - \\frac{1}{n} \\cdot \\log_q(\\text{Vol}_q(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n)$$ We know that $\\text{Vol}_q(\\lfloor \\frac{d-1}{2} \\rfloor, n)=\\sum_{j=0}^{\\lfloor {d-1\\over 2}\\rfloor}{n\\choose j}(q-1)^j$ but this expression is not very helpful for analysis. To address this, we use the $q$-ary entropy function, which provides a concise way to capture the volume of Hamming balls. q-ary Entropy Function: The q-ary entropy function $H_q:[0,1]\\to [0, 1]$ is defined as: $$ H_q(x)=x\\log_q(q-1)-x\\log_q(x)-(1-x)\\log_q(1-x) $$ This generalizes the binary entropy function $H_2(x)=-x\\log x - (1-x)\\log (1-x)$. Using q-ary entropy function, we can bound the volume of the Hamming ball with the following propositions, allowing us to replace the pesky volume expression with cleaner approximations. Propositions: Let $q\\ge 2$ be an integer, and let $0\\le p \\le 1 - {1\\over q}$. Then: $\\text{Vol}_q(pn, n)\\le q^{n\\cdot H_q(p)}$ $\\text{Vol}_q(pn, n)\\ge q^{n\\cdot H_q(p) - o(n)}$ Here, the $o(n)$ term is a function $f(n)$ such that $\\lim_{n\\to \\infty}{f(n)\\over n}\\to 0$. We can consider this term as negligible compared to $n\\cdot H_q(p)$. Intuitive Interpretation of q-ary entropy: The binary entropy function $H_2(p)$ is often described in terms of the number of bits needed to describe something. For example, a random string of length $n$, where each bit is 1 with probability $p$, can be described using $n\\cdot H_2(p)$ bits. There is a similar interpretation for $q$-ary entropy. Suppose we choose $x\\in \\mathbb{F}_q^n$ s.t. each $x_i$ is 0 with probability $1-p$ and random in $\\mathbb{F}_q^*$ with probability $p$. $$ x_i=\\begin{cases}0 &\\text{w/ prob. }1-p\\\\\\text{random in }\\mathbb{F}_q^* &\\text{w/ prob. }p\\end{cases} $$ Then, the number of bits needed to describe $x$ is roughly $n\\cdot H_2(p)$. Before proceeding, let’s examine how this q-ary entropy function behaves. $H_2(x)$: This function is 0 at $x=0$ and $x=1$, with a maximum value of $1$ at $x={1\\over 2}$. $H_3(x)$: It resembles $H_2(x)$ but is slightly shoved over to the right. It’s maximum value occurs at $x={2\\over 3}$. $H_6(x)$: This function is shifted further to the right, with its maximum value occurring at $x=\\frac{5}{6}$. More generally, $H_q(x)$ has the maximum value of $1$ at $x={q-1\\over q}$. As $q$ increases, curve of the function is shoved more and more over to the right. Here are some useful properties of $H_q(x)$: If $p\\in [0, 1]$ is constant and $q\\to \\infty$, then $$ H_q(p)= \\underbrace{p\\cdot \\log_q(q-1)}_{\\text{basically 1}}+\\underbrace{O(\\log_q(\\text{stuff}))}_{\\text{really small}}\\approx p $$ So, eventually the plot looks like a line of $H_q(p)=p$ and a little hicky at the end. If $q$ is constant and $p\\to 0$, then $$ \\begin{align} H_q(p)&=\\underbrace{p\\cdot \\log_q(q-1)}_{O(p)}+\\underbrace{p\\log_q\\left({1\\over p}\\right)}_{\\text{This term is the largest}}+ \\underbrace{(1-p)\\log_q\\left({1\\over 1-p}\\right )}_{\\approx p/\\ln(q) =O(p)}\\\\ &\\approx p\\log_q\\left({1\\over p}\\right) \\end{align} $$ So, near the origin, all those curves look like $x\\ln(1/x)\\over \\ln q$. The following is my own analysis for the last term $(1-p)\\cdot \\log_q\\left ({1\\over 1-p}\\right)$ using Taylor expansion. Analyze the last term using Taylor expansions: The Taylor expansion of $f(x)$ at $x=a$ is $$ \\begin{align} f(x)&amp;=\\sum_{n = 0}^\\infty\\frac{f^{(n)}(a)}{n!}(x-a)^n \\ &amp;=f(a)+f’(a)\\cdot (x-a)+\\frac{f’’(a)}{2!}\\cdot (x-a)^2+\\dots \\end{align} $$ Derivatives of $\\ln (x)$ are given as: $\\ln’(x)=x^{-1}$, $\\ln^{‘’}(x)=(-1)\\cdot x^{-2}$, $\\ln^{(3)}=(-1)\\cdot (-2)\\cdot x^{-3}$. By deduction, we have: $\\ln^{(n)}(x)=(-1)^{n-1}\\cdot (n-1)!\\cdot x^{-n}$. The Taylor expansion of $\\ln(x)$ at $x=1$ is: $$ \\begin{align} \\ln(x) &amp;= \\ln(1)+\\sum_{n=1}^{\\infty} \\frac{\\ln^{n}(1)}{n!}\\cdot (x-1)^n\\&amp;=\\sum_{n=1}^{\\infty}(-1)^{n-1}\\cdot {1\\over n}\\cdot (x-1)^n\\ &amp;=(x-1) -{(x-1)^2\\over 2}+{(x-1)^3\\over 3}-\\dots+{(-1)^{n-1}\\over n}\\cdot (x-1)^n \\end{align} $$ Applying this expansion to the last term: $$ \\begin{align} \\log_q\\left ({1\\over 1-p}\\right)&amp;=-{\\ln (1-p) \\over \\ln q}\\ &amp;=-{1\\over \\ln q}\\cdot [(-p)-\\frac{(-p)^2}{2}+\\dots]\\ &amp;\\approx {1\\over \\ln q}\\cdot p \\end{align} $$ This shows that $\\lim_{p\\to 0}(1-p)\\cdot \\log_q\\left ({1\\over 1-p}\\right)\\approx p/\\ln (q)=O(p)$ Now, we can use them to simplify our expression for GV and Hamming bounds, both involving the the volume of the q-ary Hamming ball. The strategy is to take log base $q$ of the following approximation in terms of the $\\delta$: $$\\text{Vol}_q(\\delta n, n)\\approx q^{n\\cdot H_q(\\delta)}$$ We can replace the pesky term $\\log_q \\text{Vol}_q(\\delta n,n)$ with $n\\cdot H_q(\\delta)$. Hamming Bound: For any family $\\mathcal{C}$ of the q-ary codes, we have $$ R(\\mathcal{C})\\le 1 - H_q(\\delta{\\mathcal{(C)}}/2) $$ GV Bound: GV Bound: Let $q\\ge 2$. For any $0\\le \\delta\\le 1- \\frac{1}{q}$, and for any $0< \\epsilon \\le 1 - H_q(\\delta)$, there exists a q-ary family of codes $\\mathcal{C}$ with $\\delta(\\mathcal{C})\\ge \\delta$ and $$ R(\\mathcal{C})\\ge 1 - H_q(\\delta)-\\epsilon $$ Trade-off Between Rate and DistanceNow, it’s easier to compare these two bounds: The following plot the trade-off for $q=2$ in terms of only the rate $R$ and the relative distance $\\delta$, without considering $n, k, d$. The red line represents the Hamming bound for binary codes.Notably, no point above the Hamming bound is achievable by any binary codes. The blue line represents the GV bound for binary codes.Notably, any point below the GV bound is achievable by some codes. The yellow region is an area of uncertainty. We would like to push the GV bound as much up as possible while at the same time try and push down the Hamming bound as much as possible. Note that the GV bound answers our earlier question: There do exist asymptotic good codes! But, can we find some explicit ones with efficient algorithms? Regarding the yellow uncertain region, there are several other interesting questions: Are there family of codes that beat the GV bound? The answer is both yes and no. Answer 1: Yes. For $q=49$, “Algebraic Geometry Codes” beat the GV bound. Answer 2: For binary codes, we don’t know.This remains an OPEN QUESTION! The GV bound (which is relatively straighforward to prove) is the best-known possibility of result we have for binary codes. Can we find explicit constructions of families of codes that meet the GV bound? Recall that our proof for GV bound is non-constructive; it uses the probabilistic method to show the existence of a random linear codes with decent rates. However, we are looking for explicit descriptions or efficient algorithms to construct such codes. Answer 1: Yes, for large alphabet. (We’ll see soon) Answer 2: For binary codes, recent work [Ta-Shma 2017] gives something close in a very particular parameter regime…but in general, it’s still an OPEN QUESTION!","link":"/2024/12/26/stanford-cs250-ecc-lec3/"},{"title":"「Cryptography-Boneh」:Collision Resistance","text":"上一节介绍了基于PRFs的MAC构造和基于随机的MAC： 本节我们将介绍抗碰撞的MAC(MACs from collision resistance)。 第一部分介绍了什么是抗碰撞 (Collision Resistance)，以及基于C.R.的MAC的安全性。 第二部分介绍了生日悖论，如何用生日攻击寻找2-way collision 和3-way collision. 第三部分介绍了Merkle-Damgarg范式（如果压缩函数h是C.R.，那么构造出的哈希函数H就是C.R.）以及如何构建C.R.的压缩函数（Davies-Meyer压缩函数). 最后一部分介绍了HMAC (Hash MAC)和一种针对MAC验证的timing attack and defense. Collision ResistanceLet H: M →T be a hash function (|M|&gt;&gt;|T|) 【H是一个哈希函数】 A collision for H is a pair $\\mathrm{m_0,m_1\\in M}$ such that: $$ \\mathrm{H(m_0)=H(m_1)\\quad and\\quad m_0\\ne m_1} $$ 【哈希碰撞是指，存在两个不同的消息，其哈希函数的值相等。】 A function is collision resistant if for all (explicit) “eff” algs. A: $$ \\operatorname{Adv}_{\\mathrm{CR}}[\\mathrm{A}, \\mathrm{H}]=\\operatorname{Pr}[\\text { A outputs collision for } \\mathrm{H}] \\quad \\text{is \"neg.\"} $$ 【如果对于所有多项式时间算法，输出哈希函数H的碰撞的概率是可忽略的，就说明哈希函数H是抗碰撞的（collision resistant），比如SHA-256就是一个抗碰撞的哈希函数。】 MACs from C.R.如何从抗碰撞的哈希函数构建MACs？ Let I = (S,V) be a MAC for short messages over (K,M,T) (e.g. AES) Let $\\mathrm{H}: \\mathrm{M}^{\\mathrm{big}} \\rightarrow \\mathrm{M}$ Def: $\\mathrm{I}^{\\mathrm{big}}=\\left(\\mathrm{S}^{\\mathrm{big}}, \\mathrm{V}^{\\mathrm{big}}\\right)$ over $\\left(\\mathrm{K}, \\mathrm{M}^{\\mathrm{big}}, \\mathrm{T}\\right)$ as:$$\\begin{align}\\mathrm{S}^{\\mathrm{big}(k, m)}=&amp;\\mathrm{S}(k, \\mathrm{H}(m)) \\ \\mathrm{V}^{\\mathrm{big}}(k, m, t)=&amp;\\mathrm{V}(k, \\mathrm{H}(m), t)\\end{align}$$【I是一个针对短消息的MAC算法，H是一个哈希函数，能将极大的消息映射到较短的消息空间上。】 Thm : If I is a secure MAC and H is collision resistant then $\\mathrm{I^{big}}$ is a secure MAC. 【如果I是一个secure MAC，并且H是抗碰撞的，$\\mathrm{I^{big}}$ 就是一个secure MAC】 其中，Collision resistence 是满足安全的必要条件： 假设攻击者可以找到一个碰撞：$\\mathrm{m_0\\ne m_1\\quad s.t. H(m_0)=H(m_1)}$ 那么，$\\mathrm{I^{big}}$ 在1-chosen msg 攻击下就是不安全的： 攻击者请求得到tag: $\\mathrm{t} \\longleftarrow \\mathrm{S}\\left(\\mathrm{k}, \\mathrm{m}_{0}\\right)$ 伪造pair $\\mathrm{(m_1,t)}$ 作为输出 Generic birthday attackGeneric attack on C.R. functions$\\mathrm{H}: \\mathrm{M} \\rightarrow{0,1}^{\\mathrm{n}}$ 是一个哈希函数.$\\left(|M| \\gg 2^{n}\\right)$ 一般的攻击算法可以在 $\\mathcal{O}(2^{n/2})$ 时间内找到一对哈希碰撞： 在M中随机选择 $2^{n/2}$ 不同的消息：$\\mathrm{m_1,\\dots,m_{2^{n/2} } }$ for i = 1, …, $2^{n/2}$ ，计算出其对应的哈希值： $\\mathrm{t}_{\\mathrm{i}}=\\mathrm{H}\\left(\\mathrm{m}_{\\mathrm{i}}\\right) \\quad \\in\\{0,1\\}^{\\mathrm{n}}$ 查看这其中是否有碰撞（即满足 $\\mathrm{t_i=t_j}$ ），如果没有，返回第一步。 （只需要进行很少迭代，就可以找到碰撞） The birthday paradox为什么上述算法有效？ 生日悖论告诉我们，如果 $r_{1}, \\ldots, r_{n} \\in{1, \\ldots, B}$ 是独立同分布的变量（均匀分布是最坏的情况），当 $n=1.2 \\times B^{1 / 2}$ 时，存在碰撞 $r_i=r_j(i\\ne j)$ 的概率大于等于1/2。 Let $r_{1}, \\ldots, r_{n} \\in{1, \\ldots, B}$ be indep. identically distributed integers. Thm : when $n=1.2 \\times B^{1 / 2}$ then $\\operatorname{Pr}\\left[\\exists \\mathrm{i} \\neq \\mathrm{j}: \\mathrm{r}_{\\mathrm{i}}=\\mathrm{r}_{\\mathrm{j}}\\right] \\geq 1 / 2$ . Proof: (for uniform indep. $r_{1}, \\ldots, r_{n} $) 取样数n和碰撞概率的关系如图： 因此上述的攻击算法只需要迭代两次，就能以极大的概率找到碰撞。其时间、空间复杂度都为 $\\mathcal{O}(2^{n/2})$ Sample C.R. hash functions抗碰撞哈希函数的比较： 因此SHA-1的digest size太小，可以很容易的找到碰撞，所以不推荐使用。 不管量子计算机能大幅降低寻找碰撞的时间： Three way collision简化考虑two-way collision: 哈希函数 $\\mathrm{H}: \\mathrm{M} \\rightarrow \\mathrm{T\\quad(|T|=t)} $ ，应该随机取样$n$个不同的数字，才能以极大的概率找到碰撞满足 $H(y)=H(x)$ . n个不同的数字中，有 $n\\times(n-1)$ 个pair$(x, y)$ . 而对于任意值$H(x)$，$y$满足 $H(y)=H(x)$ 的概率为 $1/t$ . 因此应该满足： $n^2=t$ 即只需要取样 $\\mathcal{O}(|T|^{1/2})$ 个值，即可找到碰撞。 同理，考虑three-way collision: 哈希函数 $\\mathrm{H}: \\mathrm{M} \\rightarrow \\mathrm{T\\quad(|T|=t)} $ ，应该随机取样$n$个不同的数字，才能以极大的概率找到碰撞满足 $H(x)=H(y)=H(z)$ . n个不同的数字中，有 $\\frac{n\\times(n-1)\\times(n-2)}{3!}$ 个pair $(x,y,z)$ . 而对于任意值H(x)，y和z满足 $H(y)=H(z)=H(x)$ 的概率为 $1/t^2$ 因此应该满足：$n^3/6 = t^2$ 所以只需要取样 $\\mathcal{O}(|T|^{2/3})$ 个值，即可找到碰撞。 The Merkle-Damgard ParadigmThe MD iterated construction通过压缩函数(compression function) $\\mathrm{h: T \\times X \\rightarrow T}$ ，我们可以得到哈希函数 $\\mathrm{H}: X^{\\leq L} \\longrightarrow T$ . $\\mathrm{H_i}:$ chaining variables $\\mathrm{PB}:$ padding block （If no space for PB add another block）. MD collision resistance上述构造是抗哈希的。 Thm: If h is collision resistant then so is H. 【如果压缩函数h是抗哈希的，那么构造出的哈希函数也是抗哈希的】 Proof: collision on H $\\Rightarrow$ collision on h 【通过反证法证明，如果能找到H的碰撞，那么也能找到h压缩函数的碰撞】 假设 $\\mathrm{H(M)=H(M’)}$ ，我们可以尝试构造出h的碰撞。 【1】只需要最后一块满足：$\\mathrm{h\\left(H_{t}, M_{t} | P B\\right)=H_{t+1}=H_{r+1}^{\\prime}=h\\left(H_{r}^{\\prime}, M_{r}^{\\prime} | P B^{\\prime}\\right)}$ . 【1】如果h函数的参数不完全相同，那么就找到了h的碰撞即 $\\mathrm{H_t\\ne H_r’}$ or $ \\mathrm{M_t\\ne M_r’}$ or $\\mathrm{PB\\ne PB’}$ . 【2】否则：如果 $\\mathrm{H_t= H_r’}$ and $ \\mathrm{M_t= M_r’}$ and $\\mathrm{PB= PB’}$ （可以得出 $t=r$ ）满足式子：$\\mathrm{h\\left(H_{t-1}, M_{t-1} \\right)=H_{t}=H_{t}^{\\prime}=h\\left(H_{t-1}^{\\prime}, M_{t-1}^{\\prime} \\right)}$ . 【2】同样的，如果h函数的参数不完全相同，就找到了一个h的碰撞： 即 $\\mathrm{H_{t-1}\\ne H_{t-1}’}$ or $ \\mathrm{M_{t-1}\\ne M_{t-1}’}$ . 【*】因此通过迭代的方式，到最后： 要么找到h压缩函数的碰撞 要么 $\\forall i: M_i=M_i’ \\Rightarrow M=M’$ ，但这就不满足是H哈希函数的碰撞了。 因此，如果能找到H哈希函数的碰撞，就一定能找到h压缩函数的碰撞。 所以，在MD结构中，如果想要构建抗碰撞的哈希函数，就需要先保证压缩函数是抗碰撞的。 Constructing Compression Functions根据上文MD的定理：如果压缩函数h是抗碰撞的，那么H哈希函数也是抗碰撞的。 因此，本节的目标就是如何构建压缩函数 $\\mathrm{h: T \\times X \\rightarrow T}$ Comp. func. from a block cipher我们可以从块密码中构建压缩函数。 block cipher: $\\mathrm{E}: \\mathrm{K} \\times{0,1}^{\\mathrm{n}} \\longrightarrow{0,1}^{\\mathrm{n}}$ Davies-Meyer compression function: $\\mathrm{h(H, m)=E(m, H) \\oplus H}$ 【把消息块m作为密钥】 Thm: Suppose E is an ideal cipher (collection of |K| random perms.)Finding a collision $\\mathrm{h(H,m)=h(H’,m’)}$ takes $\\mathcal{O}(2^{n/2})$ evaluations of (E, D). 【如果E是一个理想的密码，即每一个密钥都对应一个随机置换，那么通过生日攻击，找到一个碰撞需要 $\\mathcal{O}(2^{n/2})$ 次(E, D)的计算。】 如果DM压缩函数是 $\\mathrm{h(H, m)=E(m, H)}$ ，函数h就不抗碰撞。 其他通过块密码构建的压缩函数： 简化 $\\mathrm{E}:{0,1}^{\\mathrm{n}} \\times{0,1}^{\\mathrm{n}} \\rightarrow{0,1}^{\\mathrm{n}}$ Miyaguchi-Preneel: $\\mathrm{h(H, m)=E(m, H) \\oplus H \\oplus m}$ (Whirlpool) $\\mathrm{h(H, m)=E(H \\oplus m, m) \\oplus m}$ and so on. 同样的，对于这样的变体 $\\mathrm{h}(\\mathrm{H}, \\mathrm{m})=\\mathrm{E}(\\mathrm{m}, \\mathrm{H}) \\oplus \\mathrm{m}$ 是不安全的。 Case study: SHA-256SHA-256哈希函数的组成要件： Merkle-Damgard function Davies-Meyer compression function Block cipher: SHACAL-2 其中512-bit的key是从msg block中截取的，256-bit block是chaining variable. Provable compression functions还有一种可证明的压缩函数，即如果你能找到该压缩函数的碰撞，必须解决困难问题。 对于任意 $\\mathrm{m}, \\mathrm{h} \\in{0, \\ldots, \\mathrm{p}-1}$ ，定义压缩函数 $\\mathrm{h(H, m)=u^{H} \\cdot v^{m} \\quad(\\bmod p)}$ . Fact: ﬁnding collision for h(.,.) is as hard as solving “discrete‐log” modulo p. Problem: slow. HMAC通过上述Merkle-Damgard的构造，如果压缩函数h是抗碰撞的，H哈希函数也是抗碰撞的。 那么，我们能否直接使用该哈希函数H(·)构造MAC? $\\mathrm{\\mathrm{H}: X^{\\leq L} \\longrightarrow T}$ a C.R. Merkle-Damgard Hash Function Attempt： $\\mathrm{s(k, m)=H(k | m)}$ 这样是不安全的，类似上篇文章中提到的NMAC，如果没有最后一步加密，就是不安全的。 通过Extension Attack ，对给定哈希值 $\\mathrm{H(k| m)}$ ，可以计算出任意 $\\mathrm{H(k| m|w)}$ . HMAC: Hash MAC因此，通过哈希函数构造MAC的标准方法是HMAC(Hash MAC)，H是哈希函数，通过不同的哈希函数，该MAC值也有不同的输出长度。比如用SHA-256作为哈希函数，MAC的输出就是256比特。 HMAC可以将任意哈希函数作为黑盒，因此HMAC广泛应用于互联网中的协议。 HMAC:$$\\mathrm{S}(\\mathrm{k}, \\mathrm{m})=\\mathrm{H}(\\mathrm{k} \\oplus \\mathrm{opad} | \\mathrm{H}(\\mathrm{k} \\oplus \\mathrm{ipad} | \\mathrm{m}))$$（opad: outer pad, 512-bit) （ipad: inner pad, 512-bit） HMAC的结构和基于PRF的NMAC结构很像： $\\mathrm{h(IV,k\\oplus ipad)}$ : 是NMAC的 $\\mathrm{k_1}$ . $\\mathrm{h(IV,k\\oplus opad)}$ : 是NMAC的 $\\mathrm{k_2}$ . 主要的不同点在于，HMAC中的两个密钥 $\\mathrm{k_1,k_2}$ 是相关的。 Timing Attacks on MAC Verification有一种针对MAC验证的时间攻击。 比如在Keyczar crypto library中，验证MAC的函数简化为： Problem: ‘==’在python中的实现是按字节对比 (byte-by-byte comparison)。 因此，如果对比出了第一个不相等的字节，就会返回false. Timing AttackTiming attack: 对目标消息m计算tag 向服务器随机请求一个随机tag，记录下验证时间。 遍历随机tag的第一个字节，发送给服务器。当验证时间比第一步中的时间略长时，停止，即找到了第一个字节的值。 重复tag的所有字节，直到找到最终有效的tag。 保护方法的核心就是：让所有字符串的比较时间都相同。 Defense 1第一种是要比较完所有字节。 但囿于编译器的优化等因素，这很难实现。 Defense 2第二种则是通过再次MAC来比较两个字符是否相同。 由此可见，密码算法的实现也会让算法变得不安全。 Boneh: Don’t implement crypto yourself!","link":"/2021/12/28/stanford-integrity2/"},{"title":"「Cryptography-Boneh」:Integrity","text":"这篇文章主要介绍消息验证码，即MAC (Message Auth. Code)。 文章首先介绍了secure MAC的模型和安全定义，当攻击者能伪造出新的msg/tag对时，MAC就不再安全。 文章的第二部分介绍了基于PRF的MAC构造，根据相关定理，只要PRF的输出空间足够大，且这是一个安全的PRF，则基于PRF的MAC就是安全的。 但基于PRF的MAC只能计算固定消息大小的MAC，如何利用这个工具构造出更大消息空间的MAC？ 文章后半部分给出了一些主流的MAC构造： 串行构造：CBC-MAC、NMAC、CMAC 并行构造：PMAC、HMAC（下一篇文章） 基于one-time MAC: CW MAC 此外，文章还介绍了MAC Padding技术。 Message Auth. Codes本节的目标是保证消息的完整性，暂时不考虑消息机密性。 比如保护磁盘上的公共二进制文件不会被篡改，保护web页面上的广告消息不会被替换篡改。 Def: MAC I = (S, V) defined over (K, M, T) is a pair of algs: S(k, m) outputs t in T V(k, m, t) outputs ‘yes’ or ‘no’ MAC是一组定义在(K, M, T)上的算法(S, V)，生成tag的算法S(k, m)和验证tag的算法V(k, m, tag)。 消息的完整性需要发送方和接收方共享一个密钥。 如果是用CRC来保证消息的完整性，攻击者可以修改消息，并轻易的计算出新的CRC。 所以CRC可以用于检测随机错误（信道噪声），而不能用于检测恶意修改。 Secure MACs: Model在定义安全MACs的模型中： Attacker’s power: chosen message attack选择明文攻击: 对于任意的消息 $\\mathrm{m_1,m_2,…,m_q}$ ，攻击者能得到其对应的消息验证码 $\\mathrm{t}_{\\mathrm{i}} \\leftarrow \\mathrm{S}\\left(\\mathrm{k}, \\mathrm{m}_{\\mathrm{i}}\\right)$ Attacker’s goal: existential forgery伪造: 希望能伪造 新的 message/tag (m,t)。新的，意味着 $(\\mathrm{m}, \\mathrm{t}) \\notin\\left\\{\\left(\\mathrm{m}_{1}, \\mathrm{t}_{1}\\right), \\ldots,\\left(\\mathrm{m}_{\\mathrm{q}}, \\mathrm{t}_{\\mathrm{q}}\\right)\\right\\}$ new valid message/tag pair: attacker cannot produce a valid tag for a new message.意味着不能为新的消息生成有效的tag given (m,t) attacker cannot even produce (m,t’) for $\\mathrm{t’\\ne t}$也不能为原消息生成新的tag Secure MACs: Def在定义的MAC游戏中: 首先挑战者会随机选择密钥 $\\mathrm{k\\leftarrow K}$ ，作为MAC的签名算法的密钥。 攻击者向挑战者发起q个询问，每次询问都可以得到该消息对应的消息验证码。 攻击者会生成一对msg/tag pair: (m, t)，如果该pair通过了挑战者的验证算法，并且是一对新的msg/tag pair，挑战者输出：1，否则输出0。 所以secure MACs定义为：攻击者生成new message/tag pair 通过挑战者验证算法的概率为neg. Def：I=(S, V) is a secure MAC if for all “efficient” A:$$\\mathrm{Adv_{MAC}[A,I]=Pr[Chal. output 1]\\quad is “negligible”}$$ e.g.1: 两个消息具有同样tag的概率为1/2，所以Adv[A, I]=1/2. e.g.2: 同理，tag的长度是5bits，所以两个不同消息有同样tag的概率为1/32. MACs Based on PRFs任何PRF都可以转换为MAC。 对于一个PRF: $\\mathrm{F:K\\times X\\longrightarrow Y}$ 由该PRF定义的MAC $\\mathrm{I_F=(S,V)}$ 为： S(k, m) := F(k, m) V(k, m, t): output ‘yes’ if t = F(k,m) and ‘no’ otherwise. 但安全的PRFs不一定可以生成安全的MACs，比如： 由上式子生成的MACs并不是一个安全的MACs，因为tag的长度太短，攻击者可以为任何消息猜测其tag。 Security所以，要保证基于PRFs的MACs的安全性，还需要PRF的输出空间足够大，否则攻击者可以通过猜测攻击成功。 Thm: If $\\mathrm{F:K\\times X\\rightarrow Y}$ is a secure PRF and 1/|Y| is negligible(i.e. |Y| is large) then $\\mathrm{I_F}$ is a secure MAC. In particular, for every eff. MAC adversary A attacking $\\mathrm{I_F}$ there exists an eff. PRF adversary B attacking F s.t.:$$\\mathrm{Adv_{MAC}[A,I_F]\\le Adv_{PRF}[B, F]+1/|Y|}$$$\\mathrm{I_F}$ is secure as long as |Y| is large, say |Y| = $2^{80}$. 对于每一个攻击MAC的攻击者A，都存在攻击对应PRF的攻击者B，满足上述式子。所以如果不等式右边都为neg.，那不等式左边一定为neg. Truncating MACs based on PRFsTruncating MACs的定义如下： Easy lemma: suppose $\\mathrm{F:K\\times X\\rightarrow {0,1}}$ is a secure PRF. Then so is $\\mathrm{F_t(k,m)=F(k,m)[1\\dots t]}$ for all $\\mathrm{1\\le t\\le n}$ 基于PRFs的MACs，截断后的MACs，如果tag长度足够长，也是安全的。 if (S,V) is a MAC is based on a secure PRF outputing n‐bit tags the truncated MAC outputing w bits is secure as long as $\\mathrm{1/2^w}$ is still negligible (say w≥64) ExamplesAES可以看作为：a MAC for 16-byte messages. 所以引入了一个新的问题：how to convert small-MAC into a Big-MAC ? 实践中有两种主流构造方法： CBC-MAC (banking – ANSI X9.9, X9.19, FIPS 186‐3) HMAC (Internet protocols: SSL, IPsec, SSH, …) Sequential Constructions本节的主要目标是: given a PRF for short messages (AES) construct a PRF for long messages. From here on let $\\mathrm{X={0,1}^n}$ (e.g. n=128) 1. CBC-MACCBC-MAC 也叫 encrypted CBC-MAC (ECBC) PRP: $\\mathrm{F:K\\times X\\rightarrow X}$ . ECBC定义新的PRF $\\mathrm{F_{ECBC}:K^2\\times X^{\\le L}\\rightarrow X}$ . 新的PRF的输入为：一对密钥 和 long messages (L: blocks) 绿色部分就是原生的CBC（使用密钥k），而encrypted表现在对CBC的输出再进行加密操作（使用密钥 $\\mathrm{k_1}$ ，得到消息的MAC。 需要注意的是，最后对CBC输出进行加密的密钥是独立于CBC的密钥。 2. NMACNMAC即 nested MAC。 PRF: $\\mathrm{F:K\\times X\\rightarrow K}$ NMAC定义新的PRF $\\mathrm{F_{NMAC}:K^2\\times X^{\\le L}\\rightarrow K}$ . 新的PRF的输入为：一对密钥 和 long messages (L: blocks) 绿色部分是层叠(cascade)部分，把本次消息块PRF的输出作为下次消息块PRF运算的密钥，cascade部分的输出为t。 最后对t做padding操作（fixed pad），对于padding后的结果，用新的密钥 $\\mathrm{k_1}$ 再做一次PRF运算，得到最终的MAC。 Last Encryption在ECBC和NMAC中，最后都有加密的步骤。 Why the last encryption step in ECBC-MAC ? NMAC: 假设NMAC只有cascade部分，即S(k,m)=cascade(k,m) Then this MAC can be forged with one chosen msg query. Extension Attack: 已知cascade(k, m)，对于任意消息块w，可以轻易得到cascade(k, m||w)的值。 因为cascade(k, m)是作为下一个消息块运算的密钥，所以cascade(k, m||w) = PRF(cascade(k, m), w)。 所以最后的加密步骤是为了阻止extension attack。 ECBC： 同样，假设ECBC只有raw CBC部分，没有最后加密的步骤，即S(k,m)=rawCBC(k,m). Then $\\mathrm{I_{RAW}=(S,V)}$ is easily broken using a 1-chosen msg attack. 1-chosen msg attack: 攻击者随机选择one-block message m. 请求得到m对应的tag t = F(k,m) 攻击者可以伪造2-block message $\\mathrm{(m,t\\oplus m)}$ 的MAC，其实也就是 t。 $\\operatorname{rawCBC}(\\mathrm{k},(\\mathrm{m}, \\mathrm{t} \\oplus \\mathrm{m}))=\\mathrm{F}(\\mathrm{k}, \\mathrm{F}(\\mathrm{k}, \\mathrm{m}) \\oplus(\\mathrm{t} \\oplus \\mathrm{m}))=\\mathrm{F}(\\mathrm{k}, \\mathrm{t} \\oplus(\\mathrm{t} \\oplus \\mathrm{m}))=\\mathrm{t}$ Security AnalysisTheorem: For any L &gt; 0, For every eff. q-query PRF adv. A attacking $\\mathrm{F_{ECBC}}$ or $\\mathrm{F_{NMAC}}$ , there exists an eff. adversary B s.t.: $$ \\begin{array}{l}\\operatorname{Adv}_{\\mathrm{PRF}}\\left[\\mathrm{A}, \\mathrm{F}_{\\mathrm{ECBC}}\\right] \\leq \\mathrm{Adv}_{\\mathrm{PRP}}[\\mathrm{B}, \\mathrm{F}]+2 \\mathrm{q}^{2} /|\\mathrm{X}| \\\\ \\operatorname{Adv}_{\\mathrm{PRF}}\\left[\\mathrm{A}, \\mathrm{F}_{\\mathrm{NMAC}}\\right] \\leq \\mathrm{q} \\cdot \\mathrm{L} \\cdot \\mathrm{Adv}_{\\mathrm{PRF}}[\\mathrm{B}, \\mathrm{F}]+\\mathrm{q}^{2} / 2|\\mathrm{~K}|\\end{array} $$ CBC‐MAC is secure as long as q &lt;&lt; $|X|^{1/2}$ NMAC is secure as long as q &lt;&lt; $|K|^{1/2}$ (q &lt;&lt; $2^{64}$ for AES-­128) 为了简化讨论，考虑不等式： $\\operatorname{Adv}_{\\mathrm{PRF}}\\left[\\mathrm{A}, \\mathrm{F}_{\\mathrm{ECBC}}\\right] \\leq \\mathrm{Adv}_{\\mathrm{PRP}}[\\mathrm{B}, \\mathrm{F}]+ \\mathrm{q}^{2} /|\\mathrm{X}|$ q = #messages MAC-ed with k 如果我们想得到 $\\operatorname{Adv}_{\\mathrm{PRF}}\\left[\\mathrm{A}, \\mathrm{F}_{\\mathrm{ECBC}}\\right] \\leq 1 / 2^{32}$ 的结果，必须满足： $\\mathrm{q^{2} /|X|","link":"/2021/11/22/stanford-integrity1/"},{"title":"「机器学习-李宏毅」：Tips for Deep Learning","text":"这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。tips从Training和Testing两个方面展开。在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。 Recipe of Deep LearningDeep Learning 的三个步骤： 如果在Training Data中没有得到好的结果，需要重新训练Neural Network。 如果在Training Data中得到好的结果，在Testing Data（这里的Testing Data是指有Label的Data，比如Kaggle的Public Data或者是从Training Data中划分出的Development Data）没有得到的好的结果，说明Overfitting了，需要重新设计Neural Network的结构。 Do not always blame Overfitting 如果在Testing Data中，看到上图，20-layer的error小，56-layer的error大，56-layer一定overfitting了。 No!!!不要总把原因归咎于Overfitting。 再看Testing Data error之前，先看看Training Data的error。上图中，56-layer的DNN在Training Data的error本来就比20-layer的大，说明56-layer的DNN根本没有train好。 所以56-layer的DNN在Testing Data上的error大，原因不是overfitting，而是模型根本没有train好。 注： Overfitting是在Training Data上error小，但在Testing Data上的error大。 因此，对于在Training Data上得到不好的结果和在Training Data上得到好的结果但在Testing Data上得到不好的结果这两种情况，需要不同的解决方法。 Bad Results on Training Data在不重新设计DNN结构时，如果在Training Data中得到Bad Results，一般有两种方法来改进结果： New activation function【neuron换新的激活函数】 Adaptive Learning Rate New activation functionVanishing Gradient Problem 上图表示，在手写数字辨识中，Deeper layers并不能有好的performance。 为什么会这样呢？ 因为出现了Vanishing Gradient Problem，即gradient随着deeper layer逐渐消失的问题。 上图中，假设neuron的activation function是sigmod函数。 靠近Input layer层的参数的变化对Loss的影响很小，所以对Loss function做微分，gradient很小，参数更新慢。 而靠近Output layer层的参数的编号对Loss的影响更大，所以对Loss function做微分，gradient很大，参数更新快。 因为靠近Output Layer层的参数更新快，所以很快converge（收敛、趋于稳定）；但靠近Input Layer层的参数更新慢，几乎还处在random（随机）的状态。 当靠近Output Layer层的参数趋于稳定时，由于靠近Output Layer层的参数对Loss影响大，所以观察到的Loss的值也趋于稳定，于是，你就把training停掉了。 但是，靠近Input层的参数几乎处在random状态，所以拿模型用在Testing Data上，发现结果几乎是随机的。 怎么直观理解靠近Input Layer的参数的gradient小呢？ 用微分的直观含义来表示gradient $\\partial{l}/\\partial{w}$ : 当 $w$ 增加 $\\Delta{w}$ 时，如果 $l$ 的变化 $\\Delta{l}$ 变化大，说明 $\\partial{l}/\\partial{w}$ 大，否则 $\\partial{l}/\\partial{w}$ 小。 我们在DNN中使用的activation function是sigmod函数，sigmod函数会把值压到0和1之间。 因此，上图中，其他值不变，只有连接 $x_N$ 的参数 $w$ 增加 $\\Delta w$ 时，输入通过neuron的sigmod函数，函数的输出增加的 $\\Delta$ 会变小，随着Deeper Layer，neuron的输出的 $\\Delta$ 会越变越小，趋至0。 最后DNN输出的变化对 loss的影响小，即 $\\Delta{l}$ 趋至0，即参数的gradient $\\partial{l}/\\partial{w}$ 趋至0。（即 Vanishing Gradient） ReLu ：Rectified Linear Unit为了防止发生Vanishing Gradient Problem，在DNN中选择使用新的activation function。 ReLu长下面这个样子： z: input a: output 当 $z\\leq0$ 时， $a=0$ ；当 $z &gt;0$ 时， $a=z$ 。 Reason : Fast to compute Biological reason【有生物上的原因】 Infinite sigmod with different biases. 【是无穷个 有不同bias的sigmod函数 的叠加】 Vanishing gradient problem 【最重要的是没有vanishing gradient problem】 为什么ReLu没有vanishing gradient problem 上图DNN中，ReLu在输入是负数时，输出是0。因此这些输出是0的neuron可以去掉。 就变成了下图这个A Thinner linear network。由于ReLu函数的性质，靠近Input Layer的参数不会有smaller gradient。 这里有一个Q&amp;A: Q1: function变成linear的，会不会DNN就变弱了？ ： 当neuron的operation region不变的话，DNN的确是linear的，但是当neuron的operation region改变后，就是unlinear的。 ：即，当input的变化小，operation region不变（即输入不会从大于0变成小于0，小于0变成大于0这种），model还是linear的；但当input的变化大时，很多neuron的operation region都变化了，model其实就是unlinear的。 Q2: ReLu 怎么微分？ ：ReLu在0点不可微，那就随便指定为0这样（台湾腔QAQ）。 ReLu - variant当 $z\\leq 0$ 时，输出为0，就不能更新参数了。于是就有下图变体： 当 $z\\leq0$ 时，gradient都为0.01，为什么不能是其他值。于是就有下图变体：其中 $\\alpha$ 也是一个需要学习的参数 MaxoutMaxout，如下图，在设计neural network时，会给每一层的neuron分组，成为一个新的neuron。 Maxout也是一个Learnable activation function。 ReLu是Maxout学出来的一个特例。 上图中，左图是ReLu。 ReLu的输入 $z = wx+b$ ，输出 $a$ 如上图的绿色的线。 右图是Maxout。Maxout的输入 $z_1 =wx+b,z_2=0$ ，那么输出取max，输出 $a$ 如上图中绿色的线，和左图的ReLu相同。 Maxout is more than ReLu。 当参数更新时，Maxout的函数图像如下图： DNN中的参数是learnable的，所以Maxout也是一个learnable的activation function。 Reason ： Learnable activation function [Ian J. Goodfellow, ICML’13] Activation function in maxout network can be any piecewise linear convex function. 在maxout神经网络中的激活函数可以是任意的分段凸函数。 How many pieces depending on how many elements in a group. 分段函数分几段取决于一组中有多少个元素。 Maxout : how to trainGiven a training data x, we know which z would be the max. 【当给出每笔training data时，我们能知道Maxout neuron中哪一个最大】 如上图，在这笔training data x中，我们只train this thin and linear network 的参数，即max z相连的参数。 每笔不同的training data x，会得到不同的thin and linear network，最后，会train到每一个参数。 Adaptive Learning RateReview Adagrad在这篇文章： Gradient 第一小节讲到一种adaptive learning rate的gradient 算法：Adagrad 算法。在那篇文章中，我们得出的结论是 the best step $\\propto$ |First dertivative| / Second derivative. 在上图中，两个方向，因为蓝色方向的二阶微分更小，所以蓝色方向应该有更大的learning rate。 因此，在Adagrad中，我们用一阶微分来估量二阶微分的大小： $$ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t} $$ RMSProp但是，在训练NN时，Error Surface（Total Loss对参数的变化）的图像可能会更复杂，如下图： 因为函数图像过于复杂，可能在同一方向的不同位置，也需要有不同的learning rate。 RMSProp是Adagrad的进阶版。 RMSProp过程： $w^{1} \\leftarrow w^{0}-\\frac{\\eta}{\\sigma^{0}} g^{0} \\quad \\sigma^{0}=g^{0}$ $w^{2} \\leftarrow w^{1}-\\frac{\\eta}{\\sigma^{1}} g^{1} \\quad \\sigma^{1}=\\sqrt{\\alpha (\\sigma^{0})^2+(1-\\alpha)(g^1)^2}$ $w^{3} \\leftarrow w^{2}-\\frac{\\eta}{\\sigma^{2}} g^{2} \\quad \\sigma^{2}=\\sqrt{\\alpha (\\sigma^{1})^2+(1-\\alpha)(g^2)^2}$ … $w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sigma^{t}} g^{t} \\quad \\sigma^{t}=\\sqrt{\\alpha (\\sigma^{t-1})^2+(1-\\alpha)(g^t)^2}$ $\\sigma^t$ 也是在算gradients的 root mean squar。 但是在RMSProp中，加入了参数 $\\alpha$ (需要手动调节大小的参数)，可以给当前算出来的gradient $g^t$ 更大的权重，即更相信现在gradient的方向，不那么相信以前gradient的方向。 MomentumMomentum，则是引用物理中的惯性。 上图中，当小球到达local minima时，会因为惯性继续往前更新，则有可能到达minima的位置。 这里的Momentum，就代指上一次前进（参数更新）的方向。 Vanilla Gradient Descent 如果将Gradient的步骤画出图来，就是下图这样： 过程： Start at position $\\theta^0$ Compute gradietn at $\\theta^0$ Move to $\\theta^1=\\theta^0-\\eta\\nabla{L(\\theta^0)}$ Compute gradietn at $\\theta^1$ Move to $\\theta^2=\\theta^1-\\eta\\nabla{L(\\theta^1)}$ … …Stop until $\\nabla{L(\\theta^t)}\\approx0$ Momentum 在Momentum中，参数更新方向是当前Gradient方向和Momentum方向（上一次更新方向）的叠加。 Movement方向：上一次更新方向 - 当前gradient方向。 过程： Start at position $\\theta^0$ Movement: $v^0=0$ Compute gradient at $\\theta^0$ Movement $v^1=\\lambda v^0-\\eta\\nabla{L(\\theta^0)}$ Move to $\\theta^1=\\theta^0+v^1$ Compute gradient at $\\theta^1$ Movement $v^2=\\lambda v^1-\\eta\\nabla{L(\\theta^1)}$ Move to $\\theta^2=\\theta^1+v^2$ … …Stop until $\\nabla{L(\\theta^t)}\\approx0$ 和Vanilla Gradient Descent比较，$v^i$ 其实是过去gradient( $\\nabla{L(\\theta^0)}$ 、$\\nabla{L(\\theta^1)}$ 、… 、 $\\nabla{L(\\theta^{i-1})}$ )的加权和。 迭代过程： $v^0=0$ $v^1=-\\eta\\nabla{L(\\theta^0)}$ $v^2=-\\lambda\\eta\\nabla{L(\\theta^0)}-\\eta\\nabla{L(\\theta^1)}$ … 再用那个小球的例子来直觉的解释Momentum： 当小球在local minima时，gradient为0，但是Momentum（即上次移动方向）是继续往前，于是小球可以继续向前更新。 Adam = RMSProp + Momentum Algorithm：Adam, our proposed algorithm for stochastic optimization. 【Adam，是为了优化stochastic gradient】（至于什么是stochastic gradient，建议戳Post not found: Gradietn 这篇) $g_t^2$ indicates the elementwise square $g_t\\odot g_t$ . 【$g_t^2$ 是gradient $g_t$ 向量和 $g_t$ 的元素乘】 Good default settings for the tested machine learning problems are $\\alpha=0.001$ , $\\beta_1=0.9$ , $\\beta_2=0.999$ and $\\epsilon=10^{-8}$ . All operations on vectors are element-wise. With $\\beta_1^t$ and $\\beta_2^t$ we denote $\\beta_1$ and $\\beta_2$ to the power t. 【参数说明：算法默认的参数设置是 $\\alpha=0.001$ , $\\beta_1=0.9$ , $\\beta_2=0.999$ ， $\\epsilon=10^{-8}$ 。算法中所有vector之间的操作都是对元素操作。 $\\beta_1^t$ 和 $\\beta_2^t$ 是 $\\beta_1$ 和 $\\beta_2$ 的 $t$ 次幂】 Adam Pseudo Code： Require：$\\alpha$ : Stepsize 【步长/learning rate $\\eta$ 】 Require：$\\beta_1,\\beta_2\\in\\left[0,1\\right)$ : Exponential decay rates for the moment estimates. Require：$f(\\theta)$ : Stochastic objective function with parameters $\\theta$ .【参数 $\\theta$ 的损失函数】 Require: $\\theta_0$ ：Initial parameter vector 【初值】 $m_0\\longleftarrow 0$ (Initial 1st moment vector) 【 $m$ 是Momentum算法中的更新参数后的方向 $v$ 】 $v_0\\longleftarrow 0$ (Initial 2nd moment vector) 【 $v$ 是RMSprop算法中gradient的root mean square $\\sigma$ 】 $t\\longleftarrow 0$ (Initial timestep) 【更新次数】 while $\\theta_t$ not concerged do 【当 $\\theta$ 趋于稳定，即 $\\nabla{f(\\theta)}\\approx0$ 时】 $t\\longleftarrow t+1$ $g_t\\longleftarrow \\nabla{f_t(\\theta_{t-1})}$ (Get gradients w.r.t. stochastic objective at timestep t) 【算第t次时 $\\theta$ 的gradient】 $m_{t} \\leftarrow \\beta_{1} \\cdot m_{t-1}+\\left(1-\\beta_{1}\\right) \\cdot g_{t}$ (Update biased first momen t estimate) 【用Momentum算更新方向】 $v_{t} \\leftarrow \\beta_{2} \\cdot v_{t-1}+\\left(1-\\beta_{2}\\right) \\cdot g_{t}^{2}$ (Update biased second raw moment estimate) 【RMSprop估测最佳步长（ 和$v$ 负相关） 】 $\\widehat{m}_{t} \\leftarrow m_{t} /\\left(1-\\beta_{1}^{t}\\right)$ （Comppute bbi. as-corrected first momen t estima te) 【算出来的值有bias，论文中有具体解释为什么有。当更新次数增加时， $1-\\beta_1^t$ 也趋近于1】 $\\widehat{v}_{t} \\leftarrow v_{t} /\\left(1-\\beta_{2}^{t}\\right)$ (Compute bias-corrected second raw momen t estimate) 【和上同理】 $\\theta_{t} \\leftarrow \\theta_{t-1}-\\alpha \\cdot \\widehat{m}_{t} /(\\sqrt{\\widehat{v}_{t}}+\\epsilon)$ （Update parameters） 【 $\\widehat{m}t$ 相当于是更准确的gradient的方向，$\\sqrt{\\widehat{v}{t}}+\\epsilon$ 是为了估测最好的步长，调节learning rate】 Gradient Descent Limitation？在Gradient这篇文章中，讲到过Gradient有一些问题不能处理： Stuck at local minima Stuck at saddle point Very slow at the plateau （李老师说的，不是我说的QAQ）：但是Andrew（吴恩达）在2017年说过，不用太担心这个问题。为什么呢？ 如果要stuck at local minima，前提是每一维度都是local minima。 如果在一个维度遇到local minima的概率是p，当NN很复杂时，有很多参数时，比如1000，那么遇到local minima的概率是 $p^{1000}$ ，趋近于0了，几乎不会发生。 ：所以不用太担心Gradient Descent的局限性。 Bad Results on Testing DataEarly Stopping在更新参数时，可能会出现这样曲线图： 图中，Total Loss在training set中逐渐减小，但在validation set中逐渐增大。 而我们真正关心的其实是validation set的Loss。 所以想让参数停在validation set中loss最低时。 Keras能够实现EarlyStopping功能[1]：click here 123from keras.callbacks import EarlyStoppingearly_stopping = EarlyStopping(monitor='val_loss', patience=2)model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) RegularizationRegularization：Find a set of weight not only minimizing original cost but also close to zero. 构造一个新的loss function，除了最小化原来的loss function，还能使得参数趋紧0，使得function更平滑。 function的曲线更平滑，当输入有轻微扰动，不会太影响输出的结果。 L2 norm regularizationNew loss function: $$ \\begin{equation} \\begin{aligned} \\mathrm{L}^{\\prime}(\\theta)&=L(\\theta)+\\lambda \\frac{1}{2}\\|\\theta\\|_{2} \\\\ \\theta &={w_1,w_2,...} \\\\ \\|\\theta\\|_2&=(w1)^2+(w_2)^2+... \\end{aligned} \\end{equation} $$ 其中用第二范式 $\\lambda\\frac{1}{2}|\\theta|_2$ 作为regularization term。做regularization是为了使函数更平滑，所以一般不考虑bias) New gradient: $$ \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w}=\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda w $$ New update: $$ \\begin{equation} \\begin{aligned} w^{t+1} &\\longrightarrow w^{t}-\\eta \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w} \\\\ &=w^{t}-\\eta\\left(\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda w^{t}\\right) \\\\ &=(1-\\eta \\lambda) w^{t}-\\eta \\frac{\\partial \\mathrm{L}}{\\partial w} \\end{aligned} \\end{equation} $$ 在更新参数时，先乘一个 $(1-\\eta\\lambda)$ ，再更新。 weight decay（权值衰减）：由于 $\\eta,\\lambda$ 都是很小的值，所以 $w^t$ 每次都会先乘一个小于1的数，即逐渐趋于0，实现regularization。但是，因为更新中还有gradient部分，所以不会等于0。 L1 norm regularizationRegularization除了用第二范式，还可以用其他的，比如第一范式 $|\\theta|_1=|w_1|+|w_2|+…$ New loss function: $$ \\begin{equation}\\begin{aligned}\\mathrm{L}^{\\prime}(\\theta)&=L(\\theta)+\\lambda \\frac{1}{2}\\|\\theta\\|_1\\\\ \\theta &={w_1,w_2,...} \\\\ \\|\\theta\\|_1&=|w_1|+|w_2|+...\\end{aligned}\\end{equation} $$ 用sgn()符号函数来表示绝对值的求导。 符号函数：Sgn(number) 如果number 大于0，返回1；等于0，返回0；小于0，返回-1。 New gradient: $$ \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w}=\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda \\text{sgn}(w) $$ New update: $$ \\begin{equation} \\begin{aligned} w^{t+1} &\\longrightarrow w^{t}-\\eta \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w} \\\\ &=w^{t}-\\eta\\left(\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda \\text{sgn}(w^t)\\right) \\\\ &=w^{t}-\\eta \\frac{\\partial \\mathrm{L}}{\\partial w}-\\eta \\lambda \\operatorname{sgn}\\left(w^{t}\\right) \\end{aligned} \\end{equation} $$ 在用第一范式做regularization时，每次 $w^t$ 都要减一个值 $\\eta\\lambda\\text{sgn}(w^t)$ ，和用第二范式做regularization比较，后者每次都要乘一个小于1的值，即使是乘0.99，w下降也很快。 Weight decay（权值衰减）的生物意义： Our brain prunes（修剪） out the useless link between neurons. DropoutWiki: Dropout是Google提出的一种正则化技术，用以在人工神经网络中对抗过拟合。Dropout有效的原因，是它能够避免在训练数据上产生复杂的相互适应。Dropout这个术语代指在神经网络中丢弃部分神经元（包括隐藏神经元和可见神经元）。在训练阶段，dropout使得每次只有部分网络结构得到更新，因而是一种高效的神经网络模型平均化的方法。[2] 这里讲Dropout怎么做。 Training Each time before updating the parameters: Each neuron has p% to dropout. Using the new thin network for training. 【如上图，每个neuron有p的概率被dropout。于是NN就变成了下图thinner的NN】 For each mini-batch, we resample the dropout neurons. 【每次mini-batch，都要重新dropout，更新NN的结构】 TestingTesting中不做dropout If the dropout rate at training is p%, all the weights times 1-p%. 【如果在training中 dropout rate是 p%，在testing是，每个参数都乘 （1-p%)】 【比如dropout rate 是0.5。如果train出来的w是 1，那么testing中 w=0.5】 Why dropout in training：Intuitive Reason 这是一个比较有趣的比喻： 这也是一个有趣的比喻hhh: 即，团队合作的时候，如果每个人都认为队友在带我，那每个人都可能划水。 但是，（training中）如果你知道你的队友在划水，那你可能会做的更好。 但是，（testing中）发现每个人都有更好地做，都没有划水，那么结果就会很好。 （hhhh，李老师每次讲Intuitive Reason的时候，都觉得好有道理hhh，科学的直觉orz给我也整一个） Why multiply (1-p%) in testing: Intuitive reason为什么在testing中 weights要乘（1-p%)? 用一个具体的例子来直观说明： 上图中，如果dropout rate=0.5，假设只训练一次， $w_2,w_4$ 相连的neuron都被dropout。 在testing中，因为不对neurondropout，所以如果不改变weight，计算出的结果 $z’\\approx 2z$ 。 因此将所有weight简单地和(1-p%) 相乘，能尽量保证计算出的结果 $z’\\approx z$ 。 Dropout is a kind of ensembleEnsemble(合奏)，如下图，将testing data丢给train好的NN来估计，最后的估计值取所有NN输出的平均，如下图： 为什么说dropout is a kind of ensemble? Using one mini-batch to train one network 【dropout相当于每次用一个mini-batch来训练一个network】 Some parameters in the network are shared 【有些参数可能会在很多个mini-batch都被train到】 由于每个神经元有 p%的概率被dropout，因此理论上，如果有M个neuron，可能会训练 $2^M$ 个network。 但是在Ensemble中，将每个network存下来，testing的时候输出取平均，这样的过程太复杂了，结果也不一定会很好。 所以在testing中，no dropout，对原始network中的每个参数乘 (1-p%)，用这样简单的操作来达到ensemble的目的。 Reference Keras: how can i interrupt training when the validation loss isn’t decresing anymore. Dropout-wiki：https://zh.wikipedia.org/wiki/Dropout","link":"/2020/04/21/tips-for-DL/"},{"title":"「机器学习-李宏毅」:Unsupervised-PCA","text":"这篇文章详细讲解了无监督学习（Unsupervised learning）的PCA（主成分分析法）。 文章开篇从聚类（Clustering）引出Distributed Represention，其中粗略阐述了聚类中K-means和HAC（层次聚类）的思想。 文章的后半部分具体阐述了PCA的数学细节，PCA的去相关性性质，PCA的另一种解释角度（component的角度），PCA的不足等。 Unsupervised Learning无监督学习分为两种： Dimension Reduction：化繁为简。 function 只有input，能将高维、复杂的输入，抽象为低维的输出。 如下图，能将3D的折叠图像，抽象为一个2D的表示（把他摊开）。 Generation：无中生有。 function 只有output。 （后面的博客会提及） Dimension Reduction此前，在semi-supervised learning的最后，提及过better presentation的思想，Dimension Reduction 其实就是这样的思想：去芜存菁，化繁为简。 比如，在MNIST中，一个数字的表示是28*28维度的向量（图如左），但大多28 *28维度的向量（图为右）都不是数字。 因此，在表达下图一众“3”的图像中，根本不需要28*28维的向量表示，1-D即可表示一张图（图片的旋转角度）。28 * 28的图像表示就像左边中老者的头发，1-D的表示就像老者的头，是对头发运动轨迹一种更简单的表达。 Clustering在将Dimension Reduction之前，先将一种经典的无监督学习——clustering. clustering也是一种降维的表达，将复杂的向量空间抽象为简单的类别，用某一个类别来表示该数据点。 这里主要讲述cluster的主要思想，算法细节可参考其他资料 。（待补充） K-meansK-means的做法是： Clustering $X=\\left\\{x^{1}, \\cdots, x^{n}, \\cdots, x^{N}\\right\\}$ into K clusters. 把所有data分为K个类，K的确定是empirical的，需要自己确定 Initialize cluster center $c^i, i=1,2,…,K$ .(K random $x^n$ from $X$) 初始化K个类的中心数据点，建议从training set $X$ 中随机选K 个点作为初始点。 不建议直接在向量空间中随机初始化K个中心点，因为很可能随机的中心点不属于任何一个cluster。 Repeat：根据中心点标记所属类，再更新新的中心点，重复直收敛。 For all $x^n$ in $X$ : 标记所属类。 $$ b_{i}^{n}\\left\\{\\begin{array}{ll}1 & x^{n} \\text { is most \"close\" to } c^{i} \\\\ 0 & \\text { Otherwise }\\end{array}\\right. $$ Updating all $c^i$ : $c^{i}=\\sum_{x^{n}} b_{i}^{n} x^{n} / \\sum_{x^{n}} b_{i}^{n}$ (计算该类中心点) HAC：Hierarchical Agglomerative Clustering(HAC)另一种clustering的方法是层次聚类（Hierarchical Clustering），这里介绍Agglomerative（自下而上）的策略。 Build a tree. 如上图中，计算当前两两数据点（点或组合）的相似度（欧几里得距离或其他）。 选出最相近的两个合为一组（即连接在同一父子结点上，如最左边的两个） 重复1-2直至最后合为root。 该树中，越早分支的点集合，说明越不像。 Pick a threshold. 选一个阈值，即从哪个地方开始划开，比如选上图中红色的线作为阈值，那么点集分为两个cluseter，蓝色、绿色同理。 HAC和K-means相比，HAC不直接决定cluster的数目，而是通过决定threshold的值间接决定cluster的数目。 Distributed RepresentationCluster：an object must belong to one cluster. 在做聚类时，一个数据点必须标注为某一具体类别。这往往会丢失很多信息，比如一个人可能是70%的外向，30%的内敛，如果做clustering，就将这个人直接归为外向，这样的表示过于粗糙。 因此仍用vector来表示这个人，如下图。 Distributed Representation（也叫Dimension Reduction）就是：一个高维的vector通过function，得到一个低维的vector。 Distributed的方法有常见的两种： Feature selection： 如下图数据点的分布，可以直接选择feature $x_2$ . 但这种方法往往只能处理2-D的情况，对于下图这种3-D情况往往不好做特征选择。 Principle component analysis（PCA） 另一种方法就是著名的PCA，主成分分析法。 PCA中，这个function就是一个简单的linear function（$W$），通过 $z=Wx$ ，将高维的 $x$ 转化为低维的 $z$ . PCA：Principle Component AnalysisPCA的参考资料见Bishop, Chapter12. PCA就是要找 $z=Wx$ 中的 $W$ . Main IdeaReduce 1-D如果将dimension reduce to 1-D，那么可以得出 $z_1 = w^1\\cdot x$ . $w^1$ 是vector，$x$ 是vector，做内积。 如下图，内积即投影，将所有的点 $x$ 投影到 $w^1$ 方向上，然后得到对应的 $z_1$ 值。 而对于得到的一系列 $z_1$ 值，我们希望 $z_1$ 的variance越大越好。 因为 $z_1$ 的分布越大，用 $z_1$ 来刻画数据，才能更好的区分数据点。 如下图，如果 $w^1$ 的方向是small variance的方向，那么这些点会集中在一起，而large variance方向，$z_1$ 能更好的刻画数据。 $z_1$ 的数学表达是： $ \\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2} \\quad \\left\\|w^{1}\\right\\|_{2}=1$ (后文解释为什么要 $w^1$ 的长度为1) Reduce 2-D同理，如果将dimension reduce to 2-D . $z=Wx$ 即 $$ \\left\\{ \\begin{array}{11}z_1=w^1\\cdot x \\\\ z_2=w^2 \\cdot x \\end{array} \\right. ,\\quad W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\end{bmatrix} $$ 将所有点 $x$ 投影到 $w^1$ 方向，得到对应的 $z_1$ ，且让 $z_1$ 的分布尽可能的大： $$ \\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2} ,\\quad \\left\\|w^{1}\\right\\|_{2}=1 $$ 将所有点投影到 $w^2$ 方向，得到对应的 $z_2$ ，同样让 $z_2$ 的分布也尽可能大，再加一个约束条件，让 $w^2$ 和 $w^1$ 正交（后文会具体解释为什么） $$ \\operatorname{Var}\\left(z_{2}\\right)=\\frac{1}{N} \\sum_{z_{2}}\\left(z_{2}-\\overline{z_{2}}\\right)^{2} ,\\quad \\left\\|w^{2}\\right\\|_{2}=1 ,\\quad w^1\\cdot w^2=0 $$ 因此矩阵 $W$ 是Orthogonal matrix (正交矩阵)。 Detail[Warning of Math想跳过math部分的，可以直接看Conclusion。 1-D中： Goal：find $w^1$ to maximum $(w^1)^T S w^1$ s.t.$(w^1)^Tw^1=1$ 结论：$w^1$ 就是协方差矩阵 $S$ 最大特征值 $\\lambda_1 $ 对应的特征向量。 s.t.$(w^1)^Tw^1=1$ 2-D中： Goal：find $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ 结论：$w^2$ 就是协方差矩阵$S$ 第二大特征值 $\\lambda_2 $ 对应的特征向量。 s.t.$(w^2)^Tw^2=1$ k-D中： 结论：$w$ 就是协方差矩阵 $S$ 前 $k$ 大的特征值对应的特征向量。s.t. $W$ 是正交矩阵。 1-DGoal：Find $w^1$ to maximum the variance of $z_1$ . $\\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2}$ $z_1=w^1\\cdot x ,\\quad \\overline{z_{1}}=\\frac{1}{N} \\sum_{z_{1}}=\\frac{1}{N} \\sum w^{1} \\cdot x=w^{1} \\cdot \\frac{1}{N} \\sum x=w^{1} \\cdot \\bar{x}$ $\\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2}=(w^1)^T\\operatorname{Cov}(x)w^1$ $=\\frac{1}{N} \\sum_{x}\\left(w^{1} \\cdot x-w^{1} \\cdot \\bar{x}\\right)^{2} $ $=\\frac{1}{N} \\sum\\left(w^{1} \\cdot(x-\\bar{x})\\right)^{2}$ $a,b$ 是vector： $(a\\cdot b)^2=(a^Tb)^2=a^Tba^Tb$ $a^Tb$ 是scalar: $(a\\cdot b)^2 = (a^Tb)^2=a^Tba^Tb =a^Tb(a^Tb)^T=a^Tbb^Ta$ $=\\frac{1}{N} \\sum\\left(w^{1}\\right)^{T}(x-\\bar{x})(x-\\bar{x})^{T} w^{1}$ $ = \\left(w^{1}\\right)^{T}\\sum\\frac{1}{N}(x-\\bar{x})(x-\\bar{x})^{T} \\ w^{1}$ $=(w^1)^T\\operatorname{Cov}(x)w^1$ 令 $S=\\operatorname{Cov}(x)$ 之前遗留的两个问题： $\\left|w^1\\right|_2=1$ ? $w^1\\cdot w^2=1$ ? 现在来看第一个问题，为什么要 $\\left|w^1\\right|_2=1$ ？ 现在的目标，变成了 maximum $(w^1)^T S w^1$ ，如果不限制 $\\left|w^1\\right|_2$ ，让 $\\left|w^1\\right|_2$ 无穷大，那么 $(w^1)^T S w^1$ 的值也会无穷大，问题无解了。 Goal：maximum $(w^1)^T S w^1$ s.t. $(w^1)^Tw^1=1$ Lagrange multiplier[挖坑] 求解多元变量在有限制条件下的驻点。 构造拉格朗日函数： $g\\left(w^{1}\\right)=\\left(w^{1}\\right)^{T} S w^{1}-\\alpha\\left(\\left(w^{1}\\right)^{T} w^{1}-1\\right)$ ，$\\alpha\\neq 0$ 为拉格朗日乘数 $\\nabla_{w^1}g=0$ 的值为驻点（会单独写一篇博客来讲拉格朗日乘数） $\\frac{\\partial g}{\\partial \\alpha}=0$ 为限制函数 对矩阵微分：详情见wiki scalar-by-vector(scalar对vector微分) $S$ 是对称矩阵，不是 $w^1$ 的函数，结果用 $w^1$ 表达：$2Sw^1-2\\alpha w^1=0$ maximum: $(w^1)^T S w^1=\\alpha (w^1)^Tw^1=\\alpha$ *Goal：find $w^1$to maximum $\\alpha$ * $\\alpha$ 满足等式：$Sw^1=\\alpha w^1$ $\\alpha$ 是 $S$ 的特征向量，$w^1$ 是 $S$ 对应于特征值 $\\alpha$ 的特征向量。 关于特征值和特征向量的知识参考：参考下面线代知识 $w^1$ is the eigenvector(特征向量) of the covarivance matrix S corresponding to the largest eigenvalue $\\lambda_1$ . 结论：$w^1$ 就是协方差矩阵最大特征值对应的特征向量。 梦回线代QWQ（自己线代学的太差啦 啊这！ 特征向量，特征值定义： $A$ 是n阶方阵，如果存在数 $\\lambda$ 和n维非零向量 $\\alpha$ ，满足 $A\\alpha=\\lambda \\alpha$ , 则称 $\\lambda$ 为方阵 $A$ 的一个特征值，$\\alpha$ 为方阵 $A$ 对应于特征值 $\\lambda$ 的一个特征向量。 求解特征向量和特征值： $A\\alpha -\\lambda \\alpha=(A-\\lambda I)\\alpha=0$ 齐次方程有非零解的充要条件是特征方程 $det(A-\\lambda I)=0$ （行列式为0） 根据特征方程先求解出 $\\lambda$ 的所有值。 再根将 $\\lambda$ 代入齐次方程，求解齐次方程的解 $\\alpha$ ，即为对应 $\\lambda$ 的特征向量。 2-DGoal：find $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ 构造拉格朗日函数： $g\\left(w^{2}\\right)=\\left(w^{2}\\right)^{T} S w^{2}-\\alpha\\left(\\left(w^{2}\\right)^{T} w^{2}-1\\right)-\\beta\\left(\\left(w^{2}\\right)^{T} w^{1}-0\\right)$ 对 $w^2$ 求微分，所求点满足等式： $S w^{2}-\\alpha w^{2}-\\beta w^{1}=0$ 左乘 $(w^1) ^T$： $(w^1)^TSw^2-\\alpha (w^1)^Tw^2-\\beta(w^1)^Tw^1=0$ 已有： $(w^1)^Tw^2=0, (w^1)^Tw^1=1$ 证明：$ (w^1)^TSw^2=0$ $\\because (w^1)^TSw^2$ 是scalar $\\therefore (w^1)^TSw^2=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1$ $\\because S^T=S$ (协方差矩阵是对称矩阵) $\\because Sw^1=\\lambda_1 w^1$ $\\therefore (w^1)^TSw^2=(w^2)^TSw^1=\\lambda_1(w^2)^Tw^1=0$ $(w^1)^TSw^2-\\alpha (w^1)^Tw^2-\\beta(w^1)^Tw^1=0-\\alpha\\cdot 0-\\beta \\cdot 1=0$ $\\therefore \\beta=0$ $w^2$ 满足等式：$S w^{2}-\\alpha w^{2}=0$ 和1-D的情况相同：find $w^2$ maximum $(w^2)^TSw^2$ $(w^2)^TSw^2=\\alpha$ $w^2$ is the eigenvector(特征向量) of the covarivance matrix S corresponding to the largest eigenvalue $\\lambda_2$ . OVER! Conclusion最后解决之前的Q2：$(w^1)^Tw^2=0$ ? 先说明一下$S$ 的性质： 是对称矩阵，对应不同特征值对应的特征向量都是正交的。 （参考1，2） 也是半正定矩阵，其特征值都是非负的。 （参考4，5，6） 其次关于 $W$ 的性质 $ W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\\\ ...\\end{bmatrix}$ ,易得 $W$ 是orthogonal matrix(正交矩阵)。 所以这是一个约束条件，能让PCA的最优化问题转化为求其特征值的问题。 （具体见下一小节：PCA-decorrelation） 其次 $z=Wx$ ，也因为 $W$ 的正交性质，让 $z$ 的各维度（特征）decorrelation，去掉相关性，降维后的特征相互独立，方便后面generative model的假设。 $S=Cov(x)$ 为实对称矩阵。 实对称矩阵的性质：$A$ 是一个实对称矩阵，对于于 $A$ 的不同特征值的特征向量彼此正交。 正交矩阵的性质：$W^TW=WW^T=I$ $Var(z)=(w^1)^T S w^1\\geq 0$ ，方差一定大于等于0 。 半正定矩阵的定义： 实对称矩阵 $A$ ，对任意非零实向量 $X$ ，如果二次型 $f(X)=X^TAX\\geq0$ ， 则有实对称矩阵 $A$ 是半正定矩阵。 半正定矩阵的性质：半正定矩阵的特征值都是非负的。 1-D中： Goal：find $w^1$ to maximum $(w^1)^T S w^1$ s.t.$(w^1)^Tw^1=1$ 结论：$w^1$ 就是协方差矩阵 $S$ 最大特征值 $\\lambda_1 $ 对应的特征向量。 s.t.$(w^1)^Tw^1=1$ 2-D中： Goal：find $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ 结论：$w^2$ 就是协方差矩阵$S$ 第二大特征值 $\\lambda_2 $ 对应的特征向量。 s.t.$(w^2)^Tw^2=1$ k-D中： 结论：$w$ 就是协方差矩阵 $S$ 前 $k$ 大的特征值对应的特征向量。s.t. $W$ 是正交矩阵。 PCA-decorrelation$z=Wx$ 通过PCA找到的 $W$ ，$x$ 得到新的presentation $z$ ，如下图。 可见，经过PCA后，original data变为decorrelated data，各维度（feature）是去相关性的，即各维度是独立的，方便generative model的假设（比如Gaussian distribution). $z$ 是docorrelated，即 $Cov(z)=D$ 是diagonal matrix(对角矩阵) 证明：$Cov(z)=D$ is diagonal matrix $W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\\\ ...\\end{bmatrix}$ ，$S=\\operatorname{Cov}(x)$ $\\operatorname{Cov}(z)=\\frac{1}{N} \\sum(z-\\bar{z})(z-\\bar{z})^{T}=W S W^{T}$ $=W S\\left[\\begin{array}{lll}w^{1} & \\cdots & w^{K}\\end{array}\\right]=W\\left[\\begin{array}{lll}S{w}^{1} & \\cdots & S w^{K}\\end{array}\\right]$ $=W\\left[\\lambda_{1} w^{1} \\quad \\cdots \\quad \\lambda_{K} w^{K}\\right]=\\left[\\lambda_{1} W w^{1} \\quad \\cdots \\quad \\lambda_{K} W w^{K}\\right]$ ($\\lambda$ is scalar) $=\\left[\\begin{array}{lll}\\lambda_{1} e_{1} & \\cdots & \\lambda_{K} e_{K}\\end{array}\\right]=D$ ($W$ is orthogonal matrix) PCA-Another Point of ViewMain Idea: ComponentPCA看作是一些basic component的组成，如下图，手写数字都是一些基本笔画组成的，记做 $\\{u^1,u^2,u^3,...\\}$ 因此，下图的”7”的组成为 $\\{u^1,u^3,u^5\\}$ 所以原28*28 vector $x$ 表示的图像能近似表示为： $$ x \\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}+\\bar{x} $$ 其中 $\\{u^1,u^2,u^3,...\\}$ 是compoment的vector表示， $\\{c^1,c^2,c^3,...\\}$ 是component的系数，$\\bar{x}$ 是所有images的平均值。 因此 $\\begin{bmatrix}c_1 \\\\c_2 \\\\... \\\\ c_k \\end{bmatrix}$ 也能表示一个数字图像。 现在问题是找到这些component $\\{u^1,u^2,u^3,...\\}$ , 再得到 他的线形表出 $\\begin{bmatrix}c_1 \\\\c_2 \\\\... \\\\ c_k \\end{bmatrix}$ 就是我们想得到的better presentation. Detail要满足：$x \\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}+\\bar{x}$ 即，$x -\\bar{x}\\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}$ ，等式两边的误差要尽量小。 问题变成：找 $\\{u^1,u^2,u^3,...\\}$ minimize the reconstruction error = $\\|(x-\\bar{x})-\\hat{x}\\|_2$ . 损失函数： $L=\\min _{\\left\\{u^{1}, \\ldots, u^{K}\\right\\}} \\sum\\left\\|(x-\\bar{x})-\\left(\\sum_{k=1}^{K} c_{k} u^{k}\\right)\\right\\|_{2}$ 而求解PCA的过程就是在minimize损失函数 $L$ ，PCA中求解出的 $\\{w^1,w^2,...,w^K\\}$ 就是这里的component $\\{u^1,u^2,...,u^K\\}$ .(Proof 见Bisho, Chapter 12.1.2) *Goal: minimize the reconstruction error = $\\|(x-\\bar{x})-\\hat{x}\\|_2$ * $x -\\bar{x}\\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}$ 每个sample: $\\left\\{ \\begin{matrix} x^{1}-\\bar{x} \\approx c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\\cdots \\\\ x^{2}-\\bar{x} \\approx c_{1}^{2} u^{1}+c_{2}^{2} u^{2}+\\cdots \\\\x^{3}-\\bar{x} \\approx c_{1}^{3} u^{1}+c_{2}^{3} u^{2}+\\cdots \\\\ ...\\end{matrix} \\right.$ 下图中 $X=x-\\bar{x}$ 矩阵的第一列都和上面的 $x^1-\\bar{x}$ 对应： 而上面的 $c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\\cdots$ 和下图的component矩阵乘系数矩阵的第一列对应： 因此，是要让下图矩阵的结果 minimize error： 如何求解: SVD矩阵分解-其实就是最大近似分解（挖坑） SVD能将一个任意的矩阵，分解为下面三个矩阵的乘积。 $X = U\\Sigma V$ $U,V$ 都是orthogonal matrix，$\\Sigma$ 是diagonal matrix。 组成$U$ (M*K) 的K个列向量是 $XX^T$ 矩阵的前K大特征值对应的特征向量。 组成 $V$ (K*N)的K个行向量是 $X^TX$ 矩阵的前K大特征值对应的特征向量。 $XX^T$ 和 $X^TX$ 的特征值相同 $\\Sigma$ 的对角值 $\\sigma_i=\\sqrt{\\lambda_i}$ 解：$U$ 矩阵作为 component矩阵， $\\Sigma V$ 乘在一起作为系数矩阵。 $U=\\{u^1,u^2,u^3,...\\}$ 矩阵是$XX^T$ 的特征向量组成正交矩阵。 而PCA的解 $W^T=\\{w^1,w^2,...,w^K\\}$ 也是特征向量组成的正交矩阵。 所以和PCA的关系：$U$ 矩阵是 $XX^T=Cov(x)$ 的特征向量，所以$U$ 矩阵就是PCA的解。 PCA-NN：Autoencoder上文说到求解PCA的解 $\\{w^1,w^2,...,w^K\\}$ 就是在最小化restruction error $x -\\bar{x}\\approx \\sum_{k=1}^K c_kw^k$ . 两者的联系就是PCA的解 $\\{w^1,w^2,...,w^K\\}$ 就是component $\\{u^1,u^2,u^3,...\\}$ ,且PCA的表示是 $z$ 对应这里的 $c_k$ (第k个image的表示）. PCA视角： $z=c_k=(x-\\bar{x})\\cdot w^k$ PCA looks like a neural network with one hidden layer(linear activation function)。 把PCA视角看作一个NN，如下图，其hidden layer的激活函数是一个简单的线性激活函数。 再看component视角： $\\hat{x}=\\sum_{k=1}^K c_kw^k\\approx x-\\bar{x}$ PCA就构成了下面的NN，hidden layer可以是deep，这就是autoencoder(后面的博客会再详细讲)。 用Gradient Descent对输入输出做minimize error，hidden layer的输出 $c$ 就是我们想要的编码（降维后的编码）。 Q：用PCA求出的结果和用Gradient Descent训练NN的结果一样吗？ A：当然不一样，PCA的 $w$ 都是正交的，而NN的结果是gradient descent迭代出来的，并且该结果还会于初值有关。 Q：有了PCA，为什么还要用NN呢？ A：因为PCA只能处理linear的情况，对前文那种高维的非线形的无法处理，而NN可以是deep的，能较好处理非线形的情况。 tips: how many components?比如在对Pokemon进行PCA时，有六个features，如何确定principle component的数目？ 往往在实际操作中，会对每个component计算一个ratio，如图中的公式： 因为每一个component对应一个eigenvector，每个eigenvector对应一个eigenvalue，而这个eigenvalue的值代表了在这个component的维度的variance有多大，越大当然能更好的表示。 因此计算eigenvalue的ratio，来找出分布较大的component作为主成分。 More About PCA如果对MNIST做PCA分析，结果如下图，会发现下面eigen-digits这些并不像数字的某个组成部分： 同样，对face做PCA分析，结果下图： 为什么呢？ 在MNIST中，一张image的表示如下图： 其中，$\\alpha$ 可以是任意实数，那么就有正有负，所以PCA的解包含了一些真正component的adding up and subtracting，所以MNIST的解不像这些数字的一部分。 如果想得到的解看起来像真正的component，可以规定图像只能是加，即 $\\alpha$ 都是非负的。 Non-negative matrix factorization(NMF) Forcing $\\alpha$ be non-negative: additive combination Forcing $w$ be non-negative: components more like “parts of digits” Weakness of PCA PCA是unsupervised，因此可能不能区分本来是两个类别的东西。 如图，PCA的结果可能是上图的维度方向，但如果引入labeled data，更好的表达应该按照下图LDA的维度方向。 LDA (Linear Discriminant Analysis) 是一种supervised的分析方法。 PCA是Linear的，前文已经提及过，除了可以用NN的方式也有很多其他的non-linear的解法。 Reference HAC的算法细节待补充完善：https://zhuanlan.zhihu.com/p/34168766 PCA: Bishop, Chapter12. 线代知识：特征值、特征向量、实对称矩阵等： 拉格朗日乘数：Bishop, Appendix E 矩阵微分：https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors Proof-PCA的过程就是在minimize损失函数 $L$ :Bisho, Chapter 12.1.2 SVD： https://www.cnblogs.com/pinard/p/6251584.html https://www.youtube.com/watch?v=rYz83XPxiZo NMF：Non-negative matrix factorization LDA：Linear Discriminant Analysis","link":"/2020/10/31/unsupervised-learning-pca/"},{"title":"「Tools-VSCode」:Remote SSH-跳板机设置","text":"VSCode就是最棒的IDE！ 最近遇到一个Remote SSH的问题：想要连接校内的服务器，必须经过两个跳板机。 即需要三次ssh，才能连接到目标服务器D：A➡️B➡️C➡️D Terminal：如何无痛免密登录校内服务器 （需要输入三次ssh命令） VSCode：如何无痛用VSCode连接远程服务器以进行开发（一键式） Terminal如果只需要在terminal中免密连接远程服务器，步骤比较直观，但仍需要三次输入ssh命令： A、B、C主机通过ssh-keygen 生成公私钥 公钥认证链： A的公钥内容加入B主机下的~/.ssh/authorized_keys 中 B的公钥内容加入C主机下的~/.ssh/authorized_keys 中 C的公钥内容加入D主机下的~/.ssh/authorized_keys 中 Done VSCode在vscode官方博客: Remote SSH: Tips and Tricks 有提到，配置文件的ProxyCommand设置，可以让remote ssh通过一个跳板机再连接到受保护的目标主机。 To use a jump-box setup with the Remote - SSH extension, you can use the ProxyCommand config option. 通过一个跳板机A➡️B➡️C：A是用户主机，B是跳板机，C是目标主机 把A的公钥内容加入到B主机下的~/.ssh/authorized_keys 和C主机下的~/.ssh/authorized_keys 注意：这里和上面terminal的方式不同。上面是链式验证，这里是中心式的验证。（意会就好，乱取的名字）如果不加入，VSCode中每次跳转都要输密码。（懒zzz，并不想） Q：为什么要把A的公钥加入到B主机和C主机的已认证密钥中？原因如下。 This configuration will open a background SSH connection to the jump box, and then connect via a private IP address to the target. A主机下生成两个文件：b_private 和c_private A主机： ~/.ssh/b_private : B跳板机的私钥内容。（注意是私钥） ~/.ssh/c_private : C跳板机的私钥内容。 编辑VSCode ssh的配置文件~/.ssh/config 12345678910111213# B is Jump Box with public IP addressHost B HostName &lt;IP address of B&gt; User fred IdentityFile ~/.ssh/b_private# C is Target Machine with private IP addressHost C HostName &lt;IP address of C&gt; User fred Port 6000 IdentityFile ~/.ssh/c_private ProxyCommand ssh -q -W %h:%p B IdentityFile: 私钥文件的路径 Done：在VSCode中一键连接远程服务器C。 通过多个跳板机由此可以实现多次跳转。 比如经过两次跳转A➡️B➡️C➡️D：A是用户主机，B、C是跳板机，D是目标主机。 把A的公钥内容加入到B、C、D主机下的~/.ssh/authorized_keys A主机下生成两个文件：b_private 、c_private 和d_private A主机： ~/.ssh/b_private : B跳板机的私钥内容。（注意是私钥） ~/.ssh/c_private : C跳板机的私钥内容。 ~/.ssh/d_private : D跳板机的私钥内容。 编辑VSCode ssh的配置文件~/.ssh/config 1234567891011121314151617181920# B is Jump Box with public IP addressHost B HostName &lt;IP address of B&gt; User fred IdentityFile ~/.ssh/b_private# C is Jump Box with private IP addressHost C HostName &lt;IP address of C&gt; User fred Port 6000 IdentityFile ~/.ssh/c_private ProxyCommand ssh -q -W %h:%p B# D is Target Machine with private IP addressHost D HostName &lt;IP address of D&gt; User abc IdentityFile ~/.ssh/d_private ProxyCommand ssh -q -W %h:%p C Done：在VSCode中一键连接远程服务器D。","link":"/2021/10/19/vscode-remote-ssh/"},{"title":"「机器学习-李宏毅」:Unsupervised Learning：Word Embedding","text":"这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。 文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。 Word Embedding可以很好的解决这个问题。 Word Embedding有count based和prediction based两种方法。文章主要介绍了prediction based的方法，包括如何predict the word vector? 为什么这样的模型works？介绍了prediction based的变体；详细阐述了该模型中sharing parameters的做法和其必要性。 文章最后简单列举了word embedding的相关应用，包括multi-lingual embedding, multi-domain embedding, document embedding 等。 Word to Vector如何把word转换为vector? 1-of-N Encoding第一种方法是1-of-N Encoding： Vector的维度是单词总数，每一维度都代表一个单词。 1-of-N Encoding的方法简单，但这种向量的表示方式not imformative，即向量表示不能体现单词之间的语义关系。 Word Class对1-of-N Encoding方式改进，Word Class采用聚类cluster的方式，根据类别训练一个分类器。 但这种人为分类的方式，信息是会部分丢失的，即光做clustering是不够的，会丢失单词的部分信息。 Word Embedding第三种方式是Word Embedding。（词嵌入） Word Embedding: Machine learns the meaning of words from reading a lot of documents without supervision. Word Embedding，机器通过阅读大量文章学习单词的含义，用vector的形式表示单词的语义。训练时只需要给机器大量文章，不需要label，因此是无监督学习。 Word Embedding如何做Word Embedding呢？ auto-encoder？能否用auto-encoder的方式来做词嵌入呢？ 即用1-of-N encoding的方式对单词编码，作为训练的输入和输出。 word2vec时，把model中的某一hidden layer的输出作为该单词的向量表示。 这种方式是不可以的，不可以用auto-encoder。因为auto-encoder不能学到informative的信息，即用auto-encoder表示的向量不能表达word的语义。 Exploit the ContextA word can be understood by its context. 所以Word Embedding可以利用上下文来学习word的语义。 如何利用单词的上下文来学习呢？ Count based 如果两个单词 $w_i$ 和 $w_j$ 在文章中经常同时出现，那么 $V(w_i)$ ( $w_i$ 的向量表示)和 $V(w_j)$ 的向量表示会很close. E.g. Glove Vector: https://nlp.stanford.edu/projects/glove/ GloVe的表示法有两个亮点： Nearest neighbors：vectors之间的欧几里得距离（或者余弦相似度）能较好表示words之间的语义相似度。 Linear substructures：用GloVe方法表示的vectors有有趣的线性子结构。 Prediction based 使用预测的方式来表示。 Prediction basedHow to predict？prediction based的方法是用前一个单词来预测当前单词。 训练时： $w_{i-1}$ 的1-of-N encoding编码作为输入，$w_i$ 的1-of-N encoding的编码作为输出。 NN如上图，$w_{i-1}$ 的1-of-N encoding编码作为输入，输出的vector表示下一个单词是 $w_i$ 的概率。 word2vec : $w_{i-1}$ 的1-of-N encoding编码作为NN的输入，$w_i$ 的向量表示为第一个hidden layer的neurons的输入 $z$ 。 Why it works?直觉的解释他为什么能work。 如上图，蔡英文 宣誓就职 和 马英九 宣誓就职，虽然 $w_{i-1}$ 不同，但NN的输出中，“宣誓就职”的概率应该最大。 即hidden layers必须把不同的 $w_{i-1}$ project到相同的space，要求hidden layer的input是相近的，NN的输出才是相近的。 Prediction-based ：Various Architecture因为一个单词的下一个单词范围非常大，所以使用前一个单词预测当前单词的方法，performance是较差的。 因此常常会使用多个单词来预测下一个单词，NN的输入是多个单词连接在一起组成的向量，一般NN的输入至少为10个单词，word embedding的performance较好。 除了使用多个单词的方法，prediction-based的方法还用两种变体结构。 Continuous bag of word (CBOW) model: predicting the word given its context. 使用单词的前后文（前一个单词和后一个单词）来预测当前单词。 Skip-gram: predicting the context given a word. 使用中间单词来预测单词的前一个单词和后一个单词。 Sharing Parameters使用多个单词作为NN的输入，提高了word embedding的performance，但也大幅增加了模型训练的参数数量。 使用sharing parameters（共享参数）能大量减少模型的参数数量。 如上图，输入单词连接到neurons的权重应该是相同的。 除了能减少参数，sharing parameters也是必要的。否则，如果NN的输入的单词顺序交换，那么得到的单词向量是不同的。 How to train sharing parameters? 假设两个单词相同维度连接到neuron的weight是 $w_i,w_j$ ，在训练中，如何让 $w_i=w_j$ ? Given the same initialization.(相同的初始化) 原来的参数更新：$$w_i \\longleftarrow w_i - \\frac{\\partial C}{\\partial w_i} \\w_j \\longleftarrow w_j - \\frac{\\partial C}{\\partial w_j}$$虽然有相同的初始化，但在Backpropagation求偏微分时，$\\frac{\\partial C}{\\partial w_i}$ 和 $\\frac{\\partial C}{\\partial w_j}$ 不一样，那么参数 $w_i$ 和 $w_j$ 更新一次后就不同了。 在训练sharing parameters的参数更新：$$w_i \\longleftarrow w_i - \\frac{\\partial C}{\\partial w_i} -\\frac{\\partial C}{\\partial w_j}\\w_j \\longleftarrow w_j - \\frac{\\partial C}{\\partial w_j}-\\frac{\\partial C}{\\partial w_i}$$这样更新后，$w_i$ 和 $w_j$ 仍保持一致。如果有多个单词，亦然。 Word2Vec 在word2vec时，根据sharing parameters的性质，计算单词的向量表示时，可以简化运算。 如上图，用前文单词 $x_{i-1},x_{i-2}$ 表示单词 $x_i$ 的向量表示 $z=W_1x_{i-2}+W_2x_{i-1}=W(x_{i-2}+x_{i-1})$ . 其中 $x_{i-1},x_{i-2}$ 的维度是|V|，$x_i$ 的向量表示 $z$ 的维度是 |Z|，$W_1=W_2=W$ 的维度为|Z|*|V|。 Advantages of Word EmbeddingWord Embedding能得到一些有趣的特性。 向量之间有趣的线性子结构 相近的向量有相近的语义 向量之间表示的语义特性 其他应用Multi-lingual Embedding：实现翻译 不同语言之间分开训练，训练出的不同语言所对应词汇的向量表示肯定不同，再将对应词汇的向量project到同一点，即实现了翻译。 Multi-domain Embedding还可以做影像嵌入。 Document Embedding：将文件表示为一个向量 Bag of Word: 用Bag-of-word的方式编码文件，再实现semantic embedding。得到的文件表示向量可以表示文件的语义主题。 Beyond Bag of Word: 句子中单词的顺序也很大程度影响句子的语义。 因此，下图的两句话有相同的bag-of-word，但表达的含义完全相反。 关于beyond bag of word的相关工作参考reference 2. Reference GloVe: Global Vectors for Word Representation beyond bag of word:","link":"/2020/10/11/unsupervised-learning-word-embedding/"},{"title":"「Cryptography-ZKP」: Lec4-SNARKs via IP","text":"In this series, I will learn Zero Knowledge Proofs (ZKP) on this MOOC, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. Any corrections and advice are welcome. ^ - ^ Topics: Differences between Interactive Proofs and SNARKs Outline of SNARKs from IP Brief intro to Functional Commitments SZDL Lemma Multilinear Extensions Sum-check Protocol and its application. Counting Triangles SNARK for Circuit-satisfiability Before proceeding to today’s topic, let’s briefly recall what is a SNARK? SNARK is a succinct proof that a certain statement is true.For example, such a statement could be “I know an m such that SHA256(m)=0”. SNARK indicates that the proof is “short” and “fast” to verify.Note that if m is 1GB then the trivial proof, i.e. the message m, is neither short nor fast to verify. Interactive Proofs: Motivation and ModelIn traditional outsourcing, the Cloud Provider stores the user’s data and the user can ask the Cloud Provider to run some program on its data. The user just blindly trusts the answer returned by the Cloud Provider. The motivation for Interactive Proofs (IP) is that the above procedure can be turned into the following Challenge-Response procedure. The user is allowed to send a challenge to the Cloud Provider and the Cloud Provider is required to respond for such a challenge. The user has to accept if the response is valid or reject as invalid. Hence, the Challenge-Response procedure or IP can be modeled as follows. P solves a problem and tells V the answer. Then they have a conversation. P’s goal is to convince V that the answer is correct. Requirements: Completeness: an honest P can convince V to accept the answer. (Statistical) Soundness: V will catch a lying P with high probability. Note that statistical soundness is information-theoretically soundness so IPs are not based on cryptographic assumptions. Hence, the soundness must hold even if P is computationally unbounded and trying to trick V into accepting the incorrect answer. If soundness holds only against polynomial-time provers, then the protocol is called an interactive argument. It is worth noting that SNARKs are arguments so it is not statistically sound. IPs v.s. SNARKsThere are three main differences between Interactive Proofs and SNARKs. We’ll list them first and elaborate in the section. SNARKs are not statistically sound. SNARKs have knowledge soundness. SNARKs are non-interactive. Not Statistically SoundThe first one is mentioned above that SNARKs are arguments so the soundness is only against polynomial-time provers. Knowledge Soundness v.s. SoundnessThe second one is that SNARKs has knowledge soundness. SNARKs that don’t have knowledge soundness are called SNARGs, they are studied too. Considering a public arithmetic circuit such that $C(x,w)=0$ where $x$ is the public statement and $w$ is the secret witness. Compare soundness to knowledge soundness for such a circuit-satisfiability. Sound: V accepts → There exist $w$ s.t. $C(x,w)=0$. Knowledge sound: V accepts → P actually “knows” $w$ s.t. $C(x,w)=0$.The prover is establishing that he necessarily knows the witness. As for the soundness, the prover is only establishing the existence of such a witness. The knowledge soundness is establishing that the prover necessarily knows the witness. Hence, knowledge soundness is stronger. But sometimes standard soundness is meaningful even in contexts where knowledge soundness isn’t, and vice versa. Standard soundness is meaningful. Because there’s no natural “witness”. E.g., P claims the output of V’s program on $x$ is 42. Knowledge soundness is meaningful. E.g., P claims to know the secret key that controls a certain Bitcoin wallet. It is actually claimed that the prover knows a pre-image such that the hash is 0. The hash function is surjective so a witness for this claim always exists. In fact, there are many and many witnesses for this claim. It turns to a trivial sound protocol. Hence, it needs to establish that the prover necessarily knows the witness. Non-interactive and Public VerifiabilityThe final difference is that SNARKs are non-interactive. Interactive proof and arguments only convince the party that is choosing or sending the random challenges. This is bad if there are many verifiers as in most blockchain applications. P would have to convince each verifier separately. For public coin protocols, we have a solution, Fiat-Shamir, which renders the protocol non-interactive and publicly verifiable. In quiz 4, it is a false statement that non-interactive implies publicly verifiable. In my perspective, it only holds for non-interactive protocols rendered from the public coin protocols where the verifier only sample random coins and send the sampled coins to the provers. SNARKs from Interactive Proofs: OutlineWe’ll describe the outline to build SNARKs from interactive proofs in this section. Trivial SNARKsThe first thing to point out is that the trivial SNARK is not a SNARK. The trivial SNARK is as follows: Prover sends $w$ to verifier. Verifier checks if $C(x,w)=0$ and accepts if so.The verifier is required to rerun the circuit. The above trivial SNARK has two problems: The witness $w$ might be long. We want a “short” proof $\\pi$ → $\\text{len}(\\pi)=\\text{sublinear}(|w|)$ Computing $C(x,w)$ may be hard. We want a “fast” verifier → $\\text{time}(V)=O_\\lambda(|x|, \\text{sublinear(|C|)})$. As described in Lecture 2, succinctness means the proof length is sublinear in the length of the witness and the verification time is linear to the length of public statement $x$ and sublinear to the size of the circuit $C$. Note that the verification time linear to $|x|$ means that the verifier at least read the statement $x$. Less TrivialWe can make it less trivial as follows: Prover sends $w$ to verifier. Prover uses an IP to prove that $w$ satisfies the claimed property. It gives a fast verifier, but the proof is still too long. Actual SNARKsIn actual SNARKs, instead of sending $w$, the prover commits cryptographically to $w$. Consequently, the actual SNARKs is described as follows: Prover commits cryptographically to $w$. Prover uses an IP to prove that $w$ satisfies the claimed property. The IP procedure reveals just enough information about the committed witness $w$ to allow the verifier to run its checks. Moreover, the IP procedure can be rendered non-interactive via Fiat-Shamir. Functional CommitmentsThere are several important functional families introduced in Lecture 2 we want to build commitment schemes. Polynomial commitments: commit to a univariate $f(X)$ in $\\mathbb{F}_p^{(\\le d)}[X]$. $f(X)$ is a univariate polynomial in the variable $X$ that has a degree at most $d$. The prover commits to a univariate polynomial of degree $\\le d$ . Later the verifier requests to know the evaluation of this polynomial at a specific point $r$. The prover can reveal $f(r)$ and provide proof that the revealed evaluation is consistent with the committed polynomial. Note that the proof size and verifier time should be $O_\\lambda(\\log d)$ in SNARKs. Multilinear commitments: commit to multilinear $f$ in $\\mathbb{F}_p^{(\\le 1)}[X_1,\\dots, X_k]$. $f$ is a multilinear polynomial in variables $X_1,\\dots, X_k$ where each variable has a degree at most 1. E.g., $f(x_1,\\dots, x_k)=x_1x_3 + x_1 x_4 x_5+x_7$. Vector commitments (e.g. Merkle trees): commit to a vector $\\vec{u}=(u_1,\\dots, u_d)\\in \\mathbb{F}_p^d$. The prover commits to a vector of $d$ entries. Later the verifier requests the prover to open a specific entry of the vector, e.g. the $i$th entry $f_{\\vec{u}}(i)$. The prover can open the $i$th entry $f_{\\vec{u}}(i)=u_i$ and provide a short proof that the revealed entry is consistent with the committed vector. Inner product commitments (inner product arguments - IPA): commit to a vector $\\vec{u}=(u_1,\\dots, u_d)\\in \\mathbb{F}_p^d$. The prover commits to a vector $\\vec{u}$. Later the verifier requests the prover to open an inner product $f_{\\vec{u}}(\\vec{v})$ that takes a vector $\\vec{v}$ as input. The prover can open the inner product $f_{\\vec{u}}(\\vec{v})=(\\vec{u},\\vec{v})$ and provide a short proof that the revealed inner product is consistent with the committed vector. Vector Commitments: Merkle TreesIn vector commitments, the prover wants to commit a vector. We can pair up the values in the vector and hash them to form a binary tree. A Merkle tree is a binary tree where the leaf node stores the values of the vector that we want to commit and the other internal nodes calculate the hash value of its two children nodes. The root hash is the commitment of the vector so the prover just sends the root hash to the verifier as the commitment. Then the verifier wants to know the 6th entry in the vector. The prover provides the 6th entry (T) and proof that the revealed entry is consistent with the committed vector. The proof is also called the authentication information. The authentication information is the sibling hashes of all nodes on the root-to-leaf path that includes $C, m_4, h_1$. Hence, the proof size is $O(\\log n)$ hash values. The verifier can check these hashes are consistent with the root hash. Under the assumption that $H$ is a collision-resistant hash family, the vector commitment has the binding property that once the root hash is sent, the committer is bound to a fixed vector. Because opening any leaf to two different values requires finding a hash collision along the root-to-leaf path. Poly Commitments via Merkle TreesA natural way of constructing polynomial commitments is to use the Merkle trees. For example, we can commit to a univariate $f(X)$ in $\\mathbb{F}_7^{(\\le d)}[X]$ with the following Merkle tree. When the verifier requests to reveal $f(4)$, the prover can provide $f(4)$ and the following sibling hashes as proof. In summary, if we want to commit a univariate $f(X)$ in $\\mathbb{F}^{(\\le d)}[X]$, the prover needs to Mekle-commit to all evaluations of the polynomial $f$. When the verifier requests $f(r)$, the prover reveals the associated leaf along with opening information. However, it has two problems. The number of leaves is $|\\mathbb{F}|$ which means the time to compute the commitment is at least $|\\mathbb{F}|$. It is a big problem when working over large fields, e.g., $|\\mathbb{F}|\\approx 2^{64}$ or $|\\mathbb{F}|\\approx 2^{128}$.→ We want the time proportional to the degree bound $d$. The verifier does not know if $f$ has a degree at most $d$ !. In lecture 5, we will introduce KZG polynomial commitment scheme using bilinear groups, which addresses both issues. Tech PreliminariesSZDL LemmaThe heart of IP design is based on a simple observation. For a non-zero $f\\in \\mathbb{F}_p^{(\\le d)}[X]$, if we sample a random $r$ from the field $\\mathbb{F}_p$, the probability of $f(r)=0$ is at most $d/p$. Suppose $p\\approx 2^{256}$ and $d\\le 2^{40}$, then $d/p$ is negligible. If $f(r)=0$ for a random $r\\in \\mathbb{F}_p$, then $f$ is identically zero w.h.p. It gives us a simple zero test for a committed polynomial. Moreover, we can achieve a simple equality test for two committed polynomials. Let $p,q$ be univariate polynomials of degree at most $d$. Then $\\operatorname{Pr}_{r\\overset{\\$}\\leftarrow \\mathbb{F}}[p(r)=q(r)]\\le d/p$. If $f(r)=g(r)$ for a random $\\overset{\\$}\\leftarrow \\mathbb{F}_p$, then $f=g$ w.h.p. The Schwartz-Zippel-Demillo-Lipton lemma is a multivariate generalization of the above facts. Schwartz-Zippel-Demillo-Lipton Lemma (SZDL Lemma): Let $p,q$ be $\\ell$-variate polynomials of total degree at most $d$. Then $\\operatorname{Pr}_{r\\in \\mathbb{F}^{\\ell}}[p(r)=q(r)]\\le d/{|\\mathbb{F}|}$. ”Total degree” refers to the maximum sum of degree of all variables in any term. Low-Degree and Multilinear ExtensionsUsing many variables, we are able to keep the total degree of polynomials quite low, which ensures the proof is short and fast to verify. Definition of Polynomial Extensions: Given a function $f:\\{0,1\\}^{\\ell}\\rightarrow \\mathbb{F}$, a $\\ell$-variate polynomial $g$ over $\\mathbb{F}$ is said to extend $f$ if $f(x)=g(x)$ for all $x\\in \\{0,1\\}^{\\ell}$. Note that the original domain of $f$ is $\\{0,1\\}^{\\ell}$ and the domain of extension $g$ is much bigger, that’s $\\mathbb{F}^{\\ell}$. Definition of Multilinear Extensions: Any function $f:\\{0,1\\}^{\\ell}\\rightarrow \\mathbb{F}$ has a unique multilinear extension (MLE) denoted by $\\tilde{f}$. The total degree of the multilinear extension can be vastly smaller than the degree of the original univariate polynomial. Consider a univariate polynomial $f:\\{0,1\\}^2\\rightarrow \\mathbb{F}$ as follows. It maps $00$ to $1$, maps $01$ to 2, and so on. The multilinear extension $\\tilde{f}:\\mathbb{F}^2\\rightarrow \\mathbb{F}$ is defined as $\\tilde{f}(x_1,x_2)=(1-x_1)(1-x_2)+2(1-x_1)x_2+8x_1(1-x_2)+10x_1x_2$. Its domain is field by field and it’s easy to check that $\\tilde{f}(0,0)=1,\\tilde{f}(0,1)=2,\\tilde{f}(1,0)=8$ and $\\tilde{f}(1,1)=10$. Another non-multilinear extension of $f$ could be defined as $g(x_1,x_2)=-x_1^2+x_1x_2+8x_1+x_2+1$. Evaluating multilinear extensions quicklyThe sketch of evaluating the multilinear extension is Lagrange interpolation. Fact: Given as input all $2^{\\ell}$ evaluations of a function $f:\\{0,1\\}^\\ell \\rightarrow \\mathbb{F}$, for any point $r\\in \\mathbb{F}^{\\ell}$, there is an $O(2^{\\ell})$-time algorithm for evaluating $\\tilde{f}(r)$. Algorithm: Define $\\tilde{\\delta}_w(r)=\\prod_{i=1}^\\ell (r_iw_i+(1-r_i)(1-w_i)).$ This is called the multilinear Lagrange basis polynomial corresponding to $w$. For any input $r$, $\\tilde{\\delta}_w(r)=1$ if $r=w$, and $0$ otherwise. Hence, we can evaluate the multilinear extension of any input $r$ as follows. $$ \\tilde{f}(r)=\\sum_{w\\in \\{0,1\\}^\\ell}f(w)\\cdot \\tilde{\\delta}_w(r) $$ Complexity: For each $w\\in \\{0,1\\}^{\\ell}$, $\\tilde{\\delta}_w(r)$ can be computed with $O(\\ell)$ field operations, which yields an $O(\\ell 2^\\ell)$-time algorithm. It can be reduced to time $O(2^\\ell)$ via dynamic programming. If we feed this algorithm with the description of $f$ whose domain is $\\{0,1\\}^\\ell$ as inputs and the description consists of all $2^\\ell$ evaluations of $f$, then it is possible to evaluate the multilinear extension of $f$ at any desired point. This fact means that evaluating multilinear extension is essentially as fast as $O(2^\\ell)$, which is constantly slower than reading the whole description of $f$. The Sum-Check ProtocolIn this part, we’ll introduce the sum-check protocol [Lund-Fortnow-Karloff-Nissan’90]. We have a verifier with an oracle access to a $\\ell$-variate polynomial $g$ over field $\\mathbb{F}$. The verifier’s goal is to compute the following quantity: $$ \\sum_{b_1\\in\\{0,1\\}}\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(b_1,\\dots, b_\\ell) $$ As described above (functional commitments), the prover commit a multilinear polynomial, later the verifier can request the prover to evaluate at some specific points. Then the prover provide the evaluation and the proof that the revealed evaluation is consistent with the committed polynomial. In this part, we consider this process as a black box or an oracle. The verifier can go to the oracle and requests the evaluation of $g$ at some points. Note that this sum is the sum of all $g$’s evaluations over inputs $\\{0,1\\}^\\ell$ so the verifier can compute it on his own by just asking the oracle for the evaluations. But it costs the verifier $2^\\ell$ oracle queries. ProtocolInstead, we can offload the work of the verifier to the prover where the prover computes the sum and convince the verifier that the sum is correct. It turns out that the verifier only have to run $\\ell$-rounds to check the prover’s answer with only $1$ oracle query. Denote $P$ as prover and $V$ as verifier. Let’s dive into the start phase and the first round. Start: $P$ sends claimed answer $C_1$. The protocol must check that: $C_1=\\sum_{b_1\\in\\{0,1\\}}\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(b_1,\\dots, b_\\ell)$ Round 1: $P$ sends a univariate polynomial $s_1(X_1)$ claimed to equal: $H_1(X_1):=\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(X_1,\\dots, b_\\ell)$ $V$ checks that $C_1=s_1(0)+s_1(1)$. If this check passes, it is safe for $V$ to believe that $C_1$ is the correct answer as long as $V$ believes that $s_1=H_1$. It can be checked that $s_1$ and $H_1$ agree at a random point $r_1\\in \\mathbb{F}_p$ by SZDL lemma. $V$ picks $r_1$ at random from $\\mathbb{F}$ and sends $r_1$ to $P$. In round 1, $s_1(X_1)$ is the univariate polynomial that prover actually sends while $H_1(X_1)$ is what the prover claim to send if the prover is honest. Note that $H_1(X_1)$ is the true answer except that we cut off the first sum, which leave the first variable free. It reduce $2^\\ell$ terms to $2^{\\ell-1}$ terms. $g$ is supposed to have low degree (2 or 3) in each variable so the univariate polynomial $H_1$ derived from $g$ has low degree. Note: We can benefit from the low degree of the univariate polynomial. One is that specifying $H_1$ can be done by just sending 2 or 3 coefficients. Moreover, the low degree gives us acceptable or negligible sound error. After receiving the $s_1$, $V$ can compute $s_1(r_1)$ directly, but not $H_1(r_1)$. It turns out that $P$ can compute $H_1(r_1)$ and sends claimed $H_1(r_1)$ where $H_1(r_1)$ is the sum of $2^{\\ell-1}$ terms where $r_1$ is fixed so the first variable in $g$ is bound to $r_1\\in \\mathbb{F}$. $$ H_1(r_1):=\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,b_2,\\dots, b_\\ell) $$ Hence, the round 2 is indeed a recursive sub-protocol that checks $s_1(r_1)=H_1(r_1)$ where $s_1(r_1)$ is computed on $V$’s own. Round 2: They recursively check that $s_1(r_1)=H_1(r_1)$, i.e. that $$ s_1(r_1)=\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,b_2,\\dots, b_\\ell) $$ $P$ sends univariate polynomial $s_2(X_2)$ claimed to equal: $$ H_2(X_1):=\\sum_{b_3\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,X_2,\\dots, b_\\ell) $$ $V$ checks that $s_1(r_1)=s_2(0)+s_2(1)$. If this check passes, it is safe for $V$ to believe that $s_1(r_1)$ is the correct answer as long as $V$ believes that $s_2=H_2$. It can be checked that $s_2$ and $H_2$ agree at a random point $r_2\\in \\mathbb{F}_p$ by SZDL lemma. $V$ picks $r_2$ at random from $\\mathbb{F}$ and sends $r_2$ to $P$. Round $i$: They recursively check that $$ s_{i-1}(r_{i-1})=\\sum_{b_i\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,\\dots,r_{i-1},b_i,\\dots b_\\ell) $$ $P$ sends univariate polynomial $s_i(X_i)$ claimed to equal: $$ H_i(X_i):=\\sum_{b_{i+1}\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,\\dots,r_{i-1},X_i,\\dots b_\\ell) $$ $V$ checks that $s_{i-1}(r_{i-1})=s_i(0)+s_i(1)$. $V$ picks $r_i$ at random from $\\mathbb{F}$ and sends $r_i$ to $P$. Round $\\ell$: (Final round): They recursively check that $$ s_{\\ell-1}(r_{\\ell-1})= \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,\\dots,r_{\\ell-1},b_\\ell) $$ $P$ sends univariate polynomial $s_{\\ell}(X_\\ell)$ claimed to equal : $$ H_\\ell(X_\\ell):= g(r_1,\\dots,r_{\\ell-1},X_\\ell) $$ $V$ checks that $s_{\\ell-1}(r_{\\ell-1})=s_\\ell(0)+s_\\ell(1)$. $V$ picks $r_\\ell$ at random, and needs to check that $s_\\ell(r_\\ell)=g(r_1,\\dots,r_\\ell)$. No need for more rounds. $V$ can perform this check with one oracle query. Consequently, the final claim that the verifier is left to check $s_\\ell(r_\\ell)=g(r_1,\\dots,r_\\ell)$ where $s_\\ell(r_\\ell)$ can be computed on its own and $g(r_1,\\dots, r_\\ell)$ can be computed with just a single query to the oracle. If the final checks passes, then the verifier is convinced that $s_\\ell(r_\\ell)=g(r_1,\\dots,r_\\ell)$ and recursively convinced the claims left in the previous rounds, i.e. $s_i=H_i$. Finally, the verifier accepts the first claim that $C_1$ is the correct sum. AnalysisCompleteness: Completeness holds by design. If $P$ sends the prescribed message, then all of $V$’s checks will pass. Soundness: If $P$ dose not send the prescribed messages, then $V$ rejects with probability at least $1-\\frac{\\ell\\cdot d}{|\\mathbb{F}|}$, where $d$ is the maximum degree of $g$ in any variable. Proof of Soundness (Non-Inductive): It is conducted by the union bound. Specifically, if $C_1\\ne \\sum_{(b_1,\\dots, b_\\ell)\\in \\{0,1\\}^{\\ell}}g(b_1,\\dots, b_\\ell)$, then the only way the prover convince the verifier to accept is if there at least one round $i$ such that the prover sends a univariate polynomial $s_i(X_i)$ that dose not equal the prescribed polynomial $$ H_i(X_i)=\\sum_{(b_{i+1},\\dots, b_\\ell)}g(r_1, r_2,\\dots, X_i,b_{i+1},\\dots, b_\\ell) $$ yet $s_i(r_i)=H_i(r_i)$. For every round $i$, $s_i$ and $H_i$ both have degree at most $d$, and hence if $s_i\\ne H_i$, then probability that $s_i(r_i)=H_i(r_i)$ is at most $d/|\\mathbb{F}|$. By a union bound over all $\\ell$ rounds, the probability that there (is a bad event) is any round $i$ such that the prover send a polynomial $s_i\\ne H_i$ yet $s_i(r_i)=H_i(r_i)$ is at most $\\frac{\\ell\\cdot d}{|\\mathbb{F}|}$ Proof of Soundness by Induction: Base case: $\\ell=1$. In this case, $P$ sends a single message $s_1(X_1)$ claimed to equal $g(X_1)$. $V$ picks $r_1$ at random, and checks $s_1(r_1)=g(r_1)$. If $s_1\\ne g$, then the probability that $s_1(r_1)=g(r_1)$ is at most $d/|\\mathbb{F}|$. Inductive case: $\\ell &gt;1$. Recall that $P$’s first message $s_1(X_1)$ is claimed to equal $H_1(X_1)$. Then $V$ picks a random $r_1$ and sends $r_1$ to $P$. They recursively invoke sum-check to confirm $s_1(r_1)=H_1(r_1)$. If $s_1\\ne H_1$, then then probability that $s_1(r_1)=H_1(r_1)$ is at most $d/|\\mathbb{F}|$. Conditioned on $s_1(r_1)=H_1(r_1)$, $P$ is left to prove a false claim in the recursive call. The recursive call applies sum-check to $g(r_1, X_2, \\dots, X_\\ell)$, which is $\\ell-1$ variate. By induction hypothesis, $P$ convinces $V$ in the recursive call with probability at most $\\frac{d(\\ell-1)}{|\\mathbb{F}|}$. In summary, if $s_1\\ne H_1$, the probability $V$ accepts is at most $$ \\begin{aligned} \\le &\\operatorname{Pr}_{r_1\\in \\mathbb{F}}[s_1(r_1)=H_1(r_1)]+\\operatorname{Pr}_{r_2,\\dots, r_\\ell\\in \\mathbb{F}}[V \\text{ accepts}\\mid s_1(r_1)\\ne H_1(r_1)] \\\\ \\le & \\frac{d}{|\\mathbb{F}|}+ \\frac{d(\\ell-1)}{|\\mathbb{F}|}\\le \\frac{d\\ell}{|\\mathbb{F}|}\\end{aligned} $$ CostsLet $\\mathrm{deg}_i(g)$ denote the degree of variable $X_i$ in $g$ and each variable has degree at most $d$. $T$ denotes the time required to evaluate $g$ at one point. Total communication is $O(d\\cdot \\ell)$ field elements. The total prover-to-verifier communication is $\\sum_{i=1}^\\ell(\\mathrm{deg}_i(g)+1)=\\ell+\\sum_{i=1}^\\ell \\mathrm{deg}_i(g)=O(d\\cdot \\ell)$ field elements. The total verifier-to-prover communication is $\\ell-1$ field elements. Verifier’s runtime is $O(d\\ell+T)$. The running time of the verifier over the entire execution of the protocol is proportional to the total communication, plus the cost of a single oracle query to $g$ to compute $g(r_1,r_2, \\dots, r_\\ell)$. Prover’s runtime is $O(d\\cdot 2^\\ell\\cdot T)$.Counting the number of evaluations over $g$ required by the prover is less straightforward. In round $i$, $P$ is required to send a univariate polynomial $s_i$, which can be specified by $\\mathrm{deg}_i(g)+1$ points. Hence, $P$ can specify $s_i$ by sending for each $j\\in {0, \\dots, \\mathrm{deg}_i(g)}$ the value: $$ s_i(j)=\\sum_{(b_{i+1},\\dots, b_\\ell)}g(r_1,\\dots,r_{i-1},j,b_{i+1},\\dots, b_\\ell) $$ An important insight is that the number of the terms defining $s_i(j)$ falls geometrically with $i$: in the $i$th sum, there are only $(1+\\mathrm{deg}_i(g))\\cdot 2^{\\ell-i}\\approx d\\cdot 2^{\\ell-i}$ terms, with the $2^{\\ell-i}$ factor due to the number of vectors in $\\{0,1\\}^{\\ell-i}$. Thus, the total number of terms that must be evaluated is $\\sum_{i=1}^\\ell d\\cdot 2^{\\ell-i}=O(d\\cdot 2^{\\ell})$. Application of Sum-check ProtocolAn IP for counting triangles with linear-time verifierThe sum-check protocol can be applied to design an IP for counting triangles in a graph with linear-time verifier. The input is an adjacent matrix of a graph $A\\in \\{0,1\\}^{n\\times n}$. The desired output is $\\sum_{(i,j,k)\\in [n]^3}A_{ij}A_{jk}A_{ik}$, which counts the number of triangles in the graph. The fastest known algorithm runs in matrix-multiplication time, currently about $n^{2.37}$, which is super linear time in the size of the matrix. Likewise, we can offload the work to the prover to have a linear-time verifier. To design an IP derived from sum-check protocol, we need to view the matrix $A$ to a function mapping $\\{0,1\\}^{\\log n}\\times \\{0,1\\}^{\\log n}$ to $\\mathbb{F}$. It can be done easily by Lagrange interpolation. As for the following matrix $A\\in \\mathbb{F}^{4\\times 4}$, we can interpret the entry location $(i,j)\\in \\{0,1\\}^{\\log n}\\times \\{0,1\\}^{\\log n}$ as input and maps to the corresponding value $A_{i,j}\\in \\mathbb{F}$. E.g., $A(0,0,0,0)=1,A(0,0,0,1)=3$ and so on. Note that the domain of function $A$ is $\\{0,1\\}^{2\\log n}$, which has $2\\log n$ variables as inputs. It make sense to extend function $A$ to its multilinear polynomial $\\tilde{A}$ with domain $\\mathbb{F}^{2\\log n}$, each variable having degree at most 1. Hence, we can define a polynomial $g(X,Y,Z)=\\tilde{A}(X,Y)\\tilde{A}(Y,Z),\\tilde{A}(X,Z)$ that has $3\\log n$ variables, each variable having degree at most 2. Having defined the function $g$ with domain $\\{0,1\\}^{3\\log n}$, the prover and the verifier simply apply the sum-check protocol to $g$ to compute: $$ \\sum_{(a,b,c)\\in \\{0,1\\}^{3\\log n}}g(a,b,c) $$ In summary, the design of the protocol is as follows. Protocol: View $A$ as a function mapping $\\{0,1\\}^{\\log n}\\times \\{0,1\\}^{\\log n}$ to $\\mathbb{F}$. Extend $A$ to obtain its multilinear extension denoted by $\\tilde{A}$. Define the polynomial $g(X,Y,Z)=\\tilde{A}(X,Y)\\tilde{A}(Y,Z),\\tilde{A}(X,Z)$. Apply the sum-check protocol to $g$ to compute $\\sum_{(a,b,c)\\in \\{0,1\\}^{3\\log n}}g(a,b,c)$. Costs: Note that $g$ has $3\\log n$ variables and it has degree at most 2 in each variable. Total communication is $O(\\log n)$. Verifier runtime is $O(n^2)$. The total communication is logarithmic. Hence, the verifier runtime is dominated by evaluating $g$ at one point $g(r_1,r_2,r_3)=\\tilde{A}(r_1,r_2)\\tilde{A}(r_2,r_3)\\tilde{A}(r_1,r_3)$, which amounts to evaluating $\\tilde{A}$ at three points. The matrix $A$ gives the lists of all $n^2$ evaluations of the multilinear extension $\\tilde{A}:\\{0,1\\}^{2\\log n}\\rightarrow \\mathbb{F}$. As described above, the **verifier** can in linear time evaluate the multilinear extension function at any desired point in $\\mathbb{F}^{2\\log n}$. Note that the verifier runtime is linear to the size of the input/matrix, that’s $O(n^2)$. Prover runtime is $O(n^3)$. The prover’s runtime is clearly at most $O(n^5)$ since there are $3\\log n$ rounds and $g$ can be evaluated at any point in $O(n^2)$ time. But more sophisticated algorithm insights can bring the prover runtime down to $O(n^3)$. We recommend reader to refer to Chapter 4 and Chapter 5 in Thaler A SNARK for circuit-satisfiabilityWe can apply the sum-check protocol to design SNARKs for circuit satisfiability. Given an arithmetic circuit $C$ over $\\mathbb{F}$ of size $S$ and output $y$. $P$ claims to know a $w$ such that $C(x,w)=y$. For simplicity, let’s take $x$ to be the empty input. A transcript $T$ for $C$ is an assignment of a value to every gate as follows. $T$ is a correct transcript if it assigns the gate values obtained by evaluating $C$ on a valid witness $w$. The main idea is to assign each gate in $C$ a $(\\log S)$-bit label and view such a transcript as a function with domain $\\{0,1\\}^{\\log S}$ mapping to $\\mathbb{F}$. Hence, $P$’s first message is a $(\\log S)$-variate polynomial $h$ claimed to extend a correct transcript $T$, which means $$ h(x)=T(x) \\text{ }\\forall x\\in \\{0,1 \\}^{\\log S} $$ As usual, $T$ is defined over the hypercube $\\{0,1\\}^{\\log S}$ and $h$ is multilinear extension of $T$ with domain $\\mathbb{F}^{\\log S}$. $V$ can check this claim by evaluating all $S$ evaluations on $h$. Like the sum-check protocol, suppose the verifier is only able to learn a few evaluations of $h$ rather than $S$ points. Intuition of extension functionBefore describing the design details, let’s dig the intuition for why we use the extension polynomial $h$ of the transcript $T$ for $P$ to send. Intuitively, we think $h$ as a distance-amplified encoding of the transcript $T$. The domain of $T$ is $\\{0,1\\}^{\\log S}$. The domain of $h$ is $\\mathbb{F}^{\\log S}$, which is vastly bigger. By Schwart-Zippel lemma, if two transcripts disagree at even a single gate value, their extension polynomial $h,h’$ disagree at almost all points in $\\mathbb{F}^{\\log S}$. Specifically, a $1-\\log (S)/|\\mathbb{F}|$ fraction. The distance-amplifying nature of the encoding will enable $V$ to detect even a single “inconsistency” in the entire transcript. As a result, it kind of blows up the tiny difference in transcripts by the extension polynomials into easily detectable difference so that it can be detectable even by the verifier that is only allowed to evaluate the extension polynomials at a single point or a handful points. Two-step plan of attackThe original claim the prover makes is that the $(\\log S)$-variate polynomial $h$ extends the correct transcript. In order to offload work of the verifier and apply the sum-check protocol, the prover instead claims a related $(3\\log S)$-variate polynomial $g_h=0$ at every single boolean input, i.e. $h$ extends a correct transcript $T$ ↔ $g_h(a,b,c)=0$ $\\forall (a,b,c)\\in \\{0,1\\}^{3\\log S}$. Moreover, to evaluate $g_h(r)$ at any input $r$, suffices to evaluate $h$ at only 3 inputs. Specifically, the first step is as follows. Step 1: Given any $(\\log S)$-variate polynomial $h$, identify a related $(3\\log S)$-variate polynomial $g_h(a,b,c)$ via $$ \\widetilde{add}(a,b,c)\\cdot (h(a)-(h(b)+h(c))+\\widetilde{mult}(a,b,c)\\cdot (h(a)-h(b)\\cdot h(c)) $$ $\\widetilde{add},\\widetilde{mult}$ are multilinear extension called wiring predicates of the circuit. $\\widetilde{add}(a,b,c)$ splits out 1 iff $a$ is assigned to an addition gate and its two input neighbors are $b$ and $c$. Likewise, $\\widetilde{mult}(a,b,c)$ splits out 1 iff $a$ is assigned to the product of values assigned to $b$ and $c$. $g_h(a,b,c)=h(a)-(h(b)+h(c))$ if $a$ is the label of a gate that computes the sum of gates $b$ and $c$. $g_h(a,b,c)=h(a)-(h(b)\\cdot h(c))$ if $a$ is the label of a gate that computes the product of gates $b$ and $c$. $g_h(a,b,c)=0$ otherwise. Then we need to design an interactive proof to check that $g_h(a,b,c)=0 \\text{ } \\forall (a,b,c)\\in \\{0,1\\}^{3\\log S}$ in which $V$ only needs to evaluate $g_h(r)$ at one random point $r\\in \\mathbb{F}^{3\\log S}$. It is very different from the zero test. Using zero test, we are able to check $g_h=0$ for any input in $\\mathbb{F}^{3\\log S}$ by evaluating a random point $r$, but now we need to check $g_h=0$ over a hypercube $\\{0,1\\}^{3\\log S}$. Imagine for a moment that $g_h$ were a univariate polynomial $g_h(X)$. And rather than needing to check that $g_h$ vanishes over input set $\\{0,1\\}^{3\\log S}$, we need to check that $g_h$ vanishes over some set $H\\subseteq \\mathbb{F}$. We can design the polynomial IOP based on following fact. Fact: $g_h(x)=0$ for all $x\\in H$ ↔ $g_h$ is divisible by $Z_H(x)=\\prod_{a\\in H}(x-a)$. We call $Z_H$ the vanishing polynomial for $H$. The polynomial IOP works as follows. More details can be referred to the next Lecture. Polynomial IOP: $P$ sends a polynomial $q$ such that $g_h(X)=q(X)\\cdot Z_H(X)$. $V$ checks this by picking a random $r\\in \\mathbb{F}$ and checking that $g_h(r)=q(r)\\cdot Z_H(r)$. However, it dosen’t work when $g_h$ is not a univariate polynomial. Moreover, having $P$ find and send the quotient polynomial is expensive for high-degree polynomial. In the final SNARK, this would mean applying polynomial commitment to additional polynomials. This is what Marlin, Plonk and Groth16 do. In the next lecture, we will elaborate on the Plonk. Instead, the solution is to use the sum-check protocol. Concretely speaking, the sum-check protocol is able to handle multivariate polynomials and dosen’s require $P$ to send additional large polynomials. For simplicity, imagine working over the integers instead of $\\mathbb{F}$. The general idea is as follows. (Note that it is not a full version of solution.) Step2: General Idea of IP $V$ checks this by running sum-check protocol with $P$ to compute: $$ \\sum_{a,b,c\\in \\{0,1\\}^{\\log S}}g_h(a,b,c)^2 $$ If all terms in the sum are 0, the sum is 0. If working over the integers, any non-zero term in the sum will cause the sum to be strictly positive. At end of sum-check protocol, $V$ needs to evaluate $g_h(r_1, r_2, r_3)$. Suffices to evaluate $h(r_1),h(r_2),h(r_3)$. Outside of these evaluations, $V$ runs in time $O(\\log S)$ with $3\\log S$ rounds. $P$ performs $O(S)$ field operations given a witness $w$.","link":"/2023/07/18/zkp-lec4/"},{"title":"「Cryptography-ZKP」: Lec6 Poly-commit based on Pairing and Discret-log","text":"In this series, I will learn Zero Knowledge Proofs (ZKP) on this MOOC, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. Any corrections and advice are welcome. ^ - ^ Topics: KZG poly-commit based on bilinear pairing KZG scheme Powers-of-tau Ceremony Security Analysis Knowledge of exponent assumption Variants: multivariate; ZK; batch openings Bulletproofs poly-commit based on discrete logarithm Before proceeding to today’s topic, let’s recall the common recipe for building an efficient SNARK. The common way of building SNARK is to combine a poly-IOP with a functional commitment scheme. Lecture 4 uses Plonk (poly-IOP) combined with KZG (a univariate polynomial commitment) to build SNARK for general circuits. Lecture 3 uses Sumcheck protocol combined with a multivariate polynomial commitment to build SNARK. In this lecture, we are going to introduce polynomial commitments based on bilinear pairing and discrete log. When building polynomial commitment schemes, we first choose a family of polynomial $\\mathcal{F}$, then prover commits to a function $f(x)\\in \\mathcal{F}$. Verifier receives $\\text{com}_f$ as the commitment, then verifier is able to query $f$ at point $u$. Finally, prover sends the evaluation $v$ and the proof $\\pi$ that $f(u)=v$ and $f\\in \\mathcal{F}$, and verifier accepts if proof is valid. The above procedure is depicted as follows. For ease of use, we give a formal definition for polynomial commitment schemes (PCS). It consists of four algorithms as follows. $\\text{keygen}(\\lambda, \\mathcal{F})\\rightarrow gp$In setup, this algorithm takes the family as inputs and outputs the global parameters used in the commitment and proof. $\\text{commit}(gp,f)\\rightarrow \\text{com}_f$Prover calls this algorithm to commit to a function. $\\text{eval}(gp,f,u)\\rightarrow v,\\pi$Prover calls this algorithm to compute the evaluation at the point $u$ and the corresponding proof. $\\text{verify}(gp,\\text{com}_f,u,v,\\pi)\\rightarrow \\text{accept or reject}$Verifier calls this algorithm to check the validity of the proof and accept the answer if valid. It is complete if an honest prover can convince the verifier to accept the answer. It is sound if a verifier can catch a lying prover with high probability. We compare the soundness and knowledge soundness in Lecture 3. To put it simply, soundness establishes the existence of the witness while knowledge soundness establishes that the prover necessarily knows the witness. As a result, knowledge soundness is stronger. We give a formal definition of knowledge soundness. Knowledge Sound: For every poly. time adversary $A(A_0, A_1)$ such that $$ \\text{keygen}(\\lambda,\\mathcal{F})\\rightarrow gp, A_0(gp)\\rightarrow \\text{com}_f, A_1(gp, u)\\rightarrow v, \\pi: \\\\ \\operatorname{Pr}[V(vp, x,\\pi)=\\text{accept}]=1 $$ there is an efficient extractor $E$ (that uses $A$) such that $$ \\text{keygen}(\\lambda,\\mathcal{F})\\rightarrow gp, A_0(gp)\\rightarrow \\text{com}_f, E(gp, \\text{com}_f)\\rightarrow f:\\\\ \\operatorname{Pr}[f(u)=v \\text{ and } f(x)\\in \\mathcal{F}]> 1-\\epsilon $$ where $\\epsilon$ is negligible. BackgroundLet’s quickly go through the basic conceptions in number theory, which is widely used in the following sections. I refer readers to This Blog for a detailed description. Group: A set $\\mathbb{G}$ and an operation $*$ Closure: For all $a,b\\in \\mathbb{G}$, $a* b \\in \\mathbb{G}$ Associativity: For all $a,b,c\\in \\mathbb{G}$, $(a*b)*c =a*(b*c)$ Identity: There exists a unique element $e\\in \\mathbb{G}$ s.t. for every $a\\in \\mathbb{G}$, $e*a=a*e=a$. Inverse: For each $a\\in \\mathbb{G}$, there exists $b\\in \\mathbb{G}$ s.t. $a*b=b*a=e$. A simple example is the group that contains integers $\\{\\dots, -2,-1,0,1,2,\\dots\\}$ under addition operation $+$. The common Groups considered in Cryptography are the group that contains positive integers mod prime $p:\\{1,2,\\dots, p-1\\}$ under multiplication operation $\\times$ and the groups defined by elliptic curves. Generator of a group: An element $g$ that generates all elements in the group by taking all powers of $g$. For example, $3$ is a generator of the group $\\mathbb{Z}_7^*={1,2,3,4,5,6}$. We can write every group element in the power of $3$. $3^1=3;3^2=2;3^3=6;3^4=4;3^5=5;3^6=1 \\mod 7$ Discrete-logA group $\\mathbb{G}$ has an alternative representation as the powers of the generator $g:\\{g,g^2, g^3,\\dots,g^{p-1}\\}$. Discrete logarithm problem: Given $y\\in \\mathbb{G}$, find $x$ such that $g^x=y$. The quantum computer can actually solve the discrete logarithm problem in polynomial time. Discrete logarithm assumption: Discrete-log problem is computationally hard. Note that the DL assumption does not hold in all groups but it is believed to hold in certain groups. Computational Diffie-Hellman assumption: Given $\\mathbb{G},g,g^x,g^y$, cannot compute $g^{xy}$. It is worth noting that a stronger assumption means the underlying problem is easier. Hence, the CDH assumption is a stronger assumption than the DL assumption since the CDH problem is reducible to the DL problem. Bilinear PairingThe bilinear pairing is defined over $(p,\\mathbb{G},g,\\mathbb{G}_T,e)$. $\\mathbb{G}$: the base group of order $p$, a multiplicative cyclic group $\\mathbb{G}_T$: the target group of order $p$, a multiplicative cyclic group $g$: the generator of $\\mathbb{G}$ $e:\\mathbb{G}\\times \\mathbb{G} \\rightarrow \\mathbb{G}_T$, the pairing operation The pairing possesses the following bilinear property: $$ \\forall P,Q\\in \\mathbb{G}: e(P^x,Q^y)=e(P,Q)^{xy} $$ The pairing takes two elements in the base group $\\mathbb{G}$ as inputs, and outputs an element of the target group. For example, $e(g^x,g^y)=e(g,g)^{xy}=e(g^{xy},g)$. By the CDH assumption, we know computing $g^{xy}$ is hard given $g^x$ and $g^y$. It means that computing the product in the exponent is hard. But with pairing, we can check that some element $h=g^{xy}$ without knowing $x$ and $y$. Note that with pairing, we cannot break the CDH assumption with pairing. It actually gives us a tool to verify the product relationship in the exponent rather than computing the product in the exponent. A pairing example is the BLS signature proposed by Boneh, Lynn, and Shacham in 2001. [Bonth-Lynn-Shacham’2001] $\\text{Keygen}(p,\\mathbb{G},g, \\mathbb{G}_T,e):$ private key $x$ and public key $g^x$. $\\text{Sign}(sk,m)\\rightarrow \\sigma:H(m)^x$ where $H$ is a cryptographic hash that maps the message space to $\\mathbb{G}$. $\\text{Verify}(\\sigma, m):e(H(m),g^x)=e(\\sigma,g)$ The verification is to check the pairing equation. The LHS is the pairing of the hash of the message and the public key. The verifier cannot compute $H(m)^x$ without knowing $x$. The RHS is the pairing of the signature and the generator. Security Analysis: The correctness holds since the verifier will pass if the signer honestly computes $H(m)^x$. The idea of proving soundness is by contradiction. Assuming there is an adversary that can forge a signature to pass the verification, then we can break CDH assumption using this bilinear group. For your information, not all groups in which DL is hard are believed to support efficient computable pairing, but some groups especially those defined by elliptic curves. KZG based on Bilinear PairingLecture 4 has introduced KZG polynomial commitment scheme [Kate-Zaverucha-Goldberg’2010] with the multiplication notation but omits the details of pairing. In this section, we use the exponent notation and consider a bilinear group defined by $p, \\mathbb{G},g,\\mathbb{G}_T,e$ and the univariate polynomials $\\mathcal{F}=\\mathbb{F}_p^{(\\le d)}[X]$ with degree $\\le d$. Note that the degree $d$ is public to the verifier. KZG schemeLet’s elaborate on the four algorithms one by one. keygen $(\\lambda, \\mathcal{F})\\rightarrow gp$ : compute the global parameters $\\text{keygen}(\\lambda, \\mathcal{F})\\rightarrow gp$ Sample random $\\tau\\in \\mathbb{F}_p$ $gp=(g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})$ delete $\\tau$ !! (trusted setup) Intuitively, it uses a group where DL assumption is hard so that no one can compute $\\tau$. It suffices to use $gp$ to commit and generate the proof for the prover, and to check the pairing equation for the verifier, without knowing the secret $\\tau$. Once the prover learns the secret $\\tau$, the prover can generate fake proof to fool the verifier and break the security of the polynomial commitment scheme. It is worth noting that the secret $\\tau$ should be deleted so that it requires a trusted setup, which is the main drawback of KZG. For some practical applications, it is actually hard to find a trusted party to run a trusted setup. commit $(gp,f)\\rightarrow \\text{com}_f$ $f(x)=f_0+f_1x+f_2x^2+\\dots+f_d x^d$ $gp=(g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})$ Compute the commitment using $gp$ $$ \\begin{aligned}\\text{com}_f&=g^{f(\\tau)} \\\\ &=g^{f_0+f_1\\tau +f_2\\tau^2+\\dots+f_d \\tau^d} \\\\ &=(g)^{f_0} \\cdot (g^{\\tau})^{f_1} \\cdot (g^{\\tau^2})^{f_2}\\dots (g^{\\tau^d})^{f_d} \\end{aligned} $$ ​ eval $(gp,f,u)\\rightarrow v,\\pi$ $f(x)-f(u)=(x-u)q(x)$ as $u$ is a root of $f(x)-f(u)$ Compute $q(x)$ and $\\pi=g^{q(\\tau)}$ using $gp$ Note that the proof can be computed without accessing $\\tau$ and the proof size is only one group element. Finally, we are going to introduce the verification part, which is the highlight of KZG. The equation that the verifier wants to check is $f(x)-f(u)=(x-u)q(x)$. A naive idea is to verify the equation at the point $\\tau$ in the exponent on the base $g$. $$ g^{f(\\tau)-f(u)}=g^{(\\tau-u)q(\\tau)} $$ Verify has received $\\text{com}_f=g^{f(\\tau)}$ as commitment to $f$, $\\pi=g^{q(\\tau)}$ as eval proof, and $v=f(u)$ as evaluation from an honest prover. Verifier can compute $g^{(\\tau-u)}$ and $g^{q(\\tau)}$ using $gp$. Unfortunately, under CDH assumption, the verifier cannot compute $g^{(\\tau-u)q(\\tau)}$, which is the product in the exponent. The solution is pairing, which gives us a way to check the relation in the exponent of the equation instead of computing it. verify $(gp,\\text{com}_f,u,v,\\pi)$ Idea: check the equation at point $\\tau$ Challenge: only know $g^{\\tau-u}$ and $g^{q(\\tau)}$ Solution: pairing! Pairing! $$ \\begin{aligned}e(\\text{com}_f/g^v,g)&=e(g^{\\tau-u},\\pi) \\\\ e(g,g)^{f(\\tau)-f(u)}&=e(g,g)^{(\\tau-u)q(\\tau)}\\end{aligned} $$ ​ With pairing, the verifier can check the equation at the point $u$ in the exponent. The complete protocol is depicted as follows. Properties of KZGLet’s sum up the properties of KZG poly-commit. Properties of KZG: Keygen: trusted setup! Commit: $O(d)$ group exponentiations, $O(1)$ commitment size. Eval: $O(d)$ group exponentiations where $q(x)$ can be computed efficiently in linear time!Note: The polynomial division algorithm with nearly linear time is referred to this Lecture. Proof size: $O(1)$, 1 group element. Verifier time: $O(1)$, 1 pairing. Powers-of-tau CeremonyThe main drawback of KZG is the requirement of a trusted setup. A way to relax the trusted setup is Ceremony which uses a distributed generation of $gp$ so that no one can reconstruct the trapdoor if at least one of the participants is honest and discards their secrets. The main idea of distributed generation is using the product of secrets from all parties. The first party generates global parameters $gp=(g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})=(g_1,g_2,\\dots, g_d)$. Then the next party samples random $s$, and update $$ \\begin{aligned}gp' &=(g_1',g_2',\\dots, g_d') \\\\ &=(g_1^s,g_2^{s^2},\\dots, g_d^{s^d}) \\\\ &= (g^{\\tau s},g^{(\\tau s)^2},\\dots, g^{(\\tau s)^d})\\end{aligned} $$ with secret $\\tau\\cdot s$. It introduces a secret $s$ from updating /ma Finally, if all parties are honest, then the above procedure can generate the global parameters with the product of secrets from all parties. Meanwhile, it is required to check the correctness of $gp’$. Correctness of : (See [Nikolaenko-Ragsdale-Bonneau-Boneh’22]) The contributor knows $s$ s.t. $g_1’=(g_1)^s$.It can be verified by the $\\Sigma$ protocol. $gp’$ consists of consecutive powers $e(g_i’,g_1’)=e(g’_{i+1},g)$, and $g_1’\\ne 1$. Note that the check of $g_1’\\ne 1$ guarantees that the next party cannot remove the product of the preceding secrets and change it to 0. Security AnalysisThe completeness of KZG is evident. The soundness of KZG is based on the following assumption. $q$-Strong Bilinear Diffie-Hellman ($q$-SBDH) assumption: Given $(p,\\mathbb{G},g,\\mathbb{G}_T,e)$ and $(g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})$, cannot compute $e(g,g)^{\\frac{1}{\\tau-u}}$ for any $u$. There is an exposition [Tanaka-Saito] of reductions among the $q$-strong Diffie-Hellman problem and related problems. The $q$-SBDH problem is reducible to the CDH problem, so $q$-SBDH is a strictly stronger assumption. Proof of Soundness: (By Contradiction) Assume for contradiction that an adversary returns a wrong answer $v^*\\ne f(u)$ but the fake proof $\\pi^*$ pass the verification. Then we can break $q$-SBDH assumption, which arrives to a contradiction. $e(\\text{com}_f/g^{v^*})=e(g^{\\tau-u},\\pi^*)$ The pairing equation of verification holds by assumption. $e(g^{f(\\tau)-v^{*}},g)=e(g^{\\tau-u},\\pi^*)$ Knowledge of Exponent (KoE) assumption Later we are going to introduce the KoE assumption, which proves that the prover necessarily knows $f$ s.t. $\\text{com}_f=g^{f(\\tau)}$ rather than a random element. Because a random element as the commitment cannot be written in $g^{f(\\tau)}$ for some $f$. By KoE assumption, it means the prover necessarily knows an explicit $f$ so prover can compute $f(u)$. Define $\\delta=f(u)-v^*$, which is $\\ne 0$ by assumption. This is the key idea of the proof that decomposes $v^*$ to the correct answer $f(u)$ and $\\delta$. $\\Leftrightarrow e(g^{\\color{red}{f(\\tau)-f(u)+f(u)-v^*}},g)=e(g^{\\tau-u},\\pi ^*)$ $\\Leftrightarrow e(g^{\\color{red}{(\\tau-u)q(\\tau)+\\delta}},g)=e(g^{\\tau-u},\\pi ^*)$ $\\Leftrightarrow e(g,g)^{(\\tau-u)q(\\tau)+\\delta}=e(g,\\pi ^*)^{\\tau-u}$ Then we can extract the common factor $\\tau-u$ and put them outside the pairing to achieve our goal of computing$e(g,g)^{\\frac{1}{\\tau-u}}$. $\\Leftrightarrow e(g,g)^{\\delta}=(e(g,\\pi ^*)/e(g,g)^{q(\\tau)})^{\\tau-u}$ $\\Leftrightarrow e(g,g)^{\\frac{\\delta}{\\tau -u}}=e(g,\\pi^*)/e(g^{q(\\tau)},g)$ which breaks $q$-SBDH assumption. Then we are going to prove knowledge soundness by the knowledge of exponent assumption. Knowledge of Exponent assumptionIn the above security proof, we assume that the prover necessarily knows $f$ such that $\\text{com}_f=g^{f(\\tau)}$ rather than a random element. We make use of the Knowledge of Exponent (KoE) assumption to refine KZG protocol, achieving knowledge soundness. I excerpt the following descriptions from [Bellare-Palacio’04] to intuitively introduce the knowledge of exponent assumption. Let $q$ be a prime such that $2q+1$ is also prime (safe prime), and let $g$ be a generator of the order $q$ subgroup of $Z_{2q+1}^*$. Suppose we are given input $q,g,g^a$ and want to output a pair $(C,Y)$ such that $Y=C^a$. One way to do this is to pick some $c\\in \\mathbb{Z}_q$, let $C=g^c$, and let $Y=(g^a)^c$. Intuitively, it can be viewed as saying that this is the “only” way to produce such a pair. The assumption captures this by saying that any adversary outputting such a pair must “know” an exponent $c$ such that $g^c=C$. The formalization asks that there be an “extractor” that can return $c$. Knowledge of Exponent assumption : For any adversary $A$ that takes input $q,g,g^a$ and returns $(C,Y)$ with $Y=C^a$, there exists an “extractor” $\\bar{A}$, which given the same inputs as $A$ returns $c$ such that $g^c=C$. KZG with Knowledge SoundnessHaving this assumption, we can refine the KZG protocol. The goal is to prove that prover necessarily “knows” an exponent $f(\\tau)$ such that $\\text{com}_f=g^{f(\\tau)}$. We’d like to ask the prover to generate such a pair $g^{f(\\tau)}$ and $g^{\\alpha f(\\tau)}$ given “ $gp$ and $(gp)^{\\alpha}$”. The sketch of design is as follows. Sketch: $gp=(g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})$ Sample random $\\alpha$, compute $g^\\alpha,g^{\\alpha \\tau},g^{\\alpha \\tau^2},\\dots, g^{\\alpha \\tau^d}$ $\\text{com}_f=g^{f(\\tau)}$ and $\\text{com}’_f=g^{\\alpha f(\\tau)}$. If $e(\\text{com}_f,g^\\alpha)=e(\\text{com}_f’,g)$, there exists an extractor $E$ that extracts $f$ s.t. $\\text{com}_f=g^{f(\\tau)}$. Let’s elaborate on the details. In addition to the original global parameters $gp$, we need to sample random $\\alpha$ and compute $(gp)^\\alpha:\\{g^\\alpha, g^{\\alpha \\tau}, g^{\\alpha \\tau^2},\\dots, g^{\\alpha\\tau^d}\\}$, which is raising each element of the original $gp$ to random $\\alpha$. Note that the random $\\alpha$ is secret as $\\tau$. The prover commits to $f$ by computing such a pair, $\\text{com}_f=g^{f(\\tau)}$ and $\\text{com}_f’=g^{\\alpha f(\\tau)}$. Finally, the verifier can check the relation of these two commitments in the exponent by pairing. If the pairing equation $e(\\text{com}_f,g^\\alpha)=e(\\text{com}_f’,g)$ holds, by KoE assumption, there exists an extractor $E$ that extracts $f$ such that $\\text{com}_f=g^{f(\\tau)}$. Here is the KZG scheme with knowledge soundness. KZG with Knowledge Soundness: $\\text{Keygen}$: $gp$ includes both $g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d}$ and $g^\\alpha,g^{\\alpha\\tau},g^{\\alpha\\tau^2},\\dots, g^{\\alpha\\tau^d}$ where $\\tau$ and $r$ are both secret and required to be deleted. (trusted setup) $\\text{Commit}$: $\\text{com}_f=g^{f(\\tau)}$ and $\\text{com}_f’=g^{\\alpha f(\\tau)}$. $\\text{Verify}$: additionally checks $e(\\text{com}_f,g^\\alpha)=e(\\text{com}_f’,g)$. The idea of proving knowledge soundness is to extract $f$ in the first step by the KoE assumption. But it brings an overhead that the prove size is two group elements and the verifier time involves two pairing equations. Generic Group Model (GGM) [Shoup’97, Maurer’05] can replace the KoE assumption and reduce the commitment size in KZG. Informally speaking, the adversary is only given an oracle to compute the group operation. E.g., given $g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d}$, the adversary can only compute their linear combinations. As a result, the adversary cannot sample a random element that happens to be the power of another group element. See book “A Graduate Course in Applied Cryptography” by Dan Boneh and Victor Shoup, section 16.3 for more details. Variants of KZGMultivariate poly-commit Reference: [Papamanthou-Shi-Tamassia’13] KZG can be generalized to the multivariate polynomial commitment. The key idea is the following equation: $$ f(x_1,\\dots, x_k)-f(u_1,\\dots,u_k)=\\sum_{i=1}^k (x_i-u_i) q_i(\\vec{x}) $$ where $q(\\vec{x})$ is a multivariate polynomial. Keygen: sample $\\tau_1,\\tau_2,\\dots, \\tau_k$, each representing one variable, and compute $gp$ as $g$ raised to all possible monomials of $\\tau_1,\\tau_2,\\dots, \\tau_k$.e.g. $2^k$ monomials for multilinear polynomials. Commit: $\\text{com}=g^{f(\\tau_1,\\tau_2,\\dots, \\tau_k)}$ Eval: compute $\\pi_i=g^{q_i(\\vec{\\tau})}$.Note that the proof consists of multiple elements. Verify: $e(\\text{com}_f/g^v,g)=\\prod_{i=1}^ke(g^{\\tau_i-u_i},\\pi_i)$ Let $N\\le 2^k$ denote the total size of the polynomial. The proof size and verifier time are $O(\\log N)$. Achieving zero-knowledge Reference: [ZGKPP’2018] We say a poly-commit scheme is ZK if there is a simulator without knowing the polynomial can simulate the view of the verifier. The plain KZG is not ZK. E.g. the commit algorithm $\\text{com}_f=g^{f(\\tau)}$ is deterministic. The solution to achieve zero-knowledge is masking the commitment and proof with randomizers. Commit: $\\text{com}_f=g^{f(\\tau)+r\\eta}$ Eval: the main idea is to check whether a polynomial with randomizers is of a certain form. bivariate polynomial with randomizer: $\\begin{aligned} f(x)+ry-f(u) =(x-u)(q(x)+r’y)+y(r-r’(x-u))\\end{aligned}$ proof: $\\pi=g^{q(\\tau)+r’\\eta},g^{r-r’(\\tau-u)}$ Let’s elaborate on the details. First look at the commitment $\\text{com}_f=g^{f(\\tau)+r\\eta}$. The commitment to $f$ is randomized by $r$ randomly chosen by the prover. Note that $\\eta$ is another secret sampled in the trusted setup so $g^\\eta$ is included in the global parameters, which enables the prover to compute it. With random $r$ in the commitment, the idea of evaluation is to check the randomized bivariate polynomial is of a certain form: $$\\begin{aligned} f(x)+ry-f(u) =(x-u)(q(x)+r’y)+y(r-r’(x-u))\\end{aligned}$$ Likewise the check $f(x)-f(u)=(x-u)q(x)$ in the univariate polynomial, we can check the above relation in the exponent with pairing yet it is split into two terms of products. Consequently, the proof consists of two elements, the first evaluating $q(x)+r’y$ and the second evaluating $r-r’(x-u)$ at point $(x=\\tau,y=\\eta)$. Note that the verifier can compute $g^{\\tau-u}$ and $g^\\eta$ using $gp$. Finally, the verifier can check the relation in the pairing equation. Batch opening: single polynomialsAnother variant of KZG is batch opening or batch proofs. Let’s consider the batch proofs for a single polynomial in which the prover wants to prove $f$ at $u_1,\\dots, u_m$ for $m&lt;d$. Note that $m&lt;d$ is necessary since $m (&gt;d)$ points can interpolate the polynomial in clear. The key idea is to extrapolate $f(u_1),\\dots, f(u_m)$ to get $h(x)$ such that $h(u_i)=f(u_i)$. Recall that in Lecture 5 we introduce a vanishing polynomial in ZeroTest on the set $\\Omega$. It’s actually the same that we can prove $f(x)-h(x)=0$ on the set ${u_1,\\dots, u_m}$ using ZeroTest: $$f(x)-h(x)=\\prod_{i=1}^m (x-u_i) q(x)$$ $f(x)-h(x)$ is zero over the set ${u_1,\\dots, u_m}$ if and only if it is divisible by the vanishing polynomial $\\prod_{i=1}^m (x-u_i)$. The prover needs to compute the quotient polynomial $q(x)$ and generates the proof $\\pi =g^{q(\\tau)}$, a single group element as the batch proofs. The verifier checks the pairing equation $$ e(\\text{com}_f/g^{h(\\tau)},g)=e(g^{\\prod_{i=1}^m(\\tau-u_i)},\\pi) $$ where $g^{h(\\tau)}$ and $g^{\\prod_{i=1}^m(\\tau-u_i)}$ can be computed using $gp$. Note that the proof size is only one group element but the verifier time grows linearly in the number of evaluations. Batch opening: multiple polynomialsThen we extend batch opening for multiple polynomials (and multiple evaluations) where the prover wants to prove $f_i(u_{i,j})=v_{i,j}$ for $i\\in [n]$, $j\\in [m]$. The key idea kind of similar to the single polynomial case that extrapolates multiple polynomials. Specifically, the extrapolates $f_i(u_1),\\dots, f_i(u_m)$ to get $h_i(x)$ for each $i\\in [n]$. For each polynomial, we have $f_i(x)-h_i(x)=\\prod_{i=1}^m (x-u_m)q_i(x)$. The prover needs to compute every quotient polynomial $q_i(x)$ combine them via a random linear combination. Then prover can compute the proof as $g$ to the equation of the random linear combination, which is a single element. The verifier can check the relation in the exponent using bilinear pairing. KZG and its variants play an important role in building SNARKs. Plonk poly-IOP is combined with the univariate version of KZG to build SNARK for general circuits. vSQL and Libra both combine the Sumcheck protocol (Lecture 4) and the multivariate KZG. Before ending up discussing the poly-commit scheme based on the bilinear pairing, let’s sum up the pros and cons of KZG poly-commit. The pros contains that the commitment and proof size is $O(1)$, 1 group element and the verifier time involves $O(1)$ pairing. The main cons is that KZG requires a trusted setup to generate $gp$. The trusted setup is a fundamental problem to solve although the ceremony process relaxes trust a little bit and it is good for many applications in practice. Bulletproofs based on discrete-logBulletproofs is proposed by [BCCGP’16] and refined by [BBBPWM’18] to build SNARKs using a transparent setup. Moreover, they proposed an inner product protocol and a special protocol for range proof that can be generalized to build SNARKs for a general arithmetic circuit. Poly-commit based on BulletproofsIn this section, we rephrase the Bulletproofs as a poly-commit because it still shows the key idea of the reduction but significantly simplifies the protocol. The transparent setup samples random $gp=(g_0,g_1,g_2,\\dots, g_d)\\in \\mathbb{G}$ without the trapdoor $\\tau$ whose size is still linear to the degree $d$. The prover commits to $f(x)=f_0+f_1x+f_2x^2+\\dots +f_dx^d$ as usual, which raises each element of $gp$ to the corresponding coefficients of the polynomials and multiply them together to get a single element. $$\\text{com}_f=g_0^{f_0}g_1^{f_1}g_2^{f_2}\\dots g_d^{f_d}$$ Note that the random term is omitted. Let’s describe the high-level idea of Bulletproofs reduction using an example of a degree-3 polynomial. After receiving the commitment to $f$ from the prover, verifier queries at $u$. Prover replies with the evaluation $v$. The key idea is to reduce the claim of evaluating $v$ at point $u$ for the polynomial $f$ inside the commitment $\\text{com}_f$ to a new claim about a new polynomial of only half of the size. In our example, we reduce the original polynomial of degree 3 to a new polynomial of degree only 1 with only two coefficients $f_0’$ and $f_1’$. Furthermore, the verifier will receive a new instance of the commitment $\\text{com}_{f’}$ to this new polynomial of only half of the size. Then we keep doing recursively to reduce the polynomial of degree $d/2$ to a new polynomial of degree $d/4,d/8,\\dots,$ to a constant degree. Finally, in the last round, the prover can just send a polynomial of constant size to the verifier directly, and the verifier opens the polynomial and checks the evaluation of the last round is indeed true. It completes the entire reduction and guarantees that the claim of the evaluation of $v$ for the original polynomial is correct. The main challenge of the reduction is how to go from the original polynomial to a new polynomial of half of the size. We can’t have the prover commit to a random polynomial of half of the size without any relationship. Otherwise, the prover can cheat and lie about the evaluation since this new polynomial has no relation to the original polynomial. It has to check the relationship between the two polynomials. Let’s elaborate on the details of the reduction. Prover first sends the evaluation $v=f_0+f_1u+f_2u^2+f_3u^3$ at point $u$. A common way of reduction for the prover is reducing the polynomial to two polynomials of half of size. Then prover evaluates these two reduced polynomials at point $u$ to get $v_L=f_0+f_1u$ and $v_R=f_2+f_3u$ such that $v=v_L+v_Ru^2$. It is safe for the verifier to believe that $v$ is the correct evaluation if and only if $v_L$ and $v_R$ are correctly evaluated for the reduced polynomials. So prover also commits to the two reduced polynomials with two cross terms $L=g_2^{f_0}g_3^{f_1}$ and $R=g_0^{f_2}g_1^{f_3}$, which uses $g_2,g_3$ as bases to commit to the left reduced polynomial and $g_0,g_1$ to commit to the right reduced polynomial. As depicted follows, prover sends two commitments $L$ and $R$ and two evaluations $v_L$ and $v_R$ on the two reduced polynomials. But these two polynomials are actually temporary reduced polynomials. The actual reduced polynomial is a single polynomial with two coefficients $rf_0+f_2$ and $rf_1+f_3$, which is a randomized linear combination of the original coefficients where the randomness $r$ is sampled by the verifier. This new claim about this new reduced polynomial actually combines two claims about the old temporary polynomials through randomized linear combinations. And the claim about the evaluation in the next round is to altered to $v’=rv_L+v_R$. The only remaining challenge is to generate the commitment of this randomized reduced polynomial. We can’t let the prover commit $g_0^{rf_0+f_2}g_1^{rf_1+f_3}$ with the original global parameters because the transparent setup doesn’t know the relationship ( defined by $r$ ) between the two polynomials. In order to address the issue, Bulletproofs proposed a clever design to allow the verifier to compute the new commitment from the old commitments with the help of the commitments to the temporary polynomials. Recall that $\\text{com}_f=g_0^{f_0}g_1^{f_1}g_2^{f_2}\\dots g_d^{f_d}$, $L=g_2^{f_0}g_3^{f_1}$ and $R=g_0^{f_2}g_1^{f_3}$. Then verifier can compute the commitment $\\text{com}’$ from $L$ and $R$ such that $$ \\begin{aligned} \\text{com}' &=L^r\\cdot \\text{com}_f \\cdot R^{r^{-1}} \\\\ &=g_0^{f_0+r^{-1}f_2}g_2^{rf_0+f_2}\\cdot g_1^{f_1+r^{-1}f_3}g_3^{rf_1+f_3} \\\\ &= (g_0^{r^{-1}}g_2)^{rf_0+f_2} \\cdot (g_1^{r^{-1}}g_3)^{rf_1+f_3}\\end{aligned} $$ where the global parameter is updated to $gp’=(g_0^{r^{-1}}g_2,g_1^{r^{-1}}g_3)$. The last equation holds by extracting the common factor to commit to the new polynomial with coefficients $rf_0+f_2$ and $rf_1+f_3$. And the verifier can compute the new global parameters related to the new commitment, which allows the verifier to check some pairing equations. Let’s sum up the reduction procedure. Poly-commitment based on Bulletproofs: Recurse $\\log d$ rounds: Eval: (Prover) Compute $L,R,v_L,v_R$ Receive $r$ from the verifier, reduce $f$ to $f’$ of degree $d/2$ Update the bases $gp’$ Verify: (Verifier) Check $v=v_L+v_Ru^{d/2}$ Generate $r$ randomly Update $\\text{com}’=L^r\\cdot \\text{com}_f\\cdot R^{r^{-1}}$, $gp’$, and $v’=rv_L+v_R$ In the last round: The prover sends the constant-size polynomial to the verifier. The verifier checks the commitment and the evaluation is correct. Note that the above protocol can be rendered non-interactive via Fiat Shamir. Properties of BulletproofsLet’s sum up the properties of poly-commitment based on Bulletproofs. Properties of Bulletproofs: Keygen: $O(d)$, transparent setup! Commit: $O(d)$ group exponentiations, $O(1)$ commitment size. Eval: $O(d)$ group exponentiations Proof size: $O(\\log d)$In each round, the prover sends 4 elements as proof. Verifier time: $O(d)$The verifier has to recursively update the global parameters the number of which falls geometrically so the verifier time depends on the first round that is nearly linear in $d$. Other worksHyrax Reference: [Wahby-Tzialla-shelat-Thaler-Walfish’18] The main drawback of Bulletproofs is the linear verifier time. Hyrax improves the verifier time to $O(\\sqrt{d})$ by representing the coefficients as a 2-D matrix with proof size $O(\\sqrt{d})$. In fact, the product of verifier time and the proof size is linear in $d$ so the proof size can be reduced to $\\sqrt[n]{d}$ while the verifier time is $d^{1-1/n}$. Dory Reference: [Lee’2021] Dory improves the verifier time to $O(\\log d)$ without any asymptotic overhead on other parts. It’s a nice improvement over the poly-commitment based on Bulletproofs. The key idea is delegating the structured verifier computation to the prover using inner pairing product arguments. [BMMTV’2021] It also improves the prover time to $O(\\sqrt{d})$ exponentiations plus $O(d)$ field operations. Dark Reference: [Bünz-Fisch-Szepieniec’20] Dark achieves $O(\\log d)$ proof size and verifier time based on the cryptographic primitive of group of unknown order. Let’s end up with a summary of all works mentioned in this lecture.","link":"/2023/07/26/zkp-lec6/"},{"title":"「Cryptography-ZKP」: Lec5-The Plonk SNARK","text":"In this series, I will learn Zero Knowledge Proofs (ZKP) on this MOOC, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. Any corrections and advice are welcome. ^ - ^ Topics: Preprocessing SNARK KZG Poly-Commit Scheme Proving Properties of committed polys ZeroTest Product Check Permutation Check Prescribed Permutation Check Plonk IOP for General Circuits What is SNARK?Before proceeding to today’s topic, let’s review what is SNARK. Note that this part is well explained in Lecture 2. preprocessing NARKNARK is Non-interactive ARgument of Knowledge. Given a public arithmetic circuit $C(x,w)\\rightarrow \\mathbb{F}$ where $x$ is the public statement in $\\mathbb{F}^n$ and $w$ is the secret witness in $\\mathbb{F}^m$, a preprocessing (setup) algorithm $$S(C)\\rightarrow \\text{ public parameters } (pp,vp)$$ takes a description of the circuit as inputs, and outputs public parameters $(pp,vp)$ for prover and verifier. NARK works as follows. A preprocessing NARK is a triple of algorithms $(S,P,V)$. $S(C)\\rightarrow$ public parameters $(pp,vp)$ for prover and verifier. $P(pp,x,w)\\rightarrow$ proof $\\pi$. $V(vp,x,\\pi)\\rightarrow$ accept or reject. Note that all algorithms and the adversary have access to a random oracle. The informal requirements of NARK are completeness and adaptively knowledge soundness. Completeness means that an honest prover always convinces the verifier to accept the answer, i.e. $$ \\forall x,w: C(x,w)=0 \\text{ then } \\operatorname{Pr}[V(vp,x,P(pp,x,w))=\\text{accept}]=1 $$ Adaptively knowledge soundness means that if the verifier accepts the proof, then the prover must know a witness such that $C(x,w)=0$. In other words, there exists an extractor $E$ that can extract a valid $w$ from $P$. Zero-knowledge suggests that $(C,pp,vp,x,\\pi)$ reveals nothing new about $w$. Note that the privacy requirement, i.e. zero knowledge, in NARK is optional. So there is a trivial NARK in which the prover just sends $w$ as proof and the verifier just rerun the circuit and check if $C(x,w)=0$. SNARK: a Succinct ARgument of KnowledgeSNARK is a NARK (complete and knowledge sound) that is succinct. zk-SNARK is a SNARK that is also zero knowledge, meaning that the SNARK proof reveals nothing about the witness. A preprocessing SNARK is a triple of algorithms $(S,P,V)$. $S(C)\\rightarrow$ public parameters $(pp,vp)$ for prover and verifier. $P(pp,x,w)\\rightarrow$ short proof $\\pi$; $\\text{len}(\\pi)=\\text{sublinear}(|w|)$. $V(vp,x,\\pi)\\rightarrow$ fast to verify; $\\text{time}(V)=O_\\lambda(|x|,\\text{sublinear}(|C|))$. SNARK requires the length of proof to be sublinear in the length of the witness, and the verify runtime to be linear in the statement and sublinear in the size of the circuit. Being linear in the statement $x$ means that the verifier at least read the statements. Furthermore, a strongly succinct preprocessing NARK is not only sublinear but logarithmic in the size of the circuit. $S(C)\\rightarrow$ public parameters $(pp,vp)$ for prover and verifier. $P(pp,x,w)\\rightarrow$ short proof $\\pi$; $\\text{len}(\\pi)=O_\\lambda(\\log(|C|))$. $V(vp,x,\\pi)\\rightarrow$ fast to verify; $\\text{time}(V)=O_\\lambda(|x|,\\log (|C|))$. Note that the verifier runtime is logarithmic in the size of the circuit, which implies that the verifier even does not know what the underlying circuit is. An insight is that the verifier parameter $vp$ is the short “summary” of the circuit so the verifier is able to verify the evaluations of the circuit at an arbitrary input $x$. That’s the reason why we need the preprocessing or setup. It is worth noting that the trivial SNARK is not a SNARK. Types of preprocessing SetupThe setup for a circuit $C$ is an algorithm $S(C;r)\\rightarrow \\text{ public parameters}(pp,vp)$, which takes the circuit and random bits as inputs and generates public parameters for the prover and the verifier. The types of setup are more detailed. trusted setup per circuit: random $r$ must be kept secret from the prover. $S(C;r)$ trusted but universal (updatable) setup: secret $r$ is independent of $C$. $S=(S_{init},S_{index})$ $S_{init}(\\lambda;r)\\rightarrow gp$: onetime setup, secret $r$ $S_{index}(gp,C)\\rightarrow (pp,vp)$: deterministic algorithm transparent setup: does not use secret data $S(C)$ In the trusted setup, random $r$ must be kept secret from the prover, meaning that it has to be run for every circuit. Once the prover learns $r$, it allows the prover to prove false statements. In practice, $r$ will be destroyed so that nobody can ever know $r$. Sometimes, it is called the trusted setup ceremony. In the trusted but universal setup, it splits the setup into two parts. $S_{init}$ is a one-time setup that does not take the circuit as input and generates global parameters $gp$. Note that $r$ is required to be secret as well. Then $S_{index}$ is a deterministic algorithm executed for every circuit so it takes the circuit and $gp$ as inputs and generates public parameters for the prover and the verifier. In the transparent setup, $S(C)$ does not use secret data. General Paradigm for Building SNARKThe general paradigm for building SNARK is made up of two steps. One is the functional commitment scheme, which is a cryptographic object that depends on some assumptions. The other is the compatible interactive oracle proof (IOP). IOP is an information-theoretic object, which provides unconditional security without any assumptions. To be precise, they are not separate steps. For example, using poly-IOP, we can boost polynomial functional commitment for $\\mathbb{F}_p^{(\\le d)}[X]$ to build SNARK for any circuit $C$ where $|C|&lt;d$. Polynomial CommitmentsReview the polynomial commitments introduced in the last lecture. Prover commits to a univariate polynomial $f(X)$ in $\\mathbb{F}_p^{(\\le d)}[X]$ where the variable $X$ has degree at most $d$. Later the verifier can request to know the evaluation of this polynomial at a specific point. In other words, for public $u,v\\in \\mathbb{F}_p$, the prover can convince the verifier that the committed polynomial satisfies $$f(u)=v \\text{ and deg}(f)\\le d$$ Note that the verifier has the upper bound of the degree $d$, the commitment received from the prover, and $u,v$. In SNARK, the evaluation proof size and verifier time should be logarithmic in the degree, i.e. $O_\\lambda(\\log d)$. Equality Test ProtocolRecall the observation that if $p,q$ are univariate polynomials of degree at most $d$, then $\\operatorname{Pr}_{r\\overset{\\$}\\leftarrow \\mathbb{F}}[p(r)=q(r)]\\le d/p$. If $f(r)=g(r)$ for a random $r \\overset{\\$}\\leftarrow \\mathbb{F}_p$, then $f=g$ w.h.p. Having the polynomial commitments, we can construct the equality test protocol as follows. Prover has committed to two polynomial $f,g$, so verifier has two commitments depicted in two boxes. Verifier samples a random coin $r$ in $\\mathbb{F}_p$ and sends to prover.Hence, it is a public coin. Prover sends the evaluations $y,y’$ and proof that $y=f(r)$ and $y’=g(r)$. Verifier accepts if $y=y’$ and the proof is valid. Fiat-Shamir TransformNote that the equality teat protocol is interactive but the verifier just sends coins to prover. A solution to making it a SNARK (non-interactive) is the Fiat-Shamir transform, which can render a public-coin interactive protocol non-interactive. Let $H:M\\rightarrow R$ be a cryptographic hash function. The main idea is having prover generates verifier’s random bits on its own using $H$, i.e. $r\\leftarrow H(x)$. Prover and verifier can compute $r\\leftarrow H(x)$, making it non-interactive. The completeness is evident and one has to prove knowledge soundness. Thm via Fiat-Shamir Transform: This is a SNARK if ( i ) $d/p$ is negligible where $f,g\\in \\mathbb{F}_p^{(\\le d)}[X]$, and ( ii ) $H$ is modeled as a random oracle. In practice, we use SHA256 as $H$. The succinctness holds by a succinct poly commitment scheme. Note that it is not zk since verifier learns the value of $f(r),g(r)$ that he didn’t know before. In this blog, we’ll introduce a specific polynomial commitment scheme — KZG’10 with a trusted setup. KZG requires a trusted but universal setup that generates global parameters once, then it can commit to an arbitrary polynomial of bounded degree $d$. $\\mathcal{F}$-IOPHaving the functional commitments, we can build SNARKs for some circuits, e.g. zero test, equality test. But with $\\mathcal{F}$-IOP, we can boost functional commitment to build SNARK for any circuit $C(x,w)$. Hence, $\\mathcal{F}$-IOP is a proof system that proves $\\exist w:C(x,w)=0$ as follows. Setup: The setup algorithm outputs public parameters for prover and verifier where $vp$ contains a number of oracles for functions in $\\mathcal{F}$.Verifier can ask the oracle for evaluations at some points.The oracles will be replaced to functional commitment schemes in SNARKs. IP proving $C(x,w)=0$: In each round, prover sends an oracle for a function $f_i$ which later on the verifier can evaluate at any point of his choice.Likewise, the oracles will be compiled to the commitment scheme when instantiating actual SNARKs. Properties: Complete: if $\\exist w:C(x,w)=0$ then $\\operatorname{Pr}[\\text{verifier accepts}]=1$. (Unconditional) knowledge sound (as an IOP): extractor is given $(x,f_1, r_1, \\dots, r_{t-1},f_t)$ and outputs $w$.Note that the given functions are in clear since the functional commitments are SNARKs so the extractor can extract the functions from the commitments. Optional: zero knowledge for a zk-SNARK KZG Poly-Commit SchemeLet’s introduce today’s highlight, KZG polynomial commitment scheme [Kate-Zaverucha-Goldberg’2010]. KZG: A Binding PSCFixed a group $\\mathbb{G}={0,G,2\\cdot G, 3\\cdot G,\\dots, (p-1)\\cdot G}$ of order $p$. CommitmentIt requires a trusted but universal setup. $\\text{setup}(1^\\lambda)\\rightarrow gp$ Sample random $\\tau\\in \\mathbb{F}_p$ $gp=(H_0=G,H_1=\\tau\\cdot G, H_2=\\tau^2\\cdot G, \\dots, H_d=\\tau^d \\cdot G)\\in \\mathbb{G}^{d+1}$. delete $\\tau$!! $\\text{commit}(gp,f)\\rightarrow \\text{com}_f$ where $\\text{com}_f=f(\\tau)\\cdot G \\in \\mathbb{G}$ $f$ as prover parameter $\\text{com}_f$ as verifier parameter The setup generates global parameters $gp$ in which the random $\\tau$ used must be destroyed. Then prover can use $gp$ to generate the commitment for any specific polynomial $f\\in \\mathbb{F}_p^{(\\le d)}[X]$. It is worth noting the prover can compute $f(\\tau)\\cdot G$ without knowing $\\tau$. It can be evaluated by $gp$: $$\\begin{aligned}f(X)&amp; =f_0+f_1X+\\dots+f_d X^d \\ \\text{com}_f &amp;= f_0 \\cdot H_0 +\\dots f_d\\cdot H_d \\ &amp;=f_0\\cdot G + f_1\\tau \\cdot G +f_2 \\tau^2\\cdot G +\\cdots \\ &amp;= f(\\tau)\\cdot G\\end{aligned}$$ where $f_0,\\dots, f_d$ are coefficients of the polynomial. This is a binding commitment but not hiding since it reveals $f(\\tau)\\cdot G$. EvaluationAfter committing to $f$, verifier can request for evaluations at a specific point. For public $u,v\\in \\mathbb{F}_p$, the prover can convince the verifier that the committed polynomial satisfies $f(u)=v$. The proof hinges on some cute algebraic properties: $f(u)=v$ iff $u$ is a root of $\\hat{f}=f-v$ iff $(X-u)$ divides $\\hat{f}$ iff exists $q\\in \\mathbb{F}_p[X]$ s.t. $q(X)\\cdot (X-u)=f(X)-v$ As a result, we can construct an equality test on two polynomials to verify the original claim $f(u)=v$. The whole idea is depicted as follows. Eval: Prover compute the quotient polynomial $q(X)$ and $\\text{com}_q=q(\\tau)\\cdot G$ as the commitment. send the proof $\\pi:=\\text{com}_q\\in \\mathbb{G}$ Note that the proof is just one group element, which is const size better than logarithmic in $d$. Verifier accept if $(\\tau-u)\\cdot \\text{com}_q=\\text{com}_f -v\\cdot G$ The equality test for $(X-u)q(X)\\cdot G=(f(X)-v)\\cdot G$ can be checked by the random $\\tau$. It is worth noticing that $\\tau$ is secret. But verifier can use a “pairing” to do the above computation with only $H_0,H_1$ from $gp$, which is a fast computation for verifier. As for the prover, computing the quotient is indeed an expensive computation for large $d$. GeneralizationsThere are two ways to generalize KZG. [PST’13] Can use KZG to commit to $k$-variate polynomials. Batch proofs: Can commit to $n$ polynomials and provide a batch proof for multiple evaluations. suppose verifier has commitments $\\text{com}_{f_1},\\dots, \\text{com}_{f_n}$ prover wants to prove $f_i(u_{i,j})=v_{i,j}$ for $i\\in [n],j\\in[m]$ → batch proof $\\pi$ is only one group element ! Properties: linear time commitmentOne wonderful property of KZG is the prover’s runtime for commitment is linear in the degree $d$. There are two ways to represent a polynomial $f(X)$ in $\\mathbb{F}_p^{(\\le d)}[X]$: Coefficient representation:$f(X)=f_0+f_1X+\\dots +f_d X^d$. computing $\\text{com}_f=f_0\\cdot H_0 +\\dots +f_d\\cdot H_d$ takes linear time in $d$. Point-value representation:$((a_0,f(a_0)),\\dots,(a_d,f(a_d))$ computing $\\text{com}_f$ naively: construct coefficients $(f_0,f_1,\\dots, f_d)$ takes time $O(d\\log d)$ using Number Theoretic Transform (NTT). Using the point-value representation, the naive way of constructing coefficients takes time $O(d\\log d)$ yet we want it linear in $d$. A better way to compute the commitment with point-value representation is the Lagrange interpolation. $$ f(\\tau)=\\sum_{i=0}^d \\lambda_i (\\tau)\\cdot f(a_i) \\\\ \\text{where } \\lambda_i(\\tau)=\\frac{\\prod_{j=0,j\\ne i}^d (\\tau -a_j)}{\\prod _{j=0,j\\ne i}^{d}(a_i-a_j)}\\in \\mathbb{F}_p $$ One can transform $gp$ into Lagrange form $\\hat{gp}$ by a linear map, not involving any secrets so that anyone can fulfill this transformation. $$ \\hat{gp}=(\\hat{H}_0=\\lambda_0(\\tau)\\cdot G,\\hat{H}_1=\\lambda_1(\\tau)\\cdot G,\\dots, \\hat{H_d}=\\lambda_d(\\tau)\\cdot G) $$ Now, prover can in linear time computes the commitment from the point-value representation as follows. $$ \\text{com}_f=f(\\tau)\\cdot G=f(a_0)\\cdot \\hat{H}_0+\\dots +f(a_d)\\cdot \\hat{H}_d $$ To sum up, prover can compute the commitment in linear time from the coefficient representation or the point-value representation. Multi-point Proof GenerationLet $\\Omega\\subseteq \\mathbb{F}_p$ and $|\\Omega|=d$. Consider such a case in which prover has some $f(X)$ in $\\mathbb{F}_p^{(\\le d)}[X]$ and needs evaluation proofs $\\pi_a\\in G$ for all $a\\in \\Omega$. When it comes to generate evaluation proofs for multi-points, prover naively takes time $O(d^2)$ for $d$ proofs, each takes time $O(d)$. Feist-Khovratovich (FK2020) algorithm optimize to time $O(d\\log d)$ if $\\Omega$ is a multiplicative subgroup time $O(d\\log^2 d)$ otherwise. The Dory polynomial commitmentThe difficulties with KZG lies in two parts One has to require trusted setup for $gp$, and $gp$ size is linear in $d$. Dory (eprint/2020/1274) is proposed to get over the trusted setup. transparent setup: no secret randomness in the setup $\\text{com}_f$ is a single group element (independent of degree $d$ ) eval proof size for $f\\in \\mathbb{F}_p^{(\\le d)}[X]$ is $O(\\log d)$ group elements.Note the eval proof size is constant size in original KZG. eval verify time is $O(\\log d)$Note the eval verify time is constant. prover time is $O(d)$ Applications: vector commitmentPolynomial Commitment Scheme (PCS) have many applications. One is to perform a drop-in replacement for Merkle trees. The idea is to view the vector $(u_1,\\dots, u_k)\\in \\mathbb{F}_p^{(\\le d)}$ as a function $f$ such that $f(i)=u_i$ for $i=1,\\dots, k$, then prover can commits to this polynomial. Instead of proving the revealed entry is consistent with the committed vector, prover can generate evaluation proof that $f(2)=a,f(4)=b$ as depicted follows. The proof $\\pi$ is a single group element (using batch proof) that is shorter than a Merkle proof. Proving properties of committed polynomialsHaving PCS, not only verifier can query evaluations of a committed polynomial, but prover can convince verifier that the committed polynomials $f,g$ satisfy some properties, e.g. equality test. It can be summed up in the following process. Start: Prover has functions $f,g$ in clear and verifier has the corresponding commitments via PCS. Verifier samples a random $r\\in \\mathbb{F}_p$. Prover computes a related polynomial $q$ and commits to it. Verifier can query $f,g,q$ at point $x$ and accept if valid.Note that when we say verifier query a committed polynomial $f(x)$, it means verifier sends $x$ to prover who responds with $f(x)$ and eval proof $\\pi$. (described in here[link]) Polynomial Equality Testing with KZGAs described above, we can construct equality test for two committed polynomials. But for KZG, $f=g$ if and only if $\\text{com}_f=\\text{com}_g$, resulting that verifier can tell if $f=g$ on its own. But prover is needed to test equality of computed polynomials. For example, verifier has four individual commitments to $f,g_1,g_2,g_3$ where all four are in $\\mathbb{F}_p^{(\\le d)}[X]$ to test $f=g_1g_2g_3$. Then verifier queries all four polynomials at a random point $r\\overset{$}\\leftarrow \\mathbb{F}_p$ and tests equality. It is complete and sound assuming $3d/p$ is negligible since $\\text{deg}(g_1g_2g_3)\\le 3d$. Summary of Proof Gadgets for UnivariatesIn order to construct Poly-IOPs for an arbitrary circuit. In this section, we’ll introduce some important proof gadgets for univariates. Let $\\Omega$ be some subset of $\\mathbb{F}_p$ of size $k$. Let $f\\in \\mathbb{F}_p^{(\\le d)}[X]$ where $d\\ge k$ and verifier has the commitment to $f$. We can construct efficient Poly-IOPs for the following tasks. Zero Test: prove that $f$ is identically zero on $\\Omega$. Sum Check: prove that $\\sum_{a\\in \\Omega}f(a)=0$. Prod Check: prove that $\\prod_{a\\in \\Omega}f(a)=1$. → prove for rational functions that $\\prod_{a\\in \\Omega}f(a)/g(a)=1$ Permutation Check: prove that $g(\\Omega)$ is the same as $f(\\Omega)$, just permuted. Prescribed Permutation Check: prove that $g(\\Omega)$ is the same as $f(\\Omega)$, permuted by the prescribed $W$. Vanishing PolynomialBefore staring, let’s introduce the vanishing polynomial. Def: Vanishing Polynomial of $\\Omega$: The vanishing polynomial of $\\Omega$ is $$Z_\\Omega(X):=\\prod_{a\\in \\Omega}(X-a)$$ such that $\\text{deg}(Z_\\Omega)=k$. By definition, the vanishing polynomial is a univariate polynomial to be $0$ everywhere on subset $\\Omega$. We can construct a cute vanishing polynomial by constructing a special subset $\\Omega$. Let $\\omega\\in \\mathbb{F}_p$ be a primitive $k$-th root of unity so that $\\omega ^k=1$. If $\\Omega=\\{1,\\omega, \\omega^2, \\dots, \\omega^{k-1}\\}\\subseteq \\mathbb{F}_p$ then $Z_\\Omega(X)=X^k-1$. Then for $r\\in \\mathbb{F}_p$, evaluating $Z_\\Omega(r)$ takes $\\le 2\\log_2{k}$ field operations by repeated squaring algorithm. It’s super fast. In the following tasks, we fix $\\Omega=\\{1,\\omega, \\omega^2, \\dots, \\omega^{k-1}\\}$. ZeroTest on $\\Omega$In zero test, prove wants to convince verifier that $f$ is identically zero on $\\Omega$. We build zero test by the following lemma. Lemma: $f$ is zero on $\\Omega$ if and only if $f(X)$ is divisible by $Z_{\\Omega}(X)$. The IOP of zero test is depicted as follows. Prover computes the quotient polynomial $q(X)=f(X)/Z_{\\Omega}(X)$ and commits to this polynomial. Note that with KZG prover can only commits to a polynomial in $\\mathbb{F}_p^{(\\le d)}$ rather than a rational functions. Verifier samples a random $r\\in \\mathbb{F}_p$. Verifier query $q(X)$ and $f(X)$ at point $r$ to learn $q(r)$ and $f(r)$. And verifier evaluates $Z_\\Omega(r)$ by itself. Verifier accepts if $f(r)=q(r)\\cdot Z_\\Omega(r)$ since it implies $f(X)=q(X)\\cdot Z_\\Omega(X)$ w.h.p. Theorem: This protocol is complete and sound assuming $d/p$ is negligible. Costs: Verifier time: $O(\\log k)$ for evaluating $Z_\\Omega(r)$ plus two poly queries (that can be batch into one) Prover time: dominated by the time to compute $q(X)$ and then commit to $q(X)$. Product Check on $\\Omega$We omit the details of sum check and jump to the product check since they are nearly the same. Product check is a useful gadget to construct the permutation check introduced later. In product check, prover wants to convince verifier that the products of all evaluations over $\\Omega$ equals to $1$, i.e. $$\\prod_{a\\in \\Omega}f(a)=1$$ We construct a degree-$k$ polynomial to prove it. Set $t\\in \\mathbb{F}_p^{(\\le k)}[X]$ to be the degree-$k$ polynomial such that $$\\begin{aligned}t(1)&amp;=f(1), \\ t(\\omega^s)&amp;=\\prod_{i=0}^s f(\\omega^i) \\text{ for }s=1,\\dots, k \\end{aligned}$$ Note that a degree-$k$ polynomial can be uniquely specified by $k+1$ points. Then $t(\\omega^i)$ evaluates the prefix-products as follows. $t(\\omega)=f(1)\\cdot f(\\omega)$, $t(\\omega^2)=f(1)\\cdot f(\\omega)\\cdot f(\\omega^2)$ … … $t(\\omega^{k-1})=\\prod_{a\\in \\Omega}f(a)=1$ We can represent prefix-product in a iterative way: $$t(\\omega\\cdot x)=t(x)\\cdot f(\\omega \\cdot x) \\text{ for all }x\\in \\Omega$$ As a result, we can do the product check by the following lemma, which can be proved with the evaluation proof and a zero test. Lemma: If ( i ) $t(\\omega^{k-1})=1$ and ( ii ) $t(\\omega\\cdot x)-t(x)\\cdot f(\\omega\\cdot x)=0$ for all $x\\in \\Omega$ then $\\prod_{a\\in \\Omega}f(a)=1$. The IOP for product check is depicted as follows. We can split it two parts: Evaluation proof to prove $t(\\omega^{k-1})=1$ Prover construct $t(X)\\in \\mathbb{F}_p^{(\\le k)}$ and commits to it. Verifier queries $t(X)$ at $\\omega^{k-1}$. check1: Verifier checks that if $t(\\omega^{k-1})=1$. proof size: one commit, one evaluation. Let $t_1(X)=t(\\omega\\cdot X)-t(X)\\cdot f(\\omega\\cdot X)$. Zero test to prove $t_1$ is zero on $\\Omega$.Recall the lemma that $t_1$ is zero on $\\Omega$ iff $Z_{\\Omega}(X)$ divides $t_1(X)$. Prover computes the quotient polynomial $q(X)=t_1(X)/(X^{k}-1)\\in \\mathbb{F}_p^{(\\le d)}$ and commits to it. Verifier samples a random $r\\in \\mathbb{F}_p$ and need to learn $t_1(r)$ and $q(r)$. Verifier queries $q(X)$ at $r$. Verifier queries $t(X)$ at $\\omega r$, and $r$. Verifier u $f(X)$ at $\\omega r$. Verifier computes $r^{k}-1$ in time $O(\\log k)$. check2: Verifier checks if $t(\\omega\\cdot r)-t(\\omega)\\cdot f(\\omega\\cdot r)=q(r)\\cdot (r^k-1)$. proof size: one commit, four evaluations. Note that it is a public-coin interactive protocol that can be rendered non-interactive via Fiat-Shamir Transform. To sum up, the proof size is made up of two commits and five evaluations (can be batched into a single group element). Theorem: This protocol is complete and sound assuming $2d/p$ is negligible. ( $t(X)\\cdot f(\\omega\\cdot X)$ has degree at most $2d$ ). It takes verifier $O(\\log k)$ time to compute $r^{k-1}-1$. It takes prover $O(k\\log k)$ time to compute $t(X)$ and $q(X)$ using the naive way that constructs the coefficients from the point-value representation. Likewise, it works to prove the products on rational functions: $$\\prod_{a\\in \\Omega}(f/g)(a)=1$$ We construct a similar degree-$k$ polynomial to prove it. Set $t\\in \\mathbb{F}_p^{(\\le k)}[X]$ to be the degree-$k$ polynomial such that $$\\begin{aligned}t(1)&amp;=f(1)/g(1), \\ t(\\omega^s)&amp;=\\prod_{i=0}^s f(\\omega^i)/g(\\omega^i) \\text{ for }s=1,\\dots, k \\end{aligned}$$ We write the prefix-product in an iterative way: $$t(\\omega\\cdot x)=t(x)\\cdot \\frac{f(\\omega \\cdot x)}{g(\\omega\\cdot x)} \\text{ for all }x\\in \\Omega$$ Then we can prove the following two parts to fulfill the product check. Lemma: If ( i ) $t(\\omega^{k-1})=1$ and ( ii ) $t(\\omega\\cdot x)\\cdot g(\\omega \\cdot x)-t(x)\\cdot f(\\omega\\cdot x)=0$ for all $x\\in \\Omega$ then $\\prod_{a\\in \\Omega}f(a)/g(a)=1$. Note that the proof size is two commits and six evaluations. Theorem: This protocol is complete and sound assuming $2d/p$ is negligible. Compared to the original prod-check, the one extra evaluation comes from the query to $g(X)$ at $\\omega\\cdot r$. Permutation CheckLet $f,g$ be polynomials in $\\mathbb{F}_p^{(\\le d)}[X]$. Verifier has commitments to $f$ and $g$. In permutation check, that goal is that prover wants to prove that $(f(1),f(\\omega),f(\\omega^2),\\dots, f(\\omega^{k-1})\\in \\mathbb{F}_p^k$ is a permutation of $(g(1),g(\\omega),g(\\omega^2),\\dots, g(\\omega^{k-1}))\\in \\mathbb{F}_p^k$. It means to prove that $g(\\Omega)$ is the same as $f(\\Omega)$, just permuted. The main idea is to construct auxiliary polynomials that have the evaluations as its root. Let $\\hat{f}(X)=\\prod_{a\\in \\Omega}(X-f(a))$ and $\\hat{g}(X)=\\prod_{a\\in \\Omega}(X-g(a))$. Then $\\hat{f}(X)=\\hat{g}(X)$ if and only if $g$ is a permutation of $f$ on $\\Omega$. The thing to notice is that prover cannot just commits to $\\hat{f}$ and $\\hat{g}$, then verifier checks if $\\hat{f}(r)=\\hat{g}(r)$. Because there is a missed proof that $\\hat{f}$ is honestly constructed by the committed $f$. Instead, prover is needed to prove $\\hat{f}(r)=\\hat{g}(r)$ by performing a product check on a rational function $\\frac{r-f(X)}{r-g(X)}$. The IOP for permutation check is depicted as follows. Let’s elaborate on the details. Start: Prover has functions $f,g$ in clear and verifier has commitments to $f,g$. Verifier samples a random $r$. Prover constructs $\\hat{f}$ using the evaluations of $f$, so is $\\hat{g}$. Then prover wants to prove $\\hat{f}(r)=\\hat{g}(r)$. It can be transformed to prove $\\frac{\\hat{f}(r)}{\\hat{g}(r)}=1$ where $r$ is fixed. They can perform prod-check to prove $$ \\frac{\\hat{f}(r)}{\\hat{g}(r)}=\\prod_{a\\in \\Omega}\\left(\\frac{r-f(a)}{r-g(a)}\\right)=1 $$ where the rational function is defined as $\\frac{r-f(X)}{r-g(X)}$ on $\\Omega$. Proof size: two commits and six evaluations, same as prod-check on rational functions. It’s a public-coin protocol that can be rendered non-interactive. Theorem: This protocol is complete and sound assuming $2d/p$ is negligible. Prescribed Permutation CheckLet’s look into an embellished permutation check where the permutation is prescribed by a specific permutation $W$. We say $W:\\Omega\\rightarrow \\Omega$ is a permutation of $\\Omega$ if $\\forall i\\in [k]$: $W(\\omega^i)=\\omega^j$ is bijection. A bijection means one-to-one correspondence. A bijection function is injective and surjective. Let $f,g$ be polynomials in $\\mathbb{F}_p^{(\\le d)}[X]$. Verifier has three individual commitments to $f, g,$ and $W$. In prescribed permutation check, the goal of prover is to prove that $f(y)=g(W(y))$ for all $y\\in \\Omega$. In other works, it proves that $g(\\Omega)$ is the same as $f(\\Omega)$, permuted by the prescribed $W$. At first sight, we try to use a zero-test to prove $f(y)-g(W(y))=0$ on $\\Omega$. But the problem is the polynomial $f(y)-g(W(y))$ has degree $k^2$ since $g(W(y))$ is a composition of $f$ and $W$. Then prover would need to manipulate polynomials of degree $k^2$, resulting a quadratic time prover !! Yet we want a linear time prover. Let’s reduce this to a prod-check on a polynomial of degree $2k$. Observation: If $(W(a),f(a))_{a\\in \\Omega}$ is a permutation of $(a, g(a))_{a\\in \\Omega}$, then $f(y)=g(W(y))$ for all $y\\in \\Omega$. By the definition of permutation, for $a\\in \\Omega$, there exists a $a’\\in \\Omega$ $W(a’)=a$ and $f(a’)=g(a)$ hold. Then we have $f(a’)=g(W(a’))$. The following example illustrates the proof. Likewise, we construct auxiliary polynomials that have the evaluations as its root yet the evaluations are listed in form of the tuple. The intuition is to encode the tuple to a variable, then use a similar way to construct a bivariate polynomial that has the variables as its root. Hence, the tuple is encoded as variables $Y\\cdot W(a)+f(a)$ and $Y\\cdot a+g(a)$, respectively. And the bivariate polynomials of total degree $k$ is constructed as follows. $$ \\begin{cases} \\hat{f}(X,Y)&= \\prod _{a\\in \\Omega}(X-Y\\cdot W(a)-f(a)) \\\\ \\hat{g}(X,Y) &= \\prod_{a\\in \\Omega}(X-Y\\cdot a -g(a))\\end{cases} $$ The following lemma shows the correctness. Lemma: $\\hat{f}(X,Y)=\\hat{g}(X,Y)$ if and only if $(W(a),f(a))_{a\\in \\Omega}$ is a permutation of $(a,g(a))_{a\\in \\Omega}$. To prove, use the fact that $\\mathbb{F}_p[X,Y]$ is a unique factorization domain. Yet I’m not familiar with this fact. The complete protocol is depicted as follows, which composes a prod-check on the rational function $\\frac{r-s\\cdot W(X)-f(X)}{r-s\\cdot X-g(X)}$ where $r,s$ are fixed and randomly chosen by the verifier. Theorem: This protocol is complete and sound assuming $2d/p$ is negligible. The PLONK IOP for General CircuitsPLONK Finally, let’s introduce PLONK IOP, a widely used proof system in practice. It is a poly-IOP for a general circuit $C(x,w)$. We can think the PLONK IOP as an abstract IOP that can be combined with different polynomial commitment schemes to construct actual SNARK system. Step1: compile circuit to a computation traceThe first step is to compile a general circuit to a computation trace that can be encoded by a polynomial. Considering a general circuit $(x_1+x_2)(x_2+w_1)$ with two public inputs and one witness input, we can write the computation trace into a table as follows. This compilation is also called arithmetization. The example is illustrated as above. Let $|C|$ denote the total # of gates in $C$. Let $|I|=|I_x|+|I_w|$ denote the # inputs to $C$. Let $d=3|C|+|I|$ and $\\Omega=\\{1,\\omega, \\omega^2, \\dots, \\omega^{d-1}\\}$ where $\\omega\\in \\mathbb{F}_p$ is the primitive $k$-th root of unity so that $\\omega ^d=1$. In the above example, $|C|=3$, $|I|=3$, and $d=12$. The plan is prover can interpolates a polynomial $T\\in \\mathbb{F}_p^{(\\le d)}[X]$ that encodes the entire trace. $T$ encodes all inputs: $T(\\omega^{-j})=\\text{input }\\# j \\text{ for } j=1,\\dots, |I|$. $T$ encodes all wires: $\\forall l=0,\\dots, |C|-1$:For each gate labeled by $l$, $T(\\omega^{3l})$: left input to gate $\\#l$ $T(\\omega^{3l+1})$: right input to gate $\\#l$ $T(\\omega^{3l+2})$: output to gate $\\#l$ In our example, prover interpolates $T(X)$ of degree 11 such that: Note that prover can use FFT / NTT to compute the coefficients of $T$ in time $O(d\\log d)$. Step2: proving validity of $T$Then prover commits to the polynomial $T$ encoded by the computation trace, and needs to prove that $T$ is a correct computation trace. Proving Validity of : $T$ encodes the correct inputs. Every gate is evaluated correctly. The wiring is implemented correctly. The output of last gates is 0.(In our example, the output is 77) Proving (4) is easy that only proves $T(\\omega^{3|C|-1})=0$. The wiring constraints contains that the second input $6$ is connected with the left wire of the gate 0 and the right wire of the gate 1 as depicted as follows. (1) $T$ encodes the correct inputNote that the statement $x$ is public. Both prover and verifier interpolate a polynomial $v(X)\\in \\mathbb{F}_p^{(\\le |I_x|)}[X]$ that encodes the $x$-inputs to the circuit: $$ \\text{for }j=1,\\dots, |I_x|: v(\\omega^{-j})=\\text{input }\\# j $$ In our example, $v(\\omega^{-1})=5,v(\\omega^{-2})=6$, hence $v$ is linear. Note that constructing $v(X)$ takes time proportional to the size of input $x$ so that verifier has time to do this. Let $\\Omega_{\\text{inp}}=\\{\\omega^{-1},\\omega^{-2},\\dots, \\omega^{-|I_x|}\\}\\subseteq \\Omega$ that contains the points encoding the inputs. Prover proves (1) by using ZeroTest on $\\Omega_{\\text{inp}}$ to prove that $$T(y)-v(y)=0 ; \\forall y\\in \\Omega_{\\text{inp}}$$ Note that verifier can construct $v(X)$ explicitly so verifier only query $T(X)$ at randomly chosen $r$. (2) every gate is evaluated correctlySuppose that the circuit only composes the additional gates and multiplication gates. The idea of differentiating is to encode gate types using a selector polynomial $S(X)$. Define $S(X)\\in\\mathbb{F}_p^{(\\le d)}[X]$ such that $\\forall l=0, \\dots, |C|-1$: $$ \\begin{cases}S(\\omega^{3l})&= 1 \\text{ if gate }\\# l \\text{ is an addition gate} \\\\ S(w^{3l})&=0\\text{ if gate }\\# l \\text{ is an multiplication gate}\\end{cases} $$ In our example, the selector polynomial is interpolated as follows. The selector polynomial will be committed in the preprocessing phase because it is a function of the circuit, which just encodes what the gates represent in the circuit. With the selector polynomial, we can encode the addition gates and the multiplication gates into a single polynomial. $$ \\forall y\\in \\Omega_{\\text{gates}}=\\{1,\\omega^3,\\omega^6,\\omega^9,\\dots, \\omega^{3(|C|-1)}\\}: \\\\ S(y)\\cdot [T(y) + T(\\omega y)] + (1-S(y))\\cdot T(y)\\cdot T(\\omega y)=T(\\omega^2 y) $$ If the above equality holds, it means that all the addition gates and multiplication gates are evaluated correctly. $\\#l$ is an **addition gate** → $S(y=\\omega^{3l})=1$ Prove that the sum of the left input and right input equals to the output, i.e. $T(y)+T(\\omega y)=T(\\omega^2 y)$. $\\#l$ is a **multiplication gate** → $S(y=\\omega^{3l})=0$ Prove that the product of the left input and right input equals to the output, i.e. $T(y)\\cdot T(\\omega y)=T(\\omega^2 y)$. Then prover uses ZeroTest to prove that for $\\forall y\\in \\Omega_{\\text{gates}}$: $$S(y)\\cdot [T(y) + T(\\omega y)] + (1-S(y))\\cdot T(y)\\cdot T(\\omega y)-T(\\omega^2 y)=0$$ (3) the wiring is correctThe last thing is to prove the wiring is correct. First we construct the wiring constraints to encode the wires of $C$. In our example, the(incomplete) wiring constraints are listed as follows. The first constraint means that the second input is connected to the right input of the gate 0 and the left input of the gate 1. Then define a polynomial $W:\\Omega\\rightarrow \\Omega$ that implements a rotation: $W(\\omega^{-2},\\omega^{1},\\omega^3)=(\\omega^{1},\\omega^3,\\omega^{-2})$ $W(\\omega^{-1},\\omega^0)=(\\omega^{0},\\omega^{-1})$ … … The rotation means $W$ maps $\\omega^{-2}$ → $\\omega^{1}$, $w^1$ → $\\omega^3$, and $\\omega^3$→ $\\omega^{-2}$. It means the polynomial $T$ is invariant under this rotation. Note that $W$ actually defines a prescribed permutation. Finally, the following lemma tells us we can prove the wiring constraints using a prescribed permutation check. Lemma: $\\forall y\\in \\Omega$: if $T(y)=T(W(y))$, then wire constraints are satisfied. It’s a clever way of encoding all the wiring constraints. Note that the polynomial $W$ doesn’t depend on the inputs so it represents an intrinsic property of the circuit itself, which can be committed in the setup. Complete Plonk Poly-IOPThe complete Plonk poly-IOP (and SNARK) is depicted as follows. Let’s elaborate on the details. The setup preprocesses the circuit $C$ and outputs the commitmens to the selector polynomial $S$ and the wiring polynomial $W$. It is untrusted that anyone can check these commitments were done correctly. Prover compiles the circuit to a computation trace, and encodes the entire trace into a polynomial $T(X)$. Verifier can construct $v(X)$ explicitly from the public inputs $x$. Then prover proves validity of $T$: gates: evaluated correctly by ZeroTest inputs: correct inputs by ZeroTest wires: correct wirings by Prescribed Permutation Check output: correct output by evaluation proof Theorem: The Plonk Poly-IOP is complete and knowledge sound, assuming $7|C|/p$ is negligible. $7|C|$ bounds the degree of the polynomial of $S\\cdot T \\cdot T$. constant proof size: a short proof with $O(1)$ commitments. fast verifier: runs in logarithmic time $O(\\log |C|)$ quasi-linear prover: $O(|C|\\log |C|)$ SNARK: rendered via Fiat-Shamir Transform Note that the SNARK is not necessarily zk since the commitments are not zk and the openings are not as well. But there are generic transformations that can efficiently convert any Poly-IOP into a zk Poly-IOP, rendering a zk-SNARK. ExtensionsHyperplonk: linear proverThe main challenge in PLONK is the prover runs in quasi-linear time. Hyperplonk replaces $\\Omega$ with ${0,1}^t$ where $t=\\log _2 |\\Omega|$ to achieve a linear prover. The polynomial $T$ is now a multilinear polynomial in $t$ variables, and the computation trace is encoded on the vertices of the $t$-dim hypercube. ZeroTest is replaced by a multilinear SumCheck. Recall that the prover time in SumCheck ([Lecture 4]) has a factor $2^t$, which is linear to $|C|$. It turns out that all tools to build for proving facts about committed univariate polynomials can be generalized to work and prove properties of multilinear polynomials. Plonkish ArithmetizationAnother extension is about the arithmetization, including the custom gates and Plookup. Having these extension allows to shrink the size of the computation traces, which speed up the prover runtime. It supports custom gates other than addition gates and multiplication gates. The plonkish computation trace can be illustrated as follows: It is defined by a custom gate that computes $v_4+w_3\\cdot t_3$ and outputs $t_4$. Likewise, we can encode it into the following polynomial $$\\forall y\\in \\Omega_{\\text{gates}}: v(y\\omega)+w(y)\\cdot t(y)-t(y\\omega)=0$$ Prover uses a ZeroTest check to prove that the custom gate is evaluated correctly. All such gate checks are included in the gate check by multiplying a selector polynomial. Furthermore, Plookup can ensure some values in the computation trace are in pre-defined list.","link":"/2023/07/21/zkp-lec5/"},{"title":"「Cryptography-ZKP」: Lec7 Poly-commit based on ECC","text":"In this series, I will learn Zero Knowledge Proofs (ZKP) on this MOOC, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. Any corrections and advice are welcome. ^ - ^ Topics: Poly-commit based on Error-correcting Codes Argument for Vector-Matrix Product Proximity Test Consistency Test Linear-time encodable code based on expanders Lossless Expander Recursive Encoding with constant relative distance An important component in the common paradigm for efficient SNARK is the polynomial commitment scheme. In Lecture 6 we introduced the KZG polynomial commitment based on bilinear pairing and other polynomial commitments based on discrete-log. It is worth noting that prover time is dependent on $O(d)$ exponentiations, which is not strictly linear-time. Today we are going to present a new class of polynomial commitments based on error-correcting codes. Here are the motivations and drawbacks. Motivations: Plausibly post-quantum secure No group exponentiationsProver only uses hashes, additions and multiplications. Small global parameters Drawbacks: Large proof size Not homomorphic and hard to aggregate BackgroundError-correcting CodeLet’s briefly introduce the error-correcting code, which is allowed to correct errors. A $[n,k,\\Delta]$ code is defined below: $Enc(m)$: Encode a message of size $k$ to a codeword of size $n$. Minimum distance (Hamming distance) between any two codewords is $\\Delta$. Note that we omit the alphabet $\\Sigma$ (binary or field) here, which is another important parameter in code. A simple example is the repetition code, which just repeat each symbol three times. Consider the binary alphabet with $k=2$ and $n=6$. The codewords are Enc(00)=000000, Enc(01)=000111, Enc(10)=111000 and Enc(11)=111111 with minimum distance $\\Delta=3$. This repetition code with minimum distance $\\Delta=3$ can correct 1 error duting the transmission. E.g., suppose the transmission can induce at most 1 error, then the message 010111 received from the sender can be decoded to 01. It is worth mentioning that we are going to build poly-commit using error-correcting code without efficient decoding algorithm. The truth is that we don’t use the decoding algorithm at all. We define rate and relative distance over a $[n,k,\\Delta]$ code. The rate $\\frac{k}{n}$ represents the ratio of the minimal message in the codeword of size $n$. We want the rate close to 1. The relative distance $\\frac{\\Delta}{n}$ represents the ratio of the different locations between any two codewords. E.g., repetition code with rate $\\frac 1 a$ repeats each symbol $a$ times with $n=ak$, and has $\\Delta=a$ and relative distance $\\frac 1 k$. We want rate and relative distance as big as possible, but increasing rate could decrease the relative distance. The trade-off between the rate and the distance of a code is well studied in code theory. Linear CodeThe most common type of code is linear code. An important property is any linear combination of codewords is also a codeword. It is equivalent to say that encoding can always be represented as vector-matrix multiplication between $m$ (of size $k$) and the generator matrix (of size $k\\times n$). Moreover, the minimum (Hamming) distance is the same as the codeword with the least number of non-zeros (weight). (The weight of a codeword indicats the number of non-zeros.) The subtraction of any two codewords is also a codeword so the number of the different locations directly implies the weight of another non-zero codeword. Reed-Solomon CodeA classical construction of linear code is Reed-Solomon Code. It encodes messages in $\\mathbb{F}_p^k$ to codewords in $\\mathbb{F}_p^n$. The idea of encoding is veiwing the message of size $k$ as a unique degree $k-1$ univariate polynomial and the codeword is the evaluations at $n$ points. It treats each symbol of the message as an evaluation at a pre-defined point so the polynomial can be uniquely defined by interpolation on the fixed set of public points. Then we can evaluate $n$ pre-defined public points as the codeword. E.g., $(\\omega,\\omega^2,\\dots, \\omega^n)$ for $n$-th root-of-unity $\\omega^n=1 \\mod p$. The minimal distance is $\\Delta=n-k+1$ (indicating the least number of non-zeros) since a degree $k-1$ polynomial has at most $k-1$ roots (indicating the most number of zero evaluations). E.g., when $n=2k$, rate is $\\frac 1 2$ and relative distance is $\\frac 1 2$. It is pretty good in practice and is almost the best we can achieve. RS code is a linear code that the encoding algorithm can be represented as vector-matrix multiplication where the vector is the message and the generator matrix can be derived from Fourier matrix. The encoding time is $O(n\\log n)$ using the fast Fourier transform (FFT). Poly-commit based on error-correcting codesRecall the polynomial commitment scheme we discussed in previous lectures. keygen generates global parameters for $\\mathbb{F}_p^{(\\le d)}$. Prover commits to a univariate polynomial of degree $\\le d$ . Later verifier requests to evaluate at point $u$. Prover opens $v$ with proof that $v=f(u)$ and $f\\in \\mathbb{F}_p^{(\\le d)}$. Reduce PCS to Vec-Max ProductIn the poly-commit based on error-correcting codes, we write the polynomial coefficients in a matrix of size $\\sqrt{d}$ by $\\sqrt{d}$. For simplicity, we assume $d$ is an exact power. Note that the vectorization of the above matrix forms the original vector of polynomial coefficients, that is: $$ [f_{1,1},f_{2,1},\\dots, f_{\\sqrt{d},1},\\dots,f_{1,\\sqrt{d}},\\dots,f_{\\sqrt{d},\\sqrt{d}}]^{T} $$ Hence, the polynomial behind the matrix can be written with two indices: $$ f(u)=\\sum_{i=1}^{\\sqrt{d}}\\sum_{j=1}^{\\sqrt{d}} f_{i,j}u^{i-1+(j-1)\\sqrt{d}} $$ Then the evaluation of $f(u)$ can be viewed as two steps as follows. Two steps of evaluation: (Vecor-Matrix Product) Multiply a vector defined by point $u$ with the matrix of coefficients to get a vector of size $\\sqrt{d}$. (Inner Product) Multiply the vector of size $\\sqrt{d}$ with another vector defined by point $u$ to obtain the final evaluation. With this nice observation, we actually reduce the poly-commit to an argument for vector-matrix product. Roughly speaking, prover can only evaluate the first step and sends a vector of size $\\sqrt{d}$ as proof. Verifier checks whether the Vec-Mat product is correct using proof system and evaluates the second step locally, which is an inner product of the Vec-Mat product and the vector defined by point $u$. As a result, the argument for Vec-Mat product gives us a polynomial commitment with $\\sqrt{d}$ proof size. Argument for Vec-Mat ProductNow our goal is to design a scheme to test the Vec-Mat product without sending the matrix directly. CommitAs usual, we need to commit to the polynomial. Here we instead commit to an encoded matrix defined by the polynomial. We first use a linear code to encode the original matrix defined by the coefficients of polynomial. Concretely speaking, we encode each row with a linear code to compute an encoded matrix of size $\\sqrt{d}\\times n$ where $n$ is the size of the codeword. We will use a linear code with constant rate so that the size of the encoded matrix is asymptotical to $d$. Then we can commit to each column of the encoded matrix using Merkle tree. Recall the Merkle tree commitment introduced in [Lecture 4]. The root hash is served as the commitment to the encoded matrix. Then verifier can request to open each column individually and checks whether the opened column is altered. It is worth noting that the key generation for this Merkle tree commitment is only sampling a hash function, resulting in a constant size global parameters with no trusted setup. Eval and VerifyWe actually perform the evaluation together with verification. It consists of two tests, proximity test and consistency check. We fisrt consider how a malicious prover could cheat in the commitment. A malicious prover can commit to a matrix of inappropriate size but it can be recognized easily by the Merkle tree proof. A malicious prover can commit to an abitrary matrix of specified size in which each row is not a valid codeword. E.g., a valid RS code is a vector of evaluations of a polynomial specified by the message. Hence, verifier uses the proxomity test to test if the committed matrix indeed consists of $\\sqrt{d}$ codewords. Having checked this proximity test, verifier is convinced that the committed matrix is nearly close to the encoded matrix defined by the original matrix of coefficients. Then verifier can move to the consistency check to compute (and verifiy) the actual evaluation. Step1: Proximity Test Ligero [AHIV’2017] and [BCGGHJ’2017] are two independent works to introduce the proximity test. Ligero proposed the so-called interleaved test using Reed-Solomon code with quasi-linear prover time. [BCGGHJ’2017] instead used a linear-time encodable code to build the ideal linear commitment model, which is the first work to build SNARK with strictly linear prover time. Note that the proximity test in these two works are proposed to build general-purpose SNARKs. Here we use it to build poly-commit as a specified protocol. Ligero [AHIV’2017] and [BCGGHJ’2017] are two independent works to introduce the proximity test. Ligero proposed the so-called interleaved test using Reed-Solomon code with quasi-linear prover time. [BCGGHJ’2017] instead used a linear-time encodable code to build the ideal linear commitment model, which is the first work to build SNARK with strictly linear prover time. Note that the proximity test in these two works are proposed to build general-purpose SNARKs. Here we use it to build poly-commit as a specified protocol. We first present the description of the proximity test as below. Verifier samples a random vector $r$ of size $\\sqrt{d}$ and sends it to prover. Prover returns the vector-matrix product of the random vector $r$ and the encoded matrix. Verifier requests to open several random columns and prover reveals them with Merkle tree proof. Verifier performs 3 checks The returned vector is a codeword The opened columns are consistent with the committed Merkle tree. The inner product between $r$ and the opened column is consistent with the corresponding element of the returned vector. The completeness is evident. The vec-mat product computed by the honest prover is indeed the linear combination of rows (codewords) specified by the random vector chosen by the verifier. Recall the propery of the linear codes that a linear combination of codewords is a codeword. So these 3 checks will be passed by verifier. Let’s intuitively give the proof of soundness. Assume for the contradiction that the malicious prover commits to a fake matrix, and computes the vec-mat product by this fake matrix. Soundness (Intuition): If the vector is correctly computed, under our assumption, the product is not a codeword. → check 1 will be failed. If the vector is false meaning that the prover just returns an arbitrary codeword, there are many different locations from the correct answer. By check 2, columns are as committed. Probability of passing check 3 is extreamly small. Let’s discuss the second case where the vector sent by the prover is false and $w=r^TC$ denotes the correct answer. In the formal proof for soundness: [AHIV’2017], it defines a parameter $e&lt;\\frac{\\Delta}{4}$, which is related to the minimal distance $\\Delta$, to measure the distance between the committed matrix and the codeword space. Concretely speaking, $e$ measures the minimal distance between any row (of the committed matrix) and any codeword (in the codeword space). If the committed (fake) matrix is $e$-far from any codeword for $e&lt;\\frac{\\Delta}{4}$, then the probability that the vec-mat product $w=r^T C$ is $e$-close to any codeword is $\\le \\frac{e+1}{\\mathbb{F}}$, which is extreamly small. $$ \\operatorname{Pr}[w=r^TC\\text{ is }e\\text{-close to any codeword}]\\le \\frac{e+1}{\\mathbb{F}} $$ Then we can rule out this case, and the remaining case is that the correct answer $w=r^TC$ is $e$-far from any codeword. Under this condition, we know there are at least $e$ different positions between the codeword sent by prover and the correct answer $w$. Then the probability that check 3 is true for $t$ random columns is bounded by $(1-\\frac e n)^t$ where $\\frac e n$ is constant for the linear code with constant relative distance, e.g. RS code. $$ \\operatorname{Pr}[\\text{check 3 is true for }t \\text{ random columns}] \\le (1-\\frac{e}{n})^t $$ Hence, soundness probability can be reduced to negligible probability. That’s why we want linear codes with constant relative distance. Optimization for Proximity TestThere is one optimization for the proximity test. Instead of sending the codeword, prover can send the message behind the codeword to verifier. Note that the message is computed by the random vector and the original matrix defined by the polynomial coefficients. Then verifier can encode the message to obtain the corresponding codeword that is supposed to be sent by prover. This nice optimization reduces the proof size from $n$ to $k$. Moreover, there is no need for verifier to perform the first check explicitly that the vector is a codeword since the encoding is done by the verifier. We depict the optimized proximity test as below. Verifier samples a random vector $r$ of size $\\sqrt{d}$ and sends it to prover. Prover returns the vector-matrix product of the random vector $r$ and the original matrix of coefficients. Verifier encodes the message to compute the codeword. Verifier requests to open several random columns and prover reveals them with Merkle tree proof. Verifier performs 2 checks The returned vector is a codeword The opened columns are consistent with the committed Merkle tree. The inner product between $r$ and the opened column is consistent with the corresponding element of the returned vector. Step2: Consistency CheckWith the proximity test, the verifier knows the committed matrix is close to an encoded matrix with overwhelming probability. Next we can perform the consistency check to really test the evaluation of vec-mat product between the vector defined by point $u$ and the original matrix of size $\\sqrt{d}\\times \\sqrt{d}$. The consistency check is almost the same as the proximity test with the optimization mentioned above excetp that the vector is defined by point $u$ rather than a random vector $r$. Likewise, the verifier encodes the message to compute the codeword so the first check can be removed. Futhermore, the verifier can use the same opened columns in the proximity test to perform the third check. The cosistency check is depicted as below where the first two checks can be removed. Knowledge Soundness (Intuition): In the consistency test, we actually need to prove the knowledge soundness. By the proximity test, the committed matrix $C$ is close to an encoded matrix that can be uniquely decoded to a matrix $F$ defined by polynomial coefficients. Intuitively speaking, there exists an extractor that extracts $F$ by Merkle tree commitment and decoding $C$, s.t. $\\vec{u}\\times F=m$ with probability $1-\\epsilon$. SummaryTo put everything together, the poly-commit scheme based on linear code is described as below. PCS based on Linear Code: Keygen: sample a hash function Commit: encode the coefficient matrix $F$ of $f$ row-wise with a linear code compute the Merkle tree commitment col-wise Eval and Verify: Proximity test: random linear combination of all rows, check its consistency with $t$ random columns Consistency test: compute $\\vec{u}\\times F=m$, encode $m$ and check its consistency with $t$ random columns Evaluate $f(u)=&lt;m,u’&gt;$ by verifier where $u’$ is another vector defined by point $u$. An important thing to point out is that the proximity test is necessary for evaluation and verification although it is almost the same as the consistency test. Suppose we only perform the consistency test, then the verifier checks consistency of the inner-product of vector $\\vec{u}$ and the random columns. But it dose not work since vector $\\vec{u}$ is defined in a very structured way. Intuitively speaking, it has to use random challenges chosen by the verifier to guarantee the consistency. Properties: Keygen: $O(1)$, transparent setup with constant size $gp$. Commit: Encoding: $O(d\\log d)$ field multiplications using RS code, $O(d)$ using linear-time encodable codes. Merkle tree: $O(d)$ hashes, $O(1)$ commitment size. Eval: $O(d)$ field multiplications Proof size: $O(\\sqrt{d})$ (several vectors of size $\\sqrt{d}$ ) Verifier time: $O(\\sqrt{d})$ Look at the concrete performance in [GLSTW’21] with degree $d=2^{25}$ and linear-time encodable code. Commit: 36s Eval: 3.2s Proof size: 49MB Verifier time: 0.7s It is excellent in practice and significantly faster than PCS based on pairing or discrete-log (such as KZG, Bulletproofs) because it only uses linear operations without any exponentiations. Related WorksLet’s disscuss the following up-to-date works based on error-correcting codes. [Bootle-Chiesa-Groth’20] It proposed the tensor query IOP $&lt;f,(\\vec{u}\\otimes \\vec{u}’)&gt;$, which evaluates inner-product of vector $f$ of size $\\sqrt{d}$ and another vector generated by tensor product between two sub-vectors of size $\\sqrt{d}$. (dimentsion 2) Note that this IOP only works for the product of specific form. Moreover, it generalizes to multiple dimentsions and performs the proximity test and consistency test dimension by dimension with smaller proof size $O(n^\\epsilon)$ for constant $\\epsilon&lt;1$. Brakedown [GLSTW’21] This work proposed the polynomial commitment based on tensor query. As described above, we don’t use decoding algorithm at all in the poly-commit. The prover just sends the message and the verifier encodes the message to get the corresponding codeword. It gives relaxation on the design of the poly-commit which allows to use linear codes without efficient decoding algorithm. Unfortunately, when we prove the knowledge soundness, it has to extract the matrix of polynomial coefficients from the committed encoded matrix in which the efficient decoding is required. If the decoding algorithm is not efficient, the extractor is not polynomial as well. Back to this work, the another contribution is showing an alternative way to prove knowledge soundness without efficient decoding algorithm. As a result, it enables us to build poly-commit using any linear codes without efficient decoding algorithm. [Bootle-Chiesa-Liu’21] It improves proof size to $\\text{poly}\\log (n)$ with a proof composition of tensor IOP and PCP of proximity. [Mie’09] Orion [Xie-Zhang-Song’22] It improves the proof size to $O(\\log^2 n)$ with a proof composition of the code-switching technique [Ron-Zewi-Rothblum’20] Concretely, the proof size is 5.7MB for $d=2^{25}$, which is quite large in practice. Linear-time encodable code based on expandersIt is noteworthy that the following line of works all build SNARKs with linear prover using the linear-time encodable code with constant relative distance. In the last segment, we are going to present the construction of linear-time encodable code based on expanders. Linear-time encodable code is proposed by [Spielman’96] and generalized from binary to field by [Druk-Ishai’14], which relies on the expander graph. Expander GraphLook at the following bipartite graph where each node on the left set has 3 outgoing edges connecting to the nodes on the right edges and every two nodes on the left set connect at least 5 nodes on the right set. Bipartite Graph (from wiki) A bipartite graph is a graph whose vertices can be divided into two disjoint and independent sets U and V, that is, every edge connects a vertex in U to one in V. Vertex sets U and V are usually called the parts of the graph. It is a good expander since every two nodes on the left set have at most 6 outgoing edges. In terms of encoding, consider the left nodes as message where each symbol is put on a node and the right nodes is the codeword. The encoding of the message is to compute for each right-side node the sum of nodes connected from the left set, which can be represented as the vector-matrix multiplication between the message and the adjacent matrix of the graph so it is a linear code. A nature question is why we need such an expander graph to achieve linear codes with a good relative distance. Intuitively speaking, with such a good expander, even a single non-zero node on the left set can be expanded to many non-zero nodes on the right set, that is, it amplies the number of non-zeros (weight) from the message to the codeword, enabling us to achieve good relative distance. Lossless Expander Note that the relative distance in the above simple example is not constant. We are going to describe the lossless expander in a formal way. Let $|L|$ denote the number of left nodes and the number of the right nodes is $\\alpha |L|$ for a constant $\\alpha\\in (0,1)$. The degree of a left node is denoted by $g$. Consider a good (almost perfect) expander that for every subset $S$ of nodes on the left, the number of neighers $|\\Tau(S)|=g|S|$ for $|S|\\le \\frac{\\alpha |L|}{g}$, which is bounded by the number of right nodes. But it is too good to achieve. We need to relax the equality and the boundary. Lossless Expander: For every subset $S$ of nodes on the left, the number of neighbors $|\\Tau(S)|\\ge (1-\\beta)g|S|$ for $|S|\\le \\frac{\\delta |L|}{g}$.( $\\beta\\rightarrow 0$, $\\delta \\rightarrow \\alpha$ ) Likewise, the encoding on the lossless expander is to sum up the connected nodes from the left nodes for each right node to compute the codeword. Recursive EncodingThen we move to the construction of linear-time encodable codes, which uses the recursive encoding with the lossless expander. The encoding algorithm is depicted as below. Let’s elaborate on the detailed procedure of encoding a message $m$ of size $k$ to a codeword of size $4k$ with rate $1/4$. Recursive Encoding: Copy the message as the first part of the final codeword. Apply the lossless expander with $\\alpha=\\frac 1 2$ to compute the codeword $m_1$ of size $k/2$. Assume we already had an encoding algorithm for message $m_1$ of size $k/2$ with rate $1/4$ and good relative distance $\\Delta$, then we apply it to compute the codeword $c_1$ of size $2k$ as the second part of the final codeword. Apply another lossless expander with $\\alpha =\\frac 1 2$ for messages of size $2k$ to compute the codeword $c_2$ of size $k$ as the third part. The final codeword is the concatenation $c=m|| c_1||c_2$ The remaining thing is how we get the encoding algorithm for messages of size $k/2$ with rate $1/4$ and good relative distance. That’s exactly the recursiving encoding that we just use the entire encoding algorithm for the message of size $k/2$. Hence, we repeate the entire encoding algorithm in recursion from $k/2,k/4,\\dots$ until a constant size. Note that the lossless expanders used in each iteration are different since the size of message are different. Finally we can use any code with good distance for a constant-size message. E.g., Reed-Solomon code. Distance of the CodeThe recursive way of encoding enables to achieve a constant relative distance: $$ \\Delta'=\\min \\{\\Delta,\\frac{\\delta}{4g}\\} $$ where $\\Delta$ is the relative distance of the code used in the middle from $k/2$ to $2k$ and $\\frac{\\delta}{4g}$ depends on the expander graph. Proof of relative distance (case by case): [Druk-Ishai’14] If weight of $m$ is larger than $4k\\Delta’$, then the relative distance is larger than $\\frac{4k\\Delta’}{4k}=\\Delta’$.→ Done. It means that for all messages with large weight, we automatically get codewords with large weight. If weight of $m\\le 4k\\Delta'\\le \\frac{\\delta k}{g}$, the condition of the first lossless expander holds. (since $\\Delta'\\le \\frac{\\delta}{4g}$ ) Let $S$ be the set of non-zero nodes in $m$, then we have $|\\Tau(S)|\\ge (1-\\beta)g|S|$. We can set $g\\ge 10$ and $\\beta &lt; 0.1$, then at least $(1-2\\beta)g|S| &gt; 8|S|&gt;0$ vertices in Hamming ball have a unique neighbor in $S$. Hence, $m_1$ (the output of this lossless expander) is non-zero. After applying the encoding for $m_1$ of size $k/2$ with relative distance $\\Delta$, the wight of $c_1$ $\\ge 2k\\Delta\\ge 2k\\Delta’$.(The second inequality holds by the definition of min). If the weight of $c_1$ is larger than $4k\\Delta’$, then the relative distance is larger than $\\Delta’$.→Done Else, weight of $c_1$ is $\\le 4k\\Delta’&lt;\\frac{\\delta2k}{g}$, the condition of the second lossless expander holds. Let $S’$ be the set of non-zero nodes in $c_1$, then we can show the weight of $c_2$ is at least $|\\Tau(S’)|\\ge (1-\\beta)g|S’|&gt;8|S’|&gt;16k\\Delta’ &gt;(4k)\\Delta’$. Sampling of the Lossless ExpanderWith lossless expander, we can build the linear-time encodable codes with constant relative distance. The last piece of the puzzle is how to construct the lossless expander. [Capalbo-Reingold-Vadhan-Widgerson’2002] proposed an explicit construction of lossless expander. Note that being explicit is deterministic. Unfortunately, it has large hidden constant so the concrete efficiency is not good. An alternative way is random sampling since a random graph is supposed to have good expansion. Since the sample space is polynomial, there is a $1/\\text{poly}(n)$ failure probability instead of negligible probability. Improvements of the Code Brakedown [GLSTW’21] Instead of the plain summations when encoding, it uses random summations, which assign a random weight for each edge and performs the weighted summation, to significantly boost the distance. Orion [Xie-Zhang-Song’22] It proposes an expander testing with a negligible failure probability via maximum density of the graph. Let’s sum up the pros and cons of the polynomial commitment (and SNARK) based on linear code. Pros Transparent setup: $O(1)$ Commit and Prover time: $O(d)$ field additions and multiplications Plausibly post-quantum secure Field agnosticIt means that we can use any field. Cons Proof size: $O(\\sqrt{d})$, MBs","link":"/2023/08/02/zkp-lec7/"},{"title":"「Algebraic ECCs」: Lec4 Singleton + Plotkin Bounds and RS Code","text":"In this series, I will be learning Algebraic Error Correcting Codes, lectured by Mary Wootters. The lecture videos are available here. Feedback and sugguestions are always welcome! ^ - ^ Topics Covered: Singleton Bound Plotkin Bound Reed-Solomon Codes Dual View of RS Codes Generalized RS Code In the previous lecture, we explored the GV bound and the Hamming bound in the asymptotic setting where $n,k,d$ gets big. We would like to push the GV bound as much up as possible while at the same time push down the Hamming bound as much as possible. Today, we are going to learn two additional bounds, the Singleton and Plotkin bounds, which narrow down the yellow region a little bit. Additionally, we will learn Reed Solomon codes, which meet the Singleton bound. Singleton Bound Singleton Bound: If $\\mathcal{C}$ is an $(n,k,d)_q$ code, then $$ k\\le n-d+1 $$ Before proving the Singleton bound, let’s examine what it looks like. The Singleton bound states that $k\\le n-d+1$ for an $(n,k,d)_q$ code. In the asymptotic setting, this translates to$$R\\le 1-\\delta$$ as illustrated below: The purple line represents the Singleton bound for binary codes ($q=2$). This implies that all points above the Singleton bound are unachievable, which we already know from the Hamming bound. Hence, for $q=2$, the Singleton bound is strictly weaker than the Hamming bound, despite its simplicity. However, when $q\\to \\infty$, the Singleton bound can be better than the Hamming bound. For $q&gt;2$, the Hamming bound shifts up while the Singleton bound stays the same as illustrated below: This shows that the region to the bottom right (below the Hamming bound but above the Singleton bound) becomes unachievable. This is information we cannot infer from the Hamming bound alone. Now, let’s prove the Singleton bound. Proof of the Singleton Bound: For a codeword $c=(x_1,\\dots, x_n)\\in \\mathcal{C}$, consider throwing out the last $d-1$ coordinates. $$ c=(\\underbrace{x_1, x_2, \\dots, x_{n-d+1}}_{\\text{call this }\\varphi(c)\\in \\Sigma^{n-d+1}}, \\underbrace{x_{n-d+2}, \\dots, x_n}_{\\text{get rid of these}}) $$ Define the first $n-d+1$ coordinates as $\\varphi(c)\\in \\Sigma^{n-d+1}$. Now, we define a new code: $$ \\tilde{\\mathcal{C}}={\\varphi(c):c\\in \\mathcal{C}} $$ which is the set of all $\\varphi(c)$ for every codeword $c\\in \\mathcal{C}$. Thus, $\\tilde{\\mathcal{C}}\\subseteq \\Sigma^{n-d+1}$. We derive two key claims: Claim 1: $|\\mathcal{C}|=|\\tilde{\\mathcal{C}}|$.If not, then there must be a collision: $\\exists c, c’\\in \\mathcal{C}, c\\ne c’$ such that $\\varphi(c)=\\varphi(c’)$. This implies that $c$ and $c’$ differ only in their last $d-1$ coordinates, meaning $\\Delta(c, c’)\\le d-1$, which contradicts the distance $d$ of the original code. Claim 2: $|\\tilde{\\mathcal{C}}|\\le q^{n-d+1}$.This follows because $\\tilde{\\mathcal{C}}\\subseteq \\Sigma^{n-d+1}$. Combining these two claims, we have: $|\\mathcal{C}|=q^k\\le q^{n-d+1}$. Taking log base $q$ gives us $k\\le n-d+1$. $\\blacksquare$ Plotkin BoundRecall that the GV bound only works up to the relative distance $\\delta=d/n\\le 1-1/q$. Hence, as depicted below, there is a gap between $\\delta\\in (1-1/q, 1)$. We are wondering if it is possible to have codes with relative distance greater than $1-1/q$ and rate greater than $0$. Unfortunately, the answer is no. This is what the Plotkin bound tells us: Plotkin Bound: Let $\\mathcal{C}$ be a $(n, k, d)_q$ code. 1. If $d=(1-{1/q})\\cdot n$, then $|\\mathcal{C}|\\le 2\\cdot q\\cdot n$. 2. If $d>(1-1/q)\\cdot n$, then $|\\mathcal{C}|\\le \\frac{d}{d-(1-1/q)\\cdot n}$. This implies that the asymptotic rate $R=\\frac{\\log_q|\\mathcal{C}|}{n}$ approaches to $0$ as $n\\to \\infty$. Thus, in order to have a constant-rate code, we must have $d&lt;(1-1/q)\\cdot n$. The proof of the Plotkin bound is omitted in class. For details, refer to “Essential Coding Theory”, Section 4.4. Instead, we prove a corollary, which extends the Plotkin bound and achieves a trade-off when distance $\\delta &lt; 1 - 1/q$. Corollary of the Plotkin Bound: Let $\\mathcal{C}$ be a family of codes of rate $R$ and relative distance $\\delta< 1-1/q$. Then: $$ R\\le 1-\\left({q\\over q-1}\\right )\\cdot \\delta + o(1) $$ where $o(1)$ can be dropped since we are considering a family of codes with $n,k,d\\to \\infty$. Before proving this corollary, let’s look at what the Plotkin bound looks like when $q=2$. The green line represents the Plotkin bound for binary codes ($q=2$). When $\\delta$ is small, the Plotkin bound is a little worse than the Hamming bound. But when $\\delta$ gets larger, it is better than the Hamming bound. When $q&gt;2$, the Plotkin bound is also a straight line ending at $\\delta=1-1/q$, which is also the endpoint of the GV bound. Proof of Corollary of Plotkin Bound: Assuming the Plotkin Bound holds. Choose $n’=\\lfloor{dq \\over q-1}\\rfloor-1$ such that $d&gt;(1-1/q)\\cdot n’$.Notice that $n’&lt;{dq\\over q-1}$, so $d&gt;(1-1/q)\\cdot n’$. This will be useful when applying the Plotkin bound. For all $x\\in \\Sigma^{n-n’}$, define a new code $\\mathcal{C}_x$: $$ \\mathcal{C}_x=\\{(\\underbrace{c_{n-n'+1}, \\dots, c_n}_{n'}): c\\in \\mathcal{C} \\text{ with }(\\underbrace{c_1, \\dots, c_{n-n'}}_{n-n'})=x\\} $$ which is the set of ENDs (the last $n’$ symbols) of codewords that BEGIN with $x$. Now $\\mathcal{C}_x$ has distance $\\ge d$ with block length $n’&lt;{d\\over 1-1/q}$. Why is this true? For any two different codeword $c, c’\\in \\mathcal{C}$, they must have distance from each other at least $d$. If they have the same first $n-n’$ symbols denoted by $x$, they corresponds to the codewords in $\\mathcal{C}_x$. Thus, their distance must come from the end part, meaning that $\\mathcal{C}_x$ also has the distance $\\ge d$. Applying the Plotkin bound for $\\mathcal{C}_x$ with $d&gt;(1-1/q)\\cdot n’$, we have $$ |\\mathcal{C}_x|\\le {d\\over d-(1-1/q)\\cdot n’}= {qd\\over qd - (q-1)\\cdot n’}\\le qd $$ The second inequality follows by the fact that the denominator $qd-(q-1)\\cdot n’$ is an integer $&gt;0$. Thus, in particular, it is $\\ge 1$. We can plug this bound into the original code $\\mathcal{C}$ since each codeword in $\\mathcal{C}$ shows in only a certain $\\mathcal{C}_x$. $$ \\begin{align} |\\mathcal{C}| =\\sum_{x\\in \\Sigma^{n-n'}}|\\mathcal{C}_x| &\\le q^{n-n'} \\cdot qd \\\\ &=q^{(n-\\lfloor {qd\\over q-1}\\rfloor +1)}\\cdot qd\\\\ &=\\exp_q(n-{qd\\over q-1} + o(n))\\\\ &=\\exp_q(n (1-\\delta\\cdot ({q\\over q-1})+o(1)) \\end{align} $$ where the third equality captures the floor operation and constant by $o(n)$. Taking log base $q$ to the both sides, we have $R\\le 1-\\delta\\cdot ({q\\over q-1}+o(1))$ as desired. $\\blacksquare$ Discussion of Two BoundsBoth the Singleton and Plotkin bounds indicate the impossible results. They demonstrate that what trade-off between the distance and rate is impossible while the GV bound shows the possible trade-off. As depicted below, the Plotkin bounds seems strictly better than the Singleton bound. Why would we bother to discuss the Singleton bound? On one hand, it is true. But one the other hand, we’ll see a family of codes that indeed achieve the Singleton bound. Before we get there, you might think it impossible given what we’ve just stated that the Singleton bound shows the impossible results. We table the discussion in the next section. The trick here is the alphabet size $q$ will be growing with $n$. Reed-Solomon CodesReed-Solomon code is a family of codes that achieve the Singleton bound while admitting efficient error-correcting algorithm. This code is widely used in practice. Before getting into Reed-Solomon codes, let’s first discuss the polynomials over finite fields and the Vandermonde matrix. Polynomial: A (univariate) polynomial in variable $X$ over the finite field $\\mathbb{F}_q$ of degree $d$ is of the form: $$f(X)=a_0+a_1\\cdot X + \\dots a_d\\cdot X^d$$ where each coefficient $a_i\\in \\mathbb{F}_q$ and the top coefficient $a_d\\ne 0$. The set of all univariate polynomials with coefficients in $\\mathbb{F}_q$ is denoted by $\\mathbb{F}_q[X]$. Polynomials over finite fields behave similarly to those over $\\mathbb{R}$. There is a simple but super useful fact about polynomials that low-degree polynomial do not have too many roots. Fact: A non-zero polynomial of $f$ of degree $d$ over $\\mathbb{F}_q$ has at most $d$ roots. Proof Sketch: If $f(\\beta)=0$, then $(x-\\beta)\\mid f$. If $f$ has $d+1$ (distinct) $\\beta_1,\\dots, \\beta_{d+1}$ roots, then $(x-\\beta_1)(x-\\beta_2)\\dots(x-\\beta_{d+1})\\mid f$ . This leads to a contradiction because the grand product on the left has degree $d+1$, while $f(X)$ has degree of $d$. Example: Consider the field $\\mathbb{F}_3=\\{0, 1, 2\\}$: $f(X)=X^2-1$ has two roots $[f(2)=f(1)=0]$ $f(X)=X^2+2X+1$ has one root $[f(2)=0]$ $f(X)=X^2+1$ has zero root Note: The polynomial $X^2+1$ DOES have a root over $\\mathbb{F}_2$, showing that the choice of the field matters. Vandermonde MatrixNext, we will explore some useful facts about Vandermonde matrix. Vandermonde matrix: A Vandermonde matrix has the form $$ V=\\left[\\begin{array}{c} 1 & \\alpha_1 & \\alpha_1^2 & \\dots & \\alpha_1^m \\\\ 1 & \\alpha_2 & \\alpha_2^2 & \\dots & \\alpha_2^m \\\\ 1 & \\alpha_3 \\\\ \\vdots \\\\ 1 & \\alpha_n & \\alpha_n^2 & \\dots & \\alpha_n^m \\end{array}\\right] $$ where $\\alpha_1, \\dots, \\alpha_n\\in \\mathbb{F}_q$ are distinct. Aka, $V_{ij}=\\alpha_i^{j-1}$. Theorem: A square Vandermonde matrix is invertible. Proof 1: Consider a square Vandermonde matrix $V\\in \\mathbb{F}^{n\\times n}$. Suppose $\\vec{a}=(a_0, \\dots, a_n)$ is a vector. Then $V\\cdot \\vec{a}$ can be expressed as $$ V\\cdot \\vec{a}=\\left ( \\begin{array}{c} \\sum_{i=0}^{n-1} a_i\\cdot \\alpha_1^i \\\\ \\sum_{i=0}^{n-1} a_i\\cdot \\alpha_2^i \\\\ \\vdots \\\\ \\sum_{i=0}^{n-1} a_i\\cdot \\alpha_n^i \\end{array} \\right )=\\left ( \\begin{array}{c} f(\\alpha_1)\\\\ f(\\alpha_2)\\\\ \\vdots \\\\ f(\\alpha_n) \\end{array} \\right ) $$ where $f(X)=a_0+a_1X+\\dots a_{n-1}X^{n-1}$. To prove that the Vandermonde matrix is invertible, we’d like to show:if $V\\cdot \\vec{a}=0$, then $\\vec{a}$ itself must be $0$. This is true because Case 1: If $f$ is a zero polynomial (i.e., $\\vec{a}$ itself is the zero vector), it is clear that $V\\cdot \\vec{a}=0$. Case 2: If $f$ is a non-zero polynomial (i.e., $\\vec{a}$ is a non-zero vector), then $f(X)$ has degree at most $n-1$ and cannot have $n$ roots. So, we have $V\\cdot \\vec{a}\\ne 0$. Hence, $\\text{Ker}(V)=\\emptyset$, so $V$ is invertible. $\\blacksquare$ Proof 2: It can be proven by its determinate. The determinate of a Vandermonde matrix is $$\\det(V)=\\prod_{1\\le i&lt;j\\le n}(a_i-a_j)\\ne 0$$ since $a_i\\ne a_j$ for any $i\\ne j$. $\\blacksquare$ Corollary: Any square contiguous submatrix of a Vandermonde matrix is invertible. Caveat: If one of the evaluation points is 0, then the submatrix must include part of the all-ones column for it to be invertible. Otherwise, it is not inveritble. Proof: A $(r+1)\\times (r+1)$ square submatrix takes the following form: $$ \\left[ \\begin{array}{c} \\alpha_i^j & \\alpha_i^{j+1} & \\dots &\\alpha_i^{j+r-1} \\\\ \\alpha_{i+1}^j & \\alpha_{i+1}^{j+1} & \\dots &\\alpha_{i+1}^{j+r-1} \\\\ \\vdots \\\\ \\alpha_{i+r}^j & \\alpha_{i+r}^{j+1} & \\dots & \\alpha_{i+r}^{j+r} \\end{array} \\right]=D\\cdot V $$ where: $D$ is a diagonal matrix with non-zero diagonal entries $\\alpha_i^j, \\dots, a_{i+r}^j$, $V$ is a $(r+1)\\times (r+1)$ Vandermonde matrix. The invertibility of $D\\cdot V$ depends on the invertibility of both $D$ and $V$: $D$ is invertible if all diagonal entries are non-zero. This is guaranteed if either $j=0$ or $a_i\\ne 0$ for all $i$, as required in the caveat. $V$ is invertible by the aforementioned theorem. $\\blacksquare$ These facts about Vandermonde matrices will be useful. First, they imply that “polynomial interpolation works over $\\mathbb{F}_q$”. Theorem: Given $(\\alpha_i, y_i)\\in \\mathbb{F}_q\\times \\mathbb{F}_q$ for $i=1,\\dots, d+1$, there is a unique degree-$d$ polynomial $f$ so that $f(\\alpha_i)=y_i$ for $\\forall i$. Proof: If $f(X)=a_0+a_1X+\\dots+a_dX^d$, then the requirement that $f(\\alpha_i)=y_i$ for all $i$ can be written as $V\\cdot \\vec{a}=\\vec{y}$, where $V$ is a square Vandermonde matrix. Hence, $\\vec{a}=V^{-1}y$ is the unique solution because linear algebra “works” over $\\mathbb{F}_q$. $\\blacksquare$ Moreover, the proof implies that we can find $f$ efficiently. Actually, we can compute $f$ very efficiently by using NTT (Number Theoretic Transform), which allows for fast multiplication by certain special Vandermonde matrix . Fact: All functions $f:\\mathbb{F}_q\\mapsto \\mathbb{F}_q$ are polynomials of degree $\\le q-1$. Proof: There are only $q$ points in $\\mathbb{F}_q$, so we can interpolate a (unique) degree $\\le q-1$ polynomial through any function. Example: $f(X)=X^q$ must have some representation as a degree $\\le q-1$ polynomial over $\\mathbb{F}_q$. It is $X^q\\equiv X$ because $\\alpha^q=\\alpha$ for all $\\alpha\\in \\mathbb{F}_q$ (by Fermat’s little theorem). RS codeNow, we are finally ready to define the Reed-Solomon codes. Reed-Solomon Codes: Let $q\\ge n \\ge k$. The Reed-Solomon code of dimension $k$ over $\\mathbb{F}_q$, with (distinct) evaluation points $\\vec{\\alpha}=(\\alpha_1, \\dots, \\alpha_n)$ is $$ \\text{RS}_q(\\vec{\\alpha}, n, k)=\\{(f(\\alpha_1), f(\\alpha_2), \\dots, f(\\alpha_n)):f\\in \\mathbb{F}_q[X], \\deg(f)\\le k-1\\} $$ The basic idea is that low-degree polynomial does not have too many roots so that the RS code can achieve a fairly good trade-off between the rate and the distance. The codeword of Reed-Solomon code consists of evaluations of low-degree polynomials. This implies that these codewords don’t have too many zeros so that the distance of the code is good. Additionally, this definition implies a natural encoding map for RS code: $$\\vec{x}=(x_0, \\dots, x_{k-1})\\mapsto (f_{\\vec{x}}(\\alpha_0),\\dots,f_{\\vec{x}}(\\alpha_{n}))$$ where $f_{\\vec{x}}(X)=x_0+x_1\\cdot X+\\dots x_{k-1}\\cdot X^{k-1}$. Note that this isn't the only encoding map, but it’s the commonest one. This also implies the Reed-Solomon code is a linear code with Vandermonde matrix as the generator matrix. Property: $\\text{RS}_q(\\vec{\\alpha}, n, k)$ is a linear code, and the generator matrix is the $n\\times k$ Vandermonde matrix with rows corresponding to $\\alpha_1, \\alpha_2, \\dots, \\alpha_n$. Proof: The proof is clear when we write down the generator matrix. In the view of generator matrix, we have $\\dim(\\text{RS}(n, k))=k$ since the Vandermonde matrix has rank $k$. When $\\vec{\\alpha }$ is clear from context, $\\vec{\\alpha}$ can be omitted. Then we can easily compute distance of a linear code. Property: The distance of $\\text{RS}_q(n,k)$ is $d=n-k+1$. Proof: Since RS code is a linear code, $\\text{dist}(\\text{RS}_q(n, k)=\\min_{c\\in \\text{RS}}\\text{wt}(c)$. It suffices to show that $\\max$ #non-zeros of any non-zero codewords is $k-1$. The #zeros of a non-zero codeword corresponds to the #roots of a non-zero polynomial of degree at most $k-1$. $\\blacksquare$ The distance of the RS codes achieves the Singleton bound, and it is optimal for any $n$ and $k$ we choose. Corollary: RS codes exactly meet the Singleton bound. Maximum Distance Separable (MDS): A linear $(n, k, d)_q$ code with $d=n-k+1$ (aka, meeting the Singleton bound) is called Maximum Distance Separable. Notice that MDS-ness is equivalent to the property: “every $k\\times k$ submatrix of the generator matrix is full rank”. This property implies that if $\\mathcal{C}$ is MDS, then any $k$ positions of the codeword $c\\in \\mathcal{C}$ determine all of $c$. Proof Sketch: To prove the distance is $n-k+1$, it suffices to show the code can correct any $n-k$ erasures. For a codeword, any $n-k$ erasures leave us $k$ remaining non-erased positions, which corresponds to $k$ rows of the generator matrix, forming a $k\\times k$ submatrix. We can recover the message $x$ if and only if the submatrix is invertible/full rank. $\\blacksquare$ We have seen that the RS code is MSD and it has the Vandermonde matrix as the generator matrix, so the property also holds for RS code. Discussion of Two Bound (cont.)We previously showed that the distance-rate trade-off in the Plotkin Bound and the Singleton Bound. Both bounds indicate the impossible results, and the Plotkin bound is strictly better than the Singleton bound for any $q\\ge 2$. This suggests that the Singleton bound should never be achievable. However, the MDS codes are defined as codes that achieves the Singleton bound. Why don’t MDS codes violate the Singleton bound? The key lies in the fact that the above figure only applies to any fixed $q$. However, in RS codes, the alphabet size $q$ is NOT fixed—it’s growing with $n$. Thus, to get a MDS code, $q$ must be growing with $n$. How big does $q$ have to be? It is an open question in general! It was settled for prime field in 2012 by Ball. Dual View of RS CodesWhat is the parity-check matrix of an RS code? Before getting this, we need to recall some preliminary algebra. Multiplicative Group $\\mathbb{F}_q^*$: The set $\\mathbb{F}_q^*$ is the multiplicative group of non-zero elements in $\\mathbb{F}_q$, i.e. $\\mathbb{F}_q^*=\\mathbb{F}_q\\backslash\\{0\\}$. $\\mathbb{F}_q^*$ is CYCLIC, which means there’s some $\\gamma\\in \\mathbb{F}_q^*$ so that $$ \\mathbb{F}_q^*=\\{\\gamma, \\gamma^2, \\dots, \\gamma^{q-1}\\} $$ where $\\gamma$ is called a Primitive Element of $\\mathbb{F}_q$. Note that the multiplicative group $\\mathbb{F}_q^*$ is equipped with multiplication while the field $\\mathbb{F}_q$ is equipped with both multiplication and addition. For example, $\\mathbb{F}_5^*=\\{1, 2,3,4\\}$ and $2+3=0$ is not in the set. Example: 2 is a primitive element of $\\mathbb{F}_5$, and $\\mathbb{F}_5^*=\\{2, 2^2=4, 2^3=3, 2^4=1\\}$. 4 is NOT a primitive element, since $4^2=1,4^3=-1, 4^4=1, 4^5=-1, \\dots$, and we will never generate 2 or 3 as a power of 4. Lemma: For any integer $d$ with $0&lt;d&lt;q-1$, the sum of all $d$-th powers in $\\mathbb{F}_q^*$ is zero, i.e. $$ \\sum_{\\alpha\\in \\mathbb{F}_q}\\alpha^d =0$$ Proof: $\\sum_{\\alpha\\in \\mathbb{F}_q}\\alpha^d=\\sum_{\\alpha\\in \\mathbb{F}_q^*}\\alpha^d$ We are leaving out $\\alpha=0$ since $0^d$ contributes 0 to the sum. $=\\sum_{j=0}^{q-2}(\\gamma^j)^d$ for a primitive element $\\gamma$. The index start from 0 since $\\gamma^0=\\gamma^{q-1}=1$.) $=\\sum_{j=0}^{q-2}(\\gamma^d)^j$ for a primitive element $\\gamma$. We are switching the order of the exponents. $={1-(\\gamma^d)^{q-1}\\over 1-\\gamma^d}$ Fact: For any $x\\ne 1$, it follows that $(1-x)\\cdot (\\sum_{j=0}^{n-1}x^j)=1-x^n$, and so $\\sum_{j=0}^{n-1}x^j={1-x^n\\over 1-x}$ for any $n$. $={1-1\\over 1-\\gamma^d}=0$ $(\\gamma^d)^{q-1}\\cdot \\gamma^d = (\\gamma^d)^q=\\gamma^d$. (Fermat’s little theorem) $\\gamma^d\\ne0$ since $0&lt;d&lt;q-1$. So $(\\gamma^d)^{q-1}=1$. Now, we can view the RS code in a new perspective, which allows us to capture what the parity-check matrix of the RS code looks like. Proposition: Let $n=q-1$, and let $\\gamma$ be a primitive element of $\\mathbb{F}_q$. $$\\text{RS}_q((\\gamma^0, \\dots, \\gamma^{n-1}),n, k)\\\\ =\\{(c_0, c_1, \\dots, c_{n-1})\\in \\mathbb{F}_q^n:c(\\gamma^j)=0 \\text{ for }j=1, 2, \\dots, n-k\\}$$ where $c(X)=\\sum_{i=0}^{n-1}c_i\\cdot X^i$. This proposition kinds of flipping things around. In the original definition, the messages are coefficients of some polynomial and the codewords are evaluations of that polynomial. In this view, the codewords are coefficients of some polynomial. Proof: First, let’s prove one direction of this proposition. Let $f(X)=\\sum_{i=0}^{k-1}f_i\\cdot X^i$ be a message, so the RS codeword is $(f(\\gamma^0), f(\\gamma),\\dots, f(\\gamma^{n-1}))$. $c(\\gamma^j)=\\sum_{\\ell=0}^{n-1}c_{\\ell}\\cdot \\gamma^{j\\cdot \\ell}$ $=\\sum_{\\ell=0}^{n-1}(\\sum_{i=0}^{k-1}f_i\\cdot \\gamma^{i\\cdot \\ell})\\cdot \\gamma^{j\\cdot \\ell}$ $=\\sum_{\\ell=0}^{n-1}\\sum_{i=0}^{k-1}f_i\\cdot \\gamma^{(i+j)\\cdot \\ell}$ $=\\sum_{i=0}^{k-1}\\sum_{\\ell=0}^{n-1}f_i\\cdot \\gamma^{(i+j)\\cdot \\ell}$ (switching the order of summation) $=\\sum_{i=0}^{k-1}f_i\\cdot \\sum_{\\ell=0}^{n-1}\\gamma^{(i+j)\\cdot \\ell}$ $=\\sum_{i=0}^{k-1}f_i\\cdot \\sum_{\\ell=0}^{n-1}(\\gamma^{\\ell})^{i+j}$ $0\\le i\\le k-1$ and $1\\le j\\le n-k$ Thus, $1\\le i+j\\le n-1&lt;q-1$. (strictly less than $q-1$) Apply the aforementioned lemma to have $\\sum_{\\ell=0}^{n-1}(\\gamma^{\\ell})^{i+j}=0$ $=0$. $\\text{RS}_q((\\gamma^0, \\dots, \\gamma^{n-1}),n, k)\\\\ \\subseteq\\{(c_0, c_1, \\dots, c_{n-1})\\in \\mathbb{F}_q^n:c(\\gamma^j)=0 \\text{ for }j=1, 2, \\dots, n-k\\}$ This shows one direction of the proposition—RS code is contained in the above set. To prove the equality, we need to count dimension. We will show the set also has dimension $k$. Let $H$ be a matrix of this form: $$ H=\\left[ \\begin{array}{c} 1 & \\gamma & \\gamma^2 & \\dots & \\gamma^{n-1} \\\\ 1 & \\gamma^2 & \\gamma^4 & \\dots & \\gamma^{2(n-1)} \\\\ \\vdots \\\\ 1 & \\gamma^{n-k} & \\gamma^{2(n-k)} & \\dots & \\gamma^{(n-k)(n-1)} \\\\ \\end{array} \\right]\\in \\mathbb{F}_q^{(n-k)\\times n} $$ Consider $H\\cdot c$ where $c$ is a vector in that set: The $j$’th entry is $\\sum_{i=0}^{n-1}c_i\\cdot \\gamma^{ji}=c(\\gamma^j)=0$. This implies that the set $\\{(c_0, c_1, \\dots, c_{n-1})\\in \\mathbb{F}_q^n:c(\\gamma^j)=0 \\text{ for }j=1, 2, \\dots, n-k\\}$ is exactly $\\text{Ker}(H)$. This shows that the set has dimension $k$ since $H$ is a Vandermonde matrix of dimension $n-k$. The proposition also answer our question about the parity-check matrix of the RS-codes. Corollary: The parity-check matrix of $\\text{RS}_q((\\gamma^0, \\dots, \\gamma^{n-1}),n,k)$ is $$ H=\\left[ \\begin{array}{c} 1 & \\gamma & \\gamma^2 & \\dots & \\gamma^{n-1} \\\\ 1 & \\gamma^2 & \\gamma^4 & \\dots & \\gamma^{2(n-1)} \\\\ \\vdots \\\\ 1 & \\gamma^{n-k} & \\gamma^{2(n-k)} & \\dots & \\gamma^{(n-k)(n-1)} \\\\ \\end{array} \\right]\\in \\mathbb{F}_q^{(n-k)\\times n} $$ Notice that the dual code $\\text{RS}(n, k)^\\perp$ has a generator matrix $H^T$, which again looks a lot like a Vandermonde matrix. It suggests that the dual of the RS code is basically an RS code. This particular derivation of the proposition used the choice of evaluation points heavily. However, a statement like this is true in general. More precisely, we define a generalized RS codes with an additional parameter vector $\\vec\\lambda$. Generalized Reed-Solomon Code (GRS): A generalized RS code $\\text{GRS}_q(\\vec{\\alpha},n,k;\\vec{\\lambda})$ is $\\text{GRS}_q(\\vec{\\alpha},n,k;\\vec{\\lambda})\\\\=\\{\\lambda_0f(\\alpha_0), \\lambda_1f(\\alpha_1),\\dots, \\lambda_nf(\\alpha_n):f\\in \\mathbb{F}_q[X],\\deg(f)\\le k\\}$ where $\\vec{\\lambda}=(\\lambda_0, \\dots, \\lambda_n)\\in (\\mathbb{F}_q^*)^n$. The GRS code is almost identical to the RS code, except for the scaling factors $\\lambda_i$ applied to each coordinate. The introduction of $\\vec{\\lambda}$ enables the following property we’ve seen before: if we take the transpose of the parity-check matrix of a GRS code, it yields a generator matrix for another GRS code. Theorem: Theorem: For any distinct evaluation points $\\vec{\\alpha}$ and any $\\vec{\\lambda}\\in (\\mathbb{F}_q^*)^n$, there exists some $\\vec{\\sigma}\\in (\\mathbb{F}_q^*)^n$ s.t. $$ \\text{GRS}_q(\\vec{\\alpha},n,k;\\vec{\\lambda})^\\perp=\\text{GRS}_q(\\vec{\\alpha},n,n-k;\\vec{\\sigma}) $$","link":"/2025/01/13/stanford-cs250-ecc-lec4/"}],"tags":[{"name":"MPC","slug":"MPC","link":"/tags/MPC/"},{"name":"OT","slug":"OT","link":"/tags/OT/"},{"name":"GMW","slug":"GMW","link":"/tags/GMW/"},{"name":"ABY2.0","slug":"ABY2-0","link":"/tags/ABY2-0/"},{"name":"BeaverTriples","slug":"BeaverTriples","link":"/tags/BeaverTriples/"},{"name":"Machine-Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Gradient-Descent","slug":"Gradient-Descent","link":"/tags/Gradient-Descent/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Backpropagation","slug":"Backpropagation","link":"/tags/Backpropagation/"},{"name":"公开课","slug":"公开课","link":"/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"Logistic Regression","slug":"Logistic-Regression","link":"/tags/Logistic-Regression/"},{"name":"Softmax","slug":"Softmax","link":"/tags/Softmax/"},{"name":"open-classes","slug":"open-classes","link":"/tags/open-classes/"},{"name":"Tools","slug":"Tools","link":"/tags/Tools/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Gradient","slug":"Gradient","link":"/tags/Gradient/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"Algorithms","slug":"Algorithms","link":"/tags/Algorithms/"},{"name":"String","slug":"String","link":"/tags/String/"},{"name":"Data-Structure","slug":"Data-Structure","link":"/tags/Data-Structure/"},{"name":"Array","slug":"Array","link":"/tags/Array/"},{"name":"Math","slug":"Math","link":"/tags/Math/"},{"name":"Lectures","slug":"Lectures","link":"/tags/Lectures/"},{"name":"Secure Computation","slug":"Secure-Computation","link":"/tags/Secure-Computation/"},{"name":"Garbled Circuits","slug":"Garbled-Circuits","link":"/tags/Garbled-Circuits/"},{"name":"Mersenne Prime","slug":"Mersenne-Prime","link":"/tags/Mersenne-Prime/"},{"name":"Prime","slug":"Prime","link":"/tags/Prime/"},{"name":"Oblivious Transfer","slug":"Oblivious-Transfer","link":"/tags/Oblivious-Transfer/"},{"name":"IKNP","slug":"IKNP","link":"/tags/IKNP/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Paper","slug":"Paper","link":"/tags/Paper/"},{"name":"fuzz","slug":"fuzz","link":"/tags/fuzz/"},{"name":"AFL","slug":"AFL","link":"/tags/AFL/"},{"name":"AFL++","slug":"AFL","link":"/tags/AFL/"},{"name":"MOpt","slug":"MOpt","link":"/tags/MOpt/"},{"name":"RedQueen","slug":"RedQueen","link":"/tags/RedQueen/"},{"name":"AFLFast","slug":"AFLFast","link":"/tags/AFLFast/"},{"name":"AFLSmart","slug":"AFLSmart","link":"/tags/AFLSmart/"},{"name":"LAF-Intel","slug":"LAF-Intel","link":"/tags/LAF-Intel/"},{"name":"Information-Theory","slug":"Information-Theory","link":"/tags/Information-Theory/"},{"name":"error","slug":"error","link":"/tags/error/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"HTML","slug":"HTML","link":"/tags/HTML/"},{"name":"CSS","slug":"CSS","link":"/tags/CSS/"},{"name":"Digital Signatures","slug":"Digital-Signatures","link":"/tags/Digital-Signatures/"},{"name":"EUF-CMA Security","slug":"EUF-CMA-Security","link":"/tags/EUF-CMA-Security/"},{"name":"Lamport Signature","slug":"Lamport-Signature","link":"/tags/Lamport-Signature/"},{"name":"One-time Signature","slug":"One-time-Signature","link":"/tags/One-time-Signature/"},{"name":"Many-time Signature","slug":"Many-time-Signature","link":"/tags/Many-time-Signature/"},{"name":"Random Oracles","slug":"Random-Oracles","link":"/tags/Random-Oracles/"},{"name":"Hashed RSA","slug":"Hashed-RSA","link":"/tags/Hashed-RSA/"},{"name":"ZK Proof","slug":"ZK-Proof","link":"/tags/ZK-Proof/"},{"name":"Commitment","slug":"Commitment","link":"/tags/Commitment/"},{"name":"ZK","slug":"ZK","link":"/tags/ZK/"},{"name":"PoK","slug":"PoK","link":"/tags/PoK/"},{"name":"NIZK","slug":"NIZK","link":"/tags/NIZK/"},{"name":"3COL","slug":"3COL","link":"/tags/3COL/"},{"name":"Random Oracle Model","slug":"Random-Oracle-Model","link":"/tags/Random-Oracle-Model/"},{"name":"CRS Model","slug":"CRS-Model","link":"/tags/CRS-Model/"},{"name":"QNR","slug":"QNR","link":"/tags/QNR/"},{"name":"3SAT","slug":"3SAT","link":"/tags/3SAT/"},{"name":"Cryptography","slug":"Cryptography","link":"/tags/Cryptography/"},{"name":"MIT6875","slug":"MIT6875","link":"/tags/MIT6875/"},{"name":"Perfect Secrecy","slug":"Perfect-Secrecy","link":"/tags/Perfect-Secrecy/"},{"name":"Perfect Indistinguishability","slug":"Perfect-Indistinguishability","link":"/tags/Perfect-Indistinguishability/"},{"name":"One-time Pad","slug":"One-time-Pad","link":"/tags/One-time-Pad/"},{"name":"Shannon&#39;s lower bound","slug":"Shannon-s-lower-bound","link":"/tags/Shannon-s-lower-bound/"},{"name":"Application of NIZK","slug":"Application-of-NIZK","link":"/tags/Application-of-NIZK/"},{"name":"IND-CCA Security","slug":"IND-CCA-Security","link":"/tags/IND-CCA-Security/"},{"name":"CCA-Secure Encryption","slug":"CCA-Secure-Encryption","link":"/tags/CCA-Secure-Encryption/"},{"name":"Computational Indistinguishability","slug":"Computational-Indistinguishability","link":"/tags/Computational-Indistinguishability/"},{"name":"PRG","slug":"PRG","link":"/tags/PRG/"},{"name":"PRF","slug":"PRF","link":"/tags/PRF/"},{"name":"Hybrid Argument","slug":"Hybrid-Argument","link":"/tags/Hybrid-Argument/"},{"name":"GGM PRF","slug":"GGM-PRF","link":"/tags/GGM-PRF/"},{"name":"Number Theory","slug":"Number-Theory","link":"/tags/Number-Theory/"},{"name":"Multiplicative Group","slug":"Multiplicative-Group","link":"/tags/Multiplicative-Group/"},{"name":"Generators","slug":"Generators","link":"/tags/Generators/"},{"name":"Diffie-Hellman Assumptions","slug":"Diffie-Hellman-Assumptions","link":"/tags/Diffie-Hellman-Assumptions/"},{"name":"Public-key Encryption","slug":"Public-key-Encryption","link":"/tags/Public-key-Encryption/"},{"name":"IND-Secure","slug":"IND-Secure","link":"/tags/IND-Secure/"},{"name":"IND-CPA","slug":"IND-CPA","link":"/tags/IND-CPA/"},{"name":"Trapdoor Permutations","slug":"Trapdoor-Permutations","link":"/tags/Trapdoor-Permutations/"},{"name":"Quadratic Residue","slug":"Quadratic-Residue","link":"/tags/Quadratic-Residue/"},{"name":"QRA","slug":"QRA","link":"/tags/QRA/"},{"name":"GM Encryption","slug":"GM-Encryption","link":"/tags/GM-Encryption/"},{"name":"DH","slug":"DH","link":"/tags/DH/"},{"name":"El Gamla","slug":"El-Gamla","link":"/tags/El-Gamla/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"DEEPLIZARD","slug":"DEEPLIZARD","link":"/tags/DEEPLIZARD/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"OWF","slug":"OWF","link":"/tags/OWF/"},{"name":"OWP","slug":"OWP","link":"/tags/OWP/"},{"name":"HCB","slug":"HCB","link":"/tags/HCB/"},{"name":"GL Theorem","slug":"GL-Theorem","link":"/tags/GL-Theorem/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"LSTM","slug":"LSTM","link":"/tags/LSTM/"},{"name":"Semi-supervised","slug":"Semi-supervised","link":"/tags/Semi-supervised/"},{"name":"blockchain","slug":"blockchain","link":"/tags/blockchain/"},{"name":"solidity","slug":"solidity","link":"/tags/solidity/"},{"name":"Intro-to-Algorithms","slug":"Intro-to-Algorithms","link":"/tags/Intro-to-Algorithms/"},{"name":"Sort","slug":"Sort","link":"/tags/Sort/"},{"name":"BlockCipher","slug":"BlockCipher","link":"/tags/BlockCipher/"},{"name":"StreamCipher","slug":"StreamCipher","link":"/tags/StreamCipher/"},{"name":"ECC","slug":"ECC","link":"/tags/ECC/"},{"name":"Hamming Bound","slug":"Hamming-Bound","link":"/tags/Hamming-Bound/"},{"name":"Linear Code","slug":"Linear-Code","link":"/tags/Linear-Code/"},{"name":"GV Bound","slug":"GV-Bound","link":"/tags/GV-Bound/"},{"name":"q-ary Entropy","slug":"q-ary-Entropy","link":"/tags/q-ary-Entropy/"},{"name":"Integrity","slug":"Integrity","link":"/tags/Integrity/"},{"name":"MAC","slug":"MAC","link":"/tags/MAC/"},{"name":"collision-resistance","slug":"collision-resistance","link":"/tags/collision-resistance/"},{"name":"HMAC","slug":"HMAC","link":"/tags/HMAC/"},{"name":"birthday-paradox","slug":"birthday-paradox","link":"/tags/birthday-paradox/"},{"name":"MD-paradigm","slug":"MD-paradigm","link":"/tags/MD-paradigm/"},{"name":"DM-compression-function","slug":"DM-compression-function","link":"/tags/DM-compression-function/"},{"name":"DNN","slug":"DNN","link":"/tags/DNN/"},{"name":"Unsupervised","slug":"Unsupervised","link":"/tags/Unsupervised/"},{"name":"PCA","slug":"PCA","link":"/tags/PCA/"},{"name":"VSCode","slug":"VSCode","link":"/tags/VSCode/"},{"name":"Unsupervised-learning","slug":"Unsupervised-learning","link":"/tags/Unsupervised-learning/"},{"name":"Word Embedding","slug":"Word-Embedding","link":"/tags/Word-Embedding/"},{"name":"ZKP","slug":"ZKP","link":"/tags/ZKP/"},{"name":"IP","slug":"IP","link":"/tags/IP/"},{"name":"SNARKs","slug":"SNARKs","link":"/tags/SNARKs/"},{"name":"Sum-check","slug":"Sum-check","link":"/tags/Sum-check/"},{"name":"Poly-commit","slug":"Poly-commit","link":"/tags/Poly-commit/"},{"name":"KZG","slug":"KZG","link":"/tags/KZG/"},{"name":"Bulletproofs","slug":"Bulletproofs","link":"/tags/Bulletproofs/"},{"name":"Plonk","slug":"Plonk","link":"/tags/Plonk/"},{"name":"Singleton Bound","slug":"Singleton-Bound","link":"/tags/Singleton-Bound/"},{"name":"Plotkin Bound","slug":"Plotkin-Bound","link":"/tags/Plotkin-Bound/"},{"name":"RS Code","slug":"RS-Code","link":"/tags/RS-Code/"},{"name":"GRS Code","slug":"GRS-Code","link":"/tags/GRS-Code/"}],"categories":[{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"机器学习-李宏毅","slug":"机器学习-李宏毅","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"Tools","slug":"Tools","link":"/categories/Tools/"},{"name":"LeetCode","slug":"LeetCode","link":"/categories/LeetCode/"},{"name":"MPC","slug":"MPC","link":"/categories/MPC/"},{"name":"Math","slug":"Math","link":"/categories/Math/"},{"name":"Information-Theory","slug":"Information-Theory","link":"/categories/Information-Theory/"},{"name":"Cryptography-MIT6875","slug":"Cryptography-MIT6875","link":"/categories/Cryptography-MIT6875/"},{"name":"PyTorch","slug":"PyTorch","link":"/categories/PyTorch/"},{"name":"区块链","slug":"区块链","link":"/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"算法导论","slug":"算法导论","link":"/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/"},{"name":"Cryptography-Boneh","slug":"Cryptography-Boneh","link":"/categories/Cryptography-Boneh/"},{"name":"Cryptography-ECCs","slug":"Cryptography-ECCs","link":"/categories/Cryptography-ECCs/"},{"name":"Cryptography-ZKP","slug":"Cryptography-ZKP","link":"/categories/Cryptography-ZKP/"}]}