{"pages":[{"title":"TRAPDOOR!","text":"Congratulations!! You find something wired. There is a hidden homepage on my website. The URL structure is https://f7ed/[key]/ and the SHA-256 of this key is: eb9e5c2a708dd6c4b57088c96553de8303cf431b5e42a0630625fa08d8dd6d6b Hint: You can use social engineering methods to reveal the key (Â´â–½ï½€)","link":"/about/index.html"},{"title":"","text":"æ¢…æ£®ç´ æ•° /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; max-height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 65%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } æ¢…æ£®ç´ æ•°æ€§è´¨åº”ç”¨Modular ReductionMultiplicationReferenceæ€§è´¨æœ¬èŠ‚ç ”ç©¶ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')anâˆ’1a^n-1anâˆ’1 ç´ æ•°ã€‚é€šè¿‡ä¸‹è¡¨è§‚å¯Ÿå¯å¾—çŸ¥ï¼š@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa æ˜¯å¥‡æ•°æ—¶ï¼Œ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')anâˆ’1a^n-1anâˆ’1 æ˜¯å¶æ•°ã€‚ æ‰€ä»¥@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa åªèƒ½ä¸ºå¶æ•°ã€‚@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')anâˆ’1a^n-1anâˆ’1 æ€»æ˜¯è¢« @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aâˆ’1a-1aâˆ’1 æ•´é™¤ã€‚è¯æ˜Žï¼šå‡ ä½•çº§æ•°å…¬å¼ï¼ˆGeometric Seriesï¼‰@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xnâˆ’1=(xâˆ’1)(xnâˆ’1+xnâˆ’2+â‹¯+x2+x+1)x^{n}-1=(x-1)\\left(x^{n-1}+x^{n-2}+\\cdots+x^{2}+x+1\\right)xnâˆ’1=(xâˆ’1)(xnâˆ’1+xnâˆ’2+â‹¯+x2+x+1) å±•å¼€å³è¾¹ä¹˜æ³•å³å¯è¯æ˜Žå‡ ä½•çº§æ•°ã€‚æ‰€ä»¥@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa åªèƒ½ä¸º2ã€‚è§‚å¯Ÿä¸‹è¡¨@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2nâˆ’12^n-12nâˆ’1 çš„æƒ…å†µï¼šä¸æ˜¯æ‰€æœ‰@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2nâˆ’12^n-12nâˆ’1 éƒ½æ˜¯ç´ æ•°ã€‚å¦‚æžœ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn è¢«@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')mmm æ•´é™¤ï¼Œåˆ™@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2nâˆ’12^n-12nâˆ’1 è¢«@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2mâˆ’12^m-12mâˆ’1 æ•´é™¤ã€‚è§‚å¯Ÿå¦‚æžœ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn æ˜¯å¶æ•°ï¼Œ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2nâˆ’12^n-12nâˆ’1 è¢«@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')22âˆ’1=32^2-1=322âˆ’1=3 æ•´é™¤ã€‚å¦‚æžœ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn è¢«3æ•´é™¤ï¼Œ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2nâˆ’12^n-12nâˆ’1 è¢«@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')23âˆ’1=72^3-1=723âˆ’1=7 æ•´é™¤ã€‚å¦‚æžœ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn è¢«5æ•´é™¤ï¼Œ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2nâˆ’12^n-12nâˆ’1 è¢«@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')25âˆ’1=312^5-1=3125âˆ’1=31 æ•´é™¤ã€‚è¯æ˜Žï¼šåŒæ ·åˆ©ç”¨å‡ ä½•çº§æ•°å…¬å¼ï¼Œ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2n=(2m)k2^n=(2^{m})^k2n=(2m)k @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2nâˆ’1=(2m)kâˆ’1=(2mâˆ’1)((2m)kâˆ’1+(2m)kâˆ’2+â‹¯+(2m)2+(2m)+1)2^{n}-1=\\left(2^{m}\\right)^{k}-1=\\left(2^{m}-1\\right)\\left(\\left(2^{m}\\right)^{k-1}+\\left(2^{m}\\right)^{k-2}+\\cdots+\\left(2^{m}\\right)^{2}+\\left(2^{m}\\right)+1\\right)2nâˆ’1=(2m)kâˆ’1=(2mâˆ’1)((2m)kâˆ’1+(2m)kâˆ’2+â‹¯+(2m)2+(2m)+1) æ‰€ä»¥@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn ä¸€å®šæ˜¯ç´ æ•°ã€‚å‘½é¢˜1. å¦‚æžœæ•´æ•°@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aâ‰¥2a\\ge2aâ‰¥2 ä¸Ž@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nâ‰¥2n\\ge2nâ‰¥2 ï¼Œå¦‚æžœ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')anâˆ’1a^n-1anâˆ’1 æ˜¯ç´ æ•°ï¼Œåˆ™@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa ä¸€å®šç­‰äºŽ2ä¸”@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nnn ä¸€å®šæ˜¯ç´ æ•°ã€‚Proposition 1. If @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')anâˆ’1a^n-1anâˆ’1 is prime for some numbers @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aâ‰¥2a\\ge2aâ‰¥2 and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')nâ‰¥2n\\ge2nâ‰¥2 , then a must equal 2 and n must be a prime.ä¸æ˜¯æ‰€æœ‰çš„@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2pâˆ’12^p-12pâˆ’1 éƒ½æ˜¯ç´ æ•°ã€‚åº”ç”¨åœ¨å¯†ç å­¦ä¸­ï¼Œæœ‰é™åŸŸä¸­çš„è¿ç®—æ€§èƒ½æžå¤§å½±å“å¯†ç åè®®çš„å®žçŽ°ã€‚åœ¨è¿™äº›è¿ç®—ä¸­ï¼Œé€†è¿ç®—æ˜¯æœ€å¤æ‚çš„ï¼Œæ¨¡è¿ç®—ã€ä¹˜æ³•æ¬¡ä¹‹ã€‚å¦‚æžœæœ‰é™åŸŸé€‰æ‹©æ¢…æ£®ç´ æ•°ï¼Œå¾—ç›ŠäºŽå®ƒçš„ä¼˜è‰¯æ€§è´¨ï¼Œå¯ä»¥æžå¤§æé«˜è¿ç®—æ•ˆçŽ‡ï¼Œç‰¹åˆ«æ˜¯æœ‰é™åŸŸä¸‹çš„æ¨¡è¿ç®—ã€ä¹˜æ³•æ“ä½œã€‚Modular Reductionæ ¹æ®æ€§è´¨ï¼š@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2pâ‰¡1(mod2pâˆ’1)2^p \\equiv 1 \\pmod{2^p-1}2pâ‰¡1(mod2pâˆ’1) å¯å¾—ï¼š@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')x=aâ‹…2p+bâ‰¡a+b(mod2pâˆ’1)x=a\\cdot 2^p +b\\equiv a+b \\pmod{2^p-1}x=aâ‹…2p+bâ‰¡a+b(mod2pâˆ’1) å¦‚æžœå°†@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xxx å†™ä½œæ¯”ç‰¹å½¢å¼ï¼Œæ¨¡è¿ç®—çº¦å‡ä¸ºå°†é«˜ä½æ¯”ç‰¹ä¸²å’Œä½Žä½æ¯”ç‰¹ä¸²ç›¸åŠ ã€‚C++ä¼ªä»£ç å®žçŽ°ï¼ˆä»¥@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')p=261âˆ’1p=2^{61}-1p=261âˆ’1 ä¸ºä¾‹ï¼‰ï¼šä»£ç #define MERSENNE_PRIME_EXP 61 const uint64_t PR = 2305843009213693951; //2^61-1 uint64_t mod(uint64_t x) { uint64_t i = (x &amp; PR) + (x &gt;&gt; MERSENNE_PRIME_EXP); return (i&gt;=p) ? i - p: i; }Multiplicationä»¥@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')p=261âˆ’1p=2^{61}-1p=261âˆ’1 ä¸ºä¾‹ï¼Œ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa å’Œ@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bbb ç›¸ä¹˜åŽï¼Œå¾—åˆ°ä¸€ä¸ªæœ€å¤š122-bitçš„æ•°å­—ï¼Œç”¨åŒæ ·çš„æ–¹æ³•ï¼Œå°†é«˜ä½æ¯”ç‰¹å’Œä½Žä½æ¯”ç‰¹ç›¸åŠ ã€‚ï¼ˆå¦‚æžœç»“æžœæ›´é•¿ï¼Œå°±æŒ‰æ®µä¸€ç›´å åŠ ï¼‰åœ¨å®žçŽ°ä¸Šå¯ä»¥ä½¿ç”¨64-bitä¹˜æ³•ä¼˜åŒ–(å¯ä»¥ç”¨æŒ‡ä»¤é›†åŠ é€Ÿ)ï¼šä»£ç /** * @brief 64-bit multiplication * * @param a * @param b * @param c [out]pointer to hign 64-bit * @return uint64_t [out]low 64-bit */ #if defined(__x86_64__) &amp;&amp; defined(__BMI2__) inline uint64_t mul64(uint64_t a, uint64_t b, uint64_t *c) { return _mulx_u64((unsigned long long)a, (unsigned long long)b, (unsigned long long*)c); } #else inline uint64_t mul64(uint64_t a, uint64_t b, uint64_t *c) { __uint128_t aa = a; __uint128_t bb = b; auto cc = aa*bb; // c is a pointer to high 64-bit *c = cc&gt;&gt;64; // return low 64-bit return (uint64_t)cc; } #endif inline uint64_t mult_mod(uint64_t a, uint64_t b) { uint64_t c = 0; uint64_t e = mul64(a, b, (uint64_t*)&amp;c); // c is hign 64-bit // e is low 64-bit uint64_t ret = (e &amp; PR) + ( (e&gt;&gt;MERSENNE_PRIME_EXP) ^ (c&lt;&lt; (64-MERSENNE_PRIME_EXP))); return (ret &gt;= PR) ? (ret-PR) : ret; }Referenceæ•°è®ºæ¦‚è®ºï¼ˆåŽŸä¹¦ç¬¬4ç‰ˆæœ¬ï¼‰ A Friendly Introduction to Number Theory.ä»£ç å‚è€ƒäºŽemp-zk.","link":"/notes/%5BMersenne-Prime%5D/index.html"},{"title":"","text":"[Naor-OT05]Computationally Secure Oblivious Transfer /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 90%; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 70%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } ðŸ“œ[Naor-OT05]Computationally Secure Oblivious Transferkeywords: Cryptography, Privacy preserving computation, Secure function evaluation, Oblivious transferABSTRACT1 Introduction1.1 Oblivious Transfer1.2 Correctness and Security Definitions2 Protocols for 1-out-of-N Oblivious Transfer2.1 A Protocol for 1-out-of-N OTProtocolComplexityImprovement2.2 A Recursive Protocol for 1-out-of-N OTprotocolcomplexityABSTRACTcontributions: computationally secure protocols of 1-out-of-N OT: only log N executions of a 1-out-of-2 OT protocolk-out-of-N OT: more eff. than k repetitions of 1-out-of-N OToblivious transfer with adaptive queries.1 Introduction1.1 Oblivious TransferOblivious Transfer (OT)Note that this transformation takes DH-tuples to DH-tuples, and non-DH-tuples to non-DH-tuples. Furthermore, the new exponents are independent of the originals. This is easy to see if we start with a DH tuple.https://crypto.stanford.edu/pbc/notes/crypto/ot.html1.2 Correctness and Security DefinitionsThe Receiver's Security:receiver: è®¡ç®—ä¸Šä¸å¯åŒºåˆ†èŽ·å¾—çš„å€¼å’Œéšæœºå€¼The Sender's Security: sender: å¯¹äºŽsenderï¼Œè¦æ±‚real implementation of the protocol å’Œ ideal modelä¸å¯åŒºåˆ†ï¼Œå³ä¸å¯ä»¥èŽ·å¾—æ¯”ideal model ä¸‹æ›´å¤šçš„ä¿¡æ¯ ideal implementation: a trusted partyformal def:2 Protocols for 1-out-of-N Oblivious Transferassumption: block ciphers or keyed one-way hash functions can be modeled as PRF.PRF @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')FK(x)F_K(x)FKâ€‹(x) : block cipher with key K and encrypting xkeying a hash function with K and applying it to x main idea of 1-out-of-N: use a small set of keys(logN) and mask each input with a combination of a different subset of the keys ç”¨logNä¸ªä¸åŒçš„keysï¼Œé€šè¿‡ä¸åŒçš„å¯†é’¥ç»„åˆæ¥åŠ å¯†ä¸åŒçš„è¾“å…¥p.s.: the keys are not applied directly(insecure) input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')XIX_IXIâ€‹ : the value @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')FK(I)F_K(I)FKâ€‹(I) is used for masking ä¸èƒ½ç›´æŽ¥æŠŠå¯†é’¥åº”ç”¨åˆ°æ˜Žæ–‡ä¸ŠåŠ å¯†ï¼ˆæ¯”å¦‚å¼‚æˆ–çš„å½¢å¼ï¼‰æœ‰ä¸¤ç§å®žçŽ°1-out-of-N OTçš„åè®®ï¼š ä¸€ç§æ˜¯ä½¿ç”¨logNæ¬¡ 1-out-of-2 OTåè®® å¦ä¸€ç§æ˜¯ä½¿ç”¨é€’å½’å¼çš„æ–¹æ³•ï¼šå°†1-out-of-Nåˆ†ä¸ºä¸¤ä¸ª1-out-of-@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N\\sqrt{N}Nâ€‹ OTä¸¤ä¸ªåè®®ä¸­æœ€ä¸»è¦çš„æ˜¯transfer stageä¸­è°ƒç”¨1-out-of-2ã€‚ ä¸è¿‡åŽä¸€ç§åè®®å†åˆå§‹åŒ–é˜¶æ®µæ˜¯O(N)çš„å¤æ‚åº¦ï¼Œå¦ä¸€ä¸ªæ˜¯O(NlogN)çš„å¤æ‚åº¦ã€‚2.1 A Protocol for 1-out-of-N OTProtocolsender B input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')X1,X2,...,XNX_1,X_2,...,X_NX1â€‹,X2â€‹,...,XNâ€‹ , receiver want's to learn @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')XIX_IXIâ€‹ sender B prepares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')lll random pairs of keys Bå‡†å¤‡logNä¸ªå¯†é’¥å¯¹ï¼Œæ¯ä¸ªå¯†é’¥å¯¹ä¸­ä¸€ä¸ªå¯¹åº”æ¯”ç‰¹0ï¼Œä¸€ä¸ªå¯¹åº”æ¯”ç‰¹1ã€‚å¯¹æ¯ä¸€ä¸ª@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')XIX_IXIâ€‹ éƒ½åšmaskæ“ä½œï¼Œå…·ä½“æ¥è¯´å°±æ˜¯æ ¹æ®@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')III çš„æ¯”ç‰¹ä½æ¥é€‰æ‹©å¯†é’¥ï¼Œç”¨PRFçš„outputä¾æ¬¡maskã€‚A and B engage in a 1-out-of-2 OT on @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')&lt;Kj0,Kj1&gt;&lt;K_j^0,K_j^1&gt;&lt;Kj0â€‹,Kj1â€‹&gt; Aå’ŒBæ‰§è¡ŒlogNæ¬¡ 1-out-of-2 OTï¼Œå¾—åˆ°Açš„é€‰æ‹©@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')III å¯¹åº”æ¯”ç‰¹ä½çš„å¯†é’¥ã€‚B sends to A the string Y Bå‘Aå‘é€Nä¸ªåŠ å¯†åŽçš„Y = E(X)ã€‚A reconstructs X Aåªæœ‰ä»–æ‰€é€‰æ‹©å€¼çš„å¯†é’¥ï¼Œåªèƒ½è§£å¯†å‡ºå…¶ä¸­ä¸€ä¸ªã€‚Complexitypreprocessing(step 1): @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Nlogâ¡NN\\log{N}NlogN evaluations of the pseudo-random function FK æ¯ä¸€ä¸ªXéƒ½è¦logNä¸ªå¯†é’¥å¯¹å…¶maskã€‚transfer stage: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')logâ¡N\\log{N}logN invocations of the 1-out-of-2 OT protocol invocation: a request for helpImprovement2.2 A Recursive Protocol for 1-out-of-N OTprotocolsender B input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')X1,X2,...,XNX_1,X_2,...,X_NX1â€‹,X2â€‹,...,XNâ€‹ , receiver want's to learn @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')XIX_IXIâ€‹ B prepares two sets of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N\\sqrt{N}Nâ€‹ randomly chosen keys. B arranges the N inputs in a @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')NÃ—N\\sqrt{N}\\times\\sqrt{N}Nâ€‹Ã—Nâ€‹ matrix. Bå‡†å¤‡ä¸¤ç»„sqrt(N)çš„éšæœºå¯†é’¥ã€‚å†æŠŠè¿™Nä¸ªå€¼ç»„åˆä¸ºä¸€ä¸ªçŸ©é˜µï¼Œè¿™æ ·å°±å¯ä»¥ç”¨è¡Œåˆ—æ¥ç´¢å¼•ã€‚Bå¯¹æ¯ä¸ªå€¼éƒ½ç”¨è¡Œåˆ—ç´¢å¼•æ¥åŠ å¯†ã€‚A and B engage in a 1-out-of-@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N\\sqrt{N}Nâ€‹ OT protocol A å’ŒBæ‰§è¡Œ1-out-of-@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')N\\sqrt{N}Nâ€‹ OTï¼Œè®©Aå¾—åˆ°ç›®æ ‡é€‰æ‹©çš„è¡Œåˆ—å¯†é’¥ã€‚B sends to A all the YA reconstructs X Aåªæœ‰ç›®æ ‡é€‰æ‹©å¯¹è¡Œåˆ—å¯†é’¥ï¼Œæ‰€ä»¥åªèƒ½å¾—åˆ°ä¸€ä¸ªå€¼ã€‚complexitytotal = 2N evaluations of FK for preprocessing + 2 invocations of the 1-out-of-sqrt(N) protocol for transfer2 1-out-of-sqrt(N) OTpreprocess: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2Nlogâ¡N=Nlogâ¡N2\\sqrt{N} \\log \\sqrt{N} = \\sqrt{N}\\log N2Nâ€‹logNâ€‹=Nâ€‹logN evaluations for FKtransfer stage: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')2logâ¡N=logâ¡N2\\log\\sqrt{N} = \\log N2logNâ€‹=logN 1-out-of-2 OT æ‰€ä»¥è¯´ä¸¤ä¸ªåè®®è°ƒç”¨1-out-of 2çš„æ¬¡æ•°æ˜¯ä¸€æ ·çš„totol invocations of PRF: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Nlogâ¡N+2N\\sqrt{N}\\log N + 2NNâ€‹logN+2N total calls of 1-out-of-2 OT: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')logâ¡N\\log NlogN 2.1åè®®è¢«è®¤ä¸ºæ˜¯ lognç»´å¯†é’¥åŠ å¯†çš„ï¼Œè€Œ2.2åè®®è¢«è®¤ä¸ºæ˜¯2ç»´åŠ å¯†","link":"/paper/%5BNaor-OT05%5D/index.html"},{"title":"","text":"[Schneider13]GMW vs. Yao? Efficient Secure Two-Party Computation with Low Depth CItcuit /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 70%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } ðŸ“œ [Schneider13]GMW vs. Yao? Efficient Secure Two-Party Computation with Low Depth CItcuitKeywords: GMW protocol, optimizations, privacy-preserving face recognitionABSTRACT1 IntroductionContributions2 Preliminaries2.1 Oblivious Transfer2.2 Approaches for Secure Two-Party Computation2.2.1 Yao's Garbled Circuits Protocol2.2.2 GMW Protocol2.3 Evaluation Metrics and Notation3 Circuit Constructions with low Depth and Size3.1 Addition3.1.1 Ladner-Fscher Adder.3.1.2 Carry-Save Adder3.2 Squaring3.3 Comparison3.4 Hamming Weight4 Optimizations for Two Party GMWBenchmarking EnvironmentTable 1: Time improvements4.1 Multiplication Triples4.2 Using ASE instead of SHA as Pseudo-Random FunctionTable 2: Comparison of SHA-1 and AES128 implementation4.3 Load Balancing4.4 Implementation-Specific Optimizations4.5 Single Instruction Multiple Data (SIMD) OperationsABSTRACTæå‡ºäº†ä¸¤ç§æ–¹æ¡ˆï¼šYao's garbled circuits å’ŒGMWåè®®ã€‚ ä½†å› ä¸ºYaoä½†æ–¹æ¡ˆæœ‰constant round complexityï¼Œæ‰€ä»¥å¤§å¤šæ•°ç ”ç©¶éƒ½è¡¨æ˜Žå…¶æœ‰æ›´å¥½çš„æ•ˆçŽ‡ã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€äº›GMWåè®®çš„ä¼˜åŒ–ã€‚ æ€»ç»“äº†depth-optimized circuit constructions. è¿˜è€ƒè™‘äº†éšç§ä¿æŠ¤çš„äººè„¸è¯†åˆ«(privacy-preserving face recognition)1 IntroductionYao's garbled circuits, GMW: éƒ½æŠŠå‡½æ•°è¡¨ç¤ºä¸ºBoolean circuitï¼Œéƒ½æä¾›security against semi-honest adversaries.semi-honest adversaries: honestly follow the protocol but try to learn additional information from observed messages.Yao's protocol:a constant number of roundsrequires OTs only for the input of one of the parties åªæœ‰ä¸€æ–¹çš„è¾“å…¥éœ€è¦OTsecure evaluation of a gate: requires only symmetric cryptographic operations è®¡ç®—é—¨æ—¶åªéœ€è¦å¯¹ç§°å¯†ç æŠ€æœ¯GMW protocol: requires an interactive OT for each AND gate å¯¹æ¯ä¸€ä¸ªANDé—¨éƒ½è¦æ±‚äº¤äº’å¼çš„OTåè®®ContributionsGMWåè®®åœ¨two semi-honest partiesä¸­æ˜¯å¯è¡Œçš„ã€‚ æ­¤å¤–ï¼ŒGMWåè®®ç›¸å¯¹äºŽYao'sçš„åè®®æœ‰ä¸€äº›é¢å¤–çš„ä¼˜åŠ¿ã€‚three2 Preliminaries2.1 Oblivious Transferåœ¨1-out-of n @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')OTlm\\mathrm{OT}_l^mOTlmâ€‹ ä¸­ï¼š Sï¼šæä¾›mä¸ªnå…ƒç»„ Rï¼šæä¾›mä¸ªé€‰æ‹© æœ€åŽRä¼šæ ¹æ®æ¯ä¸€ä¸ªé€‰æ‹©å¾—åˆ°å…ƒç»„ä¸­çš„ä¸€ä¸ªæ•°å­—ã€‚Naor-Pinkas OT protocol[25]Naor, M., Pinkas, B.: Computationally secure oblivious transfer. Journal of Cryptology 18(1), 1â€“35 (2005)secure against semi-honest adversariesunder Decisional Diffie-Hellman(DDH) assumptionin the random-oracle modelrequires both parties to perform @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')O(m)\\mathcal{O(m)} O(m) modular exponentiationsæœ‰ä¸¤ç§åŠ é€ŸOTçš„æ–¹æ³•OT pre-computations[2]Beaver, D.: Precomputing oblivious transfer. In: Advances in Cryptology - CRYPTOâ€™95. LNCS, vol. 963, pp. 97â€“109. Springer (1995)offline: pre-compute the OTs on random inputsonline: use pre-computed values as one-time pads to run the OT on the actual inputs.R: sends one message of m @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')logâ¡2n\\log_2nlog2â€‹n bits to Sï¼ˆéšè—é€‰æ‹©ï¼‰S: sends back a message of size mnl bits OT extensions[16,22] 2.2 Approaches for Secure Two-Party Computation2.2.1 Yao's Garbled Circuits Protocol[33]Yao, A.C.: How to generate and exchange secrets. In: Foundations of Computer Science (FOCSâ€™86). pp. 162â€“167. IEEE (1986)Yao's protocol: 1. non-interactively 2. has a constant number of roundscreatorï¼ˆç”µè·¯ç”Ÿæˆæ–¹ï¼‰ï¼šencrypt the function to be computedFor each gate:map the plain values to random-looking symmetric keysgenerate an encryption table é€šè¿‡è¯¥è¡¨ï¼Œå¯ä»¥ç”¨ç»™å®šçš„input keysè®¡ç®—gate's output keytransmit the encrypted circuit and (creator's) encrypted inputs to evaluatorevaluatorï¼ˆç”µè·¯è®¡ç®—æ–¹ï¼‰obtains his encrypted input via 1-out-of 2 OT (evaluator's input key)use the encrypted inputs to evaluate the encrypted function gate by gatecreatorprovides a mapping from the encrypted output to plain outputsome extension 2.2.2 GMW Protocol[11]Goldreich, O.: Foundations of Cryptography, vol. 2: Basic Applications. Cambridge University Press (2004) [12]Goldreich, O., Micali, S., Wigderson, A.: How to play any mental game or a completeness theorem for protocols with honest majority. In: Symposium on Theory of Computing (STOCâ€™87). pp. 218â€“229. ACM (1987)GMW protocol: interactively compute a function using secret-shared values share each input and intermediate wire to 2 partiesç”¨2-out-of-2 secret sharing shemeæŠŠvalue v åˆ†äº«ç»™ä¸¤æ–¹ï¼Œæ¯ä¸€æ–¹å¾—åˆ°çš„ä¸€ä¸ªrandom-looking share @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')viv_iviâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=v1âŠ•v2v = v_1\\oplus v_2v=v1â€‹âŠ•v2â€‹ XOR gates: securely evaluated locally by XORing the sharesAND gates: parties run an interactively protocoloutput wire: æŠŠå’Œoutput wireç›¸å…³çš„shareså‘é€ç»™éœ€è¦å¾—åˆ°outputçš„å‚ä¸Žæ–¹ï¼Œè®¡ç®—output AND gate: ä¸¤ç§å®žçŽ°æ–¹æ³•Oblivious TransferMultiplication Triples[1] Goldreich, O., Micali, S., Wigderson, A.: How to play any mental game or a com- pleteness theorem for protocols with honest majority. In: Symposium on Theory of Computing (STOCâ€™87). pp. 218â€“229. ACM (1987) 2.3 Evaluation Metrics and Notationå› ä¸ºYao'så’ŒGMWéƒ½æä¾›free XORsï¼Œå› æ­¤åªæ¯”è¾ƒAND gatessize S(C) : the number AND gates in circuit Cdepth D(C): the maximum number of AND gatesother notations:3 Circuit Constructions with low Depth and Size3.1 Additionlinear size and depth[18]Ripple-carry adder: Kolesnikov, V., Sadeghi, A.R., Schneider, T.: Improved garbled circuit building blocks and applications to auctions and computing minima. In: Cryptology And Network Security (CANSâ€™09). LNCS, vol. 5888, pp. 1â€“20. Springer (2009)3.1.1 Ladner-Fscher Adder.[21] Ladner-Fischer adder Ladner,R.E.,Fischer,M.J.:Parallelprefixcomputation.JournaloftheACM27(4), 831â€“838 (1980)in logarthmic depth 3.1.2 Carry-Save Adder[8] carry-save adder: Earle, L.G.: Latched carry-save adder. IBM Technical Disclosure Bulletin 7(10), 909â€“910 (1965)has linear size and constant depth 3.2 Squaringa squaring circuit is smaller by a factor of about 2 @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xjxi+xixj=2xixjx_jx_i+x_ix_j = 2x_ix_jxjâ€‹xiâ€‹+xiâ€‹xjâ€‹=2xiâ€‹xjâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xixi=xix_ix_i=x_ixiâ€‹xiâ€‹=xiâ€‹ 3.3 Comparisonequal: greater than:3.4 Hamming Weight 4 Optimizations for Two Party GMWan implementation of GMW [5]Choi, S.G., Hwang, K.W., Katz, J., Malkin, T., Rubenstein, D.: Secure multi-party computation of Boolean circuits with applications to privacy in on-line market- places. In: Cryptographersâ€™ Track at the RSA Conference (CT-RSAâ€™12). LNCS, vol. 7178, pp. 416â€“432. Springer (2012)currently fastest garbled circuits implementation [15] Huang, Y., Evans, D., Katz, J., Malka, L.: Faster secure two-party computation using garbled circuits. In: Security Symposium. USENIX (2011)[5]èƒ½åœ¨nâ‰¥3æ—¶é«˜æ•ˆå®žçŽ°GMWåè®®ï¼Œå¯¹äºŽn=2çš„æƒ…å†µï¼Œ[5]è®¤ä¸ºä»–ä»¬çš„é€Ÿåº¦ä¼šæ¯”å½“å‰æœ€å¿«çš„garbled circuitså®žçŽ°æ…¢ä¸¤å€ã€‚ Benchmarking Environmentevaluate the time for: using average time for 100 executionssetup phase: Naor-Pinkas OTs: group @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Zpâˆ—\\Z_p^*Zpâˆ—â€‹ with @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âˆ£pâˆ£=512|p|=512âˆ£pâˆ£=512 bitOT extension: security parameter t = @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')808080 online phase: sharing of inputscircuit evaluationcombining output sharesoverall time:circuit constructionsetup phaseonline phase circuit: an unoprmized 512-bit multiplication circuit CS(C) = 800,227D(C)=38PC:Table 1: Time improvementslist the modifications in the order. each modification include the improvement of all previous optimizations.4.1 Multiplication Triples[1] multiplication triples Beaver, D.: Efficient multiparty protocols using circuit randomization. In: Ad- vances in Cryptology â€“ CRYPTOâ€™91. LNCS, vol. 576, pp. 420â€“432. Springer (1991)Beaver's MT: (instead of pre-computed OTs)slightly slower in setup phasemore efficient in online phasetime:4.2 Using ASE instead of SHA as Pseudo-Random FunctionSHA-1 in original implementationï¼šinstantiant the random oraclegenerate pseudo-random values in the OT extensionASE (CTR mode) as PRF:decreased the number of expensive hash function calls per AND gate é™ä½Žäº†æ¯ä¸ªANDé—¨è°ƒç”¨çš„hashå‡½æ•°æ¬¡æ•°receiver R: from 3.5 to 1sender S: from 4.5 to 4numer of calls to instantiate the PRF å®žä¾‹åŒ–PRFéœ€è¦è°ƒç”¨AESçš„æ¬¡æ•°R: 3.1 AES calls per AND gateS: 0.65 AES calls per AND gatethe sender and the receiver have different computational workloadTable 2: Comparison of SHA-1 and AES128 implementation 4.3 Load Balancingrun the OTs in either direction, each party has the same workload å› ä¸ºMTåœ¨online phase æ˜¯å¯¹ç§°å½¢å¼çš„ï¼Œæ‰€ä»¥å¯ä»¥åŒæ–¹å„è‡ªå¹¶è¡Œè¿è¡ŒOTåè®®2.5 SHA-1 invoations per AND gate1.8 AES invocations per AND gaterun the Naor-Pinkas OT protocol for the seed OTs twice, which however amortizes fairly quickly è™½ç„¶è¿è¡Œäº†ä¸¤æ¬¡ï¼Œä½†éžå¸¸å¿«ã€‚four parallel threads during setup phaseone for pseudo-random functionthe other for random oracle time:4.4 Implementation-Specific Optimizationsarithmetic for modular arithmeticsuse GMPbitwise processing order during OT extension and online phaseperform operations bytewise æŒ‰å­—èŠ‚å¼‚æˆ–ï¼Œè€Œä¸æ˜¯æŒ‰ä½SHA-1 for the random oracleuse an assembler implementation of OpenSSLtime:4.5 Single Instruction Multiple Data (SIMD) Operations","link":"/paper/%5BSchneider13%5D/index.html"},{"title":"","text":"[GMW87]How To Play Any Mental Game /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 50%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } ðŸ“œ[GMW87]How To Play Any Mental Game1 Introduction2 Preliminary Definitions2.1 Notation and Conventions for Probabilistic Algorithms2.2 Game Networks and Distributed Algorithms2.3 Adversaries4 Hints on How to Play Hm-game With Passive Adversaries4.1 A New and General OT ProtocolOur Protocol4.2 Strengthening Yao's Combined Oblivious Transfer4.3 The Tm-game Solver for passive adversariesCase 1: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâ‹…c\\sigma\\cdot cÏƒâ‹…c Case 2: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâˆ’1â‹…c\\sigma^{-1}\\cdot cÏƒâˆ’1â‹…c Case 3: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâ‹…Ï„\\sigma\\cdot \\tauÏƒâ‹…Ï„ 1 Introduction 2 Preliminary Definitions2.1 Notation and Conventions for Probabilistic Algorithmsnotions 2.2 Game Networks and Distributed AlgorithmsProbabilistic Distributed Alg.2.3 AdversariesPassive adversariespassive adversarycompute more than required don't want ro corrupt, but try to compute more than their due share of knowledgemsgs it sends and outputs are inaccordance to original programMalicious Adv.malicious adversarydeviated from original progarm in any actionnot only disrupte the privacy constraintbut also make the outcome different than in an ideal worldå›¾çµæ¸¸æˆåœ¨any number of passive adv. or with n/2 malicious adv. éƒ½æ˜¯playable. 4 Hints on How to Play Hm-game With Passive Adversaries4.1 A New and General OT ProtocolA more general and useful notion of OT has been proposed by Even, Goldreich and Lempel: 1-out-of-2 OTproposed the first omplementation of a 1-out-of-2 OT using public key cryptosystems.requires a quite strong set of assumpeions even when the adversaries are only passive. å³ä½¿æ˜¯åœ¨passive adv.é¢å‰ä¹Ÿéœ€è¦strong assumptions.Our Protocolä¸ºäº†ç®€åŒ–å¤„ç†ï¼Œ1-out-of-2 OT ä¸­çš„msgåªæœ‰1bit Bä¸èƒ½é¢„æµ‹å¦ä¸€ä¸ªå€¼ï¼ŒAä¸èƒ½é¢„æµ‹Bçš„é€‰æ‹©ã€‚input:A: a pair of bits (b0, b1) and their encryption.B: private input bit @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î±\\alphaÎ± desired properties:B can read @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bÎ±b_\\alphabÎ±â€‹ , but can't predict @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bÎ±â€¾b_{\\overline{\\alpha}}bÎ±â€‹ A cannot predict @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î±\\alphaÎ± (A) randomly select a trap-door function keep @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')fâˆ’1f^{-1}fâˆ’1 secret and sends f to B(B) randomly selects @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')x0,x1x_0, x_1x0â€‹,x1â€‹ sends A the pair (u, v)(A)computes(c0, c1) use c to mask b and sends(d0, d1) to B(B)computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bÎ±b_\\alphabÎ±â€‹ 4.2 Strengthening Yao's Combined Oblivious TransferCOTA, B: respectively owning private inputs a and bg: any chosen function gA computes g(a, b) while B don't know what A has computedå¦‚æžœæŠŠa, bå½“ä½œsecretsï¼ŒB obliviously transfer a prescribed combination of his and A's secret to A. å››è¡Œä¸­åªæœ‰ä¸€å¯¹çš„å¼‚æˆ–æ˜¯å¯†é’¥D5ï¼Œå…¶ä»–éƒ½æ˜¯å¯†é’¥D6ï¼Œè§£å¯†0ï¼ˆAND gate) E1,E2,E5,E6å’Œ0ï¼Œ1çš„å¯¹åº”å…³ç³»æ˜¯public labelled E3,E4å’Œ0,1çš„å…³ç³»æ˜¯secretly labelled(B)generates a COT AND-gate keep all decoding alg. and all strings in the rows(B)gives A the second input-wire: b bå¯èƒ½æ˜¯E3æˆ–E4ï¼Œå› ä¸ºAä¸çŸ¥é“å¯¹åº”å…³ç³»(A)get either D1, D2 according to value a by 1-out-of-2 OT, B will not know which alg. A got(A)easily compute the value of output-wire only A know AND(a,b)4.3 The Tm-game Solver for passive adversariesuse CTO as a subroutingwant to use COT as subrouting to construct a Tm-solver æƒ³ç”¨COTä½œä¸ºTm-solverçš„subroutingif two parties i and j use COT so that i will compute g(xi, yi) é€šè¿‡COT,party i å¯ä»¥è®¡ç®—g(xi, yi)ï¼Œè€Œparty j ä¸çŸ¥é“ã€‚[Ba]D. Barrington, Bounded-Width Branhing Program Recognize Exactly Those Languages in NC, Proc. 18th STOC, 1986 pp 1-5ç”¨äº†ä¸€ä¸ªsymmetric group on 5 elements in the program:0å’Œ1éƒ½è¢«åˆ†åˆ«ç¼–ç ä¸º2ä¸ª5-permutationså…¶ä¸­çš„å˜é‡éƒ½åœ¨S5ä¸­ç¨‹åºä¸­çš„æ¯ä¸ªæ“ä½œéƒ½åŒ…å«ä¸¤ä¸ª5-permutaionså€¼éƒ½ä¹˜æ³•ï¼Œå€¼å¯èƒ½æ˜¯å¸¸é‡ã€å˜é‡æˆ–è€…å˜é‡çš„é€†ç­‰ã€‚each party:(input)takes private bits and encodes it by a 5-permutaion @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ\\sigmaÏƒ (divide @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ\\sigmaÏƒ )selects n-1 random 5-permutaion and gives@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(i,Ïƒi)(i,\\sigma_i)(i,Ïƒiâ€‹) to party i iæ˜¯indexï¼Œæœ€åŽç»“æžœè®¡ç®—æ—¶çš„ä¹˜ç§¯é¡ºåºsets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒn\\sigma_nÏƒnâ€‹ and gives @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(n,Ïƒn)(n,\\sigma_n)(n,Ïƒnâ€‹) to party neach variable @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ\\sigmaÏƒ : each party x possesses an index permutation pair @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(x,Ïƒx)(x, \\sigma_x)(x,Ïƒxâ€‹) @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âˆx=1nÏƒx=Ïƒ\\prod_{x=1}^n\\sigma_x=\\sigmaâˆx=1nâ€‹Ïƒxâ€‹=Ïƒ é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤ï¼Œeach party å¯ä»¥è®¡ç®—æœ€ç»ˆç»“æžœçš„pieceCase 1: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâ‹…c\\sigma\\cdot cÏƒâ‹…c sigma is a variable and c is a constant(each party) has a piece: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(x,Ïƒx)(x, \\sigma_x)(x,Ïƒxâ€‹) (party n) sets his new piece: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(n,Ïƒnâ‹…c)(n, \\sigma_n\\cdot c)(n,Ïƒnâ€‹â‹…c) (all each party) leaves his piece untouchedthe ordered product of the new pieces is @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâ‹…c\\sigma\\cdot cÏƒâ‹…c piecesæŒ‰ç…§é¡ºåºçš„ä¹˜ç§¯ Case 2: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâˆ’1â‹…c\\sigma^{-1}\\cdot cÏƒâˆ’1â‹…c æ±‚é€†æ˜¯é€†åºæ±‚ï¼Œæ‰€ä»¥indexéœ€è¦æ”¹å˜ æ±‚å‡º@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâˆ’1\\sigma^{-1}Ïƒâˆ’1 ï¼Œå†æŒ‰ç…§case 1çš„æƒ…å†µå¤„ç† Case 3: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâ‹…Ï„\\sigma\\cdot \\tauÏƒâ‹…Ï„ both @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ\\sigmaÏƒ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„\\tauÏ„ are variables @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâ‹…Ï„=Ïƒ1â‹¯Ïƒnâ‹…Ï„1â‹¯Ï„n\\sigma\\cdot \\tau = \\sigma_1\\cdots\\sigma_n\\cdot \\tau_1\\cdots\\tau_nÏƒâ‹…Ï„=Ïƒ1â€‹â‹¯Ïƒnâ€‹â‹…Ï„1â€‹â‹¯Ï„nâ€‹ each party poseesses piece @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒi\\sigma_iÏƒiâ€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„i\\tau_iÏ„iâ€‹ S5 is not commutative party iåœ¨è®¡ç®—æœ€ç»ˆç»“æžœpieceæ—¶ï¼Œä¸èƒ½ç›´æŽ¥å°†ä¸¤ä¸ªpieceç›¸ä¹˜ï¼Œå› ä¸ºS5æ˜¯ä¸æ»¡è¶³äº¤æ¢å¾‹çš„ã€‚giving new piecesmove party 1's pieces closer by swaping é€šè¿‡äº¤æ¢ï¼Œè®©party 1çš„ä¸¤ä¸ªpiecesé è¿‘@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ1Ïƒ2Ï„1Ï„2\\sigma_1\\sigma_2\\tau_1\\tau_2Ïƒ1â€‹Ïƒ2â€‹Ï„1â€‹Ï„2â€‹ : swap @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ2\\sigma_2Ïƒ2â€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„1\\tau_1Ï„1â€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ1Ï„1â€²Ïƒ2â€²Ï„2\\sigma_1\\tau_1'\\sigma_2'\\tau_2Ïƒ1â€‹Ï„1â€²â€‹Ïƒ2â€²â€‹Ï„2â€‹ : satisfy @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ2Ï„1=Ï„1â€²Ïƒ2â€²\\sigma_2\\tau_1=\\tau_1'\\sigma_2'Ïƒ2â€‹Ï„1â€‹=Ï„1â€²â€‹Ïƒ2â€²â€‹ äº¤æ¢åŽï¼Œeach partyå°±å¯ä»¥å°†his pieces ç›¸ä¹˜ï¼Œå¾—åˆ°æœ€ç»ˆç»“æžœçš„pieceã€‚è®¡ç®—ç»“æžœæ—¶ï¼ŒæŒ‰åºç›¸ä¹˜å³å¯ã€‚@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ1â‹¯Ïƒnâ‹…Ï„1â‹¯Ï„n\\sigma_1\\cdots\\sigma_n\\cdot \\tau_1\\cdots\\tau_nÏƒ1â€‹â‹¯Ïƒnâ€‹â‹…Ï„1â€‹â‹¯Ï„nâ€‹ : swap @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒn\\sigma_nÏƒnâ€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„1\\tau_1Ï„1â€‹ move party 1's pieces closerswap @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒn\\sigma_{n}Ïƒnâ€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„1\\tau_1Ï„1â€‹ : @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ1â‹¯Ïƒnâˆ’1Ï„1â€²â‹…Ïƒnâ€²â‹¯Ï„n \\sigma_1\\cdots\\sigma_{n-1}\\tau_1'\\cdot \\sigma_n'\\cdots\\tau_nÏƒ1â€‹â‹¯Ïƒnâˆ’1â€‹Ï„1â€²â€‹â‹…Ïƒnâ€²â€‹â‹¯Ï„nâ€‹ satisfy @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ÏƒnÏ„1=Ï„1â€²Ïƒnâ€²\\sigma_n\\tau_1=\\tau_1'\\sigma_n'Ïƒnâ€‹Ï„1â€‹=Ï„1â€²â€‹Ïƒnâ€²â€‹ swap @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒnâˆ’1\\sigma_{n-1}Ïƒnâˆ’1â€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„1â€²\\tau_1'Ï„1â€²â€‹ swap party n-1's piece and party 1's piece...go on swapping(party 1 final)@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ1â‹…Ï„1â€²â‹¯Ïƒnâˆ’1â€²â‹…Ïƒnâ€²Ï„2â‹¯Ï„n\\sigma_1\\cdot \\tau_1'\\cdots\\sigma_{n-1}'\\cdot \\sigma_n'\\tau_2\\cdots\\tau_nÏƒ1â€‹â‹…Ï„1â€²â€‹â‹¯Ïƒnâˆ’1â€²â€‹â‹…Ïƒnâ€²â€‹Ï„2â€‹â‹¯Ï„nâ€‹ move party 2's pieces closerfinal: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ1â‹…Ï„1â€²â‹¯â‹¯Ïƒnâ€²Ï„n\\sigma_1\\cdot \\tau_1'\\cdots\\cdots\\sigma_n'\\tau_nÏƒ1â€‹â‹…Ï„1â€²â€‹â‹¯â‹¯Ïƒnâ€²â€‹Ï„nâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ÏƒnÏ„1=Ï„1â€²Ïƒnâ€²\\sigma_n\\tau_1=\\tau_1'\\sigma_n'Ïƒnâ€‹Ï„1â€‹=Ï„1â€²â€‹Ïƒnâ€²â€‹ violate the privacy constraint: party 1 and party n tell each other @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒn\\sigma_n Ïƒnâ€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„1\\tau_1Ï„1â€‹ å…¶ä¸­ä¸€ç§å®žçŽ°new piecesçš„æ–¹æ³•æ˜¯party nå’Œparty 1äº’ç›¸å‘ŠçŸ¥å¯¹åº”çš„piecesï¼Œä½†è¿™ä¼šç ´åéšç§æ€§respect the privacy constraint use COT to achieve @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ÏƒnÏ„1=Ï„1â€²Ïƒnâ€²\\sigma_n\\tau_1=\\tau_1'\\sigma_n'Ïƒnâ€‹Ï„1â€‹=Ï„1â€²â€‹Ïƒnâ€²â€‹ (party n)randomly selects a 5-permutation @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï\\rhoÏ party 1 posesses: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ1,Ï„1\\sigma_1,\\tau_1Ïƒ1â€‹,Ï„1â€‹ party n posesses: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒn,Ï„n,Ï\\sigma_n, \\tau_n, \\rhoÏƒnâ€‹,Ï„nâ€‹,Ï function g: for 5-permutations x, y and z @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(x,(y,z))=wwhere wz=yxg(x,(y,z))=w \\quad\\text{where }wz=yxg(x,(y,z))=wwhere wz=yx COTA(party 1): input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')a=Ï„1a=\\tau_1a=Ï„1â€‹ B(party n): input @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')b=(Ïƒn,Ï)b=(\\sigma_n,\\rho)b=(Ïƒnâ€‹,Ï) A(party 1)'s output: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(a,b)g(a,b)g(a,b) B(party n)'s output: noneset new pieces:party 1 : @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒ1,Ï„1â€²=g(a,b)\\sigma_1, \\tau_1'=g(a,b)Ïƒ1â€‹,Ï„1â€²â€‹=g(a,b) party n: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒnâ€²=Ï,Ï„n\\sigma_n'=\\rho,\\tau_nÏƒnâ€²â€‹=Ï,Ï„nâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„1â€²Ïƒnâ€²=g(a,b)â‹…Ï=ÏƒnÏ„1\\tau_1'\\sigma_n'=g(a,b)\\cdot\\rho=\\sigma_n\\tau_1Ï„1â€²â€‹Ïƒnâ€²â€‹=g(a,b)â‹…Ï=Ïƒnâ€‹Ï„1â€‹ Analyze security informallyparty n has no info about party 1's old piece nor the now one party n's new piece is a random @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï\\rhoÏ transfer g(a, b) is obliviousparty 1 has no info about party n'@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(Ï„1,(Ïƒn,Ï))g(\\tau_1,(\\sigma_n,\\rho))g(Ï„1â€‹,(Ïƒnâ€‹,Ï)) : party 1 only knows @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï„1\\tau_1Ï„1â€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')g(x,(y,â‹…)g(x,(y,\\cdot)g(x,(y,â‹…) is injective on S5 and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ï\\rhoÏ is randomly selected by party n Complexityafter n 'swaps': party 1 can get the pieces for the variable @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ïƒâ‹…Ï„\\sigma\\cdot \\tauÏƒâ‹…Ï„ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')O(n2)\\mathcal{O}(n^2)O(n2) End","link":"/paper/%5BGMW87%5D/index.html"},{"title":"","text":"[Beaver91]Efficient Multiplarty Protools Using Circuit Randomization /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 70%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } ðŸ“œ[Beaver91]Efficient Multiplarty Protools Using Circuit Randomization1 IntroductionThe IdeaOne-time tables2 An Efficient Protocol for Circuit EvaluationUnifSecretAdditive GateMultiplicative Gate:Figure 2: the whole protocol2.1 An Optimizationdifference between theory and practice: efficiencyIn practice:express F as a circuit CFcall on each player to secretly share @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xix_ixiâ€‹ proceed to perform &quot; secret addition and multiplication&quot; on secretly shared valuescost: depth of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')CFC_FCFâ€‹ times cost of secret multiplication1 IntroductionSecrete Sharing:Advantage of using polynomialseasy to combine two secrets multiplication by a publicly known constant But when secrets are multiplied: it's hard fan-in: [Electronics]the number of inputs that can be connected to the circuit.Notions: Our Solution:dose evaluate the circuit level by level, but each level is simply a reconstruction of secretsThe Ideacompletely randomize every input and output to each gate in @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')CFC_FCFâ€‹ plus a very simple error-correction procedure error-correcting procedure One-time tablesAnalogous to one-time padOne-time table: a set of precomputed values that support direct secure computation without broadcast or private channels 2 An Efficient Protocol for Circuit Evaluation UnifSecretUnifSecret:generate uniformly random secrets by adding uniformly random secrets shared by each party.Calculate Circuit:Circuitï¼šN wires carry inputsgates @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')gkâˆˆ{+,Ã—}g_k\\in \\{+,\\times\\}gkâ€‹âˆˆ{+,Ã—} : @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xk=gk(xik,xjk)x_k=g_k(x_{i_k},x_{j_k})xkâ€‹=gkâ€‹(xikâ€‹â€‹,xjkâ€‹â€‹) level(@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')gkg_kgkâ€‹ ): depth of the gateevaluate: æŠŠç”µè·¯çš„gatesæŒ‰ç…§depthåˆ†å±‚ï¼Œä¸€å±‚ä¸€å±‚çš„è®¡ç®—ï¼Œæ¯æ¬¡è®¡ç®—å‡ºè¯¥å±‚çš„è¾“å‡ºï¼ˆnew secrets)ï¼Œå°±æ˜¯ä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚ Share-Compute-Reveal Paradigm:evaluate gates at each levelthereby produce new secrets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xkx_kxkâ€‹ , until the final secret @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xNx_NxNâ€‹ is calculateduse REC reconstruct the result @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xNx_NxNâ€‹ Additive GateAn additive gate:create N uniformly random values @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rir_iriâ€‹ for every @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')xix_ixiâ€‹ ç”¨UnifySecretçš„æ–¹å¼ï¼Œevery party generate a random secretï¼Œå…¶å’Œå°±æ˜¯åˆ†é…ç»™æ¯ä¸ªwireçš„random valuesã€‚ ï¼ˆè¿™é‡Œä¸è€ƒè™‘secret sharingï¼Œåªè€ƒè™‘value calculationï¼‰for every gate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')gkg_kgkâ€‹ : compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sk=gk(rik,rjk)s_k=g_k(r_{i_k},r_{j_k})skâ€‹=gkâ€‹(rikâ€‹â€‹,rjkâ€‹â€‹) every partyå¯ä»¥ç›´æŽ¥å¯¹random value (pieces)è®¡ç®—ï¼Œè¿™é‡Œæ˜¯åŠ æ³•é—¨ï¼Œæ‰€ä»¥@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sk=rik+rjks_k=r_{i_k}+r_{j_k}skâ€‹=rikâ€‹â€‹+rjkâ€‹â€‹ ã€‚ä¸è¿‡each partyå¾—åˆ°çš„å…¶å®žæ˜¯PIECE(@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sks_kskâ€‹ )ã€‚consider corrections @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”i=riâˆ’xi\\Delta_i=r_i-x_iÎ”iâ€‹=riâ€‹âˆ’xiâ€‹ to each wire å¯¹äºŽä¸€å±‚gatesè€Œè¨€ï¼Œinput wireçš„corrections (pieces)å¯ä»¥locally calculateã€‚è€Œå½“è¦è®¡ç®—output wireçš„correctionsæ—¶ï¼Œå°±éœ€è¦REC input wireçš„correctionï¼Œæ‰€æœ‰partiesçš„pieces åŠ èµ·æ¥ï¼Œå¾—åˆ°@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”i\\Delta_iÎ”iâ€‹ ã€‚compute the correction @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”k\\Delta_kÎ”kâ€‹ : output wire of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')gk(+)g_k(+)gkâ€‹(+) praty know(pieces): random inputs @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rik,rjkr_{i_k},r_{j_k}rikâ€‹â€‹,rjkâ€‹â€‹ and their results @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sk=rik+rjks_k=r_{i_k}+r_{j_k}skâ€‹=rikâ€‹â€‹+rjkâ€‹â€‹ random output @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rkr_krkâ€‹ input wire corrections @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”ik,Î”jk\\Delta_{i_k}, \\Delta_{j_k}Î”ikâ€‹â€‹,Î”jkâ€‹â€‹ party calculate output wire correction: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”k=rkâˆ’skâˆ’Î”ikâˆ’Î”jk\\Delta_k = r_k-s_k-\\Delta_{i_k}-\\Delta_{j_k}Î”kâ€‹=rkâ€‹âˆ’skâ€‹âˆ’Î”ikâ€‹â€‹âˆ’Î”jkâ€‹â€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”k\\Delta_kÎ”kâ€‹ is a linear combination of &quot;known&quot; constant @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”ik\\Delta_{i_k}Î”ikâ€‹â€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”jk\\Delta_{j_k}Î”jkâ€‹â€‹ with the values @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rkr_krkâ€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sks_kskâ€‹ åˆšåˆšä¹Ÿè¯´è¿‡ï¼Œåœ¨è®¡ç®—è¿™ä¸€å±‚gateså‰ï¼Œéœ€è¦reconstruct input wireçš„correctionï¼Œæ‰€ä»¥è®¡ç®—æ—¶ï¼Œ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”ik\\Delta_{i_k}Î”ikâ€‹â€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”jk\\Delta_{j_k}Î”jkâ€‹â€‹ æ˜¯å·²ç»ç»è¿‡RECæ“ä½œï¼Œevery partyéƒ½æŠŠä»–çš„piecesåˆ†äº«å¤„ç†ï¼ˆä½†ä¸ä¼šreveal infoï¼‰ï¼Œå¾—åˆ° constant @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”ik\\Delta_{i_k}Î”ikâ€‹â€‹ and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”jk\\Delta_{j_k}Î”jkâ€‹â€‹ ã€‚Multiplicative Gate:Multiplicative gates:a linear combination: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”k=rkâˆ’skâˆ’rikÎ”jkâˆ’Î”ikrikâˆ’Î”ikÎ”jk\\Delta_{k}=r_{k}-s_{k}-r_{i_{k}} \\Delta_{j_{k}}-\\Delta_{i_{k}} r_{i_{k}}-\\Delta_{i_{k}} \\Delta_{j_{k}}Î”kâ€‹=rkâ€‹âˆ’skâ€‹âˆ’rikâ€‹â€‹Î”jkâ€‹â€‹âˆ’Î”ikâ€‹â€‹rikâ€‹â€‹âˆ’Î”ikâ€‹â€‹Î”jkâ€‹â€‹ Security: Cost of REC: Figure 2: the whole protocol2.1 An OptimizationCompress additive gates:by computing the corrections to outputs at the same time as the corrections to input wires no &quot;randomization&quot; of additive gatesExample:æŠŠå…ˆåšä¹˜æ³•ï¼Œåœ¨åšä»€ä¹ˆåŠ æ³•ä¸¤å±‚è¿ç®—ï¼ŒåŽ‹ç¼©ä¸ºä¸€å±‚è¿ç®—ã€‚Add gate output correctionå’Œå…¶inputæ²¡æœ‰å…³ç³»ï¼ŒåŒ…æ‹¬@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')r5,r6,Î”5,Î”6r_5,r_6,\\Delta_5,\\Delta_6r5â€‹,r6â€‹,Î”5â€‹,Î”6â€‹ ï¼Œåè€Œæ˜¯å’Œå‰ä¸€å±‚çš„MUL gatesçš„inputæœ‰å…³ç³»ã€‚Deduceï¼š","link":"/paper/%5BBeaver91%5D/index.html"},{"title":"","text":"[ABY2.0]Improved Mixed -Protocol Secure Two-Party Computation /* cspell:disable-file */ /* webkit printing magic: print all background colors */ html { -webkit-print-color-adjust: exact; } * { box-sizing: border-box; -webkit-print-color-adjust: exact; } html, body { margin: 0; padding: 0; } @media only screen { body { margin: 2em auto; max-width: 900px; color: rgb(55, 53, 47); } } body { line-height: 1.5; white-space: pre-wrap; } a, a.visited { color: inherit; text-decoration: underline; } .pdf-relative-link-path { font-size: 80%; color: #444; } h1, h2, h3 { letter-spacing: -0.01em; line-height: 1.2; font-weight: 600; margin-bottom: 0; } .page-title { font-size: 2.5rem; font-weight: 700; margin-top: 0; margin-bottom: 0.75em; } h1 { font-size: 1.875rem; margin-top: 1.875rem; } h2 { font-size: 1.5rem; margin-top: 1.5rem; } h3 { font-size: 1.25rem; margin-top: 1.25rem; } .source { border: 1px solid #ddd; border-radius: 3px; padding: 1.5em; word-break: break-all; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .collection-content { font-size: 0.875rem; } .column-list { display: flex; justify-content: space-between; } .column { padding: 0 1em; } .column:first-child { padding-left: 0; } .column:last-child { padding-right: 0; } .table_of_contents-item { display: block; font-size: 0.875rem; line-height: 1.3; padding: 0.125rem; } .table_of_contents-indent-1 { margin-left: 1.5rem; } .table_of_contents-indent-2 { margin-left: 3rem; } .table_of_contents-indent-3 { margin-left: 4.5rem; } .table_of_contents-link { text-decoration: none; opacity: 0.7; border-bottom: 1px solid rgba(55, 53, 47, 0.18); } table, th, td { border: 1px solid rgba(55, 53, 47, 0.09); border-collapse: collapse; } table { border-left: none; border-right: none; } th, td { font-weight: normal; padding: 0.25em 0.5em; line-height: 1.5; min-height: 1.5em; text-align: left; } th { color: rgba(55, 53, 47, 0.6); } ol, ul { margin: 0; margin-block-start: 0.6em; margin-block-end: 0.6em; } li > ol:first-child, li > ul:first-child { margin-block-start: 0.6em; } ul > li { list-style: disc; } ul.to-do-list { text-indent: -1.7em; } ul.to-do-list > li { list-style: none; } .to-do-children-checked { text-decoration: line-through; opacity: 0.375; } ul.toggle > li { list-style: none; } ul { padding-inline-start: 1.7em; } ul > li { padding-left: 0.1em; } ol { padding-inline-start: 1.6em; } ol > li { padding-left: 0.2em; } .mono ol { padding-inline-start: 2em; } .mono ol > li { text-indent: -0.4em; } .toggle { padding-inline-start: 0em; list-style-type: none; } /* Indent toggle children */ .toggle > li > details { padding-left: 1.7em; } .toggle > li > details > summary { margin-left: -1.1em; } .selected-value { display: inline-block; padding: 0 0.5em; background: rgba(206, 205, 202, 0.5); border-radius: 3px; margin-right: 0.5em; margin-top: 0.3em; margin-bottom: 0.3em; white-space: nowrap; } .collection-title { display: inline-block; margin-right: 1em; } .simple-table { margin-top: 1em; font-size: 0.875rem; } .simple-table-header { background: rgb(247, 246, 243); color: black; font-weight: 500; } time { opacity: 0.5; } .icon { display: inline-block; max-width: 1.2em; max-height: 1.2em; text-decoration: none; vertical-align: text-bottom; margin-right: 0.5em; } img.icon { border-radius: 3px; } .user-icon { width: 1.5em; height: 1.5em; border-radius: 100%; margin-right: 0.5rem; } .user-icon-inner { font-size: 0.8em; } .text-icon { border: 1px solid #000; text-align: center; } .page-cover-image { display: block; object-fit: cover; width: 100%; height: 30vh; } .page-header-icon { font-size: 3rem; margin-bottom: 1rem; } .page-header-icon-with-cover { margin-top: -0.72em; margin-left: 0.07em; } .page-header-icon img { border-radius: 3px; } .link-to-page { margin: 1em 0; padding: 0; border: none; font-weight: 500; } p > .user { opacity: 0.5; } td > .user, td > time { white-space: nowrap; } input[type=\"checkbox\"] { transform: scale(1.5); margin-right: 0.6em; vertical-align: middle; } p { margin-top: 0.5em; margin-bottom: 0.5em; } .paper-image { border: none; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; } .image { border: solid 1px; margin: 1.5em 0; padding: 0; border-radius: 0; text-align: center; width: 50%; } .code, code { background: rgba(135, 131, 120, 0.15); border-radius: 3px; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; tab-size: 2; } code { color: #eb5757; } .code { padding: 1.5em 1em; } .code-wrap { white-space: pre-wrap; word-break: break-all; } .code > code { background: none; padding: 0; font-size: 100%; color: inherit; } blockquote { font-size: 1.25em; margin: 1em 0; padding-left: 1em; border-left: 3px solid rgb(55, 53, 47); } .bookmark { text-decoration: none; max-height: 8em; padding: 0; display: flex; width: 100%; align-items: stretch; } .bookmark-title { font-size: 0.85em; overflow: hidden; text-overflow: ellipsis; height: 1.75em; white-space: nowrap; } .bookmark-text { display: flex; flex-direction: column; } .bookmark-info { flex: 4 1 180px; padding: 12px 14px 14px; display: flex; flex-direction: column; justify-content: space-between; } .bookmark-image { width: 33%; flex: 1 1 180px; display: block; position: relative; object-fit: cover; border-radius: 1px; } .bookmark-description { color: rgba(55, 53, 47, 0.6); font-size: 0.75em; overflow: hidden; max-height: 4.5em; word-break: break-word; } .bookmark-href { font-size: 0.75em; margin-top: 0.25em; } .sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\"; } .code { font-family: \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace; } .serif { font-family: Lyon-Text, Georgia, ui-serif, serif; } .mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; } .pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; } .pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; } .pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; } .pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, \"Apple Color Emoji\", Arial, sans-serif, \"Segoe UI Emoji\", \"Segoe UI Symbol\", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; } .pdf .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .code { font-family: Source Code Pro, \"SFMono-Regular\", Menlo, Consolas, \"PT Mono\", \"Liberation Mono\", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; } .pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; } .pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; } .pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; } .pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; } .pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; } .pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; } .pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; } .highlight-default { color: rgba(55, 53, 47, 1); } .highlight-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .highlight-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .highlight-gray_background { background: rgba(241, 241, 239, 1); } .highlight-brown_background { background: rgba(244, 238, 238, 1); } .highlight-orange_background { background: rgba(251, 236, 221, 1); } .highlight-yellow_background { background: rgba(251, 243, 219, 1); } .highlight-teal_background { background: rgba(237, 243, 236, 1); } .highlight-blue_background { background: rgba(231, 243, 248, 1); } .highlight-purple_background { background: rgba(244, 240, 247, 0.8); } .highlight-pink_background { background: rgba(249, 238, 243, 0.8); } .highlight-red_background { background: rgba(253, 235, 236, 1); } .block-color-default { color: inherit; fill: inherit; } .block-color-gray { color: rgba(120, 119, 116, 1); fill: rgba(145, 145, 142, 1); } .block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(187, 132, 108, 1); } .block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(215, 129, 58, 1); } .block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 148, 51, 1); } .block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(108, 155, 125, 1); } .block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(91, 151, 189, 1); } .block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(167, 130, 195, 1); } .block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(205, 116, 159, 1); } .block-color-red { color: rgba(212, 76, 71, 1); fill: rgba(225, 111, 100, 1); } .block-color-gray_background { background: rgba(241, 241, 239, 1); } .block-color-brown_background { background: rgba(244, 238, 238, 1); } .block-color-orange_background { background: rgba(251, 236, 221, 1); } .block-color-yellow_background { background: rgba(251, 243, 219, 1); } .block-color-teal_background { background: rgba(237, 243, 236, 1); } .block-color-blue_background { background: rgba(231, 243, 248, 1); } .block-color-purple_background { background: rgba(244, 240, 247, 0.8); } .block-color-pink_background { background: rgba(249, 238, 243, 0.8); } .block-color-red_background { background: rgba(253, 235, 236, 1); } .select-value-color-pink { background-color: rgba(245, 224, 233, 1); } .select-value-color-purple { background-color: rgba(232, 222, 238, 1); } .select-value-color-green { background-color: rgba(219, 237, 219, 1); } .select-value-color-gray { background-color: rgba(227, 226, 224, 1); } .select-value-color-orange { background-color: rgba(250, 222, 201, 1); } .select-value-color-brown { background-color: rgba(238, 224, 218, 1); } .select-value-color-red { background-color: rgba(255, 226, 221, 1); } .select-value-color-yellow { background-color: rgba(253, 236, 200, 1); } .select-value-color-blue { background-color: rgba(211, 229, 239, 1); } .checkbox { display: inline-flex; vertical-align: text-bottom; width: 16; height: 16; background-size: 16px; margin-left: 2px; margin-right: 5px; } .checkbox-on { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E\"); } .checkbox-off { background-image: url(\"data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E\"); } ðŸ“œ[ABY2.0]Improved Mixed -Protocol Secure Two-Party ComputationABSTRACT1 Introduction1.1 Our ContributionsMixed Protocol ConversionsBuilding Blocks1.2 Related Work2 Preliminaries3 2PC in Arithmetic, Boolean and Yao's World3.1 2PC in Arithmetic World3.1.2 Sharing Semantics3.1.3 ProtocolsSharing ProtocolRECLinear OperationsMultiplication ProtocolFigure 2: Multiplication Protocol3.1.1 High-level Overview of Our 2PC over RingFigure 1: High Level overview of Beaver's and ABY2.03.1.4 Multi-Input Multiplications Gates3.2 2PC in Boolean World3.3 2PC in Yao World4 Mixed Protocol Conversions4.1 Standard ConversionsY2BB2YA2YY2AA2BBit2AB2A4.2 Special Conversions5 Building Blocks for Applications5.1 Scalar Product5.2 Matrix Multiplication5.3 Depth-Optimized Circuits5.4 Comparison5.5 TruncationABSTRACTContribution:an efficient mixed-protocol frameworkextend to multi-input multiplications gatesconstruct eff. protocols for several primitives scalar product, matrix multiplication, comparison, maxpool, and equality testing.1 IntroductionCategories:low-latency: using Yao's GC result in constant-round solutionshigh-throughput: using Secret-sharing(SS) conmmunication rounds linear in the multiplicative depth of the citcuitMixed-protocol blocks:blocks: Arithmetic; Boolean; Yaoinput:A performs ops on fieldsB and Y perform ops on bitsapproach:A and B using SS-based approachY using GC-based approach Practical runtimes: Beaver multiplication triplesåœ¨input-independent setup phase ç”Ÿæˆä¸€äº›ç›¸å…³çš„éšæœºç»„ï¼ˆBeaver multiplication tripleï¼‰ï¼Œè¿™äº›ç»„ä¼šåœ¨input-dependent online phaseä½¿ç”¨ï¼ŒåŠ å¿«onlineçš„å¤„ç†é€Ÿåº¦ã€‚Beaver multiplication triples: [9]D. Beaver. Efï¬cient multiparty protocols using circuit randomization. In CRYPTO, 1991.1.1 Our Contributionsan eff. mixed-protocol framework for secure 2PC over an l-bit ringsecure against a semi-honest adv.use an input-independent setupfocus on online efficiencyComparison of ABY2.0 and other 2PC protocols[41] ABY: [13]SPDZ:2-input multiplication: same complexity as SPDZ but using a different approach.N-input multiplication gate: constant cost of 2 ring elements and 1 round of itertationsMixed Protocol Conversionsreduces the number of online rounds from 2 to 1 use correlated OTs(cOT) Optimization: cOT[5]cOT Building Blocks 1.2 Related WorkEff. SS-based solutions for the dishonest majority over fields [40] [56] Extended to the ring [35][82] extended the multiplication from 2-input to N-input using Beaver triple extension 2 Preliminaries 3 2PC in Arithmetic, Boolean and Yao's World3.1 2PC in Arithmetic Worldhighlight: its effectiveness towards efficient realisations for multiple input multiplication gates and dot product operations. 3.1.2 Sharing SemanticsSharing Notions:&lt;Â·&gt;-sharing: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')([Î´v]i,Î”v)\\left(\\left[\\delta_{v}\\right]_{i}, \\Delta_{v}\\right)([Î´vâ€‹]iâ€‹,Î”vâ€‹) @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î´v\\delta_vÎ´vâ€‹ is [Â·]-shared among P0, P1@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”v=v+Î´v\\Delta_{v}=v+\\delta_{v}Î”vâ€‹=v+Î´vâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”v\\Delta_vÎ”vâ€‹ is known to both P0, P1 in clean3.1.3 ProtocolsSharing Protocolenable party @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPiâ€‹ to generate a &lt;Â·&gt;-sharing of its input value v@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPiâ€‹ generate a &lt;Â·&gt;-sharing of its input value v(setup)generate a [Â·]-shared: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´v][\\delta_v][Î´vâ€‹] @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPiâ€‹ samples random @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´v]i[\\delta_v]_i[Î´vâ€‹]iâ€‹ two parties together sample @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´v]1âˆ’i[\\delta_v]_{1-i}[Î´vâ€‹]1âˆ’iâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPiâ€‹ knows @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î´v=[Î´v]0+[Î´v]1\\delta_{v}=\\left[\\delta_{v}\\right]_{0}+\\left[\\delta_{v}\\right]_{1}Î´vâ€‹=[Î´vâ€‹]0â€‹+[Î´vâ€‹]1â€‹ (online) @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”v\\Delta_vÎ”vâ€‹ is konwn to both@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPiâ€‹ computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”v=v+Î´v\\Delta_{v}=v+\\delta_{v}Î”vâ€‹=v+Î´vâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')PiP_iPiâ€‹ sends it to @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')P1âˆ’iP_{1-i}P1âˆ’iâ€‹ RECmutually exchange [Â·] share of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î´v\\delta_vÎ´vâ€‹ Linear OperationsGiven &lt;a&gt;, &lt;b&gt; and public constants @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')c1,c2c_1,c_2c1â€‹,c2â€‹ .Parties can locally compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨yâŸ©=c1â‹…âŸ¨aâŸ©+c2â‹…âŸ¨bâŸ©\\langle\\mathbf{y}\\rangle=c_{1} \\cdot\\langle\\mathrm{a}\\rangle+c_{2} \\cdot\\langle\\mathrm{b}\\rangleâŸ¨yâŸ©=c1â€‹â‹…âŸ¨aâŸ©+c2â€‹â‹…âŸ¨bâŸ© Pi locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”y=c1â‹…Î”a+c2â‹…Î”b\\Delta_{\\mathrm{y}}=c_{1} \\cdot \\Delta_{\\mathrm{a}}+c_{2} \\cdot \\Delta_{\\mathrm{b}}Î”yâ€‹=c1â€‹â‹…Î”aâ€‹+c2â€‹â‹…Î”bâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”a,Î”b\\Delta_a,\\Delta_bÎ”aâ€‹,Î”bâ€‹ is known constantsthen @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´y]i=c1â‹…[Î´a]i+c2â‹…[Î´b]i\\left[\\delta_{y}\\right]_{i}=c_{1} \\cdot\\left[\\delta_{a}\\right]_{i}+c_{2} \\cdot\\left[\\delta_{b}\\right]_{i}[Î´yâ€‹]iâ€‹=c1â€‹â‹…[Î´aâ€‹]iâ€‹+c2â€‹â‹…[Î´bâ€‹]iâ€‹ Multiplication Protocol@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”a,Î”b\\Delta_a,\\Delta_bÎ”aâ€‹,Î”bâ€‹ are consts.They can compute a []-sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”y\\Delta_yÎ”yâ€‹ if the parties obtain [Â·]-sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î´ab=Î´aÎ´b\\delta_{ab}=\\delta_a\\delta_bÎ´abâ€‹=Î´aâ€‹Î´bâ€‹ The problem of multiplication reduces to generating @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´ab][\\delta_{ab}][Î´abâ€‹] given @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´a][\\delta_a][Î´aâ€‹] and @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´b][\\delta_b][Î´bâ€‹] Figure 2: Multiplication ProtocolOT based setup MULTCorrelated OTs(cOT) [5]Execute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')cOTll\\mathrm{cOT}_l^lcOTllâ€‹ to [Â·] sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´a]0[Î´b]1\\left[\\delta_{\\mathrm{a}}\\right]_{0}\\left[\\delta_{\\mathrm{b}}\\right]_{1}[Î´aâ€‹]0â€‹[Î´bâ€‹]1â€‹ For j : 0 ... @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')lâˆ’1l-1lâˆ’1 ( j-th instance of cOT)P0(sender):inputs correlation function: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')fj(x)=x+2j[Î´a]0f_{j}(x)=x+2^{j}\\left[\\delta_{a}\\right]_{0}fjâ€‹(x)=x+2j[Î´aâ€‹]0â€‹ obtains: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(mj,0=rj,mj,1=rj+2j[Î´a]0)\\left(m_{j, 0}=r_{j}, m_{j, 1}=\\right.r_{j}+2^{j}\\left[\\delta_{a}\\right]_{0})(mj,0â€‹=rjâ€‹,mj,1â€‹=rjâ€‹+2j[Î´aâ€‹]0â€‹) P1(receiver):inputs choice bit @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')bjb_jbjâ€‹ : the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')jjj -th bit of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´b]1[\\delta_b]_1[Î´bâ€‹]1â€‹ obtains: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')mj,bjm_{j,b_j}mj,bjâ€‹â€‹ [Â·] shareP0: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[([Î´a]0[Î´b]1)]0=âˆ‘j=0â„“âˆ’1(âˆ’rj)\\left[\\left(\\left[\\delta_{\\mathrm{a}}\\right]_{0}\\left[\\delta_{\\mathrm{b}}\\right]_{1}\\right)\\right]_{0}=\\sum_{j=0}^{\\ell-1}\\left(-r_{j}\\right)[([Î´aâ€‹]0â€‹[Î´bâ€‹]1â€‹)]0â€‹=âˆ‘j=0â„“âˆ’1â€‹(âˆ’rjâ€‹) P1: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[([Î´a]0[Î´b]1)]1=âˆ‘j=0â„“âˆ’1mj,bj\\left[\\left(\\left[\\delta_{\\mathrm{a}}\\right]_{0}\\left[\\delta_{\\mathrm{b}}\\right]_{1}\\right)\\right]_{1}=\\sum_{j=0}^{\\ell-1} m_{j, b_{j}}[([Î´aâ€‹]0â€‹[Î´bâ€‹]1â€‹)]1â€‹=âˆ‘j=0â„“âˆ’1â€‹mj,bjâ€‹â€‹ HE-based setupMULT:P0 : use pk0 encrypts its msg @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´a]0,[Î´b]0\\left[\\delta_{\\mathrm{a}}\\right]_{0},\\left[\\delta_{\\mathrm{b}}\\right]_{0}[Î´aâ€‹]0â€‹,[Î´bâ€‹]0â€‹ P1: use pk0 encrypts @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´a]1,[Î´b]1\\left[\\delta_{\\mathrm{a}}\\right]_{1},\\left[\\delta_{\\mathrm{b}}\\right]_{1}[Î´aâ€‹]1â€‹,[Î´bâ€‹]1â€‹ and random element @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rrr P0: sends ciphertext to P1P1: computes ciphertext correspnding to v: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=[Î´2]0[Î´b]1+[Î´a]1[Î´b]0âˆ’rv=\\left[\\delta_{2}\\right]_{0}\\left[\\delta_{b}\\right]_{1}+\\left[\\delta_{a}\\right]_{1}\\left[\\delta_{b}\\right]_{0}-rv=[Î´2â€‹]0â€‹[Î´bâ€‹]1â€‹+[Î´aâ€‹]1â€‹[Î´bâ€‹]0â€‹âˆ’r (HE) and sends encryption of v to P0P0: use sk0 decrypt it[Â·] shareP0: vP1: r@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´a]0[Î´b]1+[Î´a]1[Î´b]0=v+r\\left[\\delta_{\\mathrm{a}}\\right]_{0}\\left[\\delta_{\\mathrm{b}}\\right]_{1}+\\left[\\delta_{\\mathrm{a}}\\right]_{1}\\left[\\delta_{\\mathrm{b}}\\right]_{0}=\\mathrm{v}+r[Î´aâ€‹]0â€‹[Î´bâ€‹]1â€‹+[Î´aâ€‹]1â€‹[Î´bâ€‹]0â€‹=v+r 3.1.1 High-level Overview of Our 2PC over RingBeaver's technique on gates inputsBeaver's multiplication triples: [9]Setup Phase:interactively generate the multiplication triple: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(Î´a,Î´b,Î´ab) with Î´ab=Î´aÎ´b(\\left.\\delta_{a}, \\delta_{b}, \\delta_{a b}\\right) \\text { with } \\delta_{a b}=\\delta_{a} \\delta_{b}(Î´aâ€‹,Î´bâ€‹,Î´abâ€‹) with Î´abâ€‹=Î´aâ€‹Î´bâ€‹ Online Phase:mutually exchange the shares of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”a,Î”b\\Delta_a,\\Delta_bÎ”aâ€‹,Î”bâ€‹ compute an additive sharing @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”a,Î”b\\Delta_a,\\Delta_bÎ”aâ€‹,Î”bâ€‹ locally compute a sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aâ‹…ba\\cdot baâ‹…b Figure 1: High Level overview of Beaver's and ABY2.0requires communicating 4 elements per multiplication(2 elements per reconstruction) REC @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”a,Î”b\\Delta_a,\\Delta_bÎ”aâ€‹,Î”bâ€‹ Our technique on gate outputs:sharing semantics ensure the parties to have the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”\\DeltaÎ” value the reconstructions of input wires are no longer requiredBut inorder to proceed further, a sharing a y needs to be generated. So parties locally compute an additive sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”y\\Delta_yÎ”yâ€‹ and exchange the shares to reconstruct @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î”y\\Delta_yÎ”yâ€‹ Main Idea:shifs the need of reconstruction from per input wire to the output wire.For a fan-in 2, it reduces the number of reconstructions from 2 to 1.3.1.4 Multi-Input Multiplications Gates3-Input Multiplication gate:need to generate the [Â·]-sharing of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î´ab,Î´bc,Î´ac and Î´abc\\delta_{a b}, \\delta_{b c}, \\delta_{a c} \\text { and } \\delta_{a b c}Î´abâ€‹,Î´bcâ€‹,Î´acâ€‹ and Î´abcâ€‹ Multi-Input Multiplication gateextend to N-input without inflating the online communication.3.2 2PC in Boolean Worldin a Boolean ring @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Z21\\Z_{2^1}Z21â€‹ replace additions with XORS and multiplications with ANDSNOT: Negation Protocol3.3 2PC in Yao WorldMain:Yao World from ABY [41]For a wire with @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')vâˆˆ0,1v\\in0,1vâˆˆ0,1 P0 (garbler): zero-key on the wire@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâŸ©0=Ku0\\langle v\\rangle_{0}=\\mathrm{K}_{\\mathrm{u}}^{0}âŸ¨vâŸ©0â€‹=Ku0â€‹ P1 (evaluator): actual key @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâŸ©1=Kuv\\langle v\\rangle_{1}=\\mathrm{K}_{\\mathrm{u}}^{v}âŸ¨vâŸ©1â€‹=Kuvâ€‹ GC Optimizationfree-XOR: [70] Point-and-permute: [11] 4 Mixed Protocol Conversions4.1 Standard ConversionsHighlight:invoke OT in the setup phase onlyY2BGoal: generate equivalent Boolean sharing given Yao sharingin Yao: LSB of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Ku0 and Kuu\\mathrm{K}_{\\mathrm{u}}^{0} \\text { and } \\mathrm{K}_{\\mathrm{u}}^{\\mathrm{u}}Ku0â€‹ and Kuuâ€‹ can reveal u free-XOR property: last bit of zero and one key are distinctY2B: convert: each party Boolean-shares the LSB of their @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨uâŸ©iY\\langle\\mathrm{u}\\rangle_{i}^{\\mathbf{Y}}âŸ¨uâŸ©iYâ€‹ obtain u: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨uâŸ©B=âŸ¨uâŸ©0BâŠ•âŸ¨uâŸ©1B\\langle\\mathbf{u}\\rangle^{\\mathbf{B}}=\\left\\langle\\mathbf{u}\\right\\rangle_0^{\\mathbf{B}} \\oplus\\left\\langle\\mathbf{u}\\right\\rangle_1^{\\mathbf{B}}âŸ¨uâŸ©B=âŸ¨uâŸ©0Bâ€‹âŠ•âŸ¨uâŸ©1Bâ€‹ optimization: P0å¯ä»¥åœ¨setup phase Boolean-shares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')LSBâ¡(Ku0)\\operatorname{LSB}\\left(\\mathrm{K}_{\\mathrm{u}}^{0}\\right)LSB(Ku0â€‹) ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªéšæœºé€‰çš„labelï¼Œè€Œå¦ä¸€ä¸ªæŒ‡depends on the value.Q: æ˜¯å¦æ³„æ¼äº†ä¸­é—´å€¼ï¼Ÿæ€»ç»“æ¥è¯´ï¼Œå°±æ˜¯Yå…ˆè½¬æ¢ä¸ºBä¸­çš„è¡¨ç¤ºï¼Œå†æ ¹æ®æˆç«‹çš„ç­‰å¼ï¼Œç”¨Bä¸­çš„è¿ç®—å¾—åˆ°ç›¸åŒçš„å€¼ã€‚ å…¶ä»–ä¹Ÿç±»ä¼¼ã€‚B2Yin Boolean :Pi: locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ui=(1âˆ’i)â‹…Î”uâŠ•[Î´u]i\\mathrm{u}_{i}=(1-i) \\cdot \\Delta_{\\mathrm{u}} \\oplus\\left[\\delta_{\\mathrm{u}}\\right]_{i}uiâ€‹=(1âˆ’i)â‹…Î”uâ€‹âŠ•[Î´uâ€‹]iâ€‹ P0: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')u0=Î”uâŠ•[Î´u]0u_0=\\Delta_u\\oplus [\\delta_u]_0u0â€‹=Î”uâ€‹âŠ•[Î´uâ€‹]0â€‹ P1: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')u1=[Î´u]1u_1=[\\delta_u]_1u1â€‹=[Î´uâ€‹]1â€‹ satisfy: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')u=u0âŠ•u1u=u_0\\oplus u_1u=u0â€‹âŠ•u1â€‹ B2Y:convert: each party Yao-shares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')uiu_iuiâ€‹ obtain u: compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨uâŸ©Y=âŸ¨u0âŸ©YâŠ•âŸ¨u1âŸ©Y\\langle\\mathbf{u}\\rangle^{\\mathbf{Y}}=\\left\\langle\\mathbf{u}_{0}\\right\\rangle^{\\mathbf{Y}} \\oplus\\left\\langle\\mathbf{u}_{1}\\right\\rangle^{\\mathbf{Y}}âŸ¨uâŸ©Y=âŸ¨u0â€‹âŸ©YâŠ•âŸ¨u1â€‹âŸ©Y optimization: P1å¯ä»¥åœ¨setup phase Yao-shares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')u1=[Î´u]1u_1=[\\delta_u]_1u1â€‹=[Î´uâ€‹]1â€‹ ï¼Œå› ä¸ºè¿™åœ¨setup phaseå¯ä»¥ç¡®å®šã€‚ A2Yin Arithmeticï¼šPi: locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')vi=(1âˆ’i)â‹…Î”vâˆ’[Î´v]i\\mathrm{v}_{i}=(1-i) \\cdot \\Delta_{\\mathrm{v}}-\\left[\\delta_{\\mathrm{v}}\\right]_{i}viâ€‹=(1âˆ’i)â‹…Î”vâ€‹âˆ’[Î´vâ€‹]iâ€‹ satisfy: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=v0+v1\\mathrm{v=v_0+v_1}v=v0â€‹+v1â€‹ A2Y:convert: each party Yao-shares viobtain vY2Asetup phase: P0P0 sample a random value rP0 generate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨râŸ©Y and âŸ¨râŸ©A\\langle r\\rangle^{\\mathbf{Y}} \\text { and }\\langle r\\rangle^{\\mathbf{A}}âŸ¨râŸ©Y and âŸ¨râŸ©A P0 garbles an Adder circuit and sends it to P1 (along with output decoding info)online phase: P1 evalute circuit:input: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâŸ©Y and âŸ¨râŸ©Y\\langle\\mathbf{v}\\rangle^{\\mathbf{Y}} \\text { and }\\langle r\\rangle^{\\mathbf{Y}}âŸ¨vâŸ©Y and âŸ¨râŸ©Y output: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨v+râŸ©Y\\langle\\mathrm{v}+r\\rangle^{\\mathbf{Y}}âŸ¨v+râŸ©Y decode: (v+r) in clearobtain:P1 Arithmetic-shares (v+r)parties compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâŸ©A=âŸ¨v+râŸ©Aâˆ’âŸ¨râŸ©A\\langle v\\rangle^{\\mathbf{A}}=\\langle v+r\\rangle^{\\mathbf{A}}-\\langle r\\rangle^{\\mathbf{A}}âŸ¨vâŸ©A=âŸ¨v+râŸ©Aâˆ’âŸ¨râŸ©A A2Bsimilar to A2Y, but it results in a non-constant round protocol (dependent on @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')lll )constant-round solution: use Y2B(A2Y(Â·))Bit2Arelation in Bit and A: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=v0âŠ•v1\\mathrm{v}=\\mathrm{v}_{0} \\oplus \\mathrm{v}_{1}v=v0â€‹âŠ•v1â€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')va=v0a+v1aâˆ’2v0av1a\\mathrm{v^{a}=v_{0}^{a}+v_{1}^{a}-2 v_{0}^{a} v_{1}^{a}}va=v0aâ€‹+v1aâ€‹âˆ’2v0aâ€‹v1aâ€‹ so: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')va=(Î”vâŠ•Î´v)a=Î”va+Î´vaâˆ’2Î”vaÎ´va\\mathrm{v^{a}=\\left(\\Delta_{v} \\oplus \\delta_{v}\\right)^{a}=\\Delta_{v}^{a}+\\delta_{v}^{a}-2 \\Delta_{v}^{a} \\delta_{v}^{a} }va=(Î”vâ€‹âŠ•Î´vâ€‹)a=Î”vaâ€‹+Î´vaâ€‹âˆ’2Î”vaâ€‹Î´vaâ€‹ setup phase: parties interactively generate share: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´va]\\mathrm{[\\delta_v^a]}[Î´vaâ€‹] @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î´v=[Î´v]0âŠ•[Î´v]1\\mathrm{\\delta_{v}=\\left[\\delta_{v}\\right]_{0} \\oplus\\left[\\delta_{v}\\right]_{1}}Î´vâ€‹=[Î´vâ€‹]0â€‹âŠ•[Î´vâ€‹]1â€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Î´va=[Î´va]0+[Î´va]1âˆ’2([Î´va]0[Î´va]1)\\mathrm{\\delta_{v}^{a}=\\left[\\delta_{v}^{a}\\right]_{0}+\\left[\\delta_{v}^{a}\\right]_{1}-2\\left(\\left[\\delta_{v}^{a}\\right]_{0}\\left[\\delta_{v}^{a}\\right]_{1}\\right)}Î´vaâ€‹=[Î´vaâ€‹]0â€‹+[Î´vaâ€‹]1â€‹âˆ’2([Î´vaâ€‹]0â€‹[Î´vaâ€‹]1â€‹) execute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')cOTl1\\mathrm{cOT}_l^1cOTl1â€‹ to share @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´va]0[Î´va]1\\left[\\delta_{v}^{a}\\right]_{0}\\left[\\delta_{v}^{a}\\right]_{1}[Î´vaâ€‹]0â€‹[Î´vaâ€‹]1â€‹ :P0 (sender): inputs correlation @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')fj(x)=x+[Î´v]0af_{j}(x)=x+\\left[\\delta_{v}\\right]_{0}^{a}fjâ€‹(x)=x+[Î´vâ€‹]0aâ€‹ and obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')(s0=r,s1=r+[Î´v]0a)\\left(s_{0}=r, s_{1}=r+\\left[\\delta_{v}\\right]_{0}^{a}\\right)(s0â€‹=r,s1â€‹=r+[Î´vâ€‹]0aâ€‹) P1 (reciever): inputs the choice bit as @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´v]1\\left[\\delta_{v}\\right]_{1}[Î´vâ€‹]1â€‹ and obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')s[Î´v]1=r+[Î´v]1â‹…[Î´v]0as_{[\\delta_v]_1}=r+\\left[\\delta_{v}\\right]_{1} \\cdot\\left[\\delta_{v}\\right]_{0}^{a}s[Î´vâ€‹]1â€‹â€‹=r+[Î´vâ€‹]1â€‹â‹…[Î´vâ€‹]0aâ€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´v]1=0\\left[\\delta_{v}\\right]_{1}=0[Î´vâ€‹]1â€‹=0 : obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')s0=rs_0=rs0â€‹=r @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´v]1=1\\left[\\delta_{v}\\right]_{1}=1[Î´vâ€‹]1â€‹=1 : obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')s0=r+[Î´v]0as_0=r+[\\delta_v]_0^as0â€‹=r+[Î´vâ€‹]0aâ€‹ P0 locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[([Î´v]0a[Î´v]1a)]0=âˆ’r\\left[\\left(\\left[\\boldsymbol{\\delta}_{v}\\right]_{0}^{a}\\left[\\boldsymbol{\\delta}_{v}\\right]_{1}^{a}\\right)\\right]_{0}=-r[([Î´vâ€‹]0aâ€‹[Î´vâ€‹]1aâ€‹)]0â€‹=âˆ’r P1 locally sets @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[([Î´v]0a[Î´v]1a)]0=s[Î´v]1\\left[\\left(\\left[\\boldsymbol{\\delta}_{\\mathrm{v}}\\right]_{0}^{\\mathrm{a}}\\left[\\boldsymbol{\\delta}_{\\mathrm{v}}\\right]_{1}^{\\mathrm{a}}\\right)\\right]_{0}=s_{\\left[\\delta_{\\mathrm{v}}\\right]_{1}}[([Î´vâ€‹]0aâ€‹[Î´vâ€‹]1aâ€‹)]0â€‹=s[Î´vâ€‹]1â€‹â€‹ Pi locally sets [Â·]-share of @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î´va]i=(1âˆ’i)â‹…[Î´v]0a+iâ‹…[Î´v]1aâˆ’2[([Î´v]0a[Î´v]1a)]i[\\delta_v^a]_i=(1-i)\\cdot[\\delta_v]_0^a+i\\cdot \\left[\\delta_{v}\\right]_{1}^{a}-2\\left[\\left(\\left[\\delta_{v}\\right]_{0}^{a}\\left[\\delta_{v}\\right]_{1}^{a}\\right)\\right]_{i}[Î´vaâ€‹]iâ€‹=(1âˆ’i)â‹…[Î´vâ€‹]0aâ€‹+iâ‹…[Î´vâ€‹]1aâ€‹âˆ’2[([Î´vâ€‹]0aâ€‹[Î´vâ€‹]1aâ€‹)]iâ€‹ online phase:convert: Pi locally computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[va]i=iâ‹…Î”va+(1âˆ’2Î”va)â‹…[Î´va]i\\left[\\mathrm{v}^{\\mathrm{a}}\\right]_{i}=i \\cdot \\Delta_{\\mathrm{v}}^{\\mathrm{a}}+\\left(1-2 \\Delta_{\\mathrm{v}}^{\\mathrm{a}}\\right) \\cdot\\left[\\delta_{\\mathrm{v}}^{\\mathrm{a}}\\right]_{i}[va]iâ€‹=iâ‹…Î”vaâ€‹+(1âˆ’2Î”vaâ€‹)â‹…[Î´vaâ€‹]iâ€‹ and Arithmetic-shares @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[va]i\\left[\\mathrm{v}^{\\mathrm{a}}\\right]_{i}[va]iâ€‹ obtain @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')va\\mathrm{v^a}va : @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vaâŸ©A=âŸ¨[va]0âŸ©A+âŸ¨[va]1âŸ©A\\left\\langle\\mathrm{v}^{\\mathrm{a}}\\right\\rangle^{\\mathbf{A}}=\\left\\langle\\left[\\mathrm{v}^{\\mathrm{a}}\\right]_{0}\\right\\rangle^{\\mathbf{A}}+\\left\\langle\\left[\\mathrm{v}^{\\mathrm{a}}\\right]_{1}\\right\\rangle^{\\mathbf{A}}âŸ¨vaâŸ©A=âŸ¨[va]0â€‹âŸ©A+âŸ¨[va]1â€‹âŸ©A B2Asetup phase: P0P0 sample a random value rP0 generate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨râŸ©B and âŸ¨râŸ©A\\langle r\\rangle^{\\mathbf{B}} \\text { and }\\langle r\\rangle^{\\mathbf{A}}âŸ¨râŸ©B and âŸ¨râŸ©A P0 garbles a boolean circuit and sends it to P1 (along with output decoding info)online phase: P1 evalute circuit:input: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâŸ©B and âŸ¨râŸ©B\\langle\\mathbf{v}\\rangle^{\\mathbf{B}} \\text { and }\\langle r\\rangle^{\\mathbf{B}}âŸ¨vâŸ©B and âŸ¨râŸ©B output: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâˆ’râŸ©B\\langle\\mathrm{v}-r\\rangle^{\\mathbf{B}}âŸ¨vâˆ’râŸ©B decode: (v-r) in clearobtain:P1 Arithmetic-shares (v-r)parties compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâŸ©A=âŸ¨vâˆ’râŸ©A+âŸ¨râŸ©A\\langle v\\rangle^{\\mathbf{A}}=\\langle v-r\\rangle^{\\mathbf{A}}+\\langle r\\rangle^{\\mathbf{A}}âŸ¨vâŸ©A=âŸ¨vâˆ’râŸ©A+âŸ¨râŸ©A But it results in a non-constant round protocol in the online phase.A novel round makes uses of the Bit2A protocol.setup phase:fact: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v=âˆ‘j=0â„“âˆ’12jâ‹…v[j]\\mathbf{v}=\\sum_{j=0}^{\\ell-1} 2^{j} \\cdot \\mathbf{v}[j]v=âˆ‘j=0â„“âˆ’1â€‹2jâ‹…v[j] ( v[j] denotes the jth bit of v)parties possess @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨v[j]âŸ©B for each jâˆˆ[0,â„“)\\langle v[j]\\rangle^{\\mathbf{B}} \\text { for each } j \\in[0, \\ell)âŸ¨v[j]âŸ©B for each jâˆˆ[0,â„“) : execute l instances of Bit2A conversionsonline phase:for each bit v[j], parties locally compute the [Â·]-sharing: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[(v[j])a][(v[j])^a][(v[j])a] Bit2A: @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')v[j]a=(Î”v[j]âŠ•Î´v[j])a=Î”v[j]a+Î´v[j]aâˆ’2Î”v[j]aÎ´v[j]a\\mathrm{v[j]^{a}=\\left(\\Delta_{v[j]} \\oplus \\delta_{v[j]}\\right)^{a}=\\Delta_{v[j]}^{a}+\\delta_{v[j]}^{a}-2 \\Delta_{v[j]}^{a} \\delta_{v[j]}^{a} }v[j]a=(Î”v[j]â€‹âŠ•Î´v[j]â€‹)a=Î”v[j]aâ€‹+Î´v[j]aâ€‹âˆ’2Î”v[j]aâ€‹Î´v[j]aâ€‹ each party locally computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[v]i=âˆ‘j=0â„“âˆ’12jâ‹…[(v[j])a]i[\\mathrm{v}]_{i}=\\sum_{j=0}^{\\ell-1} 2^{j} \\cdot\\left[(\\mathrm{v}[j])^{a}\\right]_{i}[v]iâ€‹=âˆ‘j=0â„“âˆ’1â€‹2jâ‹…[(v[j])a]iâ€‹ each party geneate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨[v]iâŸ©A\\left\\langle[\\mathbf{v}]_{i}\\right\\rangle^{A}âŸ¨[v]iâ€‹âŸ©A parties locally computes @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâŸ©A=âŸ¨[v0]âŸ©A+âŸ¨[v1]âŸ©A\\langle\\mathbf{v}\\rangle^{\\mathbf{A}}=\\left\\langle\\left[\\mathbf{v}_{0}\\right]\\right\\rangle^{\\mathbf{A}}+\\left\\langle\\left[\\mathbf{v}_{1}\\right]\\right\\rangle^{\\mathbf{A}}âŸ¨vâŸ©A=âŸ¨[v0â€‹]âŸ©A+âŸ¨[v1â€‹]âŸ©A 4.2 Special Conversions 5 Building Blocks for Applications5.1 Scalar Productæœ‰ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨nä¸ª3.1.3èŠ‚çš„ä¹˜æ³•ï¼Œä½†è¿™æ ·online communicationçš„å¼€é”€ä¼šå’Œvector size nçš„å¤§å°æœ‰å…³ï¼Œå³æ¯æ¬¡è®¡ç®—å‡ºä¸¤ä¸ªå‘é‡å¯¹åº”å…ƒç´ çš„ä¹˜ç§¯çš„&lt;Â·&gt;-shareï¼Œæœ€åŽæœ¬åœ°åŠ èµ·æ¥ã€‚åšå‡ºçš„ä¼˜åŒ–ï¼Œå…¶å®žå°±æ˜¯å…ˆæœ¬åœ°æ±‚å’Œï¼Œå†shareï¼›make the online communication independent of the vector size5.2 Matrix MultiplicationNotions:MATMULT: å’Œä¹˜æ³•ç±»ä¼¼ï¼Œå”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯è®¡ç®— @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[Î³AB][\\mathrm{\\gamma_{AB}}][Î³ABâ€‹] 5.3 Depth-Optimized CircuitsParallel-prefix Adders (PPA) [50] PPAç”µè·¯å¯ä»¥è¢«ä¼˜åŒ–ä¸ºæå–æœ€é«˜ä½çš„ç”µè·¯(BitExt)5.4 Comparisonchecking x &lt; y = checking the sign of v = x - yparties locally compute @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')âŸ¨vâŸ©=âŸ¨xâŸ©âˆ’âŸ¨yâŸ©\\langle v\\rangle=\\langle x\\rangle-\\langle y\\rangleâŸ¨vâŸ©=âŸ¨xâŸ©âˆ’âŸ¨yâŸ© P0, P1 boolean-share a and b respectively.v = a + b@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')a=âˆ’[Î´v]0a = -\\left[\\delta_{v}\\right]_{0}a=âˆ’[Î´vâ€‹]0â€‹ @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')b=Î”vâˆ’[Î´v]1\\mathrm{b}=\\Delta_{\\mathrm{v}}-\\left[\\delta_{\\mathrm{v}}\\right]_{1}b=Î”vâ€‹âˆ’[Î´vâ€‹]1â€‹ parties use Bit Extraction circuit to compute MSB(v)5.5 Truncationåœ¨online phaseï¼Œè®¡ç®—@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')[y]=[Î”y]âˆ’[Î´y][\\mathrm{y}]=\\left[\\Delta_{y}\\right]-\\left[\\delta_{y}\\right][y]=[Î”yâ€‹]âˆ’[Î´yâ€‹] ã€‚ Each party æˆªæ–­åŽå†åˆ†äº«ï¼Œæœ€åŽå†è¿˜åŽŸã€‚","link":"/paper/%5BABY2.0%5D/index.html"}],"posts":[{"title":"ã€ŒPaper Readingã€ï¼šæœªæ•´ç†ç‰ˆè®ºæ–‡ç¬”è®°","text":"æœ€è¿‘è¯»äº†å‡ ç¯‡è®ºæ–‡ ä»¥ä¸‹æ˜¯æœªæ•´ç†ç‰ˆè®ºæ–‡ç¬”è®°ï¼š [ABY2.0]Improved Mixed -Protocol Secure Two-Party Computation [Schneider13]GMW vs. Yao? Efficient Secure Two-Party Computation with Low Depth CItcuit [Beaver91]Efficient Multiplarty Protools Using Circuit Randomization [GMW87]How To Play Any Mental Game [Naor-OT05]Computationally Secure Oblivious Transfer","link":"/2021/11/07/2021-11-paper-reading/"},{"title":"Adagrag-demo","text":"å®žçŽ°è¿™ç¯‡æ–‡ç« ä¸­å‰é¢ä¸¤ä¸ªtipsã€‚ å®žçŽ°äº†tip1 Adagrad + tip2 Stochastic Gradient Descent demoä»£ç 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970################## 2020/03/06 ## Adagrad demo ##################import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model# datax_data = [[338.], [333.], [328.], [207.], [226.], [25.], [179.], [60.], [208.], [606.]]y_data = [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]# coordinatex = np.arange(-200, -100, 1)y = np.arange(-5, 5, 0.1)Z = np.zeros((len(y), len(x)))# cal the Loss of every point(function)for i in range(len(x)): for j in range(len(y)): b = x[i] w = y[j] for k in range(len(x_data)): Z[j][i] += (y_data[k] - b - w * x_data[k][0])**2# initialb = -120w = -4lr = 1 # learning rateiteration = 100000# record the iterationb_his = [b]w_his = [w]# Adagradb_grad_sum2 = 0.0w_grad_sum2 = 0.0for i in range(iteration): for k in range(len(x_data)): b_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-1) w_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-x_data[k][0]) b_grad_sum2 += b_grad**2 w_grad_sum2 += w_grad**2 b = b - lr / np.sqrt(b_grad_sum2) * b_grad w = w - lr / np.sqrt(w_grad_sum2) * w_grad b_his.append(b) w_his.append(w)# sklearn linear modelreg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print(reg.coef_[0])print(reg.intercept_)# display the figureplt.contourf(x, y, Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))plt.plot(reg.intercept_, reg.coef_, 'x', ms=13, lw=1.5, color='orange')plt.plot(b_his, w_his, 'o-', ms=3, lw=1.5, color='black')plt.xlim(-200, -100)plt.ylim(-5, 5)plt.xlabel('$b$', fontsize=16)plt.ylabel('$w$', fontsize=16)# plt.show()plt.savefig(&quot;Loss.png&quot;) Loss è¿­ä»£å›¾ç”»å‡ºçš„å›¾ç‰‡å¾ˆç›´è§‚","link":"/2020/03/08/Adagrad-demo/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šConvolution Neural Networkï¼ˆCNNï¼‰","text":"è¿™ç¯‡æ–‡ç« ä¸­é¦–å…ˆä»‹ç»äº†ä¸ºä»€ä¹ˆè¦ç”¨CNNåšå›¾åƒè¯†åˆ«ï¼Œæˆ–è€…è¯´å›¾åƒè¯†åˆ«é—®é¢˜çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿæ–‡ç« ä¸­ä¹Ÿè¯¦ç»†ä»‹ç»äº†CNNçš„å…·ä½“æž¶æž„ï¼Œä¸»è¦åŒ…æ‹¬Convolutionã€Max Poolingã€Flattenã€‚æ–‡ç« æœ€åŽç®€è¦ä»‹ç»äº†CNNåœ¨è¯¸å¤šé¢†åŸŸçš„åº”ç”¨ã€‚ Why CNN for Image?å›¾ç‰‡æœ¬è´¨éƒ½æ˜¯pixelsã€‚ åœ¨åšå›¾åƒè¯†åˆ«æ—¶ï¼Œæœ¬è´¨æ˜¯å¯¹å›¾ç‰‡ä¸­çš„æŸäº›ç‰¹å¾åƒç´ ï¼ˆproperities)è¯†åˆ«ã€‚ So Why CNN for image? Some patterns are much smaller than the whole image. A neuron does not have to see the whole image to discover the pattern. Connecting to small region with less parameters. ã€å¾ˆå¤šç‰¹å¾å›¾æ¡ˆçš„å¤§å°è¿œå°äºŽæ•´å¼ å›¾ç‰‡çš„å¤§å°ï¼Œå› æ­¤ä¸€ä¸ªneuronä¸éœ€è¦ä¸ºäº†è¯†åˆ«æŸä¸ªpatternè€Œçœ‹å®Œæ•´å¼ å›¾ç‰‡ã€‚å¹¶ä¸”ï¼Œå¦‚æžœåªè¯†åˆ«æŸä¸ªå°çš„regionï¼Œä¼šå‡å°‘å¤§é‡å‚æ•°çš„æ•°ç›®ã€‚ã€‘ å¦‚ä¸‹å›¾ï¼Œç”¨ä¸€ä¸ªneuronè¯†åˆ«çº¢æ¡†ä¸­çš„beakï¼Œå³èƒ½å¤§æ¦‚çŽ‡è®¤ä¸ºå›¾ç‰‡ä¸­æœ‰birdã€‚ The same patterns appear in different regions. They can use the same set of parameters. ã€åŒæ ·çš„patternå¯èƒ½å‡ºçŽ°åœ¨å›¾ç‰‡çš„ä¸åŒä½ç½®ã€‚patternå‡ ä¹Žç›¸åŒï¼Œå› æ­¤å¯ä»¥ç”¨åŒä¸€ç»„å‚æ•°ã€‚ã€‘ å¦‚ä¸‹å›¾ï¼Œä¸¤ä¸ªneuronè¯†åˆ«ä¸¤ä¸ªä¸åŒä½ç½®çš„beakã€‚è¢«è¯†åˆ«çš„beakå‡ ä¹Žæ— å·®åˆ«ï¼Œå› æ­¤neuronçš„å‚æ•°å¯ä»¥æ˜¯ç›¸åŒçš„ã€‚ Subsampling the pixels will not change the object. ã€ä¸€å¼ å›¾ç‰‡æ˜¯ç”±è®¸å¤špixelç»„æˆçš„ï¼Œå¦‚ä¸‹å›¾ï¼Œå¦‚æžœåŽ»æŽ‰å›¾ç‰‡çš„æ‰€æœ‰å¥‡æ•°è¡Œå¶æ•°åˆ—çš„pixelï¼Œå›¾ç‰‡å†…å®¹å‡ ä¹Žæ— å·®åˆ«ã€‚å¹¶ä¸”ï¼ŒSubsample pixelsï¼Œå³å‡å°‘äº†è¾“å…¥çš„sizeï¼Œä¹Ÿå¯ä»¥å‡å°‘NNçš„å‚æ•°æ•°é‡ã€‚ã€‘ The whole CNNCNNçš„æž¶æž„å¦‚ä¸‹å›¾ã€‚ ä¸€å¼ å›¾ç‰‡ç»è¿‡å¤šæ¬¡Convolutionã€Max Poolingå¾—åˆ°æ–°çš„imageï¼Œå†å°†æ–°çš„image Flattenï¼ˆæ‹‰ç›´ï¼‰å¾—åˆ°ä¸€ç»„æå–å¥½çš„featuresï¼Œå°†è¿™ç»„featuresæ”¾å…¥å‰é¦ˆç¥žç»ç½‘ç»œã€‚ Convolutionæ»¡è¶³å›¾ç‰‡è¯†åˆ«çš„ï¼š Property 1 : Some patterns are much smaller than the whole image. Property 2 : The same patterns appear in different regions. Max Poolingæ»¡è¶³å›¾ç‰‡è¯†åˆ«çš„ï¼š Property 3 : Subsamplingthe pixels will not change the object. CNN-Convolutionä¸€å¼ ç®€å•çš„é»‘ç™½å›¾ç‰‡å¦‚ä¸‹å›¾ï¼Œ0ä¸ºç™½è‰²ï¼Œ1ä¸ºé»‘è‰²ã€‚ å¦‚æžœå›¾ç‰‡æ˜¯å½©è‰²çš„ï¼Œå³ç”¨RGBä¸‰åŽŸè‰²æ¥è¡¨ç¤ºï¼Œç”¨ä¸‰ä¸ªmatrixåˆ†åˆ«è¡¨ç¤ºRã€Gã€Bçš„å€¼ï¼Œå¦‚ä¸‹å›¾ï¼š ä¸‹æ–‡ä¸­ï¼Œä»¥é»‘ç™½å›¾ä¸¾ä¾‹ã€‚ Property 1è®¾è®¡Filer matrixæ»¡è¶³Property 1ï¼Œå¦‚ä¸‹å›¾ï¼š ä¸Šå›¾ä¸­ï¼Œfilterçš„å¤§å°æ˜¯3*3ï¼Œå¯ä»¥æ£€æµ‹åˆ°å°åŒºåŸŸçš„æŸä¸ªpatternã€‚ æ¯ä¸ªfilterçš„å‚æ•°éƒ½æ˜¯NNä¸­çš„å‚æ•°ï¼Œéœ€è¦learnedã€‚ å¦‚æžœæ˜¯å½©è‰²å›¾ç‰‡ï¼Œfilteråº”è¯¥æ˜¯3å¼ 3*3matrixç»„æˆçš„ï¼Œåˆ†åˆ«ä»£è¡¨Rã€Gã€Bçš„filterã€‚ Property 2ä¸ºäº†æ»¡è¶³Property 2ï¼Œfilterå¯ä»¥åœ¨å›¾ç‰‡ä¸­ç§»åŠ¨ã€‚è®¾ç½®strideï¼Œå³æ¯æ¬¡filterç§»åŠ¨çš„æ­¥é•¿ã€‚ filterä¸Žè¦†ç›–å›¾ç‰‡çš„ä½ç½®åšå†…ç§¯ï¼Œéœ€è¦èµ°å®Œæ•´å¼ å›¾ç‰‡ï¼Œæœ€åŽå¾—åˆ°ä¸€å¼ feature mapã€‚ ä¸‹å›¾ä¸ºstride=1çš„convolutionç»“æžœï¼š Convolution layerï¼ˆå·ç§¯å±‚ï¼‰æœ‰å‡ ä¸ªfilterï¼Œå°±ä¼šå¾—åˆ°å‡ å¼ feature mapsã€‚ Convolution v.s. Fully ConnectedFully Connected: å¦‚æžœç”¨å…¨è¿žæŽ¥çš„æ–¹å¼åšå›¾ç‰‡è¯†åˆ«ï¼Œå›¾ç‰‡çš„æ¯ä¸€ä¸ªpixeléƒ½è¦å’Œç¬¬ä¸€å±‚çš„æ‰€æœ‰neuronsè¿žæŽ¥ï¼Œéœ€è¦å¤§é‡å‚æ•°ã€‚ å¦‚ä¸‹å›¾ï¼š Convolution: è€Œåœ¨Convolutionä¸­ï¼ŒæŠŠfeature mapä¸­çš„æ¯ä¸€ä¸ªå€¼ä½œä¸ºneuronçš„è¾“å‡ºï¼Œå› æ­¤å›¾ç‰‡ä¸­åªæœ‰éƒ¨åˆ†pixelsä¼šå’Œç¬¬ä¸€å±‚çš„ç¬¬ä¸€ä¸ªneuronè¿žæŽ¥ï¼Œè€Œä¸æ˜¯å…¨éƒ¨pixelsã€‚ å¯¹äºŽä¸€ä¸ª3*3çš„filterï¼Œä¸€ä¸ªneuronçš„è¿žæŽ¥å¦‚ä¸‹ï¼š filterä¸­çš„å€¼æ˜¯è¿žæŽ¥å‚æ•°ï¼Œåˆ™æ¯ä¸€ä¸ªneuronåªéœ€è¦ä¸Ž3*3ä¸ªinputè¿žæŽ¥ï¼Œä¸Žå…¨è¿žæŽ¥ç›¸æ¯”å‡å°‘äº†å¤§é‡å‚æ•°ã€‚ shared weights filteråœ¨å›¾ä¸­ç§»åŠ¨æ—¶ï¼Œfilterçš„å‚æ•°ä¸å˜ï¼Œå³ç¬¬äºŒä¸ªneuronçš„è¿žæŽ¥å‚æ•°å’Œç¬¬ä¸€ä¸ªneuronçš„è¿žæŽ¥å‚æ•°æ˜¯ç›¸åŒçš„ï¼Œè¿žæŽ¥å›¾å¦‚ä¸‹ï¼š é€šè¿‡filterå®žçŽ°äº†shared weightsï¼ˆå‚æ•°å…±äº«ï¼‰ï¼Œæ›´å¤§å¹…åº¦å‡å°‘äº†å‚æ•°æ•°é‡ã€‚ CNN-Max PoolingMax Poolingï¼šå°†convolution layerçš„neuronä½œä¸ºè¾“å…¥ï¼Œneuronçš„activation functionå…¶å®žå°±æ˜¯Maxoutï¼ˆMaxoutä»‹ç»è§ Post not found: tips-for-DL/#Maxout è¿™ç¯‡ çš„ä»‹ç»ï¼‰ã€‚ å°†convolution layerå¾—åˆ°çš„feature mapåšMax poolingï¼ˆæ± åŒ–ï¼‰ï¼Œå³å–ä¸‹å›¾ä¸­æ¯ä¸ªæ¡†ä¸­çš„æœ€å¤§å€¼ã€‚ å¦‚ä¸‹å›¾ï¼Œ6*6çš„imageç»è¿‡Convolution layer å’Œ Max Pooling layeråŽï¼Œå¾—åˆ°äº†new but smaller imageï¼Œæ–°çš„imageçš„ç”±ä¸¤å±‚channelç»„æˆï¼Œæ¯å±‚channeléƒ½æ˜¯2 * 2çš„imageã€‚ ä¸€ä¸ªimageæ¯ç»è¿‡ä¸€æ¬¡Convolution layer å’Œ Max Pooling layerï¼Œéƒ½ä¼šå¾—åˆ°a new imageã€‚ This new image is smaller than the origin image. And the number of channel (of the new image) is the number of filters. ä¸¾ä¸ªä¾‹å­ï¼š Convolution layeræœ‰25ä¸ªfiltersï¼Œå†ç»è¿‡Max Poolingï¼Œå¾—åˆ°çš„æ–°çš„imageæœ‰25 ä¸ªchannelã€‚ å†é‡å¤ä¸€æ¬¡Convolution å’ŒMax Poolingï¼Œæ–°çš„Convolution layerä¹Ÿæœ‰25ä¸ªfiltersï¼Œå†ç»è¿‡Max Poolingï¼Œå¾—åˆ°çš„æ–°çš„imageæœ‰å¤šå°‘ä¸ªchannelå‘¢ï¼Ÿ ç­”æ¡ˆæ˜¯25ä¸ªchannelã€‚ æ³¨æ„ ï¼šåœ¨ç¬¬äºŒæ¬¡Convolutionä¸­ï¼Œimageæœ‰depthï¼Œdepth=25ã€‚å› æ­¤åœ¨convolutionä¸­ï¼Œfilterå…¶å®žæ˜¯ä¸€ä¸ªcubicï¼Œä¹Ÿæœ‰depthï¼Œdepth=image-depth=25ï¼Œå†åšå†…ç§¯ã€‚ å› æ­¤ï¼Œæ–°çš„imageçš„channelæ•°æ˜¯ç­‰äºŽfilteræ•°çš„ã€‚ FlattenFlattenå¾ˆå¥½ç†è§£ï¼Œå°†æœ€åŽå¾—åˆ°çš„æ–°çš„image æ‹‰ç›´ï¼ˆFlattenï¼‰ä¸ºä¸€ä¸ªvectorã€‚ æ‹‰ç›´åŽçš„vectoræ˜¯ä¸€ç»„æå–å¥½çš„featuresï¼Œä½œä¸º å‰é¦ˆç¥žç»ç½‘ç»œçš„è¾“å…¥ã€‚ zero paddingå¦‚ä½•è®©å·ç§¯åŽçš„å›¾åƒä¸å˜å°ï¼Ÿ ç­”æ¡ˆå°±æ˜¯zero paddingï¼Œåœ¨åŽŸå›¾çš„paddingå¡«0ï¼Œå†åšå·ç§¯ã€‚ zero-paddingåŽå¦‚ä¸‹å›¾ï¼š å·ç§¯åŽï¼Œå›¾åƒå¤§å°ä¸å˜ï¼š What dose CNN learnä¸ºä»€ä¹ˆCNNèƒ½å¤Ÿå­¦ä¹ patternï¼Œæœ€ç»ˆè¾¾åˆ°è¯†åˆ«å›¾åƒçš„ç›®çš„ï¼Ÿ Filteråœ¨ä¸‹å›¾CNNè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å…ˆåˆ†æžèƒ½ä»ŽConvolution layerçš„filterèƒ½å¤Ÿå­¦åˆ°ä»€ä¹ˆï¼Ÿ æ¯ä¸ªfilteræœ¬è´¨ä¸Šæ˜¯ä¸€ç»„shared weights çš„neuronã€‚ å› æ­¤ï¼Œå®šä¹‰è¿™ç»„filterçš„æ¿€æ´»ç¨‹åº¦ï¼Œå³ï¼š Degree of the activation of the k-th filter: $a^k=\\sum_{i=1}^{11}\\sum_{j=1}^{11}a_{ij}^{k}$ . ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿k-th filteræ¿€æ´»ç¨‹åº¦æœ€å¤§çš„è¾“å…¥imageï¼Œå³ $x^{*}=\\arg \\max _{x} a^{k}$ ï¼Œ(method :gradient descent). éƒ¨åˆ†ç»“æžœå¦‚ä¸‹å›¾ï¼š (æ¯ä¸€å¼ å›¾éƒ½ä»£è¡¨ä¸€ä¸ªè®©filteræ¿€æ´»ç¨‹åº¦æœ€å¤§çš„ $x$) ä¸Šå›¾ä¸­ï¼Œæ‰¾åˆ°ä½¿filteræ¿€æ´»ç¨‹åº¦æœ€å¤§çš„imageï¼Œå³ä¸Šå›¾ä¸­æ¯ä¸ªfilterå¯ä»¥æ£€æµ‹ä¸€å®šçš„æ¡çº¹ï¼Œåªæœ‰å½“å›¾åƒä¸­æœ‰è¯¥æ¡çº¹ï¼Œfilterï¼ˆä¸€ç»„neuronï¼‰çš„æ¿€æ´»ç¨‹åº¦ï¼ˆå³è¾“å‡ºï¼‰æ‰èƒ½è¾¾åˆ°æœ€å¤§ã€‚ Neuronï¼ˆHidden layerï¼‰è¿™é‡Œçš„neuronæŒ‡å‰é¦ˆç¥žç»ç½‘ç»œä¸­çš„neuronï¼Œå¦‚ä¸‹å›¾çš„ $a_j$ : ç›®æ ‡ï¼šæ‰¾åˆ°ä½¿neuronçš„è¾“å‡ºæœ€å¤§çš„è¾“å…¥imageï¼Œå³ï¼š $x^{*}=\\arg \\max _{x} a^{j}$ . éƒ¨åˆ†ç»“æžœå¦‚ä¸‹ï¼š ï¼ˆæ¯ä¸€å¼ å›¾ä»£è¡¨ä¸€ä¸ªneuron) åœ¨ä¸Šå›¾ä¸­ï¼Œæ„Ÿè§‰è¾“å…¥åƒä¸€ä¸ªä»€ä¹ˆä¸œè¥¿å§emmmmã€‚ ä½†å’Œfilterå­¦åˆ°çš„ç›¸æ¯”ï¼Œneuronå­¦åˆ°çš„ä¸ä»…æ˜¯å›¾ä¸­çš„å°å°çš„patternï¼ˆæ¯”å¦‚æ¡çº¹ã€é¸Ÿå–™ç­‰ï¼‰ï¼Œneuronå­¦çš„æ˜¯çœ‹æ•´å¼ å›¾åƒä»€ä¹ˆã€‚ Outputï¼ˆOutput layerï¼‰å†ç”¨åŒæ ·çš„æ–¹æ³•ï¼Œçœ‹çœ‹è¾“å‡ºå±‚çš„neuronå­¦åˆ°äº†ä»€ä¹ˆï¼Œå¦‚ä¸‹å›¾çš„ $y_i$ ï¼š åœ¨æ‰‹å†™æ•°å­—è¾¨è¯†ä¸­ $y_i$ æ˜¯æ•°å­—ä¸º $i$ çš„æ¦‚çŽ‡ï¼Œå› æ­¤ç›®æ ‡æ˜¯ï¼šæ‰¾åˆ°ä¸€ä¸ªä½¿è¾“å‡ºæ˜¯æ•°å­— $i$ æ¦‚çŽ‡æœ€å¤§çš„è¾“å…¥imageï¼Œå³ï¼š $x^{*}=\\arg \\max _{x} y^{i}$ . ç»“æžœå¦‚ä¸‹å›¾ï¼š ç»“æžœå’Œæˆ‘ä»¬æœŸæœ›ç›¸å·®ç”šè¿œï¼Œæ ¹æœ¬ä¸èƒ½è¾¨åˆ«ä»¥ä¸Šå›¾ç‰‡æ˜¯æŸä¸ªæ•°å­—ã€‚ è¿™å…¶å®žä¹Ÿæ˜¯DNNçš„ä¸€ä¸ªç‰¹ç‚¹: Deep Neural Networks are Easily Fooled [1]ï¼Œå³NNå­¦åˆ°çš„ä¸œè¥¿å¾€å¾€å’Œäººç±»å­¦åˆ°çš„ä¸œè¥¿æ˜¯ä¸ä¸€æ ·çš„ã€‚ CNNæ‰€ä»¥CNNåˆ°åº•å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ ä¸Šæ–‡ä¸­ï¼Œoutput å­¦åˆ°çš„éƒ½æ˜¯ä¸€å›¢å¯†å¯†éº»éº»æ‚ä¹±çš„åƒç´ ç‚¹ï¼Œæ ¹æœ¬ä¸åƒæ•°å­—ã€‚ ä½†æ˜¯ï¼Œå†è€ƒè™‘æ‰‹å†™æ•°å­—imageçš„ç‰¹ç‚¹ï¼šå›¾ç‰‡ä¸­åº”è¯¥æœ‰å°‘é‡æ¨¡å¼ï¼Œå¤§ç‰‡ç©ºç™½éƒ¨åˆ†ã€‚ å› æ­¤ç›®æ ‡æ”¹è¿›ä¸ºï¼š $x^{*}=\\arg \\max _{x}\\left(y^{i}+\\sum_{i, j}\\left|x_{i j}\\right|\\right)$ $\\sum_{i, j}\\left|x_{i j}\\right|$ å°±åƒæ˜¯regularizationçš„é™åˆ¶ã€‚ ç»“æžœå¦‚ä¸‹ï¼š ï¼ˆæ³¨ï¼šå›¾ä¸­ç™½è‰²ä¸ºå¢¨æ°´ï¼Œé»‘è‰²ä¸ºç©ºç™½ï¼‰ ApplicationDeep DreamCNN exaggerates what it sees. CNNå¯ä»¥å¤¸å¤§å›¾ç‰‡ä¸­ä»–æ‰€çœ‹åˆ°çš„ä¸œè¥¿ã€‚ æ¯”å¦‚ï¼š å¯ä»¥æŠŠä¸‹å›¾ å˜æˆä¸‹å›¾ï¼ˆemmmmçœ‹ç€æœ‰ç‚¹éš¾å—ï¼‰ é™„ä¸Šç”Ÿæˆdeep dream imageçš„ç½‘ç«™[2] . Deep Style[3]Given a photo, make its style like famous paintings. ä¸Šå›¾ä¸­ï¼Œç”¨ä¸€ä¸ªCNNå­¦ä¹ å›¾ä¸­çš„contentï¼Œç”¨å¦ä¸€ä¸ªCNNå­¦ä¹ é£Žæ ¼å›¾ä¸­çš„styleã€‚ å†ç”¨ä¸€ä¸ªCNNä½¿å¾—è¾“å…¥çš„å›¾åƒcontentåƒåŽŸå›¾ï¼Œé£Žæ ¼åƒå¦ä¸€å¼ å›¾ã€‚ Playing GoCNN è¿˜å¯ä»¥ç”¨åœ¨ä¸‹å›´æ£‹ä¸­ï¼Œå¦‚ä¸‹å›¾ï¼Œè¾“å…¥æ˜¯19 * 19çš„å›´æ£‹å±€åŠ¿ï¼ˆmatrix/imageï¼‰ï¼Œé€šè¿‡CNNï¼Œå­¦å‡ºä¸‹ä¸€æ­¥åº”è¯¥èµ°å“ªï¼Ÿ Why CNN playing Go?ä¸‹å›´æ£‹æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªpropertyï¼š Some patterns are much smaller than the whole image. ï¼ˆå›´æ£‹æ–°æ‰‹ï¼Œåšä¸»åªä¸‹èµ¢è¿‡å‡ æ¬¡hhh) å¦‚æžœç™½æ£‹æ£‹æ‰‹ï¼Œçœ‹åˆ°ä¸Šå›¾çš„patternï¼Œä¸Šå›¾çš„ç™½å­åªæœ‰ä¸€å£æ°”äº†ï¼Œè¢«å µä½å°±ä¼šè¢«åƒæŽ‰ï¼Œé‚£ç™½æ£‹æ£‹æ‰‹å¤§æ¦‚çŽ‡ä¼šæ•‘é‚£ä¸ªç™½å­ï¼Œä¸‹åœ¨ç™½æ£‹çš„ä¸‹æ–¹ã€‚ Alpha Go uese 5 * 5 for first layer. The same patterns appear in different regions. ä½†å¦‚ä½•è§£é‡ŠCNNçš„å¦ä¸€ç»“æž„â€”â€”Max Poolingï¼Ÿ å› ä¸ºå›´æ£‹çš„æ£‹è°±matrixä¸åƒimageçš„pixelï¼ŒsubsampleåŽï¼Œå›´æ£‹çš„æ£‹è°±å°±å’ŒåŽŸæ£‹è°±å®Œå…¨ä¸åƒäº†ã€‚ Alpha Goçš„è®ºæ–‡ä¸­ï¼šAlpha Goå¹¶æ²¡æœ‰ç”¨Max Poolingã€‚ æ‰€ä»¥ï¼Œå¯ä»¥æ ¹æ®è¦è®­ç»ƒçš„ä¸œè¥¿è°ƒæ•´CNNæ¨¡åž‹ã€‚ Speechå¯ä»¥ç”¨CNNå­¦ä¹ Spectrogram ï¼Œå³è¯†åˆ«å‡ºè¿™ä¸€æ—¶æ®µè¯´çš„æ˜¯ä»€ä¹ˆè¯ã€‚ TextCNNè¿˜å¯ä»¥ç”¨åœ¨æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†æžä¸­ï¼Œå¯¹å¥å­ä¸­æ¯ä¸ªword embeddingåŽï¼Œé€šè¿‡CNNï¼Œå­¦ä¹ sentenceè¡¨è¾¾çš„æ˜¯negative è¿˜æ˜¯positiveè¿˜æ˜¯neutralçš„æƒ…ç»ªã€‚ Moreï¼ˆæŒ–å‘â€¦ç”Ÿå‘½å¾ˆæ¼«é•¿ï¼Œå­¦æ— æ­¢å¢ƒQAQï¼‰ The methods of visualization in these slidesï¼š https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html More about visualizationï¼š http://cs231n.github.io/understanding-cnn/ Very cool CNN visualization toolkit http://yosinski.com/deepvis http://scs.ryerson.ca/~aharley/vis/conv/ The 9 Deep Learning Papers You Need To Know About https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html How to let machine draw an image PixelRNN https://arxiv.org/abs/1601.06759 Variation Autoencoder (VAE) https://arxiv.org/abs/1312.6114 Generative Adversarial Network (GAN) http://arxiv.org/abs/1406.2661 Reference Deep Neural Networks are Easily Fooledï¼š https://www.youtube.com/watch?v=M2IebCN9Ht4 deep dream generator: http://deepdreamgenerator.com/ A Neural Algorithm of Artistic Style: https://arxiv.org/abs/1508.06576","link":"/2020/04/24/CNN/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šBackpropagation","text":"è¿™ç¯‡æ–‡ç« ä¸­ï¼Œè®²è§£äº†Deep Learningä¸­ä½¿ç”¨çš„ä¸€ç§é«˜æ•ˆGradient Descentçš„ç®—æ³•ï¼šBackPropagationã€‚BackPropagationé€šè¿‡æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸¤ä¸ªé˜¶æ®µï¼Œæœ€åŽèƒ½ä¸€èµ·ç®—å‡ºæŸå¤±å‡½æ•°å¯¹æ¯ä¸€ä¸ªå‚æ•°çš„gradientã€‚ Gradient Descentåœ¨Neural Networkä¸­ï¼Œå‚æ•°çš„æ›´æ–°ä¹Ÿæ˜¯é€šè¿‡Gradient Descentã€‚ ä½†æ˜¯å½“Neural Networkå±‚æ•°å¾ˆæ·±ï¼Œç»“æž„å¾ˆå¤æ‚çš„æ—¶å€™ï¼Œä¼šæœ‰millions of parapmetersã€‚ Backpropagationï¼šTo compute the gradient efficiently. Chain RuleBPä¸­éœ€è¦ç”¨åˆ°çš„æ•°å­¦çŸ¥è¯†ï¼šå¾®ç§¯åˆ†ä¸­çš„é“¾å¼æ³•åˆ™ã€‚ Backpropagation åœ¨NNä¸­ï¼Œå®šä¹‰æŸå¤±å‡½æ•° $L(\\theta)=\\sum_{n=1}^{N} C^{n}(\\theta)$ ï¼ˆ$\\theta$ ä»£æŒ‡NNä¸­æ‰€æœ‰çš„weight å’Œbiasï¼Œ$C$ ä¸ºCross-entropyï¼‰ å¯¹æŸä¸€å‚æ•°çš„gradientä¸º $\\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^{N} \\frac{\\partial C^{n}(\\theta)}{\\partial w}$ åœ¨ä¸Šå›¾NNä¸­ï¼Œæˆ‘ä»¬å…ˆåªç ”ç©¶çº¢æ¡†éƒ¨åˆ†ï¼Œå³æ˜¯ä»¥ä¸‹ç»“æž„ï¼š zï¼šæ¯ä¸ªactivation functionçš„è¾“å…¥ã€‚ æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œ $\\frac{\\partial C}{\\partial w}= \\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}$ . è¦è®¡ç®—æ¯ä¸ªå‚æ•°çš„ $\\frac{\\partial C}{\\partial w}$ ï¼Œåˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚ Forward pass: compute $\\frac{\\partial z}{\\partial w} $ for all parameters. Backward pass: compute $\\frac{\\partial C}{\\partial z} $ for all activation function inputs z. BPï¼šForward passCompute $\\frac{\\partial z}{\\partial w} $ for all parameters. è¿˜æ˜¯åªçœ‹ä¸Šå›¾è¿™ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥è½»æ˜“å¾—å‡ºï¼š $\\partial{z}/\\partial{w_1}=x_1\\qquad \\partial{z}/\\partial{w_2}=x_2$ å¾—åˆ°ç»“è®ºï¼š $\\frac{\\partial z}{\\partial w} $ ç­‰äºŽ the value of the input connected by the weight. ã€$\\frac{\\partial z}{\\partial w} $ ç­‰äºŽ è¿žæŽ¥wçš„è¾“å…¥çš„å€¼ã€‘ é‚£ä¹ˆï¼Œå¦‚ä½•è®¡ç®—å‡ºNNä¸­å…¨éƒ¨çš„ $\\frac{\\partial z}{\\partial w} $ ï¼Ÿ ï¼šForward pass. ç”¨å½“å‰å‚æ•°ï¼ˆw,b) ä»Žhidden layerçš„ç¬¬ä¸€å±‚å¼€å§‹ï¼Œè®¡ç®—å‡ºç¬¬ä¸€å±‚çš„è¾“å‡ºï¼Œå³ç¬¬äºŒå±‚çš„è¾“å…¥ã€‚ ä¾æ¬¡ç›¸å‰è®¡ç®—ï¼Œè®¡ç®—å‡ºæ¯ä¸€å±‚çš„è¾“å‡ºï¼Œå³ä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œå³è¾“å…¥æ‰€è¿žæŽ¥æƒé‡çš„ $\\frac{\\partial z}{\\partial w}$ ã€‚ BPï¼šBackward passCompute $\\frac{\\partial C}{\\partial z} $ for all activation function inputs z. zï¼šactivation functionçš„ input aï¼šactivation functionçš„ output è¿™é‡Œçš„activation function æ˜¯ sigmodå‡½æ•° $a=\\sigma(z)=\\frac{1}{1+e^{-z}}$ è¦æ±‚ $\\frac{\\partial C}{\\partial z}$ ï¼Œ å†æ ¹æ®é“¾å¼æ³•åˆ™ï¼š $\\frac{\\partial C}{\\partial z}=\\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}$ æ±‚ $\\frac{\\partial{a}}{\\partial{z}}$ : $\\frac{\\partial{a}}{\\partial{z}}=\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$ ï¼ˆæ˜¯å…¶ä»–activation function ä¹Ÿèƒ½è½»æ˜“æ±‚å‡ºï¼‰ æ±‚ $\\frac{\\partial C}{\\partial a}$ ï¼šæ ¹æ®é“¾å¼æ³•åˆ™ï¼š $\\frac{\\partial C}{\\partial a}=\\frac{\\partial z^{\\prime}}{\\partial a} \\frac{\\partial C}{\\partial z^{\\prime}}+\\frac{\\partial z^{\\prime \\prime}}{\\partial a} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}$ $\\frac{\\partial z^{\\prime}}{\\partial a} =w_3$ ï¼Œ $\\frac{\\partial z^{\\prime\\prime}}{\\partial a} =w_4$ $\\frac{\\partial C}{\\partial z^{\\prime}}$ å’Œ $\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$ ï¼Ÿå‡è®¾ï¼Œå·²ç»é€šè¿‡æŸç§æ–¹æ³•ç®—å‡ºè¿™ä¸ªå€¼ã€‚ $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ è¿™ä¸ªå¼å­ï¼Œå¯ä»¥ç”»æˆä¸€ä¸ªåå‘ä¼ æ’­çš„NNï¼Œè§ä¸‹å›¾ã€‚ $\\frac{\\partial C}{\\partial z^{\\prime}},\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$ æ˜¯è¿™ä¸ªneuronçš„è¾“å…¥ï¼Œ $w_3,w_4$ ä»ç„¶æ˜¯ neuronçš„ weightï¼ˆæ— biasï¼‰ã€‚ $\\sigmaâ€™(z)$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå› ä¸ºåœ¨forward passä¸­æ¯ä¸€ä¸ªactivationçš„è¾“å…¥å·²ç»è¢«ç®—å‡ºæ¥äº†ã€‚ å’Œforward passä¸­çš„NNçš„åŒºåˆ«æ˜¯ï¼Œforward ä¸­æ˜¯ä¸€ä¸ªactivation functionï¼Œè¾“å…¥zä½œç”¨äºŽè¿™ä¸ªå‡½æ•°ï¼› è€Œåœ¨ backward passä¸­ï¼Œè¿™æ›´åƒä¸€ä¸ªæ”¾ç¼©å™¨ï¼Œå°†ä»–çš„è¾“å…¥å˜å°ï¼Œå³ä¹˜ä¸Šä¸€ä¸ª $\\sigmaâ€™(z)$ ã€‚ é—®é¢˜è¿˜æ˜¯å¦‚ä½•è®¡ç®— $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ ï¼Ÿ åˆ†ä¸ºä¸¤ç§æƒ…å†µè®¨è®ºï¼Œ $zâ€™,zâ€™â€™$ æ˜¯å¦ä¸ºè¾“å‡ºå±‚çš„è¾“å…¥ï¼Ÿ Output Layerï¼š zâ€™,zâ€™â€™ï¼šactivation functionçš„è¾“å…¥ã€‚ y1,y2ï¼šactiavtion functionï¼ˆä¹Ÿæ˜¯NNï¼‰çš„è¾“å‡ºã€‚ Cï¼šNNè¾“å‡ºå’Œtargetçš„cross entropyã€‚ æ ¹æ®é“¾å¼æ³•åˆ™ï¼š $\\frac{\\partial C}{\\partial z^{\\prime}}=\\frac{\\partial y_{1}}{\\partial z^{\\prime}} \\frac{\\partial C}{\\partial y_{1}} \\quad \\frac{\\partial C}{\\partial z^{\\prime \\prime}}=\\frac{\\partial y_{2}}{\\partial z^{\\prime \\prime}} \\frac{\\partial C}{\\partial y_{2}}$ æ‰€ä»¥ï¼Œå·²çŸ¥activation functionï¼ˆsimodæˆ–è€…å…¶ä»–ï¼‰ï¼Œå¯ä»¥è½»æ˜“æ±‚å‡º $\\frac{\\partial y_{1}}{\\partial z^{\\prime}}(=\\sigma'(z'))$ å’Œ $\\frac{\\partial y_{2}}{\\partial z^{\\prime\\prime}}(=\\sigma''(z''))$ ã€‚ æ‰€ä»¥ï¼Œå·²çŸ¥æŸå¤±å‡½æ•°ï¼Œä¹Ÿå¯ä»¥è½»æ˜“æ±‚å‡º $\\frac{\\partial C}{\\partial y_1}$ å’Œ $\\frac{\\partial C}{\\partial y_2}$ ã€‚ï¼ˆ $C\\left(y, \\hat{y}\\right)=-\\left[\\hat{y} \\ln y+\\left(1-\\hat{y}\\right) \\ln \\left(1-y\\right)\\right]$ ) æ‰€ä»¥ï¼Œå¯ä»¥ç›´æŽ¥æ±‚å‡º $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ ã€‚ Not Output Layer: ä¸Šå›¾ä¸­ï¼Œå¦‚æžœæˆ‘ä»¬è¦è®¡ç®— $\\frac{\\partial C}{\\partial zâ€™}$ ï¼Œå¿…é¡»è¦å·²çŸ¥ä¸‹ä¸€å±‚çš„ $\\frac{\\partial C}{\\partial z_a}$ ï¼Œç„¶åŽä¸€ç›´é€’å½’ä¸‹åŽ»ï¼Œç›´åˆ°åˆ°è¾¾æœ€åŽçš„è¾“å‡ºå±‚ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢ä¸€ç§æƒ…å†µï¼Œå¯ä»¥ç›´æŽ¥è®¡ç®—å‡ºï¼Œå†é€’å½’å›žæ¥ï¼Œè®¡ç®—å½“å‰å±‚çš„ $\\frac{\\partial C}{\\partial zâ€™}$ ã€‚ ä½†æ˜¯ï¼Œè¿™æ ·è®¡ç®—æ¯ä¸ªå‚æ•°çš„ $\\frac{\\partial{C}}{\\partial{z}}$ éƒ½è¦ä¸€ç›´é€’å½’åˆ°è¾“å‡ºå±‚ï¼Œæ•ˆçŽ‡æ˜¾ç„¶å¤ªä½Žäº†ã€‚ è®¡ç®—æ–¹æ³•å¦‚ä¸Šå›¾ï¼š å½“æˆ‘ä»¬å·²çŸ¥è¾“å‡ºå±‚çš„ $\\frac{\\partial{C}}{\\partial{z'}},\\frac{\\partial{C}}{\\partial{z''}}$ æ—¶ï¼Œå†é€šè¿‡ä¸Šé¢çš„æ­¥éª¤3ï¼ˆä¸”çš„ç¡®ç®—å‡ºäº† $\\frac{\\partial{C}}{\\partial{z'}},\\frac{\\partial{C}}{\\partial{z''}}$ ï¼‰ï¼Œç”»æˆåå‘çš„NNï¼Œè®¡ç®—$\\frac{\\partial{C}}{\\partial{z}}$. å†ä¾æ¬¡åå‘ä¼ æ’­è®¡ç®—å‡ºæ¯ä¸€ä¸ªneuronçš„è¾“å‡ºz ï¼ˆä¹Ÿæ˜¯æ­£å‘ä¼ æ’­neuronçš„è¾“å…¥ï¼‰çš„ $\\frac{\\partial{C}}{\\partial{z}}$ . Backforward pass çš„åšæ³•ï¼š å…ˆè®¡ç®—å‡ºè¾“å‡ºå±‚çš„ $\\frac{\\partial{C}}{\\partial{z}}$ ï¼ˆä¹Ÿå°±æ˜¯ä¸Šå›¾çš„ $\\frac{\\partial{C}}{\\partial{z_5}}$ å’Œ $\\frac{\\partial{C}}{\\partial{z_6}}$ ï¼‰ ç”¨åå‘ä¼ æ’­çš„NNï¼Œå‘åŽä¾æ¬¡è®¡ç®—å‡ºæ¯ä¸€å±‚æ¯ä¸€ä¸ªneuronçš„ $\\frac{\\partial{C}}{\\partial{z}}$ ã€‚ Summary å…¬å¼ï¼š $\\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}=\\frac{\\partial C}{\\partial w}$ åœ¨æ­£å‘ä¼ æ’­NNä¸­ï¼Œzæ˜¯neuronçš„activation functionçš„è¾“å…¥ã€‚ åœ¨åå‘ä¼ æ’­NNä¸­ï¼Œzæ˜¯neuronçš„æ”¾ç¼©å™¨çš„è¾“å‡ºã€‚ é€šè¿‡Forward Passè®¡ç®—å‡ºæ­£å‘ä¼ æ’­NNçš„æ¯ä¸€ä¸ªneuronçš„ $\\frac{\\partial z}{\\partial w}$ ï¼Œç­‰äºŽè¯¥å±‚neuronçš„è¾“å…¥ã€‚ é€šè¿‡Backward Passè®¡ç®—å‡ºåå‘ä¼ æ’­NNçš„æ¯ä¸€ä¸ªneuronçš„ $\\frac{\\partial C}{\\partial z}$ ã€‚ ç„¶åŽï¼Œé€šè¿‡ç›¸ä¹˜ï¼Œè®¡ç®—å‡ºæ¯ä¸ªå‚æ•°çš„ $\\frac{\\partial C}{\\partial w}$ã€‚ Reference","link":"/2020/04/17/Backpropagation/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šDeep Learning-Introduction","text":"è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä»‹ç»äº†Deep Learningçš„ä¸€èˆ¬æ­¥éª¤ã€‚ Up and downs of Deep Learning 1958: Perceptron (linear model) 1969: Perceptron has limitation 1980s: Multi-layer perceptron â€‹ Do not have significant difference from DNN today 1986: Backpropagation â€‹ Usually more than 3 hidden layers is not helpful 1989: 1 hidden layer is â€œgood enoughâ€, why deep? 2006: RBM initialization (breakthrough) 2009: GPU 2011: Start to be popular in speech recognitionã€è¯­éŸ³è¾¨è¯†ã€‘ 2012: win ILSVRC image competition ã€å›¾åƒè¯†åˆ«ã€‘ Step 1: Neural Networkåœ¨å°†Regression å’Œ Classificationæ—¶ï¼ŒStep 1 æ˜¯ç¡®å®šä¸€ä¸ªfunction setã€‚ åœ¨Deep Learningä¸­ï¼Œä¹Ÿæ˜¯ç›¸åŒçš„ï¼Œåªæ˜¯è¿™é‡Œçš„function setå°±æ˜¯ä¸€ä¸ªneural networkçš„ç»“æž„ã€‚ ä¸Šå›¾ä¸­ï¼Œä¸€ä¸ªNeuronå°±æ˜¯å¦‚ä¸Šå›¾æ‰€ç¤ºçš„ä¸€ä¸ªunitï¼Œneuronä¹‹é—´ä¸åŒçš„è¿žæŽ¥æ–¹å¼æž„æˆä¸åŒçš„Neural Networkã€‚ Fully Connect Feedforward Network è¿™æ˜¯ä¸€ä¸ªFully Connect Feedforward Networkã€å…¨è¿žæŽ¥åé¦ˆç½‘ç»œã€‘ï¼Œå…¶ä¸­æ¯ä¸ªneuronçš„activation functionéƒ½æ˜¯ä¸€ä¸ªsigmodå‡½æ•°ã€‚ ä¸ºä»€ä¹ˆè¯´neural networkå…¶å®žå°±æ˜¯ä¸€ä¸ªfunctionå‘¢ï¼Ÿä¸Šé¢ä¸¤å¼ å›¾ä¸­ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªvectorï¼Œè¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªvectorï¼Œå¯ä»¥ç”¨ä¸‹é¢å‡½æ•°æ¥è¡¨ç¤ºã€‚ $$ f\\left(\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]\\right)=\\left[\\begin{array}{c}0.62 \\\\ 0.83\\end{array}\\right] f\\left(\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]\\right)=\\left[\\begin{array}{l}0.51 \\\\ 0.85\\end{array}\\right] $$ ä¸Šå›¾ä¸ºå…¨è¿žæŽ¥ç½‘ç»œçš„ä¸€èˆ¬å½¢å¼ï¼Œç¬¬ä¸€å±‚æ˜¯Input Layerï¼Œæœ€åŽä¸€å±‚æ˜¯Output Layerï¼Œä¸­é—´çš„å…¶ä»–å±‚ç§°ä¸ºHidden Layerã€‚ è€ŒDeep Learningä¸­çš„Deepçš„å«ä¹‰å°±æ˜¯Many hidden layersçš„æ„æ€ã€‚ Matrix Operation ä¸Šå›¾çš„å…¨è¿žæŽ¥ç½‘ç»œä¸­ï¼Œç¬¬ä¸€ä¸ªhidden layerçš„è¾“å‡ºå¯ä»¥å†™æˆçŸ©é˜µå’Œå‘é‡çš„å½¢å¼ï¼š $$ \\sigma\\left(\\left[\\begin{array}{cc}1 & -2 \\\\ -1 & 1\\end{array}\\right]\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]+\\left[\\begin{array}{c}1 \\\\ 0\\end{array}\\right]\\right)=\\left[\\begin{array}{c}0.98 \\\\ 0.12\\end{array}\\right] $$ æ›´ä¸ºä¸€èˆ¬çš„å…¬å¼ï¼Œç”¨Wè¡¨ç¤ºæƒé‡ï¼Œbä»£è¡¨biasï¼Œaè¡¨ç¤ºhidden layerçš„è¾“å‡ºã€‚è¾“å‡ºvector yå¯ä»¥å†™æˆ $y = f(x)$ çš„å½¢å¼ï¼Œå³ï¼š $y= f(x)=$ è½¬æ¢ä¸ºçŸ©é˜µè¿ç®—çš„å½¢å¼ï¼Œå°±å¯ä»¥ä½¿ç”¨å¹¶è¡Œè®¡ç®—çš„ç¡¬ä»¶æŠ€æœ¯ï¼ˆGPUï¼‰æ¥åŠ é€ŸçŸ©é˜µè¿ç®—ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆç”¨GPUæ¥è®­ç»ƒNeural Network æ›´å¿«çš„åŽŸå› ã€‚ Output Layeråœ¨ Logistic Regressionä¸­ç¬¬4èŠ‚è®²åˆ°Logistic Regressionæœ‰å±€é™ï¼Œæ¶ˆé™¤å±€é™çš„ä¸€ç§æ–¹æ³•æ˜¯Feature Transformationã€‚ ä½†æ˜¯Feature Transformationéœ€è¦äººå·¥è®¾è®¡ï¼Œä¸å¤ªâ€œæœºå™¨å­¦ä¹ â€ã€‚ åœ¨ä¸‹å›¾å…¨è¿žæŽ¥å›¾ä¸­ï¼ŒæŠŠOutput Layeræ¢æˆä¸€ä¸ªMulti-class Classifierï¼ˆSoftMaxï¼‰ï¼Œè€Œå…¶ä¸­Hidden Layersçš„ä½œç”¨å°±æ˜¯Feature extractorï¼Œä»Žfeature xæå–å‡ºæ–°çš„featureï¼Œä¹Ÿå°±æ˜¯ output layerçš„è¾“å…¥ã€‚ è¿™æ ·å°±ä¸éœ€è¦äººå·¥è®¾è®¡Feature Transformation/Feature engineeringï¼Œå¯ä»¥è®©æœºå™¨è‡ªå·±å­¦ä¹ ï¼šå¦‚ä½•å°†åŽŸæ¥çš„featureè½¬æ¢ä¸ºæ›´å¥½åˆ†ç±»çš„featureã€‚ Handwriting Digit Recognition åœ¨æ‰‹å†™æ•°å­—è¾¨åˆ«ä¸­ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ª16*16çš„imageï¼ˆ256ç»´çš„vectorï¼‰ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ª10ç»´çš„vectorï¼Œæ¯ä¸€ç»´è¡¨ç¤ºæ˜¯è¯¥imageæ˜¯æŸä¸ªæ•°å­—çš„æ¦‚çŽ‡ã€‚ åœ¨æ‰‹å†™æ•°å­—è¾¨åˆ«ä¸­ï¼Œéœ€è¦è®¾è®¡neural networkçš„ç»“æž„æ¥æå–è¾“å…¥çš„256ç»´featureã€‚ Step 2: Goodness of functionä¹‹å‰æˆ‘ä»¬å·²ç»ä½¿ç”¨è¿‡çš„æœ€å°äºŒä¹˜æ³•å’Œäº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ã€‚ ä¸€èˆ¬åœ¨Neural Networkä¸­ï¼Œä½¿ç”¨output vector å’Œtarget vectorçš„äº¤å‰ç†µä½œä¸ºLossã€‚ Step 3: Pick the best functionåœ¨NNä¸­ï¼Œä¹Ÿä½¿ç”¨Gradient Descentã€‚ ä½†æ˜¯ï¼ŒDeep Neural Networkä¸­ï¼Œå‚æ•°å¤ªå¤šäº†ï¼Œè®¡ç®—ç»“æž„ä¹Ÿå¾ˆå¤æ‚ã€‚ Backpropagationï¼šan efficient way to compute $\\partial{L}/\\partial{w}$ in neural network. Backpropagationæœ¬è´¨ä¹Ÿæ˜¯Gradient Descentï¼Œåªæ˜¯ä¸€ç§æ›´é«˜æ•ˆè¿›è¡ŒGradient Descentçš„ç®—æ³•ã€‚ åœ¨å¾ˆå¤š toolkitï¼ˆTensorFlowï¼ŒPyTorch ï¼ŒCaffeç­‰ï¼‰ä¸­éƒ½å®žçŽ°äº†Backpropgationã€‚ Backpropagationéƒ¨åˆ†ï¼Œè§ä¸‹ä¸€ç¯‡åšå®¢ã€‚ Reference","link":"/2020/04/17/DL-introdunction/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€:Classification-Probabilistic Generative Model","text":"Classification æœ‰Generative Modelå’ŒDiscriminative Modelã€‚è¿™ç¯‡æ–‡ç« ä¸»è¦è®²è¿°äº†ç”¨ç”Ÿæˆæ¨¡åž‹æ¥åšåˆ†ç±»çš„åŽŸç†åŠè¿‡ç¨‹ã€‚ What is Classification?åˆ†ç±»æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿåˆ†ç±»å¯ä»¥åº”ç”¨åˆ°å“ªäº›åœºæ™¯å‘¢ï¼Ÿ Credit Scoringã€è´·æ¬¾è¯„ä¼°ã€‘ Input: income, savings, profession, age, past financial history â€¦â€¦ Output: accept or refuse Medical Diagnosisã€åŒ»ç–—è¯Šæ–­ã€‘ Input: current symptoms, age, gender, past medical history â€¦â€¦ Output: which kind of diseases Handwritten character recognitionã€æ‰‹å†™æ•°å­—è¾¨åˆ«ã€‘ Inputï¼š Outputï¼šé‡‘ Face recognition ã€äººè„¸è¯†åˆ«ã€‘ Input: image of a face output: person Classificationï¼šExample Applicationã€å›¾ã€‘ å¦‚ä¸Šå›¾ï¼ŒPokemonåˆæ¥å•¦ï¼ Pokemonæœ‰å¾ˆå¤šå±žæ€§ï¼Œæ¯”å¦‚çš®å¡ä¸˜æ˜¯ç”µå±žæ€§ï¼Œæ°å°¼é¾Ÿæ˜¯æ°´å±žæ€§ä¹‹ç±»ã€‚ å…³äºŽPokemonçš„Classificationï¼šPredict the â€œtypeâ€ of Pokemon based on the information Inputï¼šInformation of Pokemon (æ•°å€¼åŒ–ï¼‰ Outputï¼šthe type Training Data: IDåœ¨å‰400çš„Pokemon Testing Data: IDåœ¨400åŽçš„Pokemon Classification as Regression?1. ç®€åŒ–é—®é¢˜ï¼Œåªè€ƒè™‘äºŒåˆ†ç±»ï¼šClass 1 ï¼Œ Class2ã€‚ å¦‚æžœæŠŠåˆ†ç±»é—®é¢˜å½“ä½œå›žå½’é—®é¢˜ï¼ŒæŠŠç±»åˆ«æ•°å€¼åŒ–ã€‚ åœ¨Trainingä¸­ï¼š Class 1 means the target is 1; Class 2 means the target is -1. åœ¨Testingä¸­ï¼šå¦‚æžœRegressionçš„å‡½æ•°å€¼æŽ¥è¿‘1ï¼Œè¯´æ˜Žæ˜¯class 1ï¼›å¦‚æžœå‡½æ•°å€¼æŽ¥è¿‘-1ï¼Œè¯´æ˜Žæ˜¯class 2. Regressionï¼šè¾“å…¥ä¿¡æ¯åªè€ƒè™‘ä¸¤ä¸ªç‰¹å¾ã€‚ Modelï¼š$y=w_1x_1+w_2x_2+b$ å½“Training dataçš„åˆ†å¸ƒå¦‚ä¸Šå›¾æ‰€ç¤ºæ—¶ï¼Œå¾—åˆ°çš„ï¼ˆæœ€ä¼˜å‡½æ•°ï¼‰åˆ†ç•Œçº¿æ„Ÿè§‰å¾ˆåˆç†ã€‚ ä½†å½“Training dataåœ¨å³ä¸‹è§’ä¹Ÿæœ‰åˆ†å¸ƒæ—¶ï¼ˆå¦‚å³å›¾ï¼‰ï¼Œè®­ç»ƒä¸­ä¸ºäº†å‡å°‘errorï¼Œè®­ç»ƒå¾—åˆ°çš„åˆ†ç•Œçº¿ä¼šå˜æˆç´«è‰²çš„é‚£ä¸€æ¡ã€‚ æ‰€ä»¥ï¼Œå¦‚æžœç”¨Regressionæ¥åšClassificationï¼šPenalize to the examples that are â€œtoo correctâ€ .[1] è®­ç»ƒä¸­ä¼šå› ä¸ºæƒ©ç½šä¸€äº›â€œè¿‡äºŽæ­£ç¡®â€ï¼ˆå³å’Œæˆ‘ä»¬å‡å®šçš„targetç¦»å¤ªè¿œï¼‰çš„exampleï¼Œå¾—åˆ°çš„æœ€ä¼˜å‡½æ•°åè€Œhave bad performance. 2. æ­¤å¤–ï¼Œå¦‚æžœç”¨Regressionæ¥è€ƒè™‘å¤šåˆ†ç±»ã€‚ Multiple class: Class 1 means the target is 1; Class 2 means the target is 2; Class 3 means the target is 3â€¦â€¦ å¦‚æžœç”¨ä¸Šé¢è¿™ç§å‡è®¾ï¼Œå¯ä»¥è®¤ä¸ºClass 3å’ŒClass 2 çš„å…³ç³»æ›´è¿‘ï¼Œå’ŒClass 1çš„å…³ç³»æ›´è¿œä¸€äº›ã€‚ä½†å®žé™…ä¸­ï¼Œå¯èƒ½è¿™äº›ç±»åˆ«have no relationã€‚ Classification: Ideal Alternativesåœ¨ä¸Šé¢ï¼Œæˆ‘ä»¬å‡è®¾äºŒå…ƒåˆ†ç±»æ¯ä¸€ä¸ªç±»åˆ«éƒ½æœ‰ä¸€ä¸ªtargetï¼Œç»“æžœä¸å°½äººæ„ã€‚ å¦‚æžœå°†æ¨¡åž‹æ”¹ä¸ºä¸‹å›¾å½¢å¼ï¼Œä¹Ÿå¯ä»¥è§£å†³åˆ†ç±»é—®é¢˜ã€‚ï¼ˆæŒ–å‘ï¼‰[2] Generative Model(ç”Ÿæˆæ¨¡åž‹)Estimate the Probabilitiesç”¨æ¦‚çŽ‡çš„çŸ¥è¯†æ¥è€ƒè™‘åˆ†ç±»è¿™ä¸ªé—®é¢˜ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ‰ä¸¤ä¸ªä¸¤ä¸ªç±»åˆ«ï¼ŒC1å’ŒC2ã€‚ åœ¨Testingä¸­ï¼Œå¦‚æžœä»»ç»™ä¸€ä¸ªxï¼Œå±žäºŽC1çš„æ¦‚çŽ‡æ˜¯ï¼ˆè´å¶æ–¯å…¬å¼ï¼‰ $$ P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} $$ æ‰€ä»¥åœ¨Trainingåº”è¯¥çŸ¥é“è¿™äº›çš„æ¦‚çŽ‡ï¼š $P(C_1),P(x|C_1),P(C_2),P(x|C_2)$ å…ˆéªŒæ¦‚çŽ‡P(C1)å’ŒP(C2)å¾ˆå®¹æ˜“å¾—çŸ¥ã€‚ è€Œä¼¼ç„¶P(x|C1)å’ŒP(x|C2)çš„æ¦‚çŽ‡åº”è¯¥å¦‚ä½•å¾—çŸ¥å‘¢ï¼Ÿ å¦‚æžœèƒ½å‡è®¾ï¼šç±»åˆ«æ˜¯C1ä¸­çš„å˜é‡xæœä»ŽæŸç§åˆ†å¸ƒï¼Œå¦‚é«˜æ–¯åˆ†å¸ƒç­‰ï¼Œå³å¯ä»¥å¾—åˆ°ä»»æ„P(x|C1)çš„å€¼ã€‚ æ‰€ä»¥Generative Modelï¼šæ˜¯å¯¹exampleså‡è®¾ä¸€ä¸ªåˆ†å¸ƒæ¨¡åž‹ï¼Œåœ¨trainingä¸­è°ƒèŠ‚åˆ†å¸ƒæ¨¡åž‹çš„å‚æ•°ï¼Œä½¿å¾—exampleså‡ºçŽ°çš„æ¦‚çŽ‡æœ€å¤§ã€‚ï¼ˆæžå¤§ä¼¼ç„¶çš„æ€æƒ³ï¼‰ Prior Probabilitiesï¼ˆå…ˆéªŒæ¦‚çŽ‡ï¼‰å…ˆåªè€ƒè™‘Waterå’ŒNormalä¸¤ä¸ªç±»åˆ«ã€‚ å…ˆéªŒæ¦‚çŽ‡ï¼šå³é€šè¿‡è¿‡åŽ»èµ„æ–™åˆ†æžå¾—åˆ°çš„æ¦‚çŽ‡ã€‚ åœ¨Pokemonçš„ä¾‹å­ä¸­ï¼ŒTraining Dataæ˜¯ID&lt;400çš„æ°´å±žæ€§å’Œä¸€èˆ¬å±žæ€§çš„Pokemonä¿¡æ¯ã€‚ Training Dataï¼š79 Waterï¼Œ61 Normalã€‚ å¾—åˆ°çš„å…ˆéªŒæ¦‚çŽ‡ P(C1)=79/(79+61)=0.56, P(C2)=61/(79+61)=0.44ã€‚ Probability from Classå…ˆåªè€ƒè™‘Defenseå’ŒSP Defenseè¿™ä¸¤ä¸ªfeatureã€‚ å¦‚æžœä¸è€ƒè™‘ç”Ÿæˆåˆ†å¸ƒæ¨¡åž‹ï¼Œåœ¨testingä¸­ç›´æŽ¥è®¡ç®—P(x|Water)çš„æ¦‚çŽ‡ï¼Œå¦‚ä¸‹å›¾å³ä¸‹è§’çš„é‚£åªé¾Ÿé¾Ÿï¼Œåœ¨training dataä¸­æ²¡æœ‰å‡ºçŽ°è¿‡ï¼Œé‚£å€¼ä¸º0å—ï¼Ÿæ˜¾ç„¶ä¸å¯¹ã€‚ å‡è®¾ï¼šä¸Šå›¾ä¸­water typeçš„examplesæ˜¯ä»ŽGaussian distributionï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰ä¸­å–æ ·å‡ºæ¥çš„ã€‚ å› æ­¤åœ¨trainingä¸­é€šè¿‡training dataå¾—åˆ°æœ€ä¼˜çš„Gaussian distributionçš„å‚æ•°ï¼Œè®¡ç®—æ ·æœ¬ä¸­æ²¡æœ‰å‡ºçŽ°è¿‡çš„P(x|Water)ä¹Ÿå°±è¿Žåˆƒè€Œè§£äº†ã€‚ Gaussian Distributionå¤šç»´çš„é«˜æ–¯åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒå°±æ˜¯æ­£æ€åˆ†å¸ƒå•¦ï¼‰çš„è”åˆæ¦‚çŽ‡å¯†åº¦ï¼š $$ f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\} $$ D: ç»´æ•° $\\mu$ : mean $\\Sigma$ :covariance matrix(åæ–¹å·®çŸ©é˜µ) åæ–¹å·®ï¼š $ cov(X,Y)=E[[X-E(X)][Y-E(Y)]]=E(XY)-E(X)E(Y)$ å…·ä½“åæ–¹å·®æ€§è´¨ï¼ŒæŸ¥é˜…æ¦‚çŽ‡è®ºè¯¾æœ¬å§ã€‚ x: vector,nç»´éšæœºå˜é‡ é«˜æ–¯åˆ†å¸ƒçš„æ€§è´¨åªå’Œ $\\mu$ å’Œ $\\Sigma$ æœ‰å…³ã€‚ $\\Sigma$ ä¸€å®šæ—¶ï¼Œ$\\mu$ ä¸åŒï¼Œå¦‚ä¸‹å›¾ï¼š $\\mu$ ä¸€å®šï¼Œ $\\Sigma$ ä¸åŒæ—¶ï¼Œå¦‚ä¸‹å›¾ï¼š Maximum Likelihoodï¼ˆæžå¤§ä¼¼ç„¶ï¼‰æ ·æœ¬åˆ†å¸ƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‡è®¾è¿™äº›æ ·æœ¬æ˜¯ä»ŽGaussian distributionä¸­å–æ ·ï¼Œé‚£å¦‚ä½•åœ¨è®­ç»ƒä¸­å¾—åˆ°é«˜æ–¯åˆ†å¸ƒçš„ $\\mu$ å’Œ $\\Sigma$ å‘¢ï¼Ÿ æžå¤§ä¼¼ç„¶ä¼°è®¡ã€‚ è€ƒè™‘Waterï¼Œæœ‰79ä¸ªæ ·æœ¬ï¼Œä¼°è®¡å‡½æ•° $L(\\mu, \\Sigma)=f_{\\mu, \\Sigma}\\left(x^{1}\\right) f_{\\mu, \\Sigma}\\left(x^{2}\\right) f_{\\mu, \\Sigma}\\left(x^{3}\\right) \\ldots \\ldots f_{\\mu, \\Sigma}\\left(x^{79}\\right)$ æžå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå³æ‰¾åˆ° $f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\}$ ä¸­çš„ $\\mu$ å’Œ$\\Sigma$ ä½¿å¾—ä¼°è®¡å‡½æ•°æœ€å¤§ï¼ˆä½¿å¾—å–å‡ºè¿™äº›æ ·æœ¬çš„æ¦‚çŽ‡æœ€å¤§åŒ–ï¼‰ã€‚ $\\mu^{*}, \\Sigma^{*}=\\arg \\max _{\\mu, \\Sigma} L(\\mu, \\Sigma)$ æ±‚å¯¼è®¡ç®—ï¼ˆè¿‡äºŽå¤æ‚ï¼Œä½†ä¹Ÿä¸æ˜¯ä¸èƒ½åšæ˜¯å§ï¼‰ èƒŒå…¬å¼[3] $\\mu^{*}=\\frac{1}{79} \\sum_{n=1}^{79} x^{n} \\qquad \\Sigma^{*}=\\frac{1}{79} \\sum_{n=1}^{79}\\left(x^{n}-\\mu^{*}\\right)\\left(x^{n}-\\mu^{*}\\right)^{T}$ å¾—åˆ°Waterå’ŒNormalçš„é«˜æ–¯åˆ†å¸ƒï¼Œå¦‚ä¸‹å›¾: Do Classification: different $\\Sigma$TestingTestingï¼š $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ P(x|C1)ç”±è®­ç»ƒå¾—å‡ºçš„Waterçš„é«˜æ–¯åˆ†å¸ƒè®¡ç®—å‡ºï¼ŒP(x|C2)ç”±Normalçš„é«˜æ–¯åˆ†å¸ƒè®¡ç®—å‡ºã€‚ï¼ˆå¦‚ä¸‹å›¾ï¼Œè¿‡äºŽéš¾æ‰“ï¼‰ å¦‚æžœP(C1|x)&gt;0.5ï¼Œè¯´æ˜Žx å±žäºŽWater(Class 1)ã€‚ Resultså¦‚æžœåªè€ƒè™‘ä¸¤ä¸ªfeatureï¼ˆDefenseå’ŒSP Defenseï¼‰ï¼Œä¸‹å›¾æ˜¯testing dataçš„æ ·æœ¬å›¾ï¼Œè“è‰²å±žäºŽWaterï¼Œçº¢è‰²å±žäºŽNormalã€‚ ç”¨è®­ç»ƒå¾—å‡ºçš„æ¨¡åž‹ï¼ŒTesting Data: 47% accuracyã€‚ï¼ˆç»“æžœå¦‚ä¸‹å›¾ï¼‰ å¦‚æžœè€ƒè™‘å…¨éƒ¨features(7ä¸ª)ï¼Œé‡æ–°è®­ç»ƒå‡ºçš„æ¨¡åž‹ï¼Œç»“æžœï¼šTesting Dataï¼š54% accuracyã€‚ï¼ˆç»“æžœå¦‚ä¸‹å›¾ï¼‰ ç»“æžœå¹¶ä¸å¥½ã€‚å‚æ•°è¿‡å¤šï¼Œæ¨¡åž‹è¿‡äºŽå¤æ‚ï¼Œæœ‰äº›è¿‡æ‹Ÿåˆäº†ã€‚ Modifying Modelï¼šsame $\\Sigma$æ¨¡åž‹ä¸­çš„å‚æ•°æœ‰ä¸¤ä¸ªçš„Gaussian Distributionä¸­çš„ $\\mu^* $ å’Œ $\\Sigma^*$ ï¼Œå…¶ä¸­åæ–¹å·®çŸ©é˜µçš„å¤§å°ç­‰äºŽfeatureçš„å¹³æ–¹ï¼Œæ‰€ä»¥è®©ä¸åŒçš„class share åŒä¸€ä¸ª $\\Sigma$ ï¼Œä»¥æ­¤æ¥å‡å°‘å‚æ•°ï¼Œç®€åŒ–æ¨¡åž‹ã€‚ æžå¤§ä¼¼ç„¶ä¼°è®¡çš„ä¼°è®¡å‡½æ•°ï¼š $$ L\\left(\\mu^{1}, \\mu^{2}, \\Sigma\\right)=f_{\\mu^{1}, \\Sigma}\\left(x^{1}\\right) f_{\\mu^{1}, \\Sigma}\\left(x^{2}\\right) \\cdots f_{\\mu^{1}, \\Sigma}\\left(x^{79}\\right)\\times f_{\\mu^{2}, \\Sigma}\\left(x^{80}\\right) f_{\\mu^{2}, \\Sigma}\\left(x^{81}\\right) \\cdots f_{\\mu^{2}, \\Sigma}\\left(x^{140}\\right) $$ å…¬å¼æŽ¨å¯¼:[3] $\\mu$ çš„å…¬å¼ä¸å˜ã€‚ $\\Sigma=\\frac{79}{140} \\Sigma^{1}+\\frac{61}{140} \\Sigma^{2}$ ,å³æ˜¯åŽŸ $\\Sigma^1\\ \\Sigma^2$çš„åŠ æƒå¹³å‡ã€‚ Resultså½“åªè€ƒè™‘ä¸¤ä¸ªfeaturesï¼Œç”¨åŒæ ·çš„åæ–¹å·®å‚æ•°ï¼Œç»“æžœå¦‚ä¸‹å›¾ï¼š å¯ä»¥å‘çŽ°ï¼Œç”¨äº†åŒæ ·çš„åæ–¹å·®çŸ©é˜µå‚æ•°åŽï¼Œè¾¹ç•Œå˜æˆäº†çº¿æ€§çš„ï¼Œæ‰€ä»¥è¿™ä¹Ÿæ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡åž‹ã€‚ å†è€ƒè™‘7ä¸ªfeaturesï¼Œç”¨åŒæ ·çš„åæ–¹å·®çŸ©é˜µå‚æ•°ï¼Œæ¨¡åž‹ä¹Ÿæ˜¯çº¿æ€§æ¨¡åž‹ï¼Œä½†ç”±äºŽåœ¨é«˜ç»´ç©ºé—´ï¼Œäººæ— æ³•ç›´æŽ¥ç”»å‡ºå…¶boundaryï¼Œè¿™ä¹Ÿæ˜¯æœºå™¨å­¦ä¹ çš„é­…åŠ›æ‰€åœ¨ï¼Œèƒ½è§£å†³ä¸€äº›äººæ— æ³•è§£å†³çš„é—®é¢˜ã€‚ ç»“æžœï¼šä»Žä¹‹å‰çš„54% accuracyå¢žåŠ åˆ° 73% accurancy. ç»“æžœæ˜Žæ˜¾å˜å¥½äº†ã€‚ SummaryThree Stepsï¼š Function Setï¼ˆModelï¼‰ï¼š Goodness of a function: The mean Âµ and convariance $\\Sigma$ that maximizing the likelihood(the probability of generating data) Find the best function:easy(å…¬å¼) Appendixä¸ºä»€ä¹ˆè¦é€‰æ‹©Gaussian DistributionYou can always use the distribution you like. å¯ä»¥é€‰æ‹©ä½ å–œæ¬¢çš„ä»»æ„åˆ†å¸ƒï¼Œtåˆ†å¸ƒï¼Œå¼€æ–¹åˆ†å¸ƒç­‰ã€‚ ï¼ˆè€å¸ˆè¯´ï¼šå¦‚æžœæˆ‘é€‰æ‹©å…¶ä»–åˆ†å¸ƒï¼Œä½ ä¹Ÿä¼šé—®è¿™ä¸ªé—®é¢˜ï¼Œh h hï¼‰ Naive Bayes ClassifierIf you assume all the dimensions are independent, then you are using Naive Bayes Classifier. å¦‚æžœå‡è®¾featuresä¹‹é—´äº’ç›¸ç‹¬ç«‹ï¼Œ $P\\left(x | C_{1}\\right)=P\\left(x_{1} | C_{1}\\right) P\\left(x_{2} | C_{1}\\right) \\quad \\ldots \\ldots \\quad P\\left(x_{k} | C_{1}\\right) $ ã€‚ xiæ˜¯xç¬¬iç»´åº¦çš„featureã€‚ å¯¹äºŽæ¯ä¸€ä¸ª P(xi|C1)ï¼Œå¯ä»¥å‡è®¾å…¶æœä»Žä¸€ç»´é«˜æ–¯åˆ†å¸ƒã€‚å¦‚æžœæ˜¯binary featuresï¼ˆå³featureå–å€¼åªæœ‰ä¸¤ä¸ªï¼‰ï¼Œä¹Ÿå¯ä»¥å‡è®¾å®ƒæœä»ŽBernoulli distribution(è´åŠªåˆ©åˆ†å¸ƒ)ã€‚ Posterior Probabilityï¼ˆåŽéªŒæ¦‚çŽ‡ï¼‰Posterior ProbabilityåŽéªŒæ¦‚çŽ‡ï¼Œå³ä½¿ç”¨è´å¶æ–¯å…¬å¼ï¼Œå·²çŸ¥ç»“æžœï¼Œå¯»æ‰¾æœ€ä¼˜å¯èƒ½å¯¼è‡´å®ƒå‘ç”Ÿçš„åŽŸå› ã€‚ å¯¹ $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ è¿›è¡Œå¤„ç†ã€‚ å¾—åˆ°ï¼š $$ \\begin{equation} \\begin{aligned} P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\ &=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z) \\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} \\end{aligned} \\end{equation} $$ Worning of Math $z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}=\\ln \\frac{P\\left(x | C_{1}\\right)}{P\\left(x | C_{2}\\right)}+\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}$ $\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}=\\frac{\\frac{N_{1}}{N_{1}+N_{2}}}{\\frac{N_{2}}{N_{1}+N_{2}}}=\\frac{N_{1}}{N_{2}}$ $P\\left(x | C_{1}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma 1|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)\\right\\}$ $P\\left(x | C_{2}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{\\left|\\Sigma^{2}\\right| 1 / 2} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right\\}$ $\\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2}\\left[\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)-\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right]$ $\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)=x^{T}\\left(\\Sigma^{1}\\right)^{-1} x-2\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1}$ $\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)=x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-2\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}$ $\\begin{aligned} z=& \\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2} x^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1} \\\\ &+\\frac{1}{2} x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} \\end{aligned}$ ç®€åŒ–æ¨¡åž‹åŽï¼Œ $\\Sigma^1=\\Sigma^2=\\Sigma$ : $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ ä»¤ $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ å½“ç®€åŒ–æ¨¡åž‹åŽï¼Œzæ˜¯çº¿æ€§çš„ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåœ¨ä¹‹å‰çš„ç»“æžœä¸­è¾¹ç•Œæ˜¯çº¿æ€§çš„åŽŸå› ã€‚ æœ€åŽæ¨¡åž‹å˜æˆè¿™æ ·ï¼š $P\\left(C_{1} | x\\right)=\\sigma(w \\cdot x+b)$ . åœ¨ç”Ÿæˆæ¨¡åž‹ä¸­ï¼Œæˆ‘ä»¬å…ˆä¼°è®¡å‡º $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ çš„å€¼ï¼Œä¹Ÿå°±å¾—åˆ°äº† $w\\ b$ çš„å€¼ã€‚ é‚£ï¼Œæˆ‘ä»¬èƒ½ä¸èƒ½è·³è¿‡ $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ ï¼Œç›´æŽ¥ä¼°è®¡ $w\\ b$ å‘¢ï¼Ÿ åœ¨ä¸‹ä¸€ç¯‡åšå®¢[4]ä¸­ä¼šç»§ç»­Classificationã€‚ Reference Classification as Regression: Bishop, P186. æŒ–å‘ï¼šClassificationï¼šPerceptronï¼ŒSVM. Maximum likelihood solutionï¼šBishop chapter4.2.2","link":"/2020/03/19/Classification1/"},{"title":"ã€ŒToolsã€ï¼šDocker","text":"æœ¬ç¯‡æ–‡ç« ä¸»è¦åˆ†å››ä¸ªéƒ¨åˆ†ï¼Œé¦–å…ˆä»‹ç»äº†Dockeræ˜¯ä»€ä¹ˆï¼šä¸ºä»€ä¹ˆä¼šæœ‰DockeræŠ€æœ¯çš„å‡ºçŽ°ï¼›è™šæ‹ŸåŒ–æŠ€æœ¯å’Œå®¹å™¨è™šæ‹ŸåŒ–æŠ€æœ¯çš„åŒºåˆ«ï¼›Dockerçš„åŸºæœ¬ç»„æˆï¼›Dockerçš„è¿è¡Œä¸ºä»€ä¹ˆä¼šæ¯”è™šæ‹Ÿæœºå¿«ã€‚ ç¬¬äºŒä¸ªéƒ¨åˆ†ä¸»è¦ä»‹ç»äº†Dockerçš„å¸¸ç”¨å‘½ä»¤ï¼ŒåŒ…æ‹¬é•œåƒå‘½ä»¤å’Œå®¹å™¨å‘½ä»¤ï¼Œæ–‡ä¸­è¿˜ä»Žåº•å±‚çš„è§’åº¦åˆ†æžDockeré•œåƒã€‚ ç¬¬ä¸‰ä¸ªéƒ¨åˆ†ä»‹ç»äº†Dockerä¸­çš„å®¹å™¨æ•°æ®å·ï¼Œå’Œå¦‚ä½•æŒ‚è½½æ•°æ®å·ã€‚ æœ€åŽä¸€ä¸ªéƒ¨åˆ†ï¼Œç®€å•ä»‹ç»äº†Dockerfileæ–‡ä»¶ã€‚ Dockerç®€ä»‹Docker æ˜¯ä»€ä¹ˆå¼€å‘å’Œè¿ç»´ä¹‹é—´çš„çŽ¯å¢ƒå’Œé…ç½®é—®é¢˜ï¼šåœ¨æˆ‘çš„æœºå™¨ä¸Šå¯ä»¥æ­£å¸¸å·¥ä½œã€‚ æŠŠä»£ç /é…ç½®/ç³»ç»Ÿ/æ•°æ®ç­‰å…¨éƒ¨æ‰“åŒ…æˆé•œåƒï¼Œè¿ç»´å·¥ç¨‹å¸ˆå¸¦çŽ¯å¢ƒå®‰è£…è½¯ä»¶ã€‚ DockeråŸºäºŽGoè¯­è¨€å®žçŽ°çš„äº‘å¼€æºé¡¹ç›®ï¼ŒDockerçš„ä¸»è¦ç›®æ ‡æ˜¯â€œBuildï¼ŒShip and Run Any App,Anywhereâ€ï¼Œåšåˆ°ä¸€æ¬¡å°è£…ï¼Œå¤„å¤„è¿è¡Œã€‚ Linux å®¹å™¨æŠ€æœ¯çš„å‡ºçŽ°å°±è§£å†³äº†è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼Œè€Œ Docker å°±æ˜¯åœ¨å®ƒçš„åŸºç¡€ä¸Šå‘å±•è¿‡æ¥çš„ã€‚å°†åº”ç”¨è¿è¡Œåœ¨ Docker å®¹å™¨ä¸Šé¢ï¼Œè€Œ Docker å®¹å™¨åœ¨ä»»ä½•æ“ä½œç³»ç»Ÿä¸Šéƒ½æ˜¯ä¸€è‡´çš„ï¼Œè¿™å°±å®žçŽ°äº†è·¨å¹³å°ã€è·¨æœåŠ¡å™¨ã€‚åªéœ€è¦ä¸€æ¬¡é…ç½®å¥½çŽ¯å¢ƒï¼Œæ¢åˆ°åˆ«çš„æœºå­ä¸Šå°±å¯ä»¥ä¸€é”®éƒ¨ç½²å¥½ï¼Œå¤§å¤§ç®€åŒ–äº†æ“ä½œã€‚ Dockerè§£å†³äº†è¿è¡ŒçŽ¯å¢ƒå’Œé…ç½®é—®é¢˜çš„è½¯ä»¶å®¹å™¨ï¼Œæ–¹ä¾¿åšæŒç»­é›†æˆå¹¶æœ‰åŠ©äºŽæ•´ä½“åˆ†å¸ƒçš„å®¹å™¨è™šæ‹ŸåŒ–æŠ€æœ¯ã€‚ èƒ½å¹²å˜›ï¼Ÿä¹‹å‰çš„è™šæ‹ŸåŒ–æŠ€æœ¯è™šæ‹Ÿæœºæ˜¯å¸¦çŽ¯å¢ƒå®‰è£…çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨ä¸€ç§æ“ä½œç³»ç»Ÿä¸­è¿è¡Œå¦ä¸€ç§æ“ä½œç³»ç»Ÿã€‚ è™šæ‹Ÿæœºç”¨è½¯ä»¶å®žçŽ°äº†ç¡¬ä»¶ã€å†…æ ¸ã€æ“ä½œç³»ç»ŸåŠåº”ç”¨ç¨‹åºï¼Œå¯¹åº•å±‚æ¥è¯´ï¼Œè™šæ‹Ÿæœºå°±æ˜¯ä¸€ä¸ªæ™®é€šæ–‡ä»¶ã€‚ è™šæ‹Ÿæœºçš„ç¼ºç‚¹ç¼ºç‚¹ï¼š èµ„æºå ç”¨å¤š å†—ä½™æ­¥éª¤ å¯åŠ¨æ…¢ å®¹å™¨è™šæ‹ŸåŒ–æŠ€æœ¯Linuxå®¹å™¨ï¼ˆLinux Containers,LXC)ï¼Œå¯¹è¿›ç¨‹éš”ç¦»ï¼Œå°†è½¯ä»¶è¿è¡Œæ‰€éœ€çš„èµ„æºæ‰“åŒ…åˆ°ä¸€ä¸ªéš”ç¦»çš„ç—›å…¶ä¸­ã€‚ Linuxå®¹å™¨ä¸æ˜¯æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„æ“ä½œç³»ç»Ÿï¼Œè€Œæ˜¯å°†è½¯ä»¶å·¥ä½œæ‰€éœ€çš„åº“èµ„æºå’Œè®¾ç½®ç­‰èµ„æºæ‰“åŒ…åˆ°ä¸€ä¸ªéš”ç¦»çš„å®¹å™¨ä¸­ï¼Œå› æ­¤Linuxå®¹å™¨å˜å¾—é«˜æ•ˆä¸”è½»é‡ï¼Œå¹¶ä¸”èƒ½ä¿è¯éƒ¨ç½²åœ¨ä»»ä½•çŽ¯å¢ƒä¸­çš„è½¯ä»¶éƒ½èƒ½å§‹ç»ˆå¦‚ä¸€åœ°è¿è¡Œã€‚åœ¨ å®¿ä¸»æœºä¸Šï¼ŒLinuxå®¹å™¨å°±æ˜¯ä¸€ä¸ªè¿è¡Œçš„è¿›ç¨‹ï¼Œæ‰€ä»¥Linuxå®¹å™¨æ˜¯å¯¹è¿›ç¨‹è¿›è¡Œéš”ç¦»ã€‚ å†çœ‹Dockerçš„å›¾æ ‡ï¼Œä¸Šé¢çš„é›†è£…ç®±å°±æ˜¯ä¸€ä¸ªä¸€ä¸ªå®¹å™¨ï¼Œé²¸é±¼å°±æ˜¯å®¿ä¸»æœºçš„ç¡¬ä»¶ã€å†…æ ¸ã€‚ æ¯”è¾ƒï¼š ä¼ ç»Ÿè™šæ‹ŸæœºæŠ€æœ¯è™šæ‹Ÿä¸€å¥—ç¡¬ä»¶ï¼Œåœ¨å…¶ä¸Šè¿è¡Œä¸€ä¸ªå®Œæ•´çš„æ“ä½œç³»ç»Ÿï¼Œå†è¿è¡Œæ‰€éœ€çš„åº”ç”¨è¿›ç¨‹ã€‚ å®¹å™¨å†…çš„åº”ç”¨ç›´æŽ¥è¿è¡ŒäºŽå®¿ä¸»çš„å†…æ ¸ï¼Œå®¹å™¨å†…æ²¡æœ‰ç¡¬ä»¶è™šæ‹Ÿï¼Œå®¹å™¨æ›´è½»ä¾¿ã€‚ å®¹å™¨ä¹‹é—´äº’ç›¸éš”ç¦»ï¼Œæ¯ä¸ªå®¹å™¨æœ‰è‡ªå·±çš„æ–‡ä»¶ç³»ç»Ÿï¼Œå®¹å™¨ä¹‹é—´è¿›ç¨‹ä¸ä¼šç›¸äº’å½±å“ã€‚ æ‰€ä»¥ï¼Œå¯ä»¥è®¤ä¸ºå®¹å™¨æ˜¯ä¸€ä¸ªè½»é‡çš„Linuxã€‚ å¼€å‘/è¿ç»´ï¼ˆDevOps)DevOps, Develop and Operations, å¯ä»¥åˆ©ç”¨Dockerå®žçŽ°å¼€å‘è‡ªè¿ç»´ã€‚ æ›´å¿«é€Ÿçš„åº”ç”¨äº¤ä»˜å’Œéƒ¨ç½²ã€‚ æ›´ä¾¿æ·çš„å‡çº§å’Œæ‰©ç¼©å®¹å™¨ã€‚ æ›´ç®€å•çš„ç³»ç»Ÿè¿ç»´ã€‚ æ›´é«˜æ•ˆçš„è®¡ç®—èµ„æºåˆ©ç”¨ã€‚ Dockerçš„åŸºæœ¬ç»„æˆDockerçš„ä¸‰è¦ç´ ï¼š é•œåƒ(image)ï¼šåªè¯»çš„æ¨¡ç‰ˆï¼Œç±»æ¯”Javaä¸­çš„ç±»ã€‚é•œåƒå¯ä»¥ç”¨æ¥åˆ›é€ Dockerå®¹å™¨ã€‚ å®¹å™¨(container)ï¼šé•œåƒçš„å®žä¾‹ï¼Œç‹¬ç«‹è¿è¡Œçš„ä¸€ä¸ªæˆ–ä¸€ç»„å®žä¾‹ã€‚å¯ä»¥æŠŠå®¹å™¨çœ‹ä½œä¸€ä¸ªç®€æ˜“ç‰ˆçš„LinuxçŽ¯å¢ƒã€‚ ä»“åº“(repository)ï¼šä¿å­˜é•œåƒçš„åœºæ‰€ã€‚ Dockeræœ¬èº«æ˜¯ä¸€ä¸ªå®¹å™¨è¿è¡Œè½½ä½“æˆ–ç®¡ç†å¼•æ“Žã€‚ æŠŠåº”ç”¨ç¨‹åºå’Œé…ç½®æ‰“åŒ…æˆä¸ºä¸€ä¸ªå¯äº¤ä»˜çš„è¿è¡ŒçŽ¯å¢ƒï¼Œæ‰“åŒ…å¥½çš„è¿è¡ŒçŽ¯å¢ƒå°±æ˜¯ä¸€ä¸ªimageé•œåƒæ–‡ä»¶ï¼Œåªæœ‰é€šè¿‡è¿™ä¸ªé•œåƒæ–‡ä»¶æ‰èƒ½ç”ŸæˆDockerå®¹å™¨ã€‚imageæ–‡ä»¶å¯ä»¥çœ‹ä½œæ˜¯å®¹å™¨çš„æ¨¡ç‰ˆã€‚Dockeræ ¹æ®imageæ–‡ä»¶ç”Ÿæˆå®¹å™¨çš„å®žä¾‹ã€‚ Dockerè¿è¡ŒåŽŸç†Dockeræ˜¯ä¸€ä¸ªC/Sç»“æž„çš„ç³»ç»Ÿã€‚ Dockerå®ˆæŠ¤è¿›ç¨‹è¿è¡Œåœ¨å®¿ä¸»æœºä¸Šï¼Œå®¢æˆ·é€šè¿‡Socketè¿žæŽ¥ä»Žå®¢æˆ·ç«¯è®¿é—®ï¼Œå®ˆæŠ¤è¿›ç¨‹ä»Žå®¢æˆ·ç«¯æŽ¥å—å‘½ä»¤å¹¶ç®¡ç†è¿è¡Œåœ¨ä¸»æœºä¸Šçš„å®¹å™¨ã€‚ ä¸ºä»€ä¹ˆæ¯”è™šæ‹Ÿæœºå¿« Dockeræœ‰æ¯”è™šæ‹Ÿæœºæ›´å°‘çš„æŠ½è±¡å±‚ï¼Œä¸éœ€è¦å®žçŽ°ç¡¬ä»¶èµ„æºè™šæ‹ŸåŒ–ï¼Œè¿è¡Œåœ¨dockerå®¹å™¨ä¸­çš„ç¨‹åºç›´æŽ¥ä½¿ç”¨çš„éƒ½æ˜¯å®žé™…ç‰©ç†æœºçš„ç¡¬ä»¶èµ„æºã€‚ Dockerä½¿ç”¨å®¿ä¸»æœºä¸Šçš„å†…æ ¸ï¼Œæ–°å»ºå®¹å™¨æ—¶ï¼Œä¸éœ€è¦å’Œè™šæ‹Ÿæœºä¸€æ ·é‡æ–°åŠ è½½ä¸€ä¸ªæ“ä½œç³»ç»Ÿå†…æ ¸ã€‚å› æ­¤æ–°å»ºä¸€ä¸ªdock erå®¹å™¨åªéœ€è¦å‡ ç§’é’Ÿã€‚ Dockeré•œåƒåŠ é€Ÿå¯ä»¥ç™»é™†é˜¿é‡Œäº‘èŽ·å¾—ä¸“å±žé•œåƒåŠ é€Ÿå™¨é“¾æŽ¥ï¼Œé…ç½®æœ¬æœºDockeræ‹‰å–é•œåƒä»“åº“çš„é“¾æŽ¥ï¼Œå°†æ‹‰å–é•œåƒçš„é“¾æŽ¥ä»ŽDockerHubæ¢æˆé˜¿é‡Œäº‘çš„ä»“åº“ï¼Œä¸‹è½½æ›´å¿«æ·ã€‚ å…·ä½“æŒ‰ç…§ç³»ç»Ÿè‡ªè¡ŒGoogleã€‚ Dockerå¸¸ç”¨å‘½ä»¤docker version docker info docker â€“help å¸®åŠ©å‘½ä»¤ é•œåƒå‘½ä»¤ åˆ—å‡ºæœ¬åœ°images docker images repo å‚æ•° -a :åŒ…æ‹¬ä¸­é—´æ˜ åƒå±‚ -q : åªæ˜¾ç¤ºé•œåƒid â€“digests :æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯ â€“no-trunc :æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯ ä»ŽDocker HubæŸ¥è¯¢é•œåƒå docker search [OPTIONS] image_name â€“no-trunc -s nï¼šæ”¶è—æ•°ä¸å°äºŽnçš„é•œåƒ â€“automated ä¸‹è½½/æ‹‰å–é•œåƒ docker pull é•œåƒå[:TAG] é»˜è®¤:latest åˆ é™¤é•œåƒ docker rmi é•œåƒå”¯ä¸€åå­—/é•œåƒID -f :å¼ºåˆ¶åˆ é™¤è¿è¡Œä¸­çš„é•œåƒæ–‡ä»¶ åˆ é™¤å•ä¸ªï¼š docker rmi -f é•œåƒID åˆ é™¤å¤šä¸ª docker rmi -f é•œåƒå1:TAG é•œåƒå2:TAG åˆ é™¤å…¨éƒ¨ï¼š docker rmi -f $(docker images -qa) å®¹å™¨å‘½ä»¤å®¹å™¨æ˜¯ä¸€ä¸ªå»ºè®®çš„Linuxã€‚ å¯åŠ¨å®¹å™¨ï¼š docker run [OPTIONS] IMAGE [COMMAND] [ARG...] --name å®¹å™¨å :ä¸ºå®¹å™¨æŒ‡å®šä¸€ä¸ªåå­— -d ï¼šåŽå°è¿è¡Œå®¹å™¨ï¼Œè¿”å›ž -i : ä»¥äº¤äº’æ¨¡å¼è¿è¡Œå®¹å™¨ï¼Œé€šå¸¸ä¸Ž-t ä¸€åŒä½¿ç”¨ -t :ä¸ºå®¹å™¨é‡æ–°åˆ†é…ä¸€ä¸ªä¼ªè¾“å…¥ç»ˆç«¯ï¼Œé€šå¸¸ä¸Ž-i ä¸€åŒä½¿ç”¨ã€‚ -p :ä¸»æœºç«¯å£å’Œå®¹å™¨ç«¯å£ -p ip:hostPort:containerPort -p ip::containerPort -p hostPort:containerPort -p containerPort -P :éšæœºåˆ†é…ç«¯å£ åˆ—å‡ºå½“å‰è¿è¡Œæ‰€æœ‰å®¹å™¨ï¼š docker ps -a : åˆ—å‡ºå½“å‰æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„å®¹å™¨å’ŒåŽ†å²ä¸Šè¿è¡Œè¿‡çš„å®¹å™¨ -l :æ˜¾ç¤ºæœ€è¿‘åˆ›å»ºçš„å®¹å™¨ -n :æ˜¾ç¤ºæœ€è¿‘åˆ›å»ºçš„numä¸ªå®¹å™¨ docker ps -n 3 -q :é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºå®¹å™¨ç¼–å· --no-trunc : ä¸æˆªæ–­è¾“å‡º é€€å‡º/åœæ­¢å®¹å™¨ å®¹å™¨åœæ­¢é€€å‡º exit å®¹å™¨ä¸åœæ­¢é€€å‡º Ctrl + P + Q å¯åŠ¨å®¹å™¨ docker start å®¹å™¨å/å®¹å™¨ID é‡å¯å®¹å™¨ docker restart å®¹å™¨å/å®¹å™¨ID é‡å¯æˆåŠŸåŽè¿”å›žå®¹å™¨å/å®¹å™¨ID åœæ­¢å®¹å™¨ docker stop å®¹å™¨å/å®¹å™¨ID å¼ºåˆ¶åœæ­¢å®¹å™¨ docker kill å®¹å™¨å/å®¹å™¨ID åˆ é™¤å·²åœæ­¢çš„å®¹å™¨ docker rm é•œåƒID ä¸€æ¬¡åˆ é™¤å¤šä¸ªå®¹å™¨ docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm ï¼ˆç®¡é“ä¼ é€’å‚æ•°ï¼‰ å¯åŠ¨å®ˆæŠ¤å¼å®¹å™¨ docker run -d é•œåƒå/é•œåƒID docker run -d -p ä¸»æœºç«¯å£:å®¹å™¨å†…ç«¯å£ å®¹å™¨ID å¦‚æžœä½¿ç”¨ docker ps -a æŸ¥çœ‹ï¼Œä¼šå‘çŽ°å®¹å™¨å·²ç»é€€å‡º Dockerå®¹å™¨åŽå°è¿è¡Œï¼Œå°±å¿…é¡»è¦æœ‰ä¸€ä¸ªå‰å°è¿›ç¨‹ä¸Žä¹‹äº¤äº’ å¦‚æžœå®¹å™¨åŽå°è¿è¡Œï¼Œå¦‚æžœä¸æ˜¯ä¸€ç›´æŒ‚èµ·çš„å‘½ä»¤ï¼Œä»–å°±ä¼šè‡ªåŠ¨é€€å‡ºã€‚ æ‰€ä»¥æœ€ä½³çš„è§£å†³æ–¹å¼æ˜¯å°†è¿è¡Œçš„è¿›ç¨‹ä»¥å‰å°è¿›ç¨‹è¿è¡Œã€‚ æŸ¥çœ‹å®¹å™¨æ—¥å¿— docker logs -f -t --tail å®¹å™¨ID -tï¼šæ˜¾ç¤ºåŠ å…¥æ—¶é—´æˆ³ -f ï¼šæŒç»­æ˜¾ç¤ºæœ€æ–°çš„æ—¥å¿— --tail ï¼šæ˜¾ç¤ºæœ€åŽå¤šå°‘æ¡ æ˜¾ç¤ºå®¹å™¨å†…è¿è¡Œçš„è¿›ç¨‹ docker top å®¹å™¨ID æŸ¥çœ‹å®¹å™¨å†…éƒ¨çš„ç»†èŠ‚ docker inspect å®¹å™¨ID è¿›å…¥æ­£åœ¨è¿è¡Œçš„å®¹å™¨å¹¶ä»¥å‘½ä»¤è¡Œä¸Žä¹‹äº¤äº’ ç›´æŽ¥è¿›å…¥å®¹å™¨å¯åŠ¨å‘½ä»¤çš„ç»ˆç«¯ docker attach å®¹å™¨ID åœ¨å®¹å™¨ä¸­æ‰“å¼€æ–°çš„ç»ˆç«¯ï¼Œå¹¶ä¸”å¯ä»¥å¯åŠ¨æ–°çš„è¿›ç¨‹ã€‚ docker exec -it å®¹å™¨ID bashShell docker exec -it å®¹å™¨ID /bin/bash å’Œdocker attach å®¹å™¨ID ç›¸åŒã€‚ æŠŠå®¹å™¨å†…æ–‡ä»¶æ‹·è´æ–‡ä»¶åˆ°ä¸»æœºä¸Š docker cp å®¹å™¨ID:å®¹å™¨å†…çš„è·¯å¾„ ç›®å½•ä¸»æœºè·¯å¾„ docker cp 130b1f6708dd:/x.txt /Users Dockeré•œåƒimageï¼š é•œåƒæ˜¯è½»é‡çº§ã€å¯æ‰§è¡Œçš„ç‹¬ç«‹è½¯ä»¶åŒ…ï¼Œç”¨æ¥æ‰“åŒ…è½¯ä»¶è¿è¡ŒçŽ¯å¢ƒå’ŒåŸºäºŽè¿è¡ŒçŽ¯å¢ƒå¼€å‘çš„è½¯ä»¶ï¼ŒåŒ…å«è¿è¡ŒæŸä¸ªè½¯ä»¶æ‰€éœ€çš„æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬ä»£ç ã€åº“ã€çŽ¯å¢ƒå˜é‡ã€é…ç½®æ–‡ä»¶ç­‰ã€‚ UnionFSUnionFSï¼ˆè”åˆæ–‡ä»¶ç³»ç»Ÿï¼‰æ˜¯ä¸€ç§åˆ†å±‚ã€è½»é‡é«˜æ€§èƒ½çš„æ–‡ä»¶ç³»ç»Ÿï¼Œæ”¯æŒå¯¹æ–‡ä»¶ç³»ç»Ÿçš„ä¿®æ”¹ä½œä¸ºä¸€æ¬¡æäº¤æ¥ä¸€å±‚å±‚çš„å åŠ ï¼ŒåŒæ—¶å°†ä¸åŒç›®å½•æŒ‚è½½åˆ°åŒä¸€ä¸ªè™šæ‹Ÿæ–‡ä»¶ç³»ç»Ÿä¸‹ã€‚ Unionæ–‡ä»¶ç³»ç»Ÿæ—¶Dockeré•œåƒçš„åŸºç¡€ã€‚ é•œåƒé€šè¿‡åˆ†å±‚æ¥è¿›è¡Œç»§æ‰¿ï¼ŒåŸºäºŽåŸºç¡€é•œåƒå¯ä»¥åˆ¶ä½œå„ç§å…·ä½“çš„åº”ç”¨é•œåƒã€‚ ç‰¹ç‚¹ï¼šä¸€æ¬¡åŠ è½½å¤šä¸ªæ–‡ä»¶ç³»ç»Ÿï¼Œä½†ä»Žå¤–é¢çœ‹èµ·æ¥ï¼Œåªèƒ½çœ‹åˆ°ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿï¼Œè”åˆåŠ è½½ä¼šæŠŠå„å±‚æ–‡ä»¶ç³»ç»Ÿå åŠ èµ·æ¥ï¼Œæœ€ç»ˆçš„æ–‡ä»¶ç³»ç»ŸåŒ…å«æ‰€æœ‰åº•å±‚çš„æ–‡ä»¶å’Œç›®å½•ã€‚ Dockeré•œåƒçš„åŠ è½½Dockeré•œåƒå®žé™…æ˜¯ç”±ä¸€å±‚ä¸€å±‚çš„æ–‡ä»¶ç³»ç»Ÿç»„æˆã€‚ bootfs(boot file system)åŒ…å«bootloaderå’Œkernelï¼Œbootloaderä¸»è¦æ˜¯å¼•å¯¼åŠ è½½kernelï¼ŒLinuxåˆšå¯åŠ¨æ—¶ä¼šåŠ è½½bootfsæ–‡ä»¶ç³»ç»Ÿã€‚ Dockeré•œåƒçš„æœ€åº•å±‚å°±æ˜¯bootfsï¼Œè¿™ä¸€å±‚å’Œå…¸åž‹çš„Linux/Unixç³»ç»Ÿæ˜¯ä¸€æ ·çš„ï¼ŒåŒ…å«bootloaderå’Œkernelã€‚ å½“bootåŠ è½½å®ŒæˆåŽï¼Œæ•´ä¸ªkernelå°±åœ¨å†…å­˜ä¸­äº†ï¼Œæ­¤æ—¶å†…å­˜çš„ä½¿ç”¨æƒå·²ç”±bootfsè½¬äº¤ç»™kernelï¼Œæ­¤æ—¶ç³»ç»Ÿä¹Ÿä¼šå¸è½½bootfsã€‚ rootfsï¼ˆroot file system)ï¼Œåœ¨bootfsä¹‹ä¸Šï¼ŒåŒ…å«çš„å°±æ˜¯å…¸åž‹Linuxç³»ç»Ÿä¸­çš„/dev, /proc, /bin, /etcç­‰æ ‡å‡†ç›®å½•å’Œæ–‡ä»¶ã€‚rootfså°±æ˜¯å„ç§ä¸åŒçš„æ“ä½œç³»ç»Ÿå‘è¡Œç‰ˆï¼Œæ¯”å¦‚Linuxï¼ŒCentosç­‰ã€‚ å¹³å¸¸å®‰è£…ç­‰è™šæ‹Ÿæœºçš„CentOSéƒ½æ˜¯å‡ ä¸ªGï¼Œä¸ºä»€ä¹ˆdockerç‰ˆçš„centosåªæœ‰å‡ ç™¾å…†ï¼Ÿ å¯¹äºŽä¸€ä¸ªç²¾ç®€çš„OSï¼Œrootfså¯ä»¥å¾ˆå°ï¼Œåªéœ€è¦åŒ…æ‹¬æœ€åŸºæœ¬çš„å‘½ä»¤ã€å·¥å…·å’Œç¨‹åºåº“ï¼Œå› ä¸ºåº•å±‚ç›´æŽ¥ä½¿ç”¨å®¿ä¸»æœºçš„kernelï¼Œè‡ªå·±åªéœ€è¦æä¾›rootfså°±è¡Œäº†ã€‚ å› æ­¤ï¼Œå¯¹äºŽä¸åŒçš„Linuxå‘è¡Œç‰ˆï¼ŒbootfsåŸºæœ¬ä¸€è‡´ï¼Œrootfsä¼šæœ‰å·®åˆ«ï¼Œå› æ­¤ä¸åŒçš„å‘è¡Œç‰ˆå¯ä»¥å…±ç”¨bootfsã€‚ åˆ†å±‚çš„é•œåƒåœ¨docker imageä¸‹è½½ã€åˆ é™¤æ—¶ï¼Œå¯ä»¥å‘çŽ°æ˜¯ä¸€å±‚ä¸€å±‚çš„ã€‚ åˆ†å±‚çš„é•œåƒçš„ä¸€ä¸ªæœ€å¤§çš„å¥½å¤„æ˜¯å…±äº«èµ„æºã€‚ å¦‚æžœæœ‰å¤šä¸ªé•œåƒéƒ½æ˜¯ä»Žç›¸åŒçš„baseé•œåƒbuildè€Œæ¥ï¼Œé‚£å®¿ä¸»æœºä¸­åªéœ€åœ¨ç£ç›˜ä¸Šä¿å­˜ä¸€ä»½baseé•œåƒï¼ŒåŒæ—¶å†…å­˜ä¸­ä¹Ÿåªéœ€è¦åŠ è½½ä¸€ä»½baseé•œåƒï¼Œå°±å¯ä»¥ä¸ºæ‰€æœ‰çš„å®¹å™¨æœåŠ¡äº†ã€‚ é•œåƒcommitæ“ä½œDockeré•œåƒéƒ½æ˜¯åªè¯»çš„ï¼Œä½†å½“é•œåƒå®žä¾‹åŒ–ï¼Œå¯åŠ¨å®¹å™¨æ—¶ï¼Œä¸€ä¸ªæ–°çš„å¯å†™å±‚è¢«åŠ è½½åˆ°é•œåƒçš„é¡¶éƒ¨ï¼Œè¿™ä¸€å±‚é€šå¸¸è¢«ç§°ä½œâ€œå®¹å™¨å±‚â€ï¼Œâ€œå®¹å™¨å±‚â€ä¹‹ä¸‹çš„éƒ½å«â€œé•œåƒå±‚â€ã€‚ docker commitæäº¤å®¹å™¨å±‚å‰¯æœ¬ä½¿ä¹‹æˆä¸ºä¸€ä¸ªæ–°çš„é•œåƒã€‚ docker commit -m &quot;message&quot; -a &quot;author&quot; å®¹å™¨ID å‘½åç©ºé—´/æ–°å»ºé•œåƒå[:TAGS] å®¹å™¨æ•°æ®å·Dockerç†å¿µï¼š å°†ä»£ç å’Œè¿è¡Œçš„çŽ¯å¢ƒæ‰“åŒ…å½¢æˆå®¹å™¨ï¼Œè¿è¡Œä¼´éšç€å®¹å™¨ï¼Œä½†å¸Œæœ›è¿è¡Œä¸­çš„æ•°æ®æ˜¯æŒä¹…åŒ–çš„ï¼Œå¸Œæœ›å®¹å™¨ä¹‹é—´æ˜¯å…±äº«æ•°æ®çš„ã€‚ å¦‚æžœä¸é€šè¿‡docker commitç”Ÿæˆæ–°çš„é•œåƒï¼Œä½¿å¾—æ•°æ®ä½œä¸ºé•œåƒçš„ä¸€éƒ¨åˆ†ä¿å­˜ä¸‹æ¥ï¼Œé‚£ä¹ˆå®¹å™¨åˆ é™¤åŽï¼Œæ•°æ®ä¹Ÿæ²¡æœ‰äº†ï¼Œä¸ºäº†ä¿å­˜æ•°æ®ï¼Œä½¿ç”¨å®¹å™¨æ•°æ®å·ã€‚ å¦‚æžœä¸ä½¿ç”¨commit ç”Ÿæˆæ–°çš„é•œåƒï¼ŒDockerå®¹å™¨äº§ç”Ÿçš„æ•°æ®å°†éšç€å®¹å™¨çš„åˆ é™¤è€Œä¸€èµ·åˆ é™¤ï¼Œä¸ºäº†ä¿å­˜æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨å·ã€‚ å·å·å°±æ˜¯ç›®å½•æˆ–è€…æ–‡ä»¶ï¼Œå­˜åœ¨äºŽä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨ä¸­ï¼Œç”±dockeræŒ‚è½½åˆ°å®¹å™¨ï¼Œä½†ä¸å±žäºŽUnionFSï¼ˆè”åˆæ–‡ä»¶ç³»ç»Ÿï¼‰ï¼Œå› æ­¤èƒ½ç»•è¿‡UnionFSï¼Œæä¾›ä¸€äº›ç”¨äºŽæŒç»­å­˜å‚¨æˆ–å…±äº«æ•°æ®çš„ç‰¹æ€§ã€‚ å·çš„è®¾è®¡ç›®çš„å°±æ˜¯ä¸ºäº†æ•°æ®æŒä¹…åŒ–ï¼Œå®Œå…¨ç‹¬ç«‹äºŽå®¹å™¨çš„ç”Ÿå­˜å‘¨æœŸï¼Œå› æ­¤Dockerä¸ä¼šåœ¨å®¹å™¨åˆ é™¤çš„æ—¶å€™åˆ é™¤å…¶æŒ‚è½½çš„æ•°æ®å·ã€‚ æ•°æ®å·çš„ç‰¹ç‚¹ï¼š æ•°æ®å·å¯ä»¥åœ¨å®¹å™¨ä¹‹é—´å…±äº«æˆ–é‡ç”¨æ•°æ®ã€‚ å·ä¸­çš„æ›´æ”¹ç›´æŽ¥åœ¨æ‰€æœ‰å…±äº«è¯¥å·å®¹å™¨ä¸­ç”Ÿæ•ˆã€‚ æ•°æ®å·ä¸­çš„æ›´æ”¹ä¸ä¼šåŒ…å«åœ¨é•œåƒçš„æ›´æ–°ä¸­ã€‚ æ•°æ®å·çš„ç”Ÿå‘½å‘¨æœŸä¸€ç›´æŒç»­åˆ°æ²¡æœ‰å®¹å™¨ä½¿ç”¨å®ƒä¸ºæ­¢ã€‚ æ•°æ®å·æŒ‚è½½ç›´æŽ¥å‘½ä»¤æ·»åŠ  æ•°æ®æŒ‚è½½(-v value) docker run -it -v /å®¿ä¸»æœºç›®å½•:/å®¹å™¨å†…ç›®å½• é•œåƒå æŸ¥çœ‹æŒ‚è½½æ˜¯å¦æˆåŠŸ docker inspect é•œåƒå å®¿ä¸»æœºå’Œå®¹å™¨ä¹‹é—´å®žçŽ°æ•°æ®å…±äº«ï¼Œåœ¨å®¹å™¨åœæ­¢é€€å‡ºåŽï¼Œä¿®æ”¹å®¿ä¸»æœºæ•°æ®ï¼Œæ•°æ®å®Œå…¨åŒæ­¥ã€‚ å¸¦æƒé™çš„æ•°æ®æŒ‚è½½ï¼ŒåŠ :ro (readonly) docker run -it -v /å®¿ä¸»æœºç»å¯¹è·¯å¾„ç›®å½•:/å®¹å™¨å†…ç›®å½•:ro é•œåƒå æ­¤æ—¶å®¹å™¨ä¸­å¯¹æ•°æ®å·åªè¯»ã€‚ å½“æŒ‚è½½ä¸»æœºç›®å½•äº‹ï¼ŒDockerè®¿é—®å‡ºçŽ°cannot open directory .: Permission denied è§£å†³åŠžæ³•ï¼šåœ¨æŒ‚ç ¸ç›®å½•åŽåŠ å‚æ•° --privileged=true DockerFileæ·»åŠ åœ¨DockerFileä¸­å¯ä»¥ä½¿ç”¨VOLUME æŒ‡ä»¤ç»™é•œåƒæ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®å·ã€‚ æ³¨æ„ï¼š Dockerå‡ºäºŽå¯ç§»æ¤æ€§å’Œåˆ†äº«çš„è€ƒè™‘ï¼ŒæŒ‡ä»¤ä¸­åªæœ‰å®¹å™¨å†…çš„åœ°å€ï¼Œå› ä¸ºå®¿ä¸»ä¸»æœºç›®å½•ä¾èµ–äºŽç‰¹å®šçš„ä¸»æœºã€‚ Dockerfileæ–‡ä»¶æž„å»º 1234FROM centosVOLUME [&quot;/dataVolumeContainer1&quot;, &quot;/dataVolumeContainer2&quot;, &quot;/dataVolumeContainer3&quot;]CMD echo &quot;finished,-----success&quot;CMD /bin/bash ä»¥ä¸Šdockeræ–‡ä»¶ç±»ä¼¼äºŽä¸€ä¸‹å‘½ä»¤æŒ‚è½½ 1docker run -it -v /host1:/dataVolumeContainer1 -v/host1:/dataVolumeContainer2 -v /host3:/dataVolumeContainer3 centos /bin/bash buildæž„å»ºé•œåƒï¼ˆ-f file) docker build -f DockerFileæ–‡ä»¶è·¯å¾„ -t å‘½åç©ºé—´/é•œåƒå é•œåƒç”Ÿæˆè·¯å¾„ docker build -f ./Dockerfile -t fred/centos . æ•°æ®å·å®¹å™¨æ•°æ®å®¹å™¨å·ï¼š å‘½åçš„å®¹å™¨æŒ‚è½½æ•°æ®å·ï¼Œå…¶ä»–å®¹å™¨é€šè¿‡æŒ‚è½½è¿™ä¸ªçˆ¶å®¹å™¨å®žçŽ°æ•°æ®å…±äº«ï¼ŒæŒ‚è½½æ•°æ®å·çš„å®¹å™¨ç§°ä¸ºæ•°æ®å·å®¹å™¨ã€‚ å®¹å™¨ä¹‹é—´å¯ä»¥ä¼ é€’é…ç½®ä¿¡æ¯ï¼Œæ•°æ®å·çš„ç”Ÿå‘½å‘¨æœŸä¸€ç›´æŒç»­åˆ°æ²¡æœ‰å®¹å™¨ä½¿ç”¨å®ƒä¸ºæ­¢ã€‚ æŒ‚è½½æ•°æ®å·åˆ°çˆ¶å®¹å™¨ï¼ˆå‘½åä¸ºdc01 ï¼‰ä¸Šï¼šå‘½ä»¤æ·»åŠ /Dockerfileæ·»åŠ  å®¹å™¨ç»§æ‰¿çˆ¶å®¹å™¨çš„æ•°æ®å·(--volumes-from ) docker run -it --name å­å®¹å™¨å --volumes-from çˆ¶å®¹å™¨å ç”Ÿæˆå­å®¹å™¨çš„é•œåƒå e.g: docker run -it --name dc02 --volumes-from dc01 fred/centos dc01å·²ç»æŒ‚è½½æ•°æ®å·ï¼Œæ­¤æ—¶dc02ç»§æ‰¿å®ƒï¼Œé‚£ä¹ˆdc01æŒ‚è½½çš„æ•°æ®å·ï¼Œdc02ä¹Ÿå®žçŽ°äº†å…±äº«ã€‚ DockerfileDockerfileæ˜¯ç”¨æ¥æž„å»ºDockeré•œåƒçš„æž„å»ºæ–‡ä»¶ï¼Œæ˜¯ç”±ä¸€ç³»åˆ—å‘½ä»¤å’Œå‚æ•°æž„æˆçš„è„šæœ¬ã€‚ æž„å»ºå®¹å™¨å·çš„æ­¥éª¤ï¼š ç¼–å†™Dockerfileæ–‡ä»¶ docker buildæž„å»º docker runå¯åŠ¨å®¹å™¨ Centosçš„Dockerfileæ–‡ä»¶ 123456789101112131415FROM scratchADD centos-7.8.2003-x86_64-docker.tar.xz /LABEL \\ org.label-schema.schema-version=&quot;1.0&quot; \\ org.label-schema.name=&quot;CentOS Base Image&quot; \\ org.label-schema.vendor=&quot;CentOS&quot; \\ org.label-schema.license=&quot;GPLv2&quot; \\ org.label-schema.build-date=&quot;20200504&quot; \\ org.opencontainers.image.title=&quot;CentOS Base Image&quot; \\ org.opencontainers.image.vendor=&quot;CentOS&quot; \\ org.opencontainers.image.licenses=&quot;GPL-2.0-only&quot; \\ org.opencontainers.image.created=&quot;2020-05-04 00:00:00+01:00&quot;CMD [&quot;/bin/bash&quot;] Dockerfileæž„å»ºè¿‡ç¨‹åŸºç¡€è§„åˆ™ï¼š ä¿ç•™å­—æŒ‡ä»¤å¿…é¡»å¤§å†™ï¼Œä¸”åŽé¢å¿…é¡»è‡³å°‘ä¸€ä¸ªå‚æ•°ã€‚ æŒ‡ä»¤é¡ºåºæ‰§è¡Œã€‚ æ³¨é‡Šç¬¦å·ï¼š# æ¯æ¡æŒ‡ä»¤éƒ½ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„é•œåƒå±‚ï¼Œå¹¶å¯¹è¯¥é•œåƒè¿›è¡Œæäº¤ã€‚ æ‰§è¡Œæµç¨‹ï¼š ä»ŽåŸºç¡€é•œåƒè¿è¡Œä¸€ä¸ªå®¹å™¨ æ‰§è¡Œä¸€æ¡æŒ‡ä»¤åŽå¹¶å¯¹å®¹å™¨è¿›è¡Œä¿®æ”¹ æ‰§è¡Œç±»ä¼¼docker commitæ“ä½œæäº¤ä¸€ä¸ªæ–°çš„é•œåƒå±‚ dockerå†åŸºäºŽåˆšæäº¤çš„é•œåƒè¿è¡Œä¸€ä¸ªå®¹å™¨ ç›´åˆ°æ–‡ä»¶æ‰€æœ‰æŒ‡ä»¤æ‰§è¡Œå®Œæˆ è¾¨æžDockerfileï¼ŒDockeré•œåƒï¼ŒDockerå®¹å™¨ï¼š Dockerfileã€Dockeré•œåƒä¸ŽDockerå®¹å™¨ä»Žè½¯ä»¶åº”ç”¨çš„è§’åº¦åˆ†åˆ«ä»£è¡¨è½¯ä»¶çš„ä¸‰ä¸ªä¸åŒé˜¶æ®µï¼š Dockerfileæ˜¯è½¯ä»¶çš„åŽŸææ–™ï¼Œæ˜¯é¢å‘å¼€å‘çš„ã€‚ Dockerfileå®šä¹‰äº†è¿›ç¨‹éœ€è¦çš„ä¸€åˆ‡ä¸œè¥¿ã€‚Dockerfileè®¾è®¡çš„å†…å®¹åŒ…æ‹¬æ‰§è¡Œä»£ç æˆ–è€…æ˜¯æ–‡ä»¶ã€çŽ¯å¢ƒå˜é‡ã€ä¾èµ–åŒ…ã€è¿è¡Œæ—¶çŽ¯å¢ƒã€åŠ¨æ€é“¾æŽ¥åº“ã€æ“ä½œç³»ç»Ÿçš„å‘è¡Œç‰ˆã€æœåŠ¡è¿›ç¨‹å’Œå†…æ ¸è¿›ç¨‹ç­‰ç­‰ã€‚ Dockeré•œåƒæ˜¯è½¯ä»¶çš„äº¤ä»˜å“ï¼Œæ˜¯äº¤ä»˜æ ‡å‡†ã€‚ åœ¨ç”¨Dockerfileå®šä¹‰ä¸€ä¸ªæ–‡ä»¶ä¹‹åŽï¼Œdocker buildä¼šäº§ç”Ÿä¸€ä¸ªDockeré•œåƒï¼Œè¿è¡Œ Dockeré•œåƒæ—¶ï¼Œæ‰çœŸæ­£å¼€å§‹æä¾›æœåŠ¡ã€‚ Dockerå®¹å™¨åˆ™å¯ä»¥è®¤ä¸ºæ˜¯è½¯ä»¶çš„è¿è¡Œæ€ï¼Œæ¶‰åŠéƒ¨ç½²å’Œè¿ç»´ã€‚ Dockerå®¹å™¨æ˜¯ç›´æŽ¥æä¾›æœåŠ¡çš„ã€‚ Dockfileä½“ç³»ç»“æž„ FROM åŸºç¡€é•œåƒ MAINTAINER é•œåƒç»´æŠ¤è€…çš„å§“åå’Œé‚®ç®±åœ°å€ RUN å®¹å™¨æž„å»ºæ—¶éœ€è¦è¿è¡Œçš„å‘½ä»¤ EXPOSE å½“å‰å®¹å™¨å¯¹å¤–æš´éœ²çš„ç«¯å£å· WORKDIR æŒ‡å®šåœ¨åˆ›å»ºå®¹å™¨åŽï¼Œç»ˆç«¯é»˜è®¤ç™»é™†è¿›æ¥çš„å·¥ä½œç›®å½• ENV æž„å»ºå®¹å™¨ä¸­çš„è®¾ç½®çŽ¯å¢ƒå˜é‡ ADD å°†å®¿ä¸»æœºç›®å½•ä¸‹çš„æ–‡ä»¶æ‹·è´è¿›é•œåƒä¸”ADDå‘½ä»¤ä¼šè‡ªåŠ¨å¤„ç†URLå’Œè§£åŽ‹taråŽ‹ç¼©åŒ… COPY æ‹·è´æ–‡ä»¶å’Œç›®å½•åˆ°é•œåƒä¸­ COPY src dest COPY [&quot;src&quot;, &quot;dest&quot;] VOLUME å®¹å™¨æ•°æ®å· ç”¨äºŽæ•°æ®ä¿å­˜å’ŒæŒä¹…åŒ–å·¥ä½œ CMD æŒ‡å®šä¸€ä¸ªå®¹å™¨å¯åŠ¨æ—¶è¿è¡Œçš„å‘½ä»¤ shell æ ¼å¼ï¼šCMD &lt;å‘½ä»¤&gt; execæ ¼å¼ï¼šCMD[â€œå¯æ‰§è¡Œæ–‡ä»¶â€, â€œarg1â€, â€œarg2â€,â€¦] å‚æ•°åˆ—è¡¨æ ¼å¼ï¼šCMD [â€œarg1â€, â€œarg2â€,â€¦] åœ¨æŒ‡å®šæ¥ENTRYPOINTæŒ‡ä»¤åŽï¼Œç”¨âŒ˜æŒ‡å®šå…·ä½“çš„å‚æ•°ã€‚ åªæœ‰æœ€åŽä¸€ä¸ªCMDç”Ÿæ•ˆï¼ŒCMDä¼šè¢«docker runä¹‹åŽçš„å‚æ•°æ›¿æ¢ ENTRYPOINT æŒ‡å®šä¸€ä¸ªå®¹å™¨å¯åŠ¨æ—¶è¿è¡Œçš„å‘½ä»¤ ä¼šåœ¨docker runåŽé¢è¿½åŠ å‚æ•° ONBUILD å½“æž„å»ºä¸€ä¸ªè¢«ç»§æ‰¿çš„Dockerfileæ—¶ï¼Œçˆ¶é•œåƒåœ¨è¢«å­é•œåƒç»§æ‰¿åŽçˆ¶é•œåƒçš„ONBUILDè¢«è§¦å‘","link":"/2020/07/20/Docker/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šGradient","text":"æ€»ç»“ã€ŒæŽå®æ¯…è€å¸ˆ-æœºå™¨å­¦ä¹ ã€çš„Gradientï¼Œä¸»è¦ä»Žä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢å±•å¼€ï¼šè°ƒèŠ‚learning rateï¼›åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼›å¯¹æ•°æ®è¿›è¡ŒFeature Scalingã€‚ Tip 1: Tuning your learning rates carefullyVisualize æŸå¤±å‡½æ•°éšç€å‚æ•°å˜åŒ–çš„å‡½æ•°å›¾ å·¦å›¾æ˜¯Loss Functionçš„å‡½æ•°å›¾ï¼Œçº¢è‰²æ˜¯æœ€å¥½çš„Stepï¼Œå½“Stepè¿‡å°ï¼ˆè“è‰²ï¼‰ï¼Œä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´ï¼Œå½“Stepè¿‡å¤§ï¼ˆç»¿è‰²ã€é»„è‰²ï¼‰ï¼Œä¼šå‘çŽ°Lossè¶Šæ¥è¶Šå¤§ï¼Œæ‰¾ä¸åˆ°æœ€ä½Žç‚¹ã€‚ æ‰€ä»¥åœ¨Trainingä¸­ï¼Œå°½å¯èƒ½çš„visualize losså€¼çš„å˜åŒ–ã€‚ ä½†æ˜¯å½“å‚æ•°å¤§äºŽç­‰äºŽä¸‰ä¸ªæ—¶ï¼Œ $loss function$çš„å‡½æ•°å›¾å°±ä¸èƒ½visualizeäº†ã€‚ å› æ­¤ï¼Œåœ¨å³å›¾ä¸­ï¼Œvisualize Losséšç€å‚æ•°æ›´æ–°çš„å˜åŒ–ï¼Œæ¨ªè½´å³è¿­ä»£æ¬¡æ•°ï¼Œå½“å›¾åƒå‘ˆçŽ°è“è‰²ï¼ˆsmallï¼‰æ—¶ï¼Œå°±å¯ä»¥æŠŠlearning rate è°ƒå¤§ä¸€äº›ã€‚ Adaptive Learning Rates(Adagrad)ä½†æ˜¯æ‰‹åŠ¨è°ƒèŠ‚ $\\eta$æ˜¯ä½Žæ•ˆçš„ï¼Œæˆ‘ä»¬æ›´å¸Œæœ›èƒ½è‡ªåŠ¨åœ°è°ƒèŠ‚ã€‚ ç›´è§‚ä¸Šçš„åŽŸåˆ™æ˜¯ï¼š $\\eta$ çš„å¤§å°åº”è¯¥éšç€è¿­ä»£æ¬¡æ•°çš„å¢žåŠ è€Œå˜å°ã€‚ æœ€å¼€å§‹ï¼Œåˆå§‹ç‚¹ç¦»minimaå¾ˆè¿œï¼Œé‚£stepåº”è¯¥å¤§ä¸€äº›ï¼Œæ‰€ä»¥learning rateä¹Ÿåº”è¯¥å¤§ä¸€äº›ã€‚ éšç€è¿­ä»£æ¬¡æ•°çš„å¢žåŠ ï¼Œç¦»minimaè¶Šæ¥è¶Šè¿‘ï¼Œå°±åº”è¯¥å‡å° learning rateã€‚ E.g. 1/t decayï¼š $\\eta^t=\\eta/ \\sqrt{t+1}$ ä¸åŒå‚æ•°çš„ $\\eta$åº”è¯¥ä¸åŒï¼ˆcannot be one-size-fits-all)ã€‚ AdagradAdagrad çš„ä¸»è¦æ€æƒ³æ˜¯ï¼šDivide the learning rate of each parameter by the root mean squear of its previous derivatives.(é€šè¿‡é™¤è¿™ä¸ªå‚æ•°çš„ è®¡ç®—å‡ºçš„æ‰€æœ‰å¯¼æ•° çš„å‡æ–¹æ ¹) root mean squar : $ \\sqrt{\\frac{1}{n}(x_1^2+x_2^2+...+x_n^2)} $ Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $w^{t+1} \\leftarrow w^{t}-\\frac{\\eta^{t}}{\\sigma^{t}} g^{t} $ $\\eta^t$ï¼šç¬¬tæ¬¡è¿­ä»£çš„leaning rate $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}}$ $g^{t}=\\frac{\\partial L\\left(\\theta^{t}\\right)}{\\partial w} $ $\\sigma^t$ï¼šroot mean squar of previous derivatives of w $\\tau^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}} $ å¯¹æ¯”ä¸Šé¢ä¸¤ç§Adaptive Gradientï¼ŒAdagradeçš„ä¼˜åŠ¿æ˜¯learning rate æ˜¯å’Œparameter dependentï¼ˆå‚æ•°ç›¸å…³çš„ï¼‰ã€‚ Adagradæ­¥éª¤ç®€åŒ–æ­¥éª¤ï¼š ç®€åŒ–å…¬å¼ï¼š $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ ( $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}} $ , $ \\sigma^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}$ ,çº¦æŽ‰å…±åŒé¡¹å³å¯) Adagrad Contradiction? â€”â€”AdagradåŽŸç†è§£é‡ŠVanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ åœ¨Vanilla Gradient descentä¸­ï¼Œ $g^t$è¶Šå¤§ï¼Œä¹Ÿå°±æ˜¯å½“å‰æ¢¯åº¦å¤§ï¼Œä¹Ÿå°±æœ‰æ›´å¤§çš„stepã€‚ è€Œåœ¨Adagradä¸­ï¼Œå½“ $g^t$è¶Šå¤§ï¼Œæœ‰æ›´å¤§çš„step,è€Œå½“ $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ è¶Šå¤§ï¼Œåè€Œæœ‰æ›´å°stepã€‚ Contradictionï¼Ÿ ã€ŒIntuitive Reasonï¼ˆç›´è§‚ä¸Šè§£é‡Šï¼‰ã€ $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ æ˜¯ä¸ºäº†é€ æˆåå·®çš„æ•ˆæžœã€‚ ç±»æ¯”ä¸€ä¸‹ï¼Œå¦‚æžœä¸€ä¸ªä¸€ç›´å¾ˆå‡¶çš„äººï¼Œçªç„¶æ¸©æŸ”äº†ä¸€äº›ï¼Œä½ ä¼šè§‰å¾—ä»–ç‰¹åˆ«æ¸©æŸ”ã€‚æ‰€ä»¥åŒæ ·æ˜¯ $0.1$,ç¬¬ä¸€è¡Œä¸­ï¼Œä½ ä¼šè§‰å¾—ç‰¹åˆ«å¤§ï¼Œç¬¬äºŒè¡Œä¸­ï¼Œä½ ä¼šè§‰å¾—ç‰¹åˆ«å°ã€‚ å› æ­¤ $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ è¿™ä¸€é¡¹çš„å­˜åœ¨å°±èƒ½ä½“çŽ° $g^t$çš„å˜åŒ–æœ‰å¤šsurpriseã€‚ ã€Œæ•°å­¦ä¸€äº›çš„è§£é‡Šã€1. Larger Gradient,larger steps?åœ¨å‰é¢æˆ‘ä»¬éƒ½æ·±ä¿¡ä¸ç–‘è¿™ä¸€ç‚¹ï¼Œä½†è¿™æ ·çš„æè¿°çœŸçš„æ˜¯æ­£ç¡®çš„å—ï¼Ÿ åœ¨è¿™å¼ å›¾ä¸­ï¼Œåªæœ‰ä¸€ä¸ªå‚æ•°ï¼Œè®¤ä¸ºå½“è¯¥ç‚¹çš„å¯¼æ•°è¶Šå¤§ï¼Œç¦»minimaè¶Šè¿œï¼Œè¿™æ ·çœ‹æ¥ï¼ŒLarger Gradient,larger stepsæ˜¯æ­£ç¡®çš„ã€‚ åœ¨ä¸Šå›¾ä¸­çš„ $x_0$ç‚¹ï¼Œè¯¥ç‚¹è¿­ä»£æ›´æ–°çš„best step åº”è¯¥æ­£æ¯”äºŽ $|x_0+\\frac{b}{2a}|$ ï¼Œå³ $\\frac{|2,a, x_0+b|}{2a}$ã€‚ è€Œ $\\frac{|2,a, x_0+b|}{2a}$çš„åˆ†å­ä¹Ÿå°±æ˜¯è¯¥ç‚¹çš„ä¸€é˜¶å¯¼æ•°çš„ç»å¯¹å€¼ã€‚ ä¸Šå›¾ä¸­ï¼Œæœ‰ $w_1,w_2$ä¸¤ä¸ªå‚æ•°ã€‚ æ¨ªç€ç”¨è“è‰²çš„çº¿åˆ‡ä¸€åˆ€ï¼Œå¾—åˆ°çš„æ˜¯ $w_2$ å›ºå®šï¼Œlosséšç€ $w_1$å˜åŒ–çš„å›¾åƒï¼šæ¯”è¾ƒaã€bä¸¤ç‚¹ï¼Œaç‚¹å¯¼æ•°å¤§ï¼Œç¦»minimaè¿œã€‚ ç«–ç€ç”¨ç»¿è‰²çš„çº¿åˆ‡ä¸€åˆ€ï¼Œå¾—åˆ°çš„æ˜¯ $w_2$ å›ºå®šï¼Œlosséšç€ $w_1$å˜åŒ–çš„å›¾åƒï¼šæ¯”è¾ƒcã€dä¸¤ç‚¹ï¼Œcç‚¹å¯¼æ•°å¤§ï¼Œç¦»minimaè¿œã€‚ ä½†æ˜¯ï¼Œå¦‚æžœæ¯”è¾ƒaã€cä¸¤ç‚¹å‘¢ï¼Ÿ aç‚¹å¯¹ $w_1$ çš„åå¯¼æ•°å’Œcç‚¹å¯¹ $w_2$çš„åå¯¼æ•°æ¯”è¾ƒï¼Ÿ æ¯”è¾ƒå‡ºæ¥ï¼Œcç‚¹ç‚¹åå¯¼æ•°æ›´å¤§ï¼Œç¦»minimaæ›´è¿œå—ï¼Ÿ å†çœ‹å·¦å›¾çš„å›¾åƒï¼Œæ¨ªç€çš„å¼§åº¦æ›´å¹³æ»‘ï¼Œç«–ç€çš„å¼§åº¦æ›´å°–ä¸€äº›ï¼Œç›´è§‚ä¸Šçœ‹åº”è¯¥cç‚¹ç¦»minimaæ›´è¿‘ä¸€äº›ã€‚ æ‰€ä»¥Larger Gradient,larger stepsç‚¹æ¯”è¾ƒæ–¹æ³•ä¸èƒ½ï¼ˆcross parameters)è·¨å‚æ•°æ¯”è¾ƒã€‚ æ‰€ä»¥æœ€å¥½çš„step $\\propto$ ä¸€é˜¶å¯¼æ•°ï¼ˆDo not cross parameters)ã€‚ 2.** Second Derivative** å‰é¢è®¨è®ºbest step $\\frac{|2,a, x_0+b|}{2a}$çš„åˆ†å­æ˜¯è¯¥ç‚¹ä¸€é˜¶å¯¼æ•°ï¼Œé‚£ä¹ˆå…¶åˆ†æ¯å‘¢ï¼Ÿ å½“å¯¹ä¸€é˜¶å¯¼æ•°å†æ±‚å¯¼æ—¶ï¼Œå¯ä»¥å‘çŽ°å…¶äºŒé˜¶å¯¼æ•°å°±æ˜¯best stepçš„åˆ†æ¯ã€‚ å¾—å‡ºç»“è®ºï¼šthe best step $\\propto$ |First dertivative| / Second derivativeã€‚ å› æ­¤ï¼Œå†æ¥çœ‹ä¸¤ä¸ªå‚æ•°çš„æƒ…å†µï¼Œæ¯”è¾ƒaç‚¹å’Œcç‚¹ï¼Œaç‚¹çš„ä¸€é˜¶å¯¼æ•°æ›´å°ï¼ŒäºŒé˜¶å¯¼æ•°ä¹Ÿæ›´å°ï¼›cç‚¹ç‚¹ä¸€é˜¶å¯¼æ•°æ›´å¤§ï¼ŒäºŒé˜¶å¯¼æ•°ä¹Ÿæ›´å¤§ã€‚ æ‰€ä»¥å¦‚æžœè¦æ¯”è¾ƒaã€cä¸¤ç‚¹ï¼Œè°ç¦»minimaæ›´è¿œï¼Œåº”è¯¥æ¯”è¾ƒå…¶ä¸€é˜¶å¯¼æ•°çš„ç»å¯¹å€¼é™¤ä»¥å…¶äºŒé˜¶å¯¼æ•°çš„å¤§å°ã€‚ å›žåˆ° $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ ä¸Šä¸€éƒ¨åˆ†å¾—å‡ºçš„ç»“è®ºæ˜¯ï¼šthe best step $\\propto$ |First dertivative| / Second derivativeã€‚ æ‰€ä»¥æˆ‘ä»¬çš„learning rate ä¹Ÿåº”è¯¥å’Œ |First dertivative| / Second derivativeç›¸å…³ã€‚ $g^t$ä¹Ÿå°±æ˜¯ä¸€é˜¶å¯¼æ•°ï¼Œä½†ä¸ºä»€ä¹ˆ $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ èƒ½ä»£è¡¨äºŒé˜¶å¯¼æ•°å‘¢ï¼Ÿ ä¸Šå›¾ä¸­ï¼Œè“è‰²çš„å‡½æ•°å›¾æœ‰æ›´å°çš„äºŒé˜¶å¯¼æ•°ï¼Œç»¿è‰²çš„å‡½æ•°å›¾æœ‰æ›´å¤§çš„äºŒé˜¶å¯¼æ•°ã€‚ åœ¨å¤æ‚å‡½æ•°ä¸­ï¼Œæ±‚äºŒé˜¶å¯¼æ•°æ˜¯ä¸€ä¸ªå¾ˆå¤æ‚çš„è®¡ç®—ã€‚ æ‰€ä»¥æˆ‘ä»¬æƒ³ç”¨ä¸€é˜¶å¯¼æ•°æ¥åæ˜ äºŒé˜¶å¯¼æ•°çš„å¤§å°ã€‚ åœ¨ä¸€é˜¶å¯¼æ•°çš„å‡½æ•°å›¾ä¸­ï¼Œè®¤ä¸ºä¸€é˜¶å¯¼æ•°å€¼æ›´å°çš„ï¼ŒäºŒé˜¶å¯¼æ•°ä¹Ÿæ›´å°ï¼Œä½†æ˜¯å–ä¸€ä¸ªç‚¹æ˜¾ç„¶æ˜¯ç‰‡é¢çš„ï¼Œæ‰€ä»¥è€ƒè™‘å–å¤šä¸ªç‚¹ã€‚ ä¹Ÿå°±æ˜¯ç”¨ $ \\sqrt{\\text{(first derivative)}^2}$ æ¥ä»£è¡¨best stepä¸­çš„äºŒé˜¶å¯¼æ•°ã€‚ æ€»ç»“ä¸€ä¸‹Adagradçš„ä¸ºäº†æ‰¾å¯»æœ€å¥½çš„learning rateï¼Œä»Žæ‰¾å¯»best stepä¸‹æ‰‹ï¼Œç”¨ç®€å•çš„äºŒæ¬¡å‡½æ•°ä¸ºä¾‹ï¼Œå¾—å‡º best step $\\propto$ |First dertivative| / Second derivativeã€‚ ä½†æ˜¯å¤æ‚å‡½æ•°çš„äºŒé˜¶å¯¼æ•°æ˜¯éš¾è®¡ç®—çš„ï¼Œå› æ­¤è€ƒè™‘ç”¨å¤šä¸ªç‚¹çš„ä¸€é˜¶å¯¼æ•°æ¥åæ˜ å…¶äºŒé˜¶å¯¼æ•°ã€‚ å¾—å‡º $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ ã€‚ ç›´è§‚æ¥è§£é‡Šå…¬å¼ä¸­çš„ä¸€é˜¶å¯¼æ•°çš„root mean squareï¼Œå³æ¥ä¸ºè¯¥æ¬¡è¿­ä»£çš„ä¸€é˜¶å¯¼æ•°é€ æˆåå·®æ•ˆæžœã€‚ å…¶ä»–æ–‡çŒ®ä¸­çš„Adaptive Gradientç†åº”éƒ½æ˜¯ä¸ºäº†è°ƒèŠ‚learning rateä½¿ä¹‹æœ‰best stepã€‚(å¾…è¡¥å……çš„å…¶ä»–Gradient)[1] Tip 2:Stochastic Gradient DescentStochastic Gradient Descentåœ¨linear modelä¸­ï¼Œæˆ‘ä»¬è¿™æ ·è®¡ç®—Loss functionï¼š $L=\\sum_{n}\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ æ¯æ±‚ä¸€æ¬¡Loss functionï¼ŒLéƒ½å¯¹æ‰€æœ‰training examplesçš„ $\\text{error}^2$æ±‚å’Œï¼Œå› æ­¤æ¯ä¸€æ¬¡çš„loss functionçš„è®¡ç®—ï¼Œéƒ½æ˜¯ä¸€é‡å¾ªçŽ¯ã€‚ åœ¨Stochastic Gradient Descentä¸­ï¼Œæ¯ä¸€æ¬¡æ±‚loss functionï¼Œåªå–ä¸€ä¸ªexample $x^n$ï¼Œå‡å°‘ä¸€é‡å¾ªçŽ¯ï¼Œæ— ç–‘æ›´å¿«ã€‚ Stochastic Gradient Descent Pick an example $x^n$ $L=\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ ä¸Šå›¾ä¸­ï¼Œä¼ ç»Ÿçš„Gradient Descentçœ‹å®Œä¸€æ¬¡æ‰€æœ‰çš„examplesï¼Œç¦»minimaè¿˜å¾ˆè¿œï¼›è€ŒStochastic Gradient Descent ï¼Œçœ‹å®Œä¸€æ¬¡ï¼Œå·²ç»ç¦»minimaè¾ƒè¿‘äº†ã€‚ Tip 3:Feature ScalingWhat is Feature Scaling å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¸Œæœ›èƒ½è®©ä¸åŒçš„featureèƒ½æœ‰ç›¸åŒçš„scaleï¼ˆå®šä¹‰åŸŸ/è§„æ¨¡ï¼‰ Why Feature Scalingå‡è®¾modeléƒ½æ˜¯ $y = b+ w_1 x_1 +w_2 x_2$ã€‚ ä¸Šå›¾ä¸­ï¼Œå·¦è¾¹ $x_2$çš„è§„æ¨¡æ›´å¤§ï¼Œå¯ä»¥è®¤ä¸º $x_1$ å¯¹loss çš„å½±å“æ›´å°ï¼Œ $ x_2$å¯¹lossçš„å½±å“æ›´å¤§ã€‚ å³å½“ $w_1,w_2$è½»å¾®æ‰°åŠ¨æ—¶ï¼ŒåŒæ—¶åŠ ä¸Šç›¸åŒçš„ $\\Delta w$æ—¶ï¼Œ$x_2$ ä½¿ $y$çš„å–å€¼æ›´å¤§ï¼Œé‚£ä¹ˆå¯¹loss çš„å½±å“ä¹Ÿæ›´å¤§ã€‚ å¦‚å›¾ä¸­ä¸‹æ–¹çš„å‡½æ•°å›¾ $w_1$æ–¹å‘çš„Læ›´å¹³æ»‘ï¼Œ $w_2$ æ–¹å‘æ›´é™¡å³­äº›ï¼ŒGradient descentçš„æ­¥éª¤å¦‚å›¾æ‰€ç¤ºã€‚ ä½†å½“å¯¹ $x_2$è¿›è¡Œfeature scalingåŽï¼Œå›¾åƒä¼šæ›´åƒæ­£åœ†ï¼ŒGradient descentä½¿ï¼Œå‚æ•°æ›´æ–°å‘ç€åœ†å¿ƒèµ°ï¼Œæ›´æ–°ä¼šæ›´æœ‰æ•ˆçŽ‡ã€‚ How Feature Scalingæ¦‚çŽ‡è®ºçŸ¥è¯†ï¼šæ ‡å‡†åŒ–ã€‚ æ¦‚çŽ‡è®ºï¼š éšæœºå˜é‡ $X$ çš„æœŸæœ›å’Œæ–¹å·®å‡å­˜åœ¨ï¼Œä¸” $ D(X)&gt;0$,ä»¤ $X^*=\\frac{X-E(X)}{\\sqrt{D(X)}} $ ï¼Œé‚£ä¹ˆ $E(X^)=0,D(X)=1$, $X^$ç§°ä¸ºXçš„æ ‡å‡†åŒ–éšæœºå˜é‡ã€‚ å¯¹æ‰€æœ‰å‘é‡çš„æ¯ä¸€ç»´åº¦ï¼Œè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼š $x_{i}^{r} \\leftarrow \\frac{x_{i}^{r}-m_{i}}{\\sigma_{i}} $ ï¼ˆ $m_i$æ˜¯è¯¥ç»´åº¦å˜é‡çš„å‡å€¼ï¼Œ $\\sigma_i$ æ˜¯è¯¥ç»´åº¦å˜é‡çš„æ–¹å·®ï¼‰ æ ‡å‡†åŒ–åŽï¼Œæ¯ä¸€ä¸ªfeatureçš„æœŸæœ›éƒ½æ˜¯0ï¼Œæ–¹å·®éƒ½æ˜¯1ã€‚ Gradient Descent Theory(å…¬å¼æŽ¨å¯¼)å½“ç”¨Gradient Descentè§£å†³ $\\theta^*=\\arg \\min_\\theta L(\\theta)$æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æ¯æ¬¡æ›´æ–° $\\theta $ éƒ½èƒ½å¾—åˆ° $L(\\theta^0)&gt;L(\\theta^1)&gt;L(\\theta^2)&gt;â€¦$ è¿™æ ·çš„ç†è®ºç»“æžœï¼Œä½†æ˜¯ä¸æ€»èƒ½å¾—åˆ°è¿™æ ·çš„ç»“æžœã€‚ ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬è™½ç„¶ä¸èƒ½ä¸€ä¸‹çŸ¥é“minimaçš„æ–¹å‘ï¼Œä½†æ˜¯æˆ‘ä»¬å¸Œæœ›ï¼šå½“ç»™ä¸€ä¸ªç‚¹ $\\theta^0$ æ—¶ï¼Œæˆ‘ä»¬èƒ½å¾ˆå®¹æ˜“çš„çŸ¥é“ä»–é™„è¿‘ï¼ˆæžå°çš„é™„è¿‘ï¼‰çš„æœ€å°çš„loss æ˜¯å“ªä¸ªæ–¹å‘ã€‚ æ‰€ä»¥æ€Žä¹ˆåšå‘¢ï¼Ÿ Tylor Serieså¾®ç§¯åˆ†çŸ¥è¯†ï¼šTaylor Seriesï¼ˆæ³°å‹’å…¬å¼ï¼‰ã€‚ Tylor Series:å‡½æ•° $h(x)$ åœ¨ $x_0$ æ— é™å¯å¯¼ï¼Œé‚£ä¹ˆ $\\begin{aligned} \\mathrm{h}(\\mathrm{x}) &=\\sum_{k=0}^{\\infty} \\frac{\\mathrm{h}^{(k)}\\left(x_{0}\\right)}{k !}\\left(x-x_{0}\\right)^{k} \\\\ &=h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{h^{\\prime \\prime}\\left(x_{0}\\right)}{2 !}\\left(x-x_{0}\\right)^{2}+\\ldots \\end{aligned}$ å½“ x æ— é™æŽ¥è¿‘ $x_0$ æ—¶ï¼Œå¿½ç•¥åŽé¢æ— ç©·å°çš„é«˜æ¬¡é¡¹ï¼Œ $h(x) \\approx h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right) $ ä¸Šå›¾ä¸­ï¼Œç”¨ $\\pi/4$ å¤„çš„ä¸€é˜¶æ³°å‹’å±•ç¤ºæ¥è¡¨è¾¾ $\\sin(x)$ ,å›¾åƒæ˜¯ç›´çº¿ï¼Œå’Œ $\\sin(x)$ å›¾åƒç›¸å·®å¾ˆå¤§ï¼Œä½†å½“ xæ— é™æŽ¥è¿‘ $\\pi/4$ æ˜¯ï¼Œå‡½æ•°å€¼ä¼°ç®—å¾ˆå¥½ã€‚ Multivariable Taylor Series $h(x, y)=h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right) +\\text{something raleted to} (x-x_x^0)^2 \\text{and} (y-y_0)^2+â€¦$ å½“ $(x,y)$ æŽ¥è¿‘ $(x_0,y_0)$ æ—¶ï¼Œ $h(x,y)$ ç”¨ $(x_0,y_0)$ å¤„çš„ä¸€é˜¶æ³°å‹’å±•å¼€å¼ä¼°è®¡ã€‚ $ h(x, y) \\approx h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right)$ Back to Formal Derivation å½“å›¾ä¸­çš„çº¢è‰²åœ†åœˆè¶³å¤Ÿå°æ—¶ï¼Œçº¢è‰²åœ†åœˆä¸­çš„loss å€¼å°±å¯ä»¥ç”¨ $(a,b)$ å¤„çš„ä¸€é˜¶æ³°å‹’å±•å¼€å¼æ¥è¡¨ç¤ºã€‚ $ \\mathrm{L}(\\theta) \\approx \\mathrm{L}(a, b)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}\\left(\\theta_{1}-a\\right)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}}\\left(\\theta_{2}-b\\right) $ $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$ ,d è¶³å¤Ÿå°ã€‚ ç”¨ $s=L(a,b)$ , $ u=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}, v=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} $ è¡¨ç¤ºã€‚ æœ€åŽé—®é¢˜å˜æˆï¼š $L(\\theta)\\approx s+u(\\theta_1-a)+v(\\theta_2-b)$ æ‰¾ $(\\theta_1,\\theta_2)$ï¼Œä¸”æ»¡è¶³ $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$ï¼Œä½¿ $L(\\theta)$ æœ€å°ã€‚ å˜æˆäº†ä¸€ä¸ªç®€å•çš„æœ€ä¼˜åŒ–é—®é¢˜ã€‚ ä»¤ $\\Delta \\theta_1=\\theta_1-a$ , $\\Delta\\theta_2=\\theta_2-b$ é—®é¢˜ç®€åŒ–ä¸ºï¼š $\\text{min}:u \\Delta \\theta_1+v\\Delta\\theta_2$ $\\text{subject to}:{\\Delta\\theta_1}^2+{\\Delta\\theta_2}^2\\leq d^2$ ç”»å‡ºå›¾ï¼Œå°±æ˜¯åˆä¸­æ•°å­¦äº†ã€‚æ›´æ–°çš„æ–¹å‘åº”è¯¥æ˜¯ $(u,v)$ å‘é‡åå‘çš„æ–¹å‘ã€‚ æ‰€ä»¥ï¼š $\\left[\\begin{array}{l} \\Delta \\theta_{1} \\\\ \\Delta \\theta_{2} \\end{array}\\right]=-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $\\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $ \\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}} \\\\ \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} \\end{array}\\right] $ Limitation of Gradient Descent Gradient Descent å¯èƒ½ä¼šå¡åœ¨local minimaæˆ–è€…saddle pointï¼ˆéžç‚¹ï¼šä¸€ä¸ªæ–¹å‘æ˜¯æžå¤§å€¼ï¼Œä¸€ä¸ªæ–¹å‘æ˜¯æžå°å€¼ï¼Œå¯¼æ•°ä¸º0ï¼‰ å®žè·µä¸­ï¼Œæˆ‘ä»¬å¾€å¾€ä¼šåœ¨å¯¼æ•°æ— ç©·æŽ¥è¿‘0çš„æ—¶å€™åœä¸‹æ¥ï¼ˆ&lt; 1e-7)ï¼ŒGradient Descent å¯èƒ½ä¼šåœåœ¨plateau(é«˜åŽŸï¼›å¢žé•¿åŽçš„ç¨³å®š) Reference[1] å¾…è¡¥å……çš„å…¶ä»–Gradient","link":"/2020/02/29/Gradient/"},{"title":"ã€ŒLeetCodeã€ï¼šString","text":"LeetCode String ä¸“é¢˜è®°å½•ã€‚ 9æœˆæ¯•ã€‚ ã€Œæˆ‘ç¥ç¦ä½ æœ‰æ—¶æœ‰åè¿æ°”ï¼Œä½ ä¼šæ„è¯†åˆ°æ¦‚çŽ‡å’Œè¿æ°”åœ¨äººç”Ÿä¸­æ‰®æ¼”çš„è§’è‰²ï¼Œå¹¶ç†è§£ä½ çš„æˆåŠŸå¹¶ä¸å®Œå…¨æ˜¯ä½ åº”å¾—çš„ï¼Œå…¶ä»–äººçš„å¤±è´¥ä¹Ÿå¹¶ä¸å®Œå…¨æ˜¯ä»–ä»¬åº”å¾—çš„ã€‚ã€ ã€Œä¸æƒ³è¦åˆšå¥½é”™è¿‡çš„æ‚”æ¨ï¼Œé‚£å°±è¦æœ‰å®Œå…¨ç¢¾åŽ‹çš„å®žåŠ›ã€‚ã€ String28-Implement strStr()28-Implement strStr() Problem: è¿”å›žç¬¬ä¸€ä¸ªå­—ä¸²å‡ºçŽ°çš„ä¸‹æ ‡ Solutionï¼š Pythonå°±æš´åŠ›åŒ¹é…ã€‚ 12345678class Solution: def strStr(self, haystack: str, needle: str) -&gt; int: n1 = len(haystack) n2 = len(needle) for i in range(0, n1-n2+1): if haystack[i:i+n2] == needle: return i return -1 14-Longest Common Prefix14-Longest Common Prefix Problem: è¿”å›žä¸²çš„å…¬å…±æœ€é•¿å‰ç¼€ã€‚ Solutionï¼š æš´åŠ›åŒ¹é…é•¿åº¦å°±å¥½ã€‚ 1234567891011121314151617from typing import Listclass Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: LCP = 0 n = len(strs) if n == 0: return &quot;&quot; while True: for i in range(0, n): if LCP &lt; len(strs[i]) and strs[i][LCP] == strs[0][LCP]: continue else: return strs[0][0: LCP] LCP += 1 58-Length of Last Word58-Length of Last Word Problem: å•è¯ä¸²ç”±å­—æ¯å’Œç©ºæ ¼ç»„æˆï¼Œè¿”å›žæœ€åŽä¸€ä¸ªå•è¯çš„é•¿åº¦ã€‚ Solutionï¼š æ³¨æ„ä¸²æœ€åŽçš„ç©ºæ ¼ã€‚ 12345678910111213class Solution: def lengthOfLastWord(self, s: str) -&gt; int: n = len(s) length = 0 end = n-1 while end-1 &gt;= 0 and s[end] == ' ': end -= 1 for i in range(end, -1, -1): if s[i] == ' ': return length else: length += 1 return length 58-First Unique Character in a StringProblem: æ‰¾ç¬¬ä¸€ä¸ªæ²¡æœ‰é‡å¤å‡ºçŽ°çš„å­—ç¬¦ä¸‹æ ‡ã€‚ Solutionï¼š æš´åŠ›ã€‚ 12345678910class Solution: def firstUniqChar(self, s: str) -&gt; int: a_ascii = ord('a') cnt = [0]*(26+5) for i in s: cnt[ord(i)-a_ascii] += 1 for idx in s: if cnt[ord(i)-a_ascii] == 1: return s.index(i) return -1 å¯»æ‰¾å­ä¸²å¼€å§‹ç´¢å¼•ï¼š str.find(substr, beg=0, end=len(string)) substr: å­—ä¸² beg: å¼€å§‹ç´¢å¼• end: ç»“æŸç´¢å¼•ï¼Œé»˜è®¤å­—ç¬¦ä¸²é•¿åº¦ã€‚ å¦‚æžœå­—ç¬¦ä¸²ä¸åŒ…å«å­ä¸²ï¼Œåˆ™è¿”å›ž-1 str.index(str, beg=0, end=len(string)) å’Œfindå·®ä¸å¤šï¼Œå¦‚æžœä¸åŒ…å«å­ä¸²ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚ 383-Ransom Note383-Ransom Note Problem: ç»™ä¸¤ä¸ªå­—ç¬¦ä¸²ï¼Œåˆ¤æ–­ä¸²1çš„å­—ç¬¦èƒ½å¦ç”±ä¸²2çš„å­—ç¬¦ç»„æˆã€‚ Solutionï¼š å­—å…¸è®¡æ•°ã€‚ 12345678910111213141516class Solution: def canConstruct(self, ransomNote: str, magazine: str) -&gt; bool: ransomDir = {} magazineDir = {} for ch in ransomNote: ransomDir.setdefault(ch, 0) ransomDir[ch] += 1 for ch in magazine: magazineDir.setdefault(ch, 0) magazineDir[ch] += 1 for (k, v) in ransomDir.items(): if k not in magazineDir: return False if ransomDir[k] &gt; magazineDir[k]: return False return True åˆå§‹åŒ–å­—å…¸çš„å€¼ï¼šdic.setdefault(ch, 0) 344-Reverse String344-Reverse String Problem: in-place åè½¬å­—ç¬¦ä¸² with O(1) çš„é¢å¤–ç©ºé—´ã€‚ Solutionï¼š å‰åŽä¸¤ä¸ªæŒ‡é’ˆäº¤æ¢ã€‚ 123456789101112131415from typing import Listclass Solution: def reverseString(self, s: List[str]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify s in-place instead. &quot;&quot;&quot; n = len(s) l = 0 r = n-1 while l &lt; r: s[l], s[r] = s[r], s[l] l += 1 r -= 1 151-Reverse Words in a String151-Reverse Words in a String Problem: åè½¬å­—ç¬¦ä¸²word by word.(ç»“æžœä¸­å•è¯é—´åªèƒ½æœ‰ä¸€ä¸ªç©ºæ ¼) Solutionï¼š æŠŠå•è¯å­˜å…¥åˆ—è¡¨ï¼Œå†è¾“å‡ºã€‚ 1234567891011121314151617class Solution: def reverseWords(self, s: str) -&gt; str: n = len(s) i = 0 li = [] while i &lt; n: while i &lt; n and s[i] == ' ': i += 1 l = i while i &lt; n and s[i] != ' ': i += 1 r = i if r != l: li.append(s[l:r]) ans = ' '.join(li[-1::-1]) return ans Pythonè¿žæŽ¥å­—ç¬¦ä¸²æ€»ç»“ åŠ å·è¿žæŽ¥ï¼š'a' + 'b' é€—å·è¿žæŽ¥ï¼Œåªèƒ½ç”¨äºŽprintæ‰“å°: print(a, b) ç›´æŽ¥è¿žæŽ¥: print('a' 'b') ä½¿ç”¨ % æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼š'%s %s' % ('hello', 'world') format æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼š'{}{}'.format('hello', 'world') join å†…ç½®æ–¹æ³•ï¼šç”¨å­—ç¬¦æ¥è¿žæŽ¥ä¸€ä¸ªåºåˆ—ï¼Œæ•°ç»„æˆ–åˆ—è¡¨ç­‰ï¼š'-'.join(['aa', 'bb', 'cc']) f-string æ–¹æ³•ï¼šaa, bb = 'hello', 'world' , f'{aa} {bb}' * æ“ä½œç¬¦ï¼šå­—ç¬¦ä¸²ä¹˜æ³•ã€‚ åè½¬åˆ—è¡¨ï¼š[-1: : -1] 186-Reverse Words in a String II186-Reverse Words in a String II Problem: åè½¬å•è¯in-places. Solution: ä¸¤æ¬¡åè½¬ï¼Œç¬¬ä¸€æ¬¡æ•´ä½“åè½¬ï¼Œç¬¬äºŒæ¬¡å†å•è¯åè½¬ã€‚ ï¼ˆä¸é¢å¤–å¼€ä¸ªæ•°ç»„æ¥é€ä¸ªèµ‹å€¼ACä¸äº†ï¼Œä¸çŸ¥é“ä¸ºå•¥q w q) 123456789101112131415161718class Solution: def reverseWords(self, s: List[str]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify s in-place instead. &quot;&quot;&quot; temp = s[-1::-1] n = len(temp) i = 0 while i &lt; n: l = i while i &lt; n and temp[i] != ' ': i += 1 r = i temp[l:r] = list(reversed(temp[l:r])) i += 1 for index in range(n): s[index] = temp[index] Python åè½¬åˆ—è¡¨çš„æ–¹æ³•ï¼š list(reversed(a)) , reversed(a)è¿”å›žçš„æ˜¯è¿­ä»£å™¨ï¼Œè½¬æ¢æˆlistã€‚ a[::-1] Python å­—ç¬¦ä¸²(str)å’Œåˆ—è¡¨(list)äº’ç›¸è½¬æ¢ï¼š str è½¬æ¢ä¸º list list() è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦åˆ—è¡¨ str.split() æˆ–è€…str.split(' ') ç©ºæ ¼åˆ†å‰²è½¬æ¢ 1234567891011121314str1 = &quot;123&quot;list1 = list(str1)print list1# ['1', '2', '3']str2 = &quot;123 sjhid dhi&quot;list2 = str2.split() #or list2 = str2.split(&quot; &quot;)print list2# ['123', 'sjhid', 'dhi']str3 = &quot;www.google.com&quot;list3 = str3.split(&quot;.&quot;)print list3# ['www', 'google', 'com'] listè½¬æ¢ä¸ºstr: &quot;&quot;.join(list) æ— ç©ºæ ¼è¿žæŽ¥ &quot;.&quot;.join(list) 345-Reverse Vowels of a String345-Reverse Vowels of a String Problem: åè½¬å­—ç¬¦ä¸²ä¸­çš„å…ƒéŸ³å­—æ¯ã€‚ Solutionï¼š å…ƒéŸ³å­—æ¯ï¼ŒåŒ…æ‹¬å¤§å†™å…ƒéŸ³å­—æ¯å’Œå°å†™å…ƒéŸ³å­—æ¯ã€‚ 1234567891011121314151617181920class Solution: def reverseVowels(self, s: str) -&gt; str: n = len(s) s = list(s) dic = {'a': 1, 'e': 1, 'i': 1, 'o': 1, 'u': 1} rev = [0] * n for index in range(n): if s[index] in dic or s[index].lower() in dic: rev[index] = 1 l = 0 r = n - 1 while l &lt; r: while l &lt; r and rev[l] == 0: l += 1 while l &lt; r and rev[r] == 0: r -= 1 s[l], s[r] = s[r], s[l] l += 1 r -= 1 return ''.join(s) Pythonå¤§å°å†™è½¬æ¢ï¼š æ‰€æœ‰å­—ç¬¦è½¬æ¢ä¸ºå¤§å†™ï¼šstr.upper() æ‰€æœ‰å­—ç¬¦è½¬æ¢ä¸ºå°å†™ï¼šstr.lower() ç¬¬ä¸€ä¸ªå­—æ¯è½¬æ¢ä¸ºå¤§å†™å­—æ¯ï¼Œå…¶ä½™å°å†™ï¼šstr.capitalize() æŠŠæ¯ä¸ªå•è¯çš„ç¬¬ä¸€ä¸ªå­—æ¯è½¬æ¢ä¸ºå¤§å†™ï¼Œå…¶ä½™å°å†™ã€‚ 12345678910str = &quot;www.runoob.com&quot;print(str.upper()) # æŠŠæ‰€æœ‰å­—ç¬¦ä¸­çš„å°å†™å­—æ¯è½¬æ¢æˆå¤§å†™å­—æ¯print(str.lower()) # æŠŠæ‰€æœ‰å­—ç¬¦ä¸­çš„å¤§å†™å­—æ¯è½¬æ¢æˆå°å†™å­—æ¯print(str.capitalize()) # æŠŠç¬¬ä¸€ä¸ªå­—æ¯è½¬åŒ–ä¸ºå¤§å†™å­—æ¯ï¼Œå…¶ä½™å°å†™print(str.title()) # æŠŠæ¯ä¸ªå•è¯çš„ç¬¬ä¸€ä¸ªå­—æ¯è½¬åŒ–ä¸ºå¤§å†™ï¼Œå…¶ä½™å°å†™# WWW.RUNOOB.COM# www.runoob.com# Www.runoob.com# Www.Runoob.Com Pythonä¸­stringæ˜¯ä¸å¯å˜å¯¹è±¡ï¼Œä¸èƒ½é€šè¿‡ä¸‹æ ‡çš„æ–¹å¼ï¼ˆå¦‚str[0]='a' )æ”¹å˜å­—ç¬¦ä¸²ã€‚ 205-Isomorphic Strings205-Isomorphic Strings Problem: åˆ¤æ–­æ˜¯å¦åŒæž„å­—ç¬¦ä¸²ã€‚ Solutionï¼š å­—ç¬¦åˆ°å­—ç¬¦çš„æ˜ å°„ï¼Œå¿…é¡»æ˜¯å•å°„ã€‚ 12345678910111213141516class Solution: def isIsomorphic(self, s: str, t: str) -&gt; bool: n = len(s) dic = {} vSet = set() # satisfy single map for idx in range(n): ch = s[idx] # single map if ch not in dic and t[idx] not in vSet: dic[ch] = t[idx] vSet.add(t[idx]) continue if ch in dic and dic[ch] == t[idx]: continue return False return True Python é›†åˆçš„æ“ä½œï¼š åˆ›å»ºç©ºé›†åˆï¼šset() åˆ›å»ºæœ‰åˆå€¼çš„é›†åˆï¼šSET = {v0, v1, v2} æˆ–è€…SET = set(v0) åˆ¤æ–­å…ƒç´ æ˜¯å¦åœ¨é›†åˆä¸­ï¼šx in SET é›†åˆè¿ç®—ï¼š a-b :å±žäºŽaé›†åˆä¸å±žäºŽbé›†åˆ a|b :å±žäºŽaé›†åˆæˆ–å±žäºŽbé›†åˆ a&amp;b :é›†åˆaå’Œé›†åˆbéƒ½åŒ…å«çš„å…ƒç´  a^b : ä¸åŒæ—¶åŒ…å«äºŽé›†åˆaå’Œé›†åˆbçš„å…ƒç´  é›†åˆä¸­æ·»åŠ å…ƒç´ ï¼šs.add(x) é›†åˆä¸­æ·»åŠ å…ƒç´ ï¼Œä¸”å‚æ•°å¯ä»¥æ˜¯åˆ—è¡¨ã€å…ƒç»„ã€å­—å…¸(æ˜¯æ¯ä¸ªå…ƒç´ éƒ½æ·»åŠ è¿›åŽ»ï¼‰ç­‰ï¼šs.update(x) ç§»é™¤å…ƒç´ ï¼šs.remove(x) ï¼Œå¦‚æžœå…ƒç´ ä¸å­˜åœ¨ï¼Œåˆ™ä¼šå‘ç”Ÿé”™è¯¯ã€‚ ç§»é™¤å…ƒç´ ï¼šs.discard(x) ï¼Œå¦‚æžœå…ƒç´ ä¸å­˜åœ¨ï¼Œä¸ä¼šå‘ç”Ÿé”™è¯¯ã€‚ éšæœºåˆ é™¤é›†åˆä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼šs.pop() ï¼ˆåŽŸç†ï¼šå¯¹é›†åˆæ— åºæŽ’åºï¼Œç„¶åŽåˆ é™¤æ— åºæŽ’åˆ—é›†åˆçš„ç¬¬ä¸€ä¸ªï¼‰ è®¡ç®—é›†åˆå…ƒç´ çš„ä¸ªæ•°ï¼šlen(s) æ¸…ç©ºé›†åˆs.clear() List Comprehension &amp;&amp; Set Comprehension &amp;&amp; Dictionary Comprehension è¿™ä¸ªç›¸å½“äºŽæ•°å­¦ä¸­çš„ $S={2\\cdot x\\mid x\\in \\left[0,9\\right)}$ çš„è¡¨è¾¾ã€‚ List Comprehension å¦‚æžœç”¨æ•°å­¦ä¸­çš„è¿™ä¸ªè¡¨è¾¾æ¥çœ‹ä¸‹é¢çš„å¼å­ï¼Œå°±å¾ˆæ˜¾è€Œæ˜“è§äº†ã€‚ 1arr = [i for i in range(10)] å†çœ‹çœ‹åŠ äº†å…¶ä»–é™åˆ¶çš„ä¾‹å­ 12345678# filter the elementsarr1 = [x for x in arr if x % 2==0]# add more conditionsarr2 = [x**2 for x in arr if x &gt;= 3 and x % 2]# use nested for loopsarr3 = [(x, y) for x in range(3) for y in range(4)] ä½¿ç”¨List Comprehensionä¸ä»…ä¼˜ç¾Žï¼Œè€Œä¸”æ•ˆçŽ‡ä¹Ÿä¼šå¾ˆé«˜ã€‚ Set Comprehension åŒæ ·çš„ 1s = {x for x in range(100) if x%2 != 0 and x%3 != 0} Dictionary Comprehension Syntaxï¼š{expression(variable): expression(variable) for variable, variable in input_set [predicate][, â€¦]} 1234567891011121314# [(set_k), (set_v)]&gt;&gt;&gt; {k: v for k, v in [(1, 2), (3, 4)]}{1: 2, 3: 4}&gt;&gt;&gt; {n: n for n in range(2)}{0: 0, 1: 1}&gt;&gt;&gt; {chr(n): n for n in (65, 66, 66)}{'A': 65, 'B': 66}# ((k1, v1), (k2, v2))&gt;&gt;&gt; {k: v for k, v in (('I', 1), ('II', 2))}{'I': 1, 'II': 2}&gt;&gt;&gt; {k: v for k, v in (('a', 0), ('b', 1)) if v == 1}{'b': 1} 68-Text Justification68-Text Justification Problem: æ–‡æœ¬å¯¹é½ï¼Œæ€»ç»“ä¸‹æ¥æœ‰ä»¥ä¸‹å‡ ç‚¹è¦æ±‚ã€‚ å¦‚æžœä¸æ˜¯æœ€åŽä¸€è¡Œï¼Œä¸”è¯¥è¡Œä¸æ­¢ä¸€ä¸ªå•è¯ï¼Œåˆ™è¦æ±‚å·¦å³å¯¹é½ã€‚ å·¦å³å¯¹é½ï¼šå°½å¯èƒ½è®©å•è¯é—´çš„ç©ºæ ¼å‡åŒ€åˆ†å¸ƒï¼Œå¦‚æžœä¸èƒ½å‡åŒ€åˆ†å¸ƒï¼Œåˆ™å•è¯å·¦è¾¹çš„ç©ºæ ¼åº”è¯¥æ¯”å³è¾¹çš„ç©ºæ ¼å¤šã€‚ è´ªå¿ƒçš„æ€æƒ³ï¼šåº”è¯¥å°½å¯èƒ½çš„å¤šæ”¾å•è¯ã€‚ å¦‚æžœæ˜¯æœ€åŽä¸€è¡Œï¼Œæˆ–è€…è¯¥è¡Œåªæœ‰ä¸€ä¸ªå•è¯ï¼Œåˆ™è¦æ±‚å·¦å¯¹é½ã€‚ Solutionï¼š åˆ†ä¸¤ç§æƒ…å†µå¤„ç†ï¼Œåˆ¤æ–­æ˜¯å·¦å¯¹é½ï¼Œè¿˜æ˜¯å³å¯¹é½ã€‚ å·¦å¯¹é½ï¼šè¯¥è¡Œæœ‰xä¸ªå•è¯ å‰x-1ä¸ªå•è¯çš„åŽé¢éƒ½åº”è¯¥åªæœ‰ä¸€ä¸ªç©ºæ ¼ã€‚ æœ€åŽä¸€ä¸ªå•è¯åŽé¢å°±åº”è¯¥è¡¥é½æ‰€æœ‰ç©ºæ ¼ã€‚ å·¦å³å¯¹é½ï¼šè¯¥è¡Œæœ‰xä¸ªå•è¯ï¼Œæœ‰x-1ä¸ªç©ºæ ¼é—´éš™ã€‚ è®¡ç®—å¾—åˆ°è¯¥è¡Œçš„ç©ºæ ¼æ•°wï¼Œåˆ™å¦‚æžœèƒ½å‡åŒ€åˆ†é…ï¼Œåˆ™æ¯ä¸ªé—´éš™åº”è¯¥æœ‰aver = w // (x-1) ä¸ªç©ºæ ¼ã€‚ ä½†ä¹Ÿè®¸ä¸ä¼šå‡åŒ€åˆ†é…ï¼Œå› æ­¤ï¼Œå¯èƒ½ä¼šå¤šå‡ºmä¸ªç©ºæ ¼ï¼ˆm &lt; x-1 ) å³å‰mä¸ªå•è¯ï¼Œå•è¯çš„åŽé¢åº”è¯¥æœ‰ï¼ˆaver+1)ä¸ªç©ºæ ¼ï¼ŒåŽé¢çš„(x-1) - mä¸ªå•è¯åº”è¯¥æœ‰averä¸ªç©ºæ ¼ã€‚ æœ€åŽä¸€ä¸ªå•è¯çš„åŽé¢æ²¡æœ‰ç©ºæ ¼ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from typing import Listclass Solution: def __init__(self): self.words = None self.maxWidth = None self.sum = None def leftJustify(self, l: int, r: int) -&gt; str: wordsNum = r - l + 1 lengthNum = self.sum[r] if l == 0 else self.sum[r] - self.sum[l - 1] spaceNum = self.maxWidth - lengthNum temp = &quot;&quot; for i in range(l, r): temp += self.words[i] + &quot; &quot; temp += self.words[r] + &quot; &quot;*(spaceNum - (wordsNum - 1)) return temp def leftRightJustify(self, l: int, r: int) -&gt; str: wordsNum = r - l + 1 lengthNum = self.sum[r] if l == 0 else self.sum[r] - self.sum[l - 1] spaceNum = self.maxWidth - lengthNum temp = &quot;&quot; averSpace = spaceNum // (wordsNum - 1) moreSpace = spaceNum - averSpace*(wordsNum - 1) for i in range(moreSpace): temp += self.words[l+i] + &quot; &quot; * (averSpace + 1) for i in range(l+moreSpace, r): temp += self.words[i] + &quot; &quot; * averSpace temp += self.words[r] return temp def fullJustify(self, words: List[str], maxWidth: int) -&gt; List[str]: self.words = words self.maxWidth = maxWidth n = len(words) sum = [0]*n sum[0] = len(words[0]) # sum prefix length of words for i in range(1, n): sum[i] = sum[i - 1] + len(words[i]) self.sum = sum l = 0 ans = [] while l &lt; n: r = l lengthNum = len(self.words[l]) while r+1 &lt; n and lengthNum + len(self.words[r+1]) + 1 &lt;= maxWidth: lengthNum += len(self.words[r+1]) + 1 # space r += 1 # only one word or the last line if r - l + 1 == 1 or r == n - 1: ans.append(self.leftJustify(l, r)) else: ans.append(self.leftRightJustify(l, r)) l = r + 1 return ans Pythonçš„ä¸‰å…ƒè¿ç®—ç¬¦ï¼š #å¦‚æžœæ¡ä»¶ä¸ºçœŸï¼Œè¿”å›žçœŸ å¦åˆ™è¿”å›žå‡condition_is_true if condition else condition_is_false 12is_fat = Truestate = &quot;fat&quot; if is_fat else &quot;not fat&quot; Pythonçš„æ•´é™¤æ˜¯ï¼š\\\\ ï¼Œå®žæ•°é™¤æ˜¯ï¼š\\ Reference Pythonä¸­å­—ç¬¦ä¸²çš„è¿žæŽ¥æ–¹æ³•æ€»ç»“ï¼š https://segmentfault.com/a/1190000015475309","link":"/2020/09/28/LeetCode_String/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€:Classification-Logistic Regression","text":"åœ¨ä¸Šç¯‡æ–‡ç« ä¸­ï¼Œè®²è§£äº†æ€Žä¹ˆç”¨Generative Modelåšåˆ†ç±»é—®é¢˜ã€‚è¿™ç¯‡æ–‡ç« ä¸­ï¼Œè®²è§£äº†åšClassificationçš„å¦ä¸€ç§Discriminativeçš„æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯Logistic Regressionã€‚æ–‡ç« ä¸»è¦æœ‰ä¸¤éƒ¨åˆ†ï¼šç¬¬ä¸€éƒ¨åˆ†è®²è§£äº†Logistic Regressionçš„ä¸‰ä¸ªæ­¥éª¤ã€‚ç¬¬äºŒä¸ªéƒ¨åˆ†è®²è§£äº†multi-classå¤šåˆ†ç±»çš„ä¸‰ä¸ªæ­¥éª¤ï¼Œä»¥åŠsoftmaxæ˜¯å¦‚ä½•æ“ä½œçš„ã€‚ Logistic RegressionStep1: Function Setåœ¨Post not found: Classification ä¸Šä¸€ç¯‡æ–‡ç« æœ«å°¾ï¼Œæˆ‘ä»¬å¾—å‡º $P_{w, b}\\left(C_{1} | x\\right)=\\sigma(w\\cdot x+b)$ çš„å½¢å¼ï¼Œæƒ³è·³è¿‡æ‰¾ $\\mu_1,\\mu_2,\\Sigma$ çš„è¿‡ç¨‹ï¼Œç›´æŽ¥æ‰¾ $w,b$ ã€‚ å› æ­¤Function Set: $f_{w, b}(x)=P_{w, b}\\left(C_{1} | x\\right)$ ã€‚å€¼å¤§äºŽ0.5ï¼Œåˆ™å±žäºŽC1ç±»ï¼Œå¦åˆ™å±žäºŽC2ç±»ã€‚ Step2: Goodness of a Functionä½¿ç”¨æžå¤§ä¼¼ç„¶çš„æ€æƒ³ï¼ˆåœ¨å‰ä¸€ç¯‡æœºçŽ‡æ¨¡åž‹/ç”Ÿæˆæ¨¡åž‹ä¸­æœ‰è®²ï¼‰ ä¼°è®¡å‡½æ•°æ˜¯ ï¼š$L(w, b)=f_{w, b}\\left(x^{1}\\right) f_{w, b}\\left(x^{2}\\right)\\left(1-f_{w, b}\\left(x^{3}\\right)\\right) \\cdots f_{w, b}\\left(x^{N}\\right)$ ç›®æ ‡ï¼š $ w^{*}, b^{*}=\\arg \\max _{w, b} L(w, b)$ ç”±äºŽåœ¨ä¹‹å‰çš„Regressionä¸­ï¼Œæˆ‘ä»¬éƒ½æ˜¯æ‰¾æžå°å€¼ç‚¹ï¼Œä¸ºäº†æ–¹ä¾¿å¤„ç†ï¼Œå°†ä¼°è®¡å‡½æ•°è½¬æ¢ä¸ºå¦‚ä¸‹å½¢å¼çš„æŸå¤±å‡½æ•°ï¼š $$ \\begin{equation} \\begin{aligned} -\\ln L(w, b)&=-(\\ln f_{w, b}\\left(x^{1}\\right)+\\ln f_{w, b}\\left(x^{2}\\right)+\\ln \\left(1-f_{w, b}\\left(x^{3}\\right)\\right) \\cdots ) \\\\ Loss&=\\sum_{n}-\\left[\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)\\right] \\end{aligned} \\end{equation} $$ ç›®æ ‡ ï¼š $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ Cross entropyï¼ˆäº¤å‰ç†µï¼‰ å…³äºŽç†µã€äº¤å‰ç†µã€ç›¸å¯¹ç†µï¼ˆKLæ•£åº¦ï¼‰çš„ç†è§£ï¼šå¼ºçƒˆå®‰åˆ© ä¸Šå¼ä¸­çš„ $\\left[\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)\\right]$ å…¶å®žæ˜¯ä¸¤ä¸ªBernoulli distributionçš„äº¤å‰ç†µã€‚ äº¤å‰ç†µæ˜¯ä»€ä¹ˆï¼Ÿ ç®€å•æ¥è¯´ï¼Œäº¤å‰ç†µæ˜¯è¯„ä¼°ä¸¤ä¸ªdistribution æœ‰å¤šæŽ¥è¿‘ã€‚æ‰€ä»¥å½“è¿™ä¸¤ä¸ªBernoulli åˆ†å¸ƒçš„äº¤å‰ç†µä¸º0æ—¶ï¼Œè¡¨æ˜Žè¿™ä¸¤ä¸ªåˆ†å¸ƒä¸€æ¨¡ä¸€æ ·ã€‚ å¯¹äºŽ $\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)$ ï¼š Distribution p: p(x = 1) = $\\hat{y}^n$ ; p( x = 0 ) = 1 - $\\hat{y}^n$ Distribution q: q(x = 1 ) = $f(x^n)$ ; q(x = 0 ) = 1 - $f(x^n)$ äº¤å‰ç†µ $H(p,q)=-\\Sigma_xp(x)\\ln(q(x))$ pæ˜¯çœŸå®žçš„åˆ†å¸ƒï¼Œqæ˜¯é¢„æµ‹çš„åˆ†å¸ƒã€‚ å› æ­¤ï¼Œè¿™ä¸ªæŸå¤±å‡½æ•°çš„è¡¨è¾¾å¼å…¶å®žä¹Ÿæ˜¯è¾“å‡ºåˆ†å¸ƒå’Œtargetåˆ†å¸ƒçš„äº¤å‰ç†µï¼Œå³ï¼š $L(f)=\\sum_{n} C\\left(f\\left(x^{n}\\right), \\hat{y}^{n}\\right)$ ï¼ˆ $C\\left(f\\left(x^{n}\\right), \\hat{y}^{n}\\right)=-\\left[\\hat{y}^{n} \\ln f\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f\\left(x^{n}\\right)\\right)\\right]$ ï¼‰ å’ŒLinear Regressionä¸åŒï¼Œä¸ºä»€ä¹ˆLogistic Regressionä¸ç”¨square errorï¼Œè€Œè¦ä½¿ç”¨cross entropyã€‚ åœ¨1.4å°èŠ‚ä¼šç»™å‡ºè§£é‡Šã€‚ Step3: Find the best functionåœ¨ç¬¬ä¸‰æ­¥ï¼ŒåŒæ ·ä½¿ç”¨Gradientæ¥å¯»æ‰¾æœ€ä¼˜å‡½æ•°ã€‚ æŽ¨å¯¼è¿‡ç¨‹ï¼š $\\left.\\frac{-\\ln L(w, b)}{\\partial w_{i}}=\\sum_{n}-\\left[\\hat{y}^{n} \\frac{\\ln f_{w, b}\\left(x^{n}\\right)}{\\partial w_{i}}+\\left(1-\\hat{y}^{n}\\right) \\frac{\\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right.}{\\partial w_{i}}\\right)\\right]$ $\\frac{\\partial \\ln f_{w, b}(x)}{\\partial w_{i}}=\\frac{\\operatorname{\\partial\\ln} f_{w, b}(x)}{\\partial z} \\frac{\\partial z}{\\partial w_{i}}$ $\\frac{\\partial \\ln \\sigma(z)}{\\partial z}=\\frac{1}{\\sigma(z)} \\frac{\\partial \\sigma(z)}{\\partial z}=\\frac{1}{\\sigma(z)} \\sigma(z)(1-\\sigma(z))$ $\\frac{\\partial z}{\\partial w_{i}}=x_{i}$ $\\frac{\\partial \\ln \\left(1-f_{w, b}(x)\\right)}{\\partial w_{i}}=\\frac{\\operatorname{\\partial\\ln}\\left(1-f_{w, b}(x)\\right)}{\\partial z} \\frac{\\partial z}{\\partial w_{i}}$ $\\frac{\\partial \\ln (1-\\sigma(z))}{\\partial z}=-\\frac{1}{1-\\sigma(z)} \\frac{\\partial \\sigma(z)}{\\partial z}=-\\frac{1}{1-\\partial(z)} \\sigma(z)(1-\\sigma(z))$ $\\frac{\\partial z}{\\partial w_{i}}=x_{i}$ $\\frac{\\partial \\sigma(z)}{\\partial z}=\\sigma(z)\\cdot(1-\\sigma(z))$ æ³¨ï¼š$f_{w, b}(x)=\\sigma(z)$ ; $z=w \\cdot x+b=\\sum_{i} w_{i} x_{i}+b$ $$ \\begin{equation} \\begin{aligned} \\frac{-\\ln L(w, b)}{\\partial w_{i}}&=\\sum_{n}-\\left[\\hat{y}^{n}\\left(1-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n}-\\left(1-\\hat{y}^{n}\\right) f_{w, b}\\left(x^{n}\\right) x_{i}^{n}\\right] \\\\&=\\sum_{n}-\\left(\\hat{y}^{n}-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n} \\end{aligned} \\end{equation} $$ å› æ­¤Logistic Regressionçš„æŸå¤±å‡½æ•°çš„å¯¼æ•°å’ŒLinear Regressionçš„ä¸€æ ·ã€‚ è¿­ä»£æ›´æ–°ï¼š $w_{i} \\leftarrow w_{i}-\\eta \\sum_{n}-\\left(\\hat{y}^{n}-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n}$ ä¸ŽLinear Regression çš„å¯¹æ¯”å¦‚å›¾æ‰€ç¤ºã€‚ If : Logistic + Square Errorå‰é¢ä¸€å°èŠ‚æˆ‘ä»¬æåˆ°ï¼Œåœ¨Logistic Regressionä¸­ä½¿ç”¨cross entropyåˆ¤åˆ«ä¸€ä¸ªå‡½æ•°çš„å¥½å,é‚£ä¸ºä»€ä¹ˆä¸ä½¿ç”¨square erroræ¥judge the goodnessï¼Ÿ å¦‚æžœä½¿ç”¨ Square Errorçš„æ–¹æ³•ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š æ¥çœ‹Step 3: æŸå¤±å‡½æ•°çš„å¯¼æ•°æ˜¯ $2\\left(f_{w, b}(x)-\\hat{y}\\right) f_{w, b}(x)\\left(1-f_{w, b}(x)\\right) x_{i}$ è€ƒè™‘ $\\hat{y}^n=1$ ï¼ˆå³æˆ‘ä»¬çš„targetæ˜¯1ï¼‰ï¼š å¦‚æžœ $f_{w,b}(x^n)=1$ , å³é¢„æµ‹å€¼æŽ¥è¿‘ target, ç®—å‡ºæ¥çš„ $\\partial{L}/\\partial{w_i}=0$ æ˜¯æœŸæœ›çš„ã€‚ å¦‚æžœ $f_{w,b}(x^n)=0$ , å³é¢„æµ‹å€¼åŽŸç† target, ç®—å‡ºæ¥çš„ $\\partial{L}/\\partial{w_i}=0$ æ˜¯ä¸æœŸæœ›çš„ã€‚ åŒç†ï¼Œå½“è€ƒè™‘ $\\hat{y}^n=0$ æƒ…å†µæ—¶ï¼Œä¹Ÿæ˜¯å¦‚æ­¤ã€‚ æ›´ç›´è§‚çš„çœ‹ï¼š ä¸Šå›¾ä¸­ï¼Œç”»å‡ºäº†ä¸¤ç§æŸå¤±å‡½æ•°çš„å¹³é¢ï¼Œä¸­å¿ƒçš„æœ€ä½Žç‚¹æ˜¯æˆ‘ä»¬çš„targetã€‚ ä½†åœ¨Square Errorä¸­ï¼Œè¿œç¦»targetçš„è“è‰²ç‚¹ï¼Œä¹Ÿå¤„åœ¨å¾ˆå¹³å¦çš„ä½ç½®ï¼Œå…¶å¯¼æ•°å°ï¼Œå‚æ•°çš„æ›´æ–°ä¼šå¾ˆæ…¢ã€‚ å› æ­¤åœ¨Cross Entropyä¸­ï¼Œç¦»targetè¶Šè¿œï¼Œå…¶å¯¼æ•°æ›´å¤§ï¼Œæ›´æ–°æ›´å¿«ã€‚ æ‰€ä»¥Cross Entropyçš„æ•ˆæžœæ¯”Square Erroræ›´å¿«ï¼Œæ•ˆæžœæ›´å¥½ã€‚ Discriminative V.S. Generativeè¿™ç¯‡æ–‡ç« ä¸­çš„Logistic Regressionæ˜¯Discriminative Modelã€‚ ä¸Šç¯‡æ–‡ç« ä¸­Classificationæ˜¯Generative Modelã€‚ æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼Ÿ ä¸Šå›¾ä¸­ï¼ŒGenerative Modelåšäº†å‡è®¾ï¼ˆè„‘è¡¥ï¼‰ï¼Œå‡è®¾å®ƒæ˜¯ Gaussian Distributionï¼Œå‡è®¾å®ƒæ˜¯Bernoulli Distributionã€‚ç„¶åŽåŽ»æ‰¾è¿™äº›åˆ†å¸ƒçš„å‚æ•°ï¼Œåœ¨æ±‚å‡º $w,b$ã€‚ è€Œåœ¨Discriminative Modelä¸­ï¼Œæ²¡æœ‰åšä»»ä½•å‡è®¾ï¼Œç›´æŽ¥æ‰¾ $w,b$ å‚æ•°ã€‚ æ‰€ä»¥ï¼Œè¿™ä¸¤ç§Modelç»è¿‡trainingæ‰¾å‡ºæ¥çš„å‚æ•°ä¸€æ ·å—ï¼Ÿ ç­”æ¡ˆæ˜¯ä¸ä¸€æ ·çš„ã€‚ The same model(function set), but different function is selected by the same training data. åœ¨ä¸Šç¯‡Pokemonçš„ä¾‹å­ä¸­ï¼Œæ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„ç»“æžœå·®å¼‚ã€‚ å¯è§ï¼Œåœ¨Pokemonçš„ä¾‹å­æ€»ï¼ŒDiscriminativeçš„æ•ˆæžœæ¯”Generativeçš„æ•ˆæžœå¥½ä¸€äº›ã€‚ ä½†æ˜¯Generative Modelå°±ä¸å¥½å—ï¼Ÿ Benefit of generative model With the assumption of probability distribution, less training data is needed. ã€è®­ç»ƒç”Ÿæˆæ¨¡åž‹æ‰€éœ€æ•°æ®æ›´å°‘ã€‘ With the assumption of probability distribution, more robust to the noise. ã€ç”Ÿæˆæ¨¡åž‹å¯¹noise dataæ›´å…¼å®¹ã€‘ Priors and class-dependent probabilities can be estimated from different sources. ã€ç”Ÿæˆæ¨¡åž‹ä¸­çš„ å…ˆéªŒæ¦‚çŽ‡Priors å’Œ åŸºäºŽç±»åˆ«çš„åˆ†å¸ƒæ¦‚çŽ‡ä¸åŒã€‘ æ¯”å¦‚ï¼Œåšè¯­éŸ³è¾¨è¯†ç³»ç»Ÿï¼Œæ•´ä¸ªç³»ç»Ÿæ˜¯generativeçš„ã€‚ å› ä¸ºPriorï¼ˆæŸä¸€å¥è¯çš„æ¦‚çŽ‡ï¼‰å¹¶ä¸éœ€è¦ä»Ždataä¸­çŸ¥é“ï¼Œå¯ä»¥ç›´æŽ¥åœ¨ç½‘ç»œä¸Šçˆ¬è™«ç»Ÿè®¡ã€‚ è€Œclass-dependent probabilitiesï¼ˆè¿™æ®µè¯­éŸ³æ˜¯è¿™å¥è¯çš„æ¦‚çŽ‡ï¼‰éœ€è¦dataè¿›è¡Œè®­ç»ƒæ‰èƒ½å¾—çŸ¥ã€‚ Multi-class classificationsoftmax å‡è®¾æœ‰ä¸‰ä¸ªç±»åˆ«ï¼šC1ã€C2ã€C3 ã€‚æ¨¡åž‹å·²ç»å¾—åˆ°ï¼Œå‚æ•°åˆ†åˆ«æ˜¯ wã€bã€‚ å¯¹äºŽè¾“å…¥x, åˆ¤æ–­xå±žäºŽå“ªä¸€ä¸ªç±»åˆ«ã€‚ é€šè¿‡æ¯ä¸ªç±»åˆ«çš„ wã€bæ±‚å‡º $z^i=w^i\\cdot x+b_i$ Softmaxçš„æ­¥éª¤ï¼š exponentialï¼šæ¯ä¸ªzå€¼å¾—åˆ° $=e^z$ . sumï¼šå°†æŒ‡æ•°åŒ–åŽçš„å€¼åŠ èµ·æ¥$=\\Sigma_{j=1}^3e^{z_j}$ output: æ¯ä¸ªç±»åˆ«çš„è¾“å‡º $y_i=e^{z_1}/\\Sigma_{j=1}^3e^{z_j}$ ï¼Œå³xå±žäºŽç±»åˆ«içš„æ¦‚çŽ‡ã€‚ æ±‚å‡ºçš„ $1&gt;y_i&gt;0$ ä¸” $\\Sigma_iy_i=1$ ã€‚ é€šè¿‡Softmaxï¼Œå¾—åˆ° $y_i=P(C_i|x)$ ã€‚ Stepsï¼ˆæ‰‹å†™ç¬”è®°ï¼Œç•¥å€¾æ–œï¼ŒåŽŸæ¥ä¸åˆ‡ä¸€åˆ‡è¿˜ä¸çŸ¥é“è‡ªå·±æ­ªçš„è¿™ä¹ˆåŽ‰å®³ æ³ªï¼‰ Step 1: Step 2: Step 3: ä½¿ç”¨Stochastic Gradientï¼ˆå³æ¯ä¸ªæ ·æœ¬æ›´æ–°ä¸€æ¬¡ï¼‰çš„è¯ï¼š data: [x, $\\hat{y}$ ] , $\\hat{y}_i=1$ æ›´æ–° $w^j$ : $j=i$ : $w^j \\leftarrow w^j-\\eta\\cdot (y_i-1)\\cdot x$ $j\\neq i$ : $w^j \\leftarrow w^j-\\eta\\cdot y_i\\cdot x$ (ä¸‹æ¬¡ä¸€å®šï¼Œç¬”è®°å†™ç›´ä¸€ç‚¹ï¼) æ›´ä¸ºè§„èŒƒçš„æŽ¨å¯¼è§[1] Limitation of Logistic Regression å¯¹äºŽå¦‚ä¸Šæƒ…å†µï¼ŒLogistic Regressionå¹¶ä¸èƒ½è¿›è¡Œåˆ†ç±»ï¼Œå› ä¸ºä»–çš„boundary åº”è¯¥æ˜¯çº¿æ€§çš„ã€‚ Feature Transformingå¦‚æžœå¯¹featureåšè½¬æ¢åŽï¼Œå°±å¯ä»¥ç”¨Logistic Regressionå¤„ç†ã€‚ é‡å®šä¹‰featureï¼Œ $x_1â€™$ :å®šä¹‰ä¸ºåˆ°[0,0]çš„è·ç¦»ï¼Œ $x_2â€™$ :å®šä¹‰ä¸ºåˆ°[1,1]çš„è·ç¦»ã€‚ äºŽæ˜¯å›¾å˜æˆä¸‹å›¾ï¼Œå³å¯ç”¨Logistic Regressionè¿›è¡Œåˆ†ç±»ã€‚ ä½†è¿™æ ·çš„åšæ³•ï¼Œå°±ä¸åƒäººå·¥æ™ºèƒ½äº†ï¼Œå› ä¸ºFeature Transformationéœ€è¦äººæ¥è®¾è®¡ï¼Œè€Œä¸”è¾ƒéš¾è®¾è®¡ã€‚ Cascading logistic regression modelså¦ä¸€ç§åšæ³•æ˜¯ï¼Œå°†logistic regressionè¿žæŽ¥èµ·æ¥ã€‚ ä¸Šå›¾ä¸­ï¼Œå·¦è¾¹éƒ¨åˆ†çš„ä¸¤ä¸ªlogistic regressionå°±ç›¸å½“äºŽåœ¨åšFeature Transformationï¼Œå³è¾¹éƒ¨åˆ†ç›¸å½“äºŽåœ¨åšClassificationã€‚ è€Œé€šè¿‡è¿™ç§å½¢å¼ï¼Œå°†å¤šä¸ªmodelè¿žæŽ¥èµ·æ¥ï¼Œä¹Ÿå°±æ˜¯å¤§çƒ­çš„Neural Networkã€‚ Reference Multi-class ClassificationæŽ¨å¯¼ï¼šBishopï¼ŒP209-210 å…³äºŽEntropy, Cross Entropy, KL-Divergenceçš„ç†è§£ï¼šå¼ºçƒˆå®‰åˆ©ï¼šhttps://www.youtube.com/watch?v=ErfnhcEV1O8","link":"/2020/03/31/Classification2/"},{"title":"ã€ŒLeetCodeã€ï¼šMath","text":"LeetCode Math ä¸“é¢˜è®°å½•ã€‚ 10æœˆåˆã€‚ Albert Einstein: â€œI believe that not everything that can be counted counts, and not everything that counts can be countedâ€ ã€Œå¹¶éžæ‰€æœ‰é‡è¦çš„ä¸œè¥¿éƒ½æ˜¯å¯ä»¥è¢«è®¡ç®—çš„ï¼Œä¹Ÿå¹¶ä¸æ˜¯æ‰€æœ‰èƒ½è¢«è®¡ç®—çš„ä¸œè¥¿éƒ½é‚£ä¹ˆé‡è¦ã€‚ã€ 7. Reverse Integer[E]7. Reverse Integer[E] Problem: åè½¬32bitsçš„æœ‰ç¬¦å·æ•°å­—ï¼Œå¦‚æžœåè½¬åŽä¼šæº¢å‡ºåˆ™è¿”å›ž0. Solutionï¼š ç›´è§‚çš„è§£å†³å®ƒï¼Œå…ˆç®—å‡ºåè½¬åŽçš„æ•°å­—ï¼Œç”¨æ¯”è¾ƒå¤§å°æ¥çœ‹æ˜¯å¦æº¢å‡ºã€‚ï¼ˆæœ€å¼€å§‹è¿˜æƒ³ç€è½¬æ¢ä¸ºbitä¸²æ¥çœ‹ï¼Œå°±å¤æ‚äº†ï¼‰ 123456789101112131415161718class Solution: def reverse(self, x: int) -&gt; int: n_min = -(2 ** 31) n_max = 2 ** 31 - 1 s = 0 flag = True if x &lt; 0: flag = False x = -x while x != 0: r = x % 10 s = s * 10 + r x //=10 if flag is False: s = -s if s &lt; n_min or s &gt; n_max: return 0 return s åè¿›åˆ¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶ã€å…«è¿›åˆ¶ã€åå…­è¿›åˆ¶ï¼š äºŒè¿›åˆ¶ï¼šbin(a) ,ä¹Ÿå¯ä»¥ç›´æŽ¥èµ‹äºŒè¿›åˆ¶çš„å€¼0b10101 å…«è¿›åˆ¶ï¼šoct(a) ï¼Œèµ‹å€¼å…«è¿›åˆ¶çš„å€¼0o263361 åå…­è¿›åˆ¶ï¼šhex(a) ,èµ‹å€¼åå…­è¿›åˆ¶0x1839ac29 165. Compare Version Numbers[M]165. Compare Version Numbers[M] Problemï¼š æ¯”è¾ƒç‰ˆæœ¬å·ã€‚ Solutionï¼š ç›´è§‚ï½žEasyï½ž 123456789101112131415class Solution: def compareVersion(self, version1: str, version2: str) -&gt; int: li1 = version1.split('.') li2 = version2.split('.') n_1 = len(li1) n_2 = len(li2) n = max(n_1, n_2) for i in range(n): a = int(li1[i]) if i &lt; n_1 else 0 b = int(li2[i]) if i &lt; n_2 else 0 if a &gt; b: return 1 elif a &lt; b: return -1 return 0 Pythonä¸­çš„å¼ºåˆ¶è½¬æ¢ï¼š å­—ç¬¦ä¸²è½¬æ¢ä¸ºint : int_value = int(str_value) intè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼šstr.value = str(int_value) intè½¬æ¢ä¸ºunicodeï¼š unicode(int_value) unicodeè½¬æ¢ä¸ºintï¼šint(unicode_value) å­—ç¬¦ä¸²è½¬æ¢ä¸ºunicodeï¼šunicode(str_value) unicodeè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼šstr(unicode_value) Javaä¸­çš„å¼ºåˆ¶è½¬æ¢ï¼š å­—ç¬¦ä¸²Stringè½¬åŒ–ä¸ºintï¼šint_value = String.parseInt(string_value) æˆ–è€… (int)string_value) intè½¬åŒ–ä¸ºå­—ç¬¦ä¸²Stringï¼šstring_value = (String)int_value 66. Plus One[E]66. Plus One[E] Problem: ç”¨åˆ—è¡¨è¡¨ç¤ºä¸€ä¸ªæ­£æ•°ï¼Œè¿”å›žæ­£æ•°+1çš„åˆ—è¡¨ç»“æžœã€‚ Solutionï¼š è®°å½•ä¸€ä¸ªæœ€é«˜ä½çš„è¿›ä½æƒ…å†µã€‚ 123456789101112131415161718192021from typing import Listclass Solution: def plusOne(self, digits: List[int]) -&gt; List[int]: c_bit = 0 n = len(digits) i = n - 1 digits[i] += 1 while i &gt;= 0: if digits[i] &lt; 10: break digits[i] %= 10 if i == 0: c_bit = 1 else: digits[i-1] += 1 i -= 1 if c_bit == 1: digits.insert(0, c_bit) return digits Pythonä¸­listæ·»åŠ å…ƒç´ çš„é›†ä¸­æ–¹æ³•ï¼šï¼ˆappend(); extend(); insert(); +åŠ å·ï¼‰ append() ï¼šåœ¨Listå°¾éƒ¨è¿½åŠ å•ä¸ªå…ƒç´ ï¼ŒåªæŽ¥å—ä¸€ä¸ªå‚æ•°ï¼Œå‚æ•°å¯ä»¥æ˜¯ä»»æ„æ•°æ®ç±»åž‹ã€‚ extend() ï¼šåœ¨listå°¾éƒ¨è¿½åŠ ä¸€ä¸ªåˆ—è¡¨ï¼Œå°†è¯¥å‚æ•°åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ è¿žæŽ¥åˆ°åŽŸåˆ—è¡¨ã€‚ 12345&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; b = [3,4,5]&gt;&gt;&gt; a.extend(b)&gt;&gt;&gt; a[1, 2, 3, 3, 4, 5] insert(index, object)ï¼šå°†ä¸€ä¸ªå…ƒç´ æ’å…¥åˆ°åˆ—è¡¨ä¸­ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ’å…¥çš„ç´¢å¼•ç‚¹ï¼Œç¬¬äºŒä¸ªæ˜¯æ’å…¥çš„å…ƒç´ ã€‚ +åŠ å·ï¼šå°†ä¸¤ä¸ªlistç›¸åŠ ï¼Œè¿”å›žä¸€ä¸ªæ–°çš„listå¯¹è±¡ã€‚ åŒºåˆ«ï¼šå‰ä¸‰ç§æ–¹æ³•(append, extend, insert)å¯ä»¥å¯¹åˆ—è¡¨å¢žåŠ å…ƒç´ ï¼Œæ²¡æœ‰è¿”å›žå€¼ï¼Œæ˜¯ç›´æŽ¥ä¿®æ”¹åŽŸæ•°æ®å¯¹è±¡ï¼Œè€Œ+åŠ å·æ˜¯éœ€è¦åˆ›å»ºæ–°çš„listå¯¹è±¡ï¼Œéœ€è¦æ¶ˆè€—é¢å¤–çš„å†…å­˜ã€‚","link":"/2020/10/09/Leetcode-math/"},{"title":"ã€ŒMPC-Mike Rosulek ã€ï¼šOverview of Secure Computation and Yao&#39;s Protocol","text":"æœ¬ç³»åˆ—æ˜¯æ€»ç»“Mike Rosulekæ•™æŽˆåœ¨ä¸Šæµ·æœŸæ™ºç ”ç©¶é™¢çš„å¯†ç å­¦å­¦æœ¯è®²åº§ã€‚ è¿™æ˜¯Mikeæ•™æŽˆçš„ç¬¬ä¸€ä¸ªåˆ†äº«ï¼šOverview of Secure Computation and Yaoâ€™s Protocol Roadmap Secure computation: Concepts &amp; definitions Yaoâ€™s protocol: semi-honest secure computation for boolean circuits ä¸»è¦å†…å®¹åŒ…æ‹¬å®‰å…¨å¤šæ–¹è®¡ç®—çš„æ•´ä½“ä»‹ç»åŠå…¶åº”ç”¨åœºæ™¯ã€å¦‚ä½•å®šä¹‰å®‰å…¨å¤šæ–¹è®¡ç®—çš„securityã€Yaoçš„æ··æ·†ç”µè·¯åè®® (garbled circuits protocol)ã€‚ ï¼ˆå’•å’•å’•åšå®¢é€‰æ‰‹å›žæ¥äº†ï¼Œå­¦é•¿è¯´ï¼šä¸Žå…¶æ‹…å¿ƒæœ‰æ²¡æœ‰å­¦è¯»ï¼Œä¸å¦‚å¤šå­¦å­¦å¯†ç å­¦ï¼Œæ³ªç›®ï¼Œæˆ‘è§‰å¾—ä»–è¯´çš„å¯¹ï¼ï¼‰ Secure computationConceptsä»€ä¹ˆæ˜¯å®‰å…¨è®¡ç®—ï¼Ÿ Mutually distrusting parties, each with a private input. ã€å‚ä¸Žæ–¹ä¹‹é—´äº’ä¸ä¿¡ä»»ï¼Œä¸”éƒ½æœ‰ä¸€ä¸ªéšç§è¾“å…¥ã€‘ Learn the result of agreed-upon computation. ã€å‚ä¸Žæ–¹å¸Œæœ›èƒ½å¾—åˆ°ä¸€ä¸ªå„æ–¹å•†å®šçš„è®¡ç®—ç»“æžœã€‘ å®‰å…¨è®¡ç®—èƒ½ä¿è¯ä»€ä¹ˆï¼Ÿ Privacy (â€œlearn no more thanâ€ prescribed output) ã€éšç§æ€§ï¼šå‚ä¸Žæ–¹é™¤äº†å¾—åˆ°å•†å®šçš„è®¡ç®—ç»“æžœï¼Œå…¶ä»–éƒ½ä¸èƒ½å¾—åˆ°ã€‘ Input independence ã€å„å‚ä¸Žæ–¹çš„è¾“å…¥æ— å…³ã€‘ Output consistency ã€å„å‚ä¸Žæ–¹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå•†å®šä¸€è‡´çš„è®¡ç®—ç»“æžœã€‘ ä»¥ä¸Šæ€§è´¨åŠæ—¶åœ¨æŸäº›å‚ä¸Žæ–¹å‹¾ç»“æ¬ºéª—çš„æƒ…å†µä¸‹ï¼Œä¹Ÿæ˜¯æˆç«‹çš„ã€‚ scenariosExample 1: Sugar Beetsâ€”â€”ç”œèœå®šä»· Farmers make bids (â€œat price X, I will produce Y amountâ€) ã€Farmersï¼šä¼šæ ¹æ®ç”œèœçš„å®šä»·å†³å®šäº§é‡ï¼Œä»·æ ¼å¯¹äº§é‡çš„å½±å“å¦‚ä¸Šå›¾è“è‰²çš„çº¿ã€‘ Purchaser bids (â€œat price X, I will buy Y amountâ€) ã€Purchaserï¼šä¼šæ ¹æ®ç”œèœçš„å®šä»·è§‰å¾—è´­ä¹°é‡ï¼Œä»·æ ¼å¯¹è´­ä¹°é‡çš„å½±å“å¦‚ä¸Šå›¾çº¢è‰²çš„çº¿ã€‘ Market clearing price (MCP): price at which total supply = demand ã€åŒæ–¹å¸Œæœ›ç”œèœä»·æ ¼èƒ½è¾¾åˆ°å¸‚åœºå‡ºæ¸…ä»·ï¼Œå³äº§é‡=éœ€æ±‚é‡ã€‘ ä½†æ˜¯åŒæ–¹ä¸å¸Œæœ›é€éœ²è‡ªå·±å¿ƒä¸­çš„å®šä»·ï¼Œå› ä¸ºè¿™ä¼šç ´åç»æµŽå¸‚åœºçš„balabala å› æ­¤ï¼Œåœ¨2009å¹´ï¼Œé€šè¿‡åŒæ–¹å¯ä»¥é€šè¿‡å®‰å…¨è®¡ç®—å¾—åˆ°å¸‚åœºå‡ºè¯·ä»·ã€‚ Example 2: Ad conversionâ€”â€”å¹¿å‘ŠæŠ•æ”¾ ç‰¹æ–¯æ‹‰å­˜å‚¨äº†åˆ°åº—è´­ä¹°æ±½è½¦çš„ç”¨æˆ·é‚®ç®±ï¼Œè€Œè°·æ­Œå¸Œæœ›èƒ½ä¸ªæ€§åŒ–çš„æŠ•æ”¾æ±½è½¦å¹¿å‘Šã€‚ å› æ­¤ï¼Œå¦‚æžœåŒæ–¹å…¬å¼€æ•°æ®åº“ä¸­çš„æ•°æ®ï¼Œå°±å¯ä»¥é€šè¿‡ä¸Šå›¾çš„SQLè¯­å¥ç­›é€‰å‡ºç›®æ ‡ç”¨æˆ·ã€‚ ä½†è¿™æ ·ä¼šä¾µçŠ¯ç”¨æˆ·çš„ç”¨æˆ·çš„éšç§ï¼Œæ‰€ä»¥ç‰¹æ–¯æ‹‰å’Œè°·æ­Œéƒ½ä¸ä¼šç›´æŽ¥å…¬å¼€ç”¨æˆ·çš„ä¿¡æ¯ã€‚ç”±æ­¤ï¼ŒåŒæ–¹å¯ä»¥é€šè¿‡å®‰å…¨è®¡ç®—å¾—åˆ°å¹¿å‘ŠæŠ•æ”¾çš„ç›®æ ‡ç”¨æˆ·ã€‚ Example 3: Wage Equity Studyâ€”â€”è–ªèµ„å…¬å¹³æ€§ç ”ç©¶ ç ”ç©¶äººå‘˜æƒ³ç ”ç©¶Bostonçš„è–ªèµ„æ°´å¹³æ˜¯å¦æ˜¯æ€§åˆ«å¹³ç­‰çš„ï¼Œä½†å„ä¼ä¸šä¸ä¼šæ„¿æ„ç›´æŽ¥å…¬å¼€è–ªèµ„ã€‚ è€Œé€šè¿‡å®‰å…¨è®¡ç®—ï¼Œå¯ä»¥åœ¨ä¿æŠ¤ä¼ä¸šè–ªèµ„éšç§çš„æƒ…å†µä¸‹ï¼Œå¾—åˆ°ç›¸å…³ç ”ç©¶ç»“æžœã€‚ Definitionsâ€œsecurelyâ€ compute f ?å¦‚ä½•ç†è§£â€œå®‰å…¨åœ°â€œè®¡ç®—fï¼Ÿ å¯ä»¥å…ˆä»Žå®‰å…¨æ€§éœ€æ±‚/å¨èƒæ¥ç†è§£ä»€ä¹ˆæ˜¯ä¸å®‰å…¨ã€‚ What if adversary learns more than f(x, y)? ã€å¦‚æžœæ”»å‡»è€…å­¦åˆ°äº†é™¤å•†å®šè®¡ç®—ç»“æžœf(x,y)ä»¥å¤–çš„ä¿¡æ¯ï¼Ÿã€‘ What if adversary learns f(x, y) but then prevents honest party from learning it too? ã€å¦‚æžœæ”»å‡»è€…èƒ½å¤Ÿå¾—åˆ°è®¡ç®—ç»“æžœï¼Œä½†æ˜¯é˜»ç¢è¯šä¿¡çš„å‚ä¸Žæ–¹å¾—åˆ°æ­£ç¡®çš„è®¡ç®—ç»“æžœï¼Ÿã€‘ What if adversary forces several parties to have inconsistent outputs? ã€å¦‚æžœæ”»å‡»è€…é€šè¿‡æŸç§æ‰‹æ®µä½¿å¾—ä¸€äº›å‚ä¸Žæ–¹å¾—åˆ°ä¸ä¸€è‡´çš„è®¡ç®—ç»“æžœã€‘ What if adversaryâ€™s choice of input depends on honest partyâ€™s input? ã€å¦‚æžœæ”»å‡»è€…çš„è¾“å…¥ä¼šä¾èµ–ä¸€äº›è¯šä¿¡å‚ä¸Žæ–¹çš„è¾“å…¥ï¼Ÿã€‘ï¼ˆä¸å¤ªç†è§£è¿™æ˜¯ä¸€ä¸ªä»€ä¹ˆæ ·çš„æƒ…å†µï¼Ÿï¼‰ Security in ideal worldåœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œå‡è®¾æœ‰ä¸€ä¸ªå¯ä¿¡ç¬¬ä¸‰æ–¹ï¼ŒAliceå’ŒBobæŠŠè‡ªå·±çš„è¾“å…¥x, y å‘Šè¯‰å¥¹ï¼Œå¯ä¿¡ç¬¬ä¸‰æ–¹è®¡ç®—f(x, y) ï¼Œå†æŠŠè®¡ç®—ç»“æžœåˆ†åˆ«åˆ†å‘ç»™Aliceå’ŒBobã€‚ åœ¨ç†æƒ³æƒ…å†µä¸­ï¼Œå¦‚æžœæœ‰ä¸€ä¸ªæ¶æ„çš„å‚ä¸Žæ–¹ï¼Œç»“æžœä¼šæ€Žä¹ˆæ ·ï¼Ÿï¼ˆä½ çœ‹å›¾ï¼Œå˜åçš„äººå˜ä¸‘äº†2333 corrupt partyâ€™s view: éšæ„é€‰æ‹©ä¸€ä¸ªè¾“å…¥y é™¤äº†çŸ¥é“f(x, y) ï¼Œå…¶ä»–ä¿¡æ¯éƒ½ä¸çŸ¥é“ã€‚ è‡´ä½¿è¯šä¿¡å‚ä¸Žæ–¹å¾—åˆ°ä¸€è‡´çš„f(x, y) å¯è§ï¼Œåœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œå³ä½¿æœ‰æ¶æ„çš„å‚ä¸Žæ–¹ï¼Œä¹Ÿèƒ½è¾¾åˆ°ä¹‹å‰æåˆ°çš„â€œå®‰å…¨åœ°â€è®¡ç®—f(x, y) çš„è¦æ±‚ã€‚ Security goal: real protocol interaction is as secure as the ideal-world interaction For every â€œattackâ€ against real protocol, there is a way to achieve â€œsame effectâ€ in ideal world ã€å› æ­¤ï¼ŒçŽ°å®žæƒ…å†µçš„å®‰å…¨è®¡ç®—ç›®æ ‡åº”è¯¥æ˜¯è¾¾åˆ°å’Œç†æƒ³æƒ…å†µä¸­çš„åŒç­‰å®‰å…¨æ€§ã€‚ã€‘ ã€å¯¹äºŽæ¯ä¸€ä¸ªå°è¯•æ”»å‡»çœŸå®žå®‰å…¨è®¡ç®—åè®®çš„æ”»å‡»è€…æ¥è¯´ï¼Œæ€»æœ‰ä¸€ç§æ–¹å¼è¾¾åˆ° å’Œæ”»å‡»ç†æƒ³æƒ…å†µçš„ ç›¸åŒå½±å“ã€‚ã€‘ é¦–å…ˆï¼Œå…ˆè§£é‡Šåœ¨real protocolä¸­ï¼Œæ¶æ„å‚ä¸Žæ–¹ä¼šäº§ç”Ÿä»€ä¹ˆæ ·çš„effectï¼Ÿ ä¸‹å›¾æ˜¯ä¸€ä¸ªçŽ°å®žçš„MPCåè®®äº¤äº’å›¾ï¼ˆæœ‰æ¶æ„å‚ä¸Žæ–¹ï¼‰ï¼š effectï¼š Something the adversary learns / can compute about honest party ã€æ¶æ„å‚ä¸Žæ–¹é€šè¿‡MPCå¾—åˆ°çš„è®¡ç®—ç»“æžœï¼Œå¯èƒ½èƒ½è®¡ç®—ä¸€äº›å…³äºŽå¯ä¿¡å‚ä¸Žæ–¹çš„ä¿¡æ¯ã€‘ Some influence on honest partyâ€™s output ã€æ¶æ„å‚ä¸Žæ–¹èƒ½å¤Ÿå½±å“å¯ä¿¡å‚ä¸Žæ–¹å¾—åˆ°çš„è®¡ç®—ç»“æžœã€‘ Defining security å®Œæ•´çš„å®‰å…¨å®šä¹‰: RanCanetti13 Universally Composable Security: A New Paradigm for Cryptographic Protocols Security definition: For every real-world adversary $\\mathcal{A}$, there exists an ideal adversary $\\mathcal{Aâ€™}$ s.t. joint distribution (HonestOutput,AdvOutput) is indistinguishable. ã€å¯¹äºŽæ¯ä¸€ä¸ªçœŸå®žåè®®äº¤äº’ä¸‹çš„æ”»å‡»è€… $\\mathcal{A}$ ï¼Œéƒ½å­˜åœ¨ä¸€ä¸ªåœ¨ç†è®ºåè®®æƒ…å†µä¸‹çš„æ”»å‡»è€… $\\mathcal{Aâ€™}$ ï¼Œè¿™ä¸¤ä¸ªæ”»å‡»è€…äº§ç”Ÿçš„effectæ˜¯ç›¸åŒçš„ï¼Œä¸¤ç§æƒ…å†µä¸‹çš„è¯šä¿¡å‚ä¸Žæ–¹çš„è¾“å‡ºå’Œæ¶æ„å‚ä¸Žæ–¹çš„è¾“å‡ºçš„ è”åˆåˆ†å¸ƒ æ˜¯ä¸å¯åŒºåˆ†çš„ã€‚ã€‘ çŽ°åœ¨ï¼Œæ¥è§£é‡Šå¦‚ä½•äº§ç”Ÿä¸Šæ–‡æåˆ°çš„ï¼šFor every â€œattackâ€ against real protocol, there is a way to achieve â€œsame effectâ€ in ideal world. å‡è®¾åœ¨ç†æƒ³æƒ…å†µä¸­æœ‰ä¸€ä¸ªsimulatorï¼šsimulates real-world interaction in ideal world ã€å¯ä»¥åœ¨ç†æƒ³æƒ…å†µä¸­æ¨¡æ‹ŸçŽ°å®žåè®®çš„äº¤äº’ã€‚ã€‘ å¯¹äºŽè¯¥æ¨¡æ‹Ÿå™¨æ¥è¯´ï¼Œä»–å¯ä»¥æ‰®æ¼”ä¸¤ç§è§’è‰²ï¼š Send protocol messages that look like they came from honest party Demonstrates that honest partyâ€™s messages leak no more than f(x, y) ã€å’Œæ¶æ„å‚ä¸Žæ–¹äº¤äº’ï¼Œæ¨¡æ‹ŸçŽ°å®žåè®®ä¸­çš„è¯šä¿¡å‚ä¸Žæ–¹ï¼šå¯¹æ¨¡æ‹Ÿå™¨æ¥è¯´ï¼Œä»–é™¤äº†çŸ¥é“è®¡ç®—ç»“æžœf(x, y) å¤–ã€‘ Extract an f-input by examining adversaryâ€™s protocol messages â€œExplainsâ€ the effect on honest partyâ€™s output in terms of ideal world ã€åœ¨ç†æƒ³åè®®ä¸­ï¼Œæ¨¡æ‹Ÿå™¨æå–æ¥è‡ªæ¶æ„å‚ä¸Žæ–¹çš„è¾“å…¥ä¿¡æ¯ï¼Œæ¨¡æ‹Ÿç†æƒ³æƒ…å†µä¸­çš„å‚ä¸Žæ–¹ã€‚ã€‘ Q: yuyuè€å¸ˆé—®ï¼Œè¿™é‡Œçš„extractæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ Aï¼šåœ¨çŽ°å®žå®‰å…¨è®¡ç®—çš„åè®®äº¤äº’ä¸­ï¼Œä¸ç®¡æ¶æ„å‚ä¸Žæ–¹æäº¤ä»€ä¹ˆè¾“å…¥ï¼Œæ€»èƒ½å•†å®šä¸€ä¸ªç»“æžœã€‚å› æ­¤åœ¨ç†æƒ³æƒ…å†µä¸­ï¼Œä¹Ÿè®¸æ¶æ„å‚ä¸Žæ–¹åªæ˜¯ä¸Šä¼ äº†ä¸€äº›garbage bitï¼Œæ¨¡æ‹Ÿå™¨éœ€è¦ä»Žè¿™äº›garbage bitsä¸­æå–ä¸€äº›ä¿¡æ¯ï¼Œä½œä¸ºå‚ä¸Žäº¤äº’çš„è¾“å…¥ã€‚ Semi- Honest securityæœ‰ä¸€ç§ç‰¹æ®Šçš„æƒ…å†µï¼Œå³æ”»å‡»è€…æ˜¯semi-honestï¼š Adversary assumed to follow the protocol on a given input ã€æ”»å‡»è€…åœ¨ç»™å®šè¾“å…¥ä¸‹éµå®ˆåè®®ã€‘ Adversary may try to learn information based on what it sees ã€ä½†æ”»å‡»è€…ä¼šå°è¯•ä»Žå¾—åˆ°çš„ä¿¡æ¯ä¸­å­¦åˆ°ä¸€äº›å…¶ä»–ä¿¡æ¯ã€‘ No need to extract, only simulate transcript given ideal input+output ã€å› ä¸ºæ”»å‡»è€…ä¼šéµå®ˆåè®®ï¼Œæ‰€ä»¥æ¨¡æ‹Ÿå™¨ä¸éœ€è¦extractæ¥è‡ªåŠè¯šå®žçš„æ”»å‡»è€…ï¼Œåªéœ€è¦ä¼ é€æ¥è‡ªåŠè¯šå®žå‚ä¸Žæ–¹çš„è¾“å…¥å’Œå¯ä¿¡ç¬¬ä¸‰æ–¹çš„è®¡ç®—è¾“å‡ºç»“æžœã€‚ã€‘ Yaoâ€™s protocolYaoâ€™s protocol : semi-honest secure computation for boolean circuits. warm-up protocol: garbled truth tableAliceæ–¹ï¼šä¼šè®¡ç®—ä¸€ä¸ªæ··æ·†çœŸå€¼è¡¨ï¼ˆgarbled truth tableï¼‰ Write truth table of function f ã€å†™ä¸‹å‡½æ•°fçš„çœŸå€¼è¡¨ï¼Œè¯¥å‡½æ•°å¯¹äºŽAliceæ–¹æœ‰4ç§è¾“å…¥ï¼Œå¯¹Bobæ–¹æœ‰4ç§è¾“å…¥ã€‘ For each possible input, choose random cryptographic key ã€å¯¹äºŽå‡½æ•°çš„æ¯ä¸€ç§è¾“å…¥ï¼ŒAliceéƒ½é€‰æ‹©ä¸€ä¸ªéšæœºçš„å¯†é’¥ï¼ˆä¹Ÿå«labelï¼‰ï¼Œå¯¹äºŽAliceæ–¹çš„è¾“å…¥æœ‰4ç§å¯†é’¥ï¼Œå¯¹äºŽBobæ–¹çš„è¾“å…¥æœ‰4ç§å¯†é’¥ã€‚ã€‘ Encrypt each output with corresponding keys ã€å†ç”¨å¯¹åº”çš„è¾“å…¥å¯†é’¥åŠ å¯†æ¯ä¸€ä¸ªå‡½æ•°çš„è¾“å‡ºã€‘ Randomly permute ciphertexts, send to Bob ã€å¯¹ç¬¬ä¸‰æ­¥çš„åŠ å¯†çœŸå€¼è¡¨éšæœºç½®æ¢ï¼Œå¾—åˆ°garbled truth tableï¼Œå†å‘ç»™Bobã€‚ã€‘ è€Œå¯¹äºŽBobæ–¹ï¼šï¼ˆå…ˆä¸è€ƒè™‘å¦‚ä½•å¾—åˆ°æ­£ç¡®çš„å¯†é’¥Axå’ŒByï¼‰Bobå¾—åˆ°äº†å¯†é’¥Axå’ŒByï¼Œå¯¹garbled truth tableè¿›è¡Œå°è¯•æ€§è§£å¯†ï¼Œå°±åªèƒ½å¾—åˆ°f(x, y) å¯¹äºŽBobæ¥è¯´ï¼Œæ˜¯é€šè¿‡OTæ¥å¾—åˆ°æ­£ç¡®çš„å¯†é’¥Axå’ŒByã€‚åŽé¢ä¼šè§£é‡ŠOTçš„ç®€æ˜“ç‰ˆæœ¬å’Œæ›´ä¸ºå¸¸ç”¨çš„åšæ³•ã€‚ Qï¼šBobå¦‚ä½•åˆ¤æ–­è¿™æ˜¯æ­£ç¡®çš„è§£å¯†ï¼Ÿ Aï¼šåœ¨å¯¹å­—ç¬¦åŠ å¯†å‰ï¼Œå¯ä»¥å¯¹å­—ç¬¦åŠ ä¸€äº›æ ‡è®°ï¼Œæ¯”å¦‚å‰å¯¼0ï¼Œæˆ–è€…ç‰¹æ®Šå­—ç¬¦ä¸²ä¹‹ç±»ã€‚ åœ¨Bobæ”¶åˆ°garbled truth tableå’Œgarbled inputåŽï¼ŒBobçš„è§†è§’å¯ä»¥æ¨¡æ‹Ÿæˆä¸‹å›¾ï¼š å¹¶ä¸”ï¼Œå¯¹äºŽdistinguisheræ¥è¯´ï¼Œåªè¦å¯†é’¥{A, B}ä¸­æœ‰ä¸€ä¸ªä¸çŸ¥é“ï¼Œé‚£ä¹ˆ $\\mathbb{E}{A, B}(C)\\approx \\mathbb{E}{Aâ€™, Bâ€™}(Câ€™)$ ï¼Œï¼ˆå³å’Œå…¶ä»–ä¸çŸ¥é“çš„è¾“å…¥çš„åŠ å¯†æ˜¯ä¸å¯åŒºåˆ†çš„ï¼‰è¿™æ ·çš„æ¨¡æ‹Ÿå°±æ˜¯ä¸å¯åŒºåˆ†çš„ã€‚ Extending warm-up protocolä½†è¿™æ ·çš„åè®®è¿˜å­˜åœ¨ä¸€äº›é—®é¢˜ï¼š Problem: Cost scales with the truth table size of f ã€å¯¹äºŽå¤æ‚å‡½æ•°æ¥è¯´ï¼ŒçœŸå€¼è¡¨çš„å¤§å°ä¼šå¾ˆå¤§ï¼Œè®¡ç®—å¼€é”€å’Œå­˜å‚¨å¼€é”€éƒ½ä¼šå¾ˆå¤§ã€‘ How does Bob magically learn â€œcorrectâ€ Ax, By? ã€ä¹‹å‰å¿½ç•¥çš„å‡è®¾ï¼ŒBobå¦‚ä½•æ‰èƒ½å¾—åˆ°Aliceåˆ¶å®šçš„å¯†é’¥Axå’ŒByã€‘ è¿™é‡Œä¼šå…ˆè§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæƒ³æ³•æ˜¯æŠŠè¿™ä¸ªå¤æ‚å‡½æ•°æ‹†åˆ†æˆå¤šä¸ªgarbled truth tableï¼Œå¯ä»¥ç”¨ä¸€ä¸ªgarbled table çš„garbled outputsä½œä¸ºå¦ä¸€ä¸ªgarbled tableçš„keysï¼Œå°±åƒè¿™æ ·ï¼š ç¬¬äºŒä¸ªé—®é¢˜ï¼Œå…³äºŽOTçš„éƒ¨åˆ†ï¼ŒåŽæ–‡ä¼šç»§ç»­è§£é‡Šã€‚ Garbled circuit frameworkå¯¹äºŽgarbled circuit frameworkçš„æå‡ºï¼Œå‡ºè‡ªå§šæœŸæ™ºé™¢å£«86å¹´çš„è®ºæ–‡ã€‚ ç”¨booleanç”µè·¯è¡¨ç¤ºä¸€ä¸ªå‡½æ•°ï¼Œç”µè·¯ç”±ä¸€äº›é—¨ç»„æˆï¼š (Alice) Garbling a circuit: Pick random labels W0, W1 on each wire ã€å¯¹äºŽæ¯ä¸€ä¸ªwireï¼Œéƒ½ä¼šé€‰æ‹©ä¸¤ä¸ªlabelæ¥è¡¨ç¤ºtrueå’Œfalseï¼Œæ¯”å¦‚é€‰æ‹©A0æ˜¯trueï¼ŒA1æ˜¯falseï¼Œè¿™é‡Œçš„labelå…¶å®žå°±æ˜¯ä¸Šæ–‡æåˆ°çš„åŠ å¯†å¯†é’¥ã€‚ã€‘ â€œEncryptâ€ truth table of each gate ã€ç„¶åŽï¼Œå¯¹æ¯ä¸€ä¸ªé—¨ï¼Œéƒ½ç”¨å¯¹åº”çš„labelåŠ å¯†çœŸå€¼è¡¨ã€‘ Garbled circuit = all encrypted gates Garbled encoding = one label per wire ã€è¿™æ ·å°±æŠŠä¸€ä¸ªæ··æ·†ç”µè·¯å˜æˆä¸€äº›åŠ å¯†çœŸå€¼è¡¨ï¼ŒåŒæ ·çš„ï¼ŒæŠŠæ··æ·†ç”µè·¯çš„ç¼–ç å˜æˆäº†æ¯ä¸ªwireçš„ä¸€ä¸ªlabelã€‘ (Bob) Garbled evaluation: Bobæ”¶åˆ°äº†æ··æ·†ç”µè·¯ï¼ˆgarbled circuitï¼‰å’Œæ··æ·†è¾“å…¥ï¼ˆgarbled inputï¼‰ï¼Œå°±å¯ä»¥è¿›è¡Œgarbled evaluation Only one ciphertext per gate is decryptable ã€å¯¹äºŽBobæ¥è¯´ï¼Œä»–åªæœ‰è¾“å…¥wireçš„å•ä¸ªlabelï¼Œå› æ­¤å¯¹äºŽæ¯ä¸€ä¸ªé—¨ï¼Œä»–éƒ½åªèƒ½è§£å¯†åŠ å¯†çœŸå€¼è¡¨ä¸­çš„ä¸€é¡¹ã€‘ Result of decryption = value on outgoing wire ã€è¾“å‡ºé—¨çš„å€¼å°±æ˜¯æœ€åŽçš„è®¡ç®—ç»“æžœã€‘ å¯¹äºŽæŽ¥å—æ–¹Bobæ¥è¯´ï¼Œå®ƒçš„ä»»åŠ¡å°±æ˜¯ç”¨garbled inputæ¥evaluate garbled circuitsã€‚ä»–åªèƒ½å­¦ä¹ åˆ°æ¯ä¸ªwireçš„ä¸€ä¸ªlabelï¼Œè®¡ç®—ä¸Šä¸å¯èƒ½çš„åŽ»çŒœæµ‹è¿™ä¸ªwireçš„å¦ä¸€ä¸ªlabelæ˜¯ä»€ä¹ˆã€‚è€Œå¯¹äºŽè¾“å‡ºé—¨çš„è¾“å‡ºwireï¼Œå³ä½¿æ­éœ²è¾“å‡ºwireçš„ä¸¤ä¸ªlabelä¹Ÿåªä¼šæ³„æ¼ç”µè·¯çš„è¾“å‡ºã€‚ åœ¨BellareHoangRogaway12ä¸­æœ‰å¯¹gabled circuitåšä¸€äº›æ­£å¼çš„ç¬¦å·å®šä¹‰ï¼š Privacy: (F,X,d) reveals nothing beyond f and f(x) Obliviousness: (F,X) reveals nothing beyond f Authenticity: given (F, X), hard to find Yâ€˜ that decodes $\\notin$ {f(x), âŠ¥} Oblivious transferå›žåˆ°ä¹‹å‰è¯´çš„ç¬¬äºŒä¸ªé—®é¢˜ï¼Œå¯¹äºŽevaluatorï¼Œä¹Ÿå°±æ˜¯Bobï¼Œå¦‚ä½•æ‰èƒ½å¾—åˆ°garbled circuitçš„garbled inputï¼Ÿ å¯¹äºŽä¸‹å›¾çš„garbled circuitï¼Œéœ€è¦Aliceè¾“å…¥Aå’ŒBï¼Œéœ€è¦Bobè¾“å…¥Cå’ŒDã€‚ Garblerâ€™s input: Alice ä½œä¸ºgarblerï¼Œå¥¹çŸ¥é“æ¯ä¸ªwireçš„ä¸¤ä¸ªlabelï¼ˆæ¯”å¦‚A0, A1)ï¼Œä¹ŸçŸ¥é“labelå¯¹åº”çš„çœŸå€¼å…³ç³»(æ¯”å¦‚A0æ˜¯falseï¼ŒA1æ˜¯trueï¼‰ï¼Œé‚£Aliceåªéœ€è¦æŠŠå¥¹çš„è¾“å…¥çš„å¯¹åº”çš„é‚£ä¸ªlabelå‘é€ç»™Bobå°±å¥½äº†ã€‚ Evaluatorâ€™s input: æˆ‘ä»¬éœ€è¦ç”¨OTï¼ˆoblivious transferï¼‰æ¥å®žçŽ°ï¼Œè®©Bobä»ŽAliceæ‰‹ä¸­èŽ·å¾—ä»–æƒ³è¦çš„labelï¼Œä½†åˆä¸è®©AliceçŸ¥é“ä»–æƒ³è¦çš„æ˜¯å“ªä¸€ä¸ªlabelã€‚ å¦‚ä½•æž„é€ ä¸ç»æ„ä¼ è¾“(OT)ï¼Ÿ ä¸‹å›¾æ˜¯ä¸€ç§ç®€å•å®žçŽ°çš„OTï¼Œä¸æ˜¯æœ€ä¼˜çš„åè®®ï¼ŒåŽé¢çš„æ–‡ç« ä¼šç»§ç»­ä»‹ç»å¸¸ç”¨çš„OTã€‚ Aliceæœ‰ä¸¤ä¸ªlabelï¼šW0å’ŒW1ï¼ŒBobæƒ³è¦å¾—åˆ°å…¶ä¸­ä¸€ä¸ªï¼Œå‡è®¾æ˜¯Wc ( $c=0,1$ ) ã€‚ Bobéœ€è¦ç”¨KeyGenç®—æ³•ç”Ÿæˆä¸€å¯¹å…¬ç§é’¥ $(sk_c, pk_c)$ ï¼ŒåŒæ ·è¿˜éœ€è¦BlindKeyGenç®—æ³•ç”Ÿæˆä¸€ä¸ªä¸çŸ¥é“ï¼ˆæˆ–è€…ç¡•å£«æ— æ³•çŸ¥é“ï¼‰å…¶å¯¹åº”ç§é’¥çš„å…¬é’¥ $pk_{1-c}$ ã€‚ Bobå°†ä¸¤ä¸ªå…¬é’¥ $pk_0, pk_1$ å‘ç»™Alice Aliceç”¨ $pk_0$ åŠ å¯†W0ï¼Œç”¨ $pk_1$ åŠ å¯†W1ï¼ŒæŠŠä¸¤ä¸ªå¯†æ–‡å‘é€ç»™Bob å› ä¸ºBobåªçŸ¥é“Wcçš„ç§é’¥ï¼Œè€Œæ— æ³•è®¡ç®—å¾—åˆ°W1-cçš„ç§é’¥ã€‚æ‰€ä»¥Bobå¯ä»¥ä»ŽAliceæ‰‹ä¸­å¾—åˆ°æƒ³è¦çš„labelã€‚ Need public-key encryption that supports blind key generation: sample a public key without knowledge of secret key E.g.: ElGamal (sample group element without knowing discrete log) éœ€è¦ç”¨ä¸€äº›æ”¯æŒblind keyç”Ÿæˆçš„å…¬é’¥åŠ å¯†ç®—æ³•ï¼Œæ¯”å¦‚ElGamalï¼Œç¾¤ä¸­çš„æŸäº›å…ƒç´ æ²¡æœ‰å¯¹åº”çš„ç¦»æ•£å¯¹æ•°ï¼Œä¹Ÿå°±æ— æ³•çŸ¥é“å…¶å¯¹åº”çš„ç§é’¥ã€‚ Yaoâ€™s protocol: overviewYaoåè®®ï¼š Gablerç”Ÿæˆæ¯ä¸ªwireçš„labelï¼ŒæŠŠgarbled circuit fï¼ˆåŠ å¯†çš„çœŸå€¼è¡¨ï¼‰ã€garbled input xï¼ˆAliceè¾“å…¥å¯¹åº”çš„labels)å’Œoutput wire labelsï¼ˆè¾“å‡ºwireå¯¹åº”çš„labelsï¼‰å‘é€ç»™Evaluator Evaluatorç”¨næ¬¡OTï¼Œåœ¨å’ŒAliceäº¤äº’ä¸­ï¼Œå¾—åˆ°Bobæ¯ä¸€ä¸ªè¾“å…¥wireçš„labelï¼Œæ¯ä¸€æ¬¡OTï¼ŒAliceéƒ½ä¸çŸ¥é“Bobé€‰æ‹©çš„è¾“å…¥æ˜¯ä»€ä¹ˆã€‚ å¯¹äºŽEvaluatoræ¥è¯´ï¼šgarbled f + garbled inputs + all output labels â‡’ Bob learns only f(x, y)","link":"/2021/07/02/MPC1-Yaos-protocol/"},{"title":"ã€ŒMPC-Mike Rosulek ã€ï¼šAdvanced Techniques and Optimizations for Garbled Circuits","text":"æœ¬ç³»åˆ—æ˜¯æ€»ç»“Mike Rosulekæ•™æŽˆåœ¨ä¸Šæµ·æœŸæ™ºç ”ç©¶é™¢çš„å¯†ç å­¦å­¦æœ¯è®²åº§ã€‚ è¿™æ˜¯Mikeæ•™æŽˆçš„ç¬¬äºŒä¸ªåˆ†äº«ï¼šAdvanced Techniques and Optimizations for Garbled Circuits Roadmap Optimizations: How did garbled boolean circuits get so small? New frontiers: How to garble arithmetic circuits? åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä¼šä»‹ç»åœ¨garble boolean circuitsæ—¶çš„ä¼˜åŒ–æŠ€æœ¯ï¼šåŒ…æ‹¬point-and-permute, row-reduction, free-XORå’Œhalf gatesã€‚æ­¤å¤–ï¼Œè¿™ç¯‡æ–‡ç« è¿˜ä¼šä»‹ç»å¦‚ä½•garble arithmetic circuitsã€‚ Optimizing garbled circuitsåœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œä»‹ç»äº†garbled circuitsçš„æ ¸å¿ƒæ€æƒ³ï¼š Given garbled gate + one wire label per input wire: can learn only one output label (authenticity) cannot learn truth value of labels (privacy) garbled circuitsçš„è®¡ç®—æ€§èƒ½å’Œé€Ÿåº¦ä¸»è¦å–å†³äºŽä»–çš„å¤§å°ï¼ˆciphertextçš„æ•°é‡ï¼‰ï¼Œä½†çŽ°å®žä¸­garbled circuitsçš„è®¡ç®—æ˜¯éžå¸¸å¿«çš„ã€‚ å› ä¸ºçŽ°æœ‰çš„garbled boolean circuitsä½¿ç”¨äº†ä¸€äº›ä¼˜åŒ–æŠ€æœ¯ï¼Œä½¿å¾—garbled gateçš„ciphertextè¡¨ç¤ºæ•°é‡å¤§å¹…ä¸‹é™ã€‚ Ciphertext expansionåœ¨ä»‹ç»è¿™äº›ä¼˜åŒ–æŠ€æœ¯ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥åˆ†æžYaoâ€™s protocolä¸­garbled circuitsçš„å¤§å°ã€‚ æ¯ä¸ªboolean gateçš„wireæœ‰ä¸¤ç§è¾“å…¥ï¼ˆ0æˆ–è€…1ï¼‰ï¼Œå¯¹æ¯ä¸ªè¾“å…¥éƒ½éšæœºé€‰æ‹©ä¸€ä¸ªlabelï¼Œå†ç”¨å¯¹åº”çš„labelï¼ˆå¯†é’¥ï¼‰å¯¹è¾“å‡ºåŠ å¯†ï¼Œå¾—åˆ°ä¸‹å›¾çš„garbled gatesï¼ˆciphertextï¼‰ã€‚ ä½†æ˜¯è¿™æ ·çš„æŽ’åˆ—æ–¹å¼ä¼šæ³„æ¼è¯­æ„ä¿¡æ¯ï¼Œå› æ­¤éœ€è¦æ‰“ä¹±ciphertextçš„é¡ºåºã€‚ è€Œevaluatoråœ¨evaluateæ—¶ï¼Œåªèƒ½ç”¨å¾—åˆ°çš„labelsä¸€ä¸ªä¸€ä¸ªå°è¯•æ€§è§£å¯†ã€‚ ä¸ºäº†è®©evaluatoråœ¨è§£å¯†æ—¶èƒ½æ¸…æ¥šçŸ¥é“è§£å¯†å‡ºçš„labelæ˜¯æ­£ç¡®çš„ï¼Œå°±éœ€è¦ä½¿ç”¨å¸¦æœ‰ciphertext expansionçš„åŠ å¯†æ–¹æ¡ˆã€‚ï¼ˆæ¯”å¦‚åœ¨åŠ å¯†æ—¶ï¼Œå¦‚æžœlabelæ˜¯4ä½çš„ï¼Œå¯ä»¥é€šè¿‡åœ¨labelå‰åŠ 4ä¸ªå‰å¯¼0ï¼‰è¿™æ ·å¾—åˆ°çš„garbled gateçš„ciphertexté•¿åº¦æ˜¯åŽŸlabelé•¿åº¦çš„ä¸€å€ã€‚ Point-and-permuteé¦–å…ˆä»‹ç»çš„æ˜¯Point-and-permuteæŠ€æœ¯ï¼š Assign color bits red &amp; blue to wire labels. Association between (red, blue ) &lt;-&gt; (True, Flase) is random for each wire. ã€color bits å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªbit maskï¼Œæˆ–è€…è¯´æ˜¯ä¸€ä¸ª1-bit one-time padã€‚åœ¨ä¸ºæ¯ä¸€ä¸ªwireçš„labelsåˆ†é…color bitsæ—¶ï¼Œæ˜¯éšæœºåˆ†é…çš„ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªbit r (0æˆ–è€…1)ï¼Œå¯¹äºŽwire aï¼Œlabelçš„color bitsçš„è®¡ç®—æ˜¯ $a\\oplus r$ ã€‚ã€‘ A wire label reveals its own color(e.g. as last bit) ã€å¯ä»¥åœ¨labelåŽå†é™„åŠ ä¸€ä¸ªbitä½œä¸ºcolor bitï¼Œæ¥è¡¨ç¤ºè¯¥wire labelçš„é¢œè‰²ã€‘ Order the 4 ciphertexts canonically, by color of keys. ã€ç„¶åŽå¯¹è¡¨ç¤ºgarbled gateçš„4ä¸ªciphertextsæŒ‰ç…§æŒ‡å®šçš„é¢œè‰²ç»„åˆæŽ’åºã€‘ Evaluate by decrypting ciphertext indexed by your colors ã€è¿™æ ·evaluatoråœ¨evaluateæ··æ·†ç”µè·¯æ—¶ï¼Œå°±å¯ä»¥é€šè¿‡å·²çŸ¥labelsçš„é¢œè‰²ç»„åˆç´¢å¼•å‡ºæ­£ç¡®çš„å¯†æ–‡ï¼Œå†è¿›è¡Œè§£å¯†ï¼Œä¸å†éœ€è¦å¯¹æ¯ä¸€ä¸ªå¯†æ–‡è¿›è¡Œå°è¯•æ€§è§£å¯†ã€‚ã€‘ é€šè¿‡Point-and-permuteæŠ€æœ¯ï¼Œevaluatoråœ¨è¯„ä¼°ç”µè·¯æ—¶ä¸å†éœ€è¦å¯¹æ¯ä¸ªå¯†æ–‡å°è¯•æ€§è§£å¯†ï¼Œgarbleråœ¨æ··æ·†ç”µè·¯æ—¶ä¹Ÿä¸å†éœ€è¦åšciphertext expansionã€‚ åŒæ—¶ï¼Œè¯¥æŠ€æœ¯æ”¯æŒç®€å•çš„one-time padåŠ å¯†ï¼Œå³ $\\mathbb{E}_{A, B}(C)=H(A, B)\\oplus C$ ï¼Œ$H$ å¯ä»¥æ˜¯ä»»æ„çš„ä¸€ç§åŠ å¯†æ–¹å¼ï¼Œæ¯”å¦‚AESã€‚ æˆ‘ä»¬å°†Point-and-permuteä¸ŽYaoâ€™s protocolè¿›è¡Œæ¯”è¾ƒï¼š å‡å®šå•ä¸ªlabelå¤§å°æ˜¯ $\\lambda$ ï¼Œgarbled circuitsçš„å¤§å°è¡¨ç¤ºä¸ºciphertextsçš„å¤§å° ã€‚garble costçš„å€¼ä»£è¡¨garbleråŠ å¯†æ—¶çš„å¼€é”€ï¼Œeval costçš„å€¼è¡¨ç¤ºevaluatorè§£å¯†æ—¶çš„å¼€é”€ã€‚ å¯¹äºŽYaoçš„æ–¹æ¡ˆï¼Œå› ä¸ºæœ‰ciphertext expansionï¼Œæ‰€ä»¥garbled circuitsçš„å¤§å°ä¸º $8\\lambda$ ã€‚å¯¹æ¯ä¸ªgarbled gateï¼Œéƒ½éœ€è¦ç”Ÿæˆ4ä¸ªciphertextsã€‚è€Œevaluatoråœ¨è¯„ä¼°ç”µè·¯æ—¶ï¼Œæ¯ä¸ªciphertextéƒ½æœ‰ 1/4çš„æ¦‚çŽ‡æ˜¯æ­£ç¡®çš„ï¼Œæ‰€ä»¥å¼€é”€ä¸º $2.5 = 1\\times 1/4 + 2 \\times 1/4 + 3\\times 1/4 + 4\\times 1/4$ . å¯¹äºŽPoint-and-permuteçš„æ–¹æ¡ˆï¼Œä¸éœ€è¦ciphertext expansionï¼Œæ‰€ä»¥garbled circuitsçš„å¤§å°ä¸º $4\\lambda$ ã€‚åœ¨evaluatorè¯„ä¼°ç”µè·¯æ—¶ï¼Œå¯ä»¥é€šè¿‡labelsçš„é¢œè‰²ç»„åˆæ¥ç´¢å¼•å¯†æ–‡ï¼Œå› æ­¤åªéœ€è¦1çš„å¼€é”€ã€‚ Garbled Row ReductionRow Reduction[NaorPinkasSumner99] æ˜¯åœ¨Point-and-permuteæ–¹æ¡ˆä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼š ç›¸è¾ƒäºŽPoint-and-permuteæ–¹æ¡ˆä¸­çš„éšæœºç”Ÿæˆè¾“å‡ºwireçš„labelsï¼ŒRow Reductionæ–¹æ¡ˆåœ¨è¾“å‡ºwireçš„labelsç”Ÿæˆä¸­ï¼Œä½¿ç”¨äº†ä¸€ä¸ªtrickï¼šlabel $C_0$ éšæœºç”Ÿæˆï¼Œè€Œlabel $C_1$ æ˜¯ä¸€ä¸ªèƒ½è®©P&amp;Pæ–¹æ¡ˆä¸­çš„ç¬¬ä¸€ä¸ªciphertextä¸º $0^n$ ï¼Œå³ $C_1 = H(A_0, B_1)$ ã€‚è™½ç„¶$C_1 = H(A_0, B_1)$ ï¼Œä½†å¯¹é™¤garblerä»¥å¤–çš„äººæ¥è¯´ï¼Œ$C_1$ å’Œrandom bitsæ˜¯ä¸å¯åŒºåˆ†çš„ï¼Œå³ $C_1$ çœ‹èµ·æ¥å’Œéšæœºç”Ÿæˆçš„labelä¸€æ ·ã€‚ å¯¹garbleræ¥è¯´ï¼Œæ¯ä¸ªgarbled gateå°±å¯ä»¥åªç”¨3ä¸ªciphertextsè¡¨ç¤ºã€‚ ä½†å¯¹evaluatoræ¥è¯´ï¼Œä»–åªéœ€è¦æŠŠ $0^n$ åŠ ä¸Šï¼Œé‡æž„ä¸º4ä¸ªciphertextsï¼ŒåŽé¢çš„æ“ä½œå’ŒP&amp;Pæ–¹æ¡ˆç›¸åŒã€‚ åŒæ ·çš„ï¼Œæˆ‘ä»¬æŠŠRow Reductionçš„æ–¹æ¡ˆå’Œå‰ä¸¤ç§æ–¹æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼š GRR3æ–¹æ¡ˆç›¸å¯¹äºŽP&amp;Pæ–¹æ¡ˆï¼Œå¯ä»¥å°†garbled circuitsçš„æ•°é‡ä»Ž4é™ä¸º3ã€‚ Free XORFree XOR[KolesnikovSchneider08] æ˜¯å¯¹å¼‚æˆ–é—¨çš„ä¼˜åŒ–æŠ€æœ¯ã€‚ Define offset of a wire â‰¡ XOR of its two labels ã€Free XORå¯¹wireå®šä¹‰äº†ä¸€ç§offset $\\Delta$ ï¼Œè¯¥å€¼å…¶å®žç­‰äºŽä¸¤ä¸ªlabelsçš„å¼‚æˆ–ã€‘ Choose all wires in circuit to have same (secret) offset âˆ† ã€Free XORæ–¹æ¡ˆä¸­ï¼Œå¯¹æ‰€æœ‰çš„wireséƒ½é€‰æ‹©äº†ä¸€ä¸ªç›¸åŒçš„ä¿å¯†çš„offsetã€‘ Choose false output = false input âŠ• false input ã€ç›¸è¾ƒäºŽä¹‹å‰æ–¹æ¡ˆçš„éšæœºç”Ÿæˆoutput wireçš„labelï¼ŒFree XORè®©output wireçš„false output = false input âŠ• false inputã€‘ ã€ä¹Ÿå°±æ˜¯ä»¤ $C = A \\oplus B$ (A: false , B: false, C: false)ã€‘ Evaluate by xoring input wire labels (no crypto) ã€å¦‚æ­¤å®šä¹‰output wireçš„labelï¼Œoutput wireçš„labelå¯ä»¥é€šè¿‡input wireçš„labelç›´æŽ¥è®¡ç®—å‡ºæ¥ï¼Œè€Œä¸éœ€è¦å†å¯¹å…¶åŠ å¯†è§£å¯†ã€‘ ã€false xor false = false ï½œ $A \\oplus B = C$ ã€‘ ã€false xor true = true ï½œ $A \\oplus B \\oplus \\Delta = C\\oplus \\Delta$ ã€‘ ã€true xor false = true ï½œ $A \\oplus \\Delta \\oplus B = C\\oplus \\Delta$ ã€‘ ã€true xor true = false ï½œ $A \\oplus \\Delta \\oplus B \\oplus \\Delta = C$ ã€‘ åŒæ ·ï¼Œæˆ‘ä»¬æŠŠXORçš„æ–¹æ¡ˆå’Œä¹‹å‰æ–¹æ¡ˆæ¯”è¾ƒï¼Œç”±äºŽXORæ–¹æ¡ˆåªé€‚ç”¨äºŽXOR gateï¼Œå› æ­¤æŠŠXOR gateå’ŒAND gateåˆ†å¼€ï¼š Row reduction $\\times$ 2Row reduction $\\times$ 2[GueronLindellNofPinkas15] æ–¹æ¡ˆæ˜¯å¯¹Row Reductionæ–¹æ¡ˆçš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ŒRow Reductionæ–¹æ¡ˆå¯¹è¾“å‡ºwireçš„labelé€‰æ‹©ä¸Šï¼š$C_0$ éšæœºç”Ÿæˆï¼Œ$C_1$ çš„å€¼æ˜¯èƒ½è®©P&amp;Pæ–¹æ¡ˆç¬¬ä¸€ä¸ªciphertext ä¸º $0^n$ çš„æ–¹æ¡ˆï¼Œå³ $C_1 = H(A_0, B_1)$ ï¼ˆå¼å­åªæ˜¯é’ˆå¯¹ä¸‹å›¾çš„æ ·ä¾‹ï¼‰ã€‚ è€ŒRow reduction $\\times$ 2æ–¹æ¡ˆè§„å®šlabel $C_0 = H_{00}\\oplus H_{11} \\oplus H_{10}$ ï¼Œè¿™æ ·èƒ½è®©å‰©ä¸‹ä¸‰ä¸ªciphertextçš„XORå€¼ä¸º $0^n$ ï¼ŒåŒæ ·çš„ $C_0$ å¯¹é™¤garblerä¹‹å¤–çš„äººæ¥è¯´å’Œrandom bitsä¸å¯åŒºåˆ†ã€‚ å› æ­¤ï¼Œå¯¹garbleræ¥è¯´ï¼Œæ¯ä¸€ä¸ªgarbled gateåªéœ€è¦ç”¨ä¸¤ä¸ªciphertextsè¡¨ç¤ºï¼š è€Œå¯¹evaluatoræ¥è¯´ï¼Œä»–åªéœ€è¦ç”¨è¿™ä¸¤ä¸ªciphertextsé‡æž„å‡ºåŽŸæœ¬çš„garbled circuitså³å¯ï¼š åŒæ ·ï¼ŒæŠŠGRR2æ–¹æ¡ˆå’Œå…¶ä»–æ–¹æ¡ˆæ¯”è¾ƒï¼Œä¸»è¦å’ŒP&amp;På’ŒGRR3æ–¹æ¡ˆæ¯”è¾ƒï¼Œæ¯ä¸ªgarbled gateéƒ½åªéœ€è¦ç”¨ä¸¤ä¸ªciphertextså³å¯è¡¨ç¤ºï¼š è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒFree XORæŠ€æœ¯å’ŒGRR2æŠ€æœ¯æ˜¯ä¸å¯å…¼å®¹çš„ã€‚å› ä¸ºåœ¨GRR2ä¸­ï¼Œ$C_0 = H_{00}\\oplus H_{11} \\oplus H_{10}$ ï¼Œ$C_1 = H(A_0, B_1)$ ï¼Œæ— æ³•ä¿è¯ $C_0\\oplus C_1=\\Delta$ ã€‚å› æ­¤åœ¨ä¸€ä¸ªç”µè·¯ä¸­ï¼Œé‚£ä¹ˆä½¿ç”¨Free-XORæ–¹æ¡ˆï¼Œè¦ä¹ˆä½¿ç”¨GRR2æ–¹æ¡ˆã€‚ Half GatesHalf Gates æ–¹æ¡ˆå‡ºè‡ªSamee Zahur, Mike Rosulek, David Evansçš„15å¹´çš„æ–‡ç« ï¼šTwo Halves Make a Whole: Reducing Data Transfer in Garbled Circuits using Half Gates. (Eurocrypt 2015) è¿™ä¸ªæ–¹æ¡ˆgarbled circuitsçš„è®¾è®¡æ˜¯å€Ÿç”¨äº†Free-XORçš„éƒ¨åˆ†è®¾è®¡ï¼šå³æ‰€æœ‰çš„wireéƒ½é€‰æ‹©äº†ä¸€ä¸ªç›¸åŒçš„offsetã€‚ä½†output wire label $C$ä¸Žinput wireçš„labelæ— å…³ã€‚ Garbler: half gateå¦‚æžœgarbleræå‰çŸ¥é“äº†input wire açš„å€¼ï¼š $a = 0$ å¦‚æžœ$a = 0$, è¿™ä¸ªé—¨å°±å˜æˆä¸€ä¸ªä¸€å…ƒé—¨ï¼š $b \\rightarrow 0$ å¯ä»¥ç”¨2ä¸ªciphertextsæ¥è¡¨ç¤ºè¿™ä¸ªé—¨ï¼Œevaluatorè§£å¯†åŽéƒ½å¾—åˆ°output wire çš„label $C$ã€‚ $a = 1$ å¦‚æžœ$a = 1$, è¿™ä¸ªé—¨å°±å˜æˆä¸€ä¸ªä¸€å…ƒé—¨ï¼š$b\\rightarrow b$ åŒæ ·å¯ä»¥ç”¨2ä¸ªciphertextsè¡¨ç¤ºè¿™ä¸ªé—¨ï¼Œevaluatorå¯ä»¥ç”¨label $B$è§£å¯†å¾—åˆ°output wire çš„ label $C$ï¼Œå¯ä»¥ç”¨ label$B\\oplus \\Delta$ è§£å¯†å¾—åˆ°output wire çš„label $C\\oplus \\Delta$ ã€‚ ç»“åˆä¸¤ç§æƒ…å†µï¼š å¯ä»¥å½’çº³ä¸ºä¸‹å¼ï¼Œgarblerå¯ä»¥æ ¹æ®å·²çŸ¥açš„å€¼æ¥ç”Ÿæˆå¯¹åº”çš„ciphertextsï¼ŒåŒæ ·ç”¨point-and-permuteçš„color bitçš„æ€æƒ³æ¥é‡æŽ’åˆ—ciphertextsã€‚ åŒæ ·ï¼Œä¹Ÿå¯ä»¥å†ç”¨row reductionçš„æŠ€æœ¯æ¥é€‰æ‹©output wireçš„false label $C=H(B)$ ï¼Œä½¿å¾—ç¬¬ä¸€ä¸ªciphertextä¸º $0^n$ ï¼Œè¿™æ ·å¯¹äºŽgarblerçŸ¥é“ä¸€ä¸ªinput wireçœŸå€¼çš„garbled half gateå°±å¯ä»¥ç”¨ä¸€ä¸ªciphertextæ¥è¡¨ç¤ºï¼š Evaluator: half gateå¦‚æžœevaluatoræå‰çŸ¥é“äº†input wire bçš„å€¼ï¼Œå³çŸ¥é“labelçš„å¯¹åº”çœŸå€¼ï¼š Evaluator has B (knows label B is false): should obtain C (false) Evaluator has $B\\oplus \\Delta$ (knows label $B\\oplus \\Delta$ is true): should be able to transfer truth value from â€œaâ€ wire to â€œcâ€ wire. ã€å³å¦‚æžœevaluatorçŸ¥é“wire bçš„labelæ˜¯trueï¼Œé‚£evaluateçš„ç»“æžœçš„çœŸå€¼åº”è¯¥å’Œwire aä¸Šlabelå¯¹åº”çš„çœŸå€¼ç›¸åŒï¼ˆå³ä½¿å¥¹ä¸çŸ¥é“wire aä¸Šå®žé™…çš„çœŸå€¼æ˜¯ä»€ä¹ˆï¼‰ã€‘ Evaluatoråªè¦çŸ¥é“ $A\\oplus C$ çš„å€¼ï¼Œå°±å¯ä»¥å®žçŽ° â€œtransfer truth value from â€œaâ€ wire to â€œcâ€ wire.â€ false($A$) -&gt; false ($C$) : $A\\oplus A \\oplus C = C$ true($A\\oplus \\Delta$ )-&gt; true ($C\\oplus \\Delta$ ): $A\\oplus \\Delta \\oplus A \\oplus C = C\\oplus \\Delta$ åŒæ ·çš„ï¼Œå¯ä»¥ä½¿ç”¨row reductionçš„æŠ€æœ¯ï¼Œä»¤false label $C = H(B)$ ï¼Œä½¿å¾—garbled circuitsçš„ç¬¬ä¸€ä¸ªciphertextä¸º $0^n$ ï¼Œè¿™æ ·å¯¹äºŽevaluatorçŸ¥é“ä¸€ä¸ªinput wireçœŸå€¼çš„garbled half gateå°±å¯ä»¥ç”¨ä¸€ä¸ªciphertextè¡¨ç¤ºã€‚ Two halves make a wholeå¯¹äºŽåœ¨å¤æ‚ç”µè·¯ä¸­çš„ANDé—¨ï¼Œä»–çš„input wireå¯èƒ½å’ŒåŒæ–¹è¾“å…¥éƒ½ç›¸å…³ï¼Œå› æ­¤garblerä¸çŸ¥é“æŸä¸ªinput wireçš„çœŸå€¼åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œevaluatoræ›´ä¸çŸ¥é“ã€‚ å¦‚ä½•æ‰èƒ½å°†ä¸Šæ–‡ä¸­çš„half gateåº”ç”¨åˆ°ä¸€èˆ¬ANDé—¨ä¸Šï¼Ÿ å¯¹äºŽgarbleræ¥è¯´ï¼Œåœ¨garble AND gateæ—¶ï¼Œå³ç„¶ä¸çŸ¥é“æŸä¸ªinpu wireçš„çœŸå€¼ï¼Œé‚£æˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€ä¸ªéšæœºbit rï¼ˆ0æˆ–è€…1ï¼‰ï¼Œè¯¥å€¼å…¶å®žå°±æ˜¯åœ¨è®¡ç®—labelçš„color bitæ—¶å¼•å…¥çš„ï¼Œè€Œbit $a\\oplus r$ å°±æ˜¯label $A$ çš„color bitã€‚ å¯¹äºŽgarbleræ¥è¯´ï¼ŒgarblerçŸ¥é“bit $r$çš„çœŸå€¼ï¼Œtrue or falseã€‚ å¯¹äºŽevaluatoræ¥è¯´ï¼Œä»–åœ¨evaluateè¿™ä¸ªAND gateæ—¶ï¼Œä»–ä¼šå¾—åˆ°wire açš„labelï¼Œè™½ç„¶ä¸çŸ¥é“è¿™ä¸ªlabelå®žé™…æ˜¯å¯¹åº”ç€trueè¿˜æ˜¯falseï¼ˆå³ä¸çŸ¥é“ $a$ çš„å€¼ï¼‰ï¼Œä½†æ˜¯ä»–çŸ¥é“è¿™ä¸ªlabelçš„color bitæ˜¯ä»€ä¹ˆï¼ˆåƒP&amp;Pä¸­æåˆ°çš„é‚£æ ·ï¼Œå¯ä»¥æŠŠcolor bitæ”¾åœ¨labelçš„æœ€åŽä¸€ä½ï¼‰ï¼Œè€Œè¿™ä¸ªcolor bitå…¶å®žå°±æ˜¯bit $a\\oplus r$ çš„çœŸå€¼ï¼Œæ‰€ä»¥evaluatoråœ¨evaluateæ—¶æ˜¯çŸ¥é“ bit $a\\oplus r$ çš„çœŸå€¼çš„ã€‚ çŽ°åœ¨æˆ‘ä»¬é€šè¿‡å¼•å…¥äº† bit $r$ ï¼Œå®žçŽ°äº†half gateçš„ä¸¤ä¸ªå‡è®¾ï¼Œä¸€ä¸ªæ˜¯è®©garblerçŸ¥é“AND gateå…¶ä¸­ä¸€ä¸ªçš„è¾“å…¥çœŸå€¼ï¼Œä¸€ä¸ªæ˜¯è®©evaluatorçŸ¥é“AND gateå…¶ä¸­ä¸€ä¸ªçš„è¾“å…¥çœŸå€¼ã€‚ é‚£å¦‚ä½•é€šè¿‡ bit $r$ ï¼ŒæŠŠä¸€ä¸ªä¸€èˆ¬AND gateè½¬å˜ä¸ºhalf gates å‘¢ï¼Ÿ æŠŠ $a\\wedge b$ å†™æˆå¦‚ä¸‹å¼ï¼š$$\\begin{align}a\\wedge b &amp;= (a\\oplus r \\oplus r)\\wedge b \\&amp;=[(a\\oplus r)\\wedge b]\\oplus [r\\wedge b]\\end{align}$$ å¯¹äºŽå·¦è¾¹çš„ $[(a\\oplus r)\\wedge b]$ ï¼ševaluatorçŸ¥é“ $a\\oplus r$ çš„çœŸå€¼ï¼Œå¯ä»¥ç”¨ä¸Šé¢æåˆ°çš„ç¬¬äºŒç§half gateã€‚ å¯¹äºŽå³è¾¹çš„ $[r\\wedge b]$ : garblerçŸ¥é“ $r$ çš„çœŸå€¼ï¼Œå¯ä»¥ç”¨ä¸Šé¢æåˆ°çš„ç¬¬ä¸€ç§ half gateã€‚ ä¸¤ä¸ªå¼å­çš„xorè¿ç®—ï¼šå°±å¯ä»¥ä½¿ç”¨Free-XOR æŠ€æœ¯ã€‚ å› æ­¤ï¼Œæ€»å¼€é”€ = 2 â€œhalf gatesâ€ + â€œ1 XOR gateâ€ = 2 ciphertexts åŒæ ·çš„ï¼ŒæŠŠHalf gatesçš„æ–¹æ¡ˆå’Œå…¶ä»–æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼ŒAND gateçš„å¤§å°å¯ä»¥é™ä¸º2ï¼ŒåŒæ—¶Half gatesæŠ€æœ¯å’ŒFree-XORæŠ€æœ¯æ˜¯å…¼å®¹çš„ï¼Œå› æ­¤åœ¨half gatesæ–¹æ¡ˆä¸­ï¼ŒXOR gateçš„å¼€é”€ä¸º0. Garbling arithmetic circuitsGeneralized Free XORå°†Free XORçš„æ€æƒ³è¿ç§»åˆ° $\\mathbb{Z}_m$ ä¸Šã€‚ Free XOR Gerneralized Free XOR Wire carries a truth value from ${0, 1}$ Wire carries a truth value from $\\mathbb{Z}_m$ Wire labels are bit strings ${0, 1}^\\lambda$ . Wire labels are tuples $(\\mathbb{Z}_m)^\\lambda$. Global wire-label-offset $\\Delta\\in{0, 1}^\\lambda$ Global wire-label-offset $\\Delta\\in(\\mathbb{Z}_m)^\\lambda$ false wire labe lis $A$ ï¼›true wire label is $A\\oplus \\Delta$ Wire label encoding truth value $a\\in \\mathbb{Z}_m$ is $A+a\\Delta$ âŠ• is componentwise addition mod 2 + is componentwise addition mod m Generalized Free XORçš„æ ¸å¿ƒå°±æ˜¯ï¼šç”¨wire label $A+a\\Delta\\in (\\mathbb{Z}_m)^\\lambda$ æ¥ç¼–ç  $a\\in \\mathbb{Z}_m$ ã€‚ è¿™æ ·ç¼–ç åŽï¼Œevaluatorå¯ä»¥ç›´æŽ¥é€šè¿‡å¯¹labelè¿›è¡Œæ¨¡ $m$ çš„åŠ æ³•æ“ä½œæ¥å®Œæˆevaluateï¼š Garbling unary gatesä¸€å…ƒé—¨ $\\phi$ ï¼ˆunary gatesï¼‰: åªæœ‰ä¸€ä¸ªinput wire $\\in \\mathbb{Z}_m$ï¼Œ ä¸€ä¸ªoutput wire $\\in \\mathbb{Z}_l$ï¼Œæ³¨æ„ä¸¤ä¸ªwireå¯ä»¥å±žäºŽä¸åŒçš„æœ‰é™åŸŸã€‚ Generalized Free XORé™¤äº†å¯ä»¥æ··æ·†æ¨¡ $m$çš„åŠ æ³•ç”µè·¯ï¼Œè¿˜å¯ä»¥æ··æ·†ä¸€å…ƒé—¨ã€‚ ç”¨input wire label $A+a\\Delta_m\\in (\\mathbb{Z}_m)^\\lambda$ æ¥ç¼–ç  $a\\in \\mathbb{Z}_m$ ï¼Œç”¨output wire label $C+a\\Delta_l \\in (\\mathbb{Z}_l)^\\lambda$ æ¥ç¼–ç  $c\\in \\mathbb{Z}_l$ ï¼Œå› ä¸ºä¸¤ä¸ªwireå¯ä»¥å±žäºŽä¸åŒçš„æœ‰é™åŸŸï¼Œæ‰€ä»¥æ³¨æ„æ˜¯ä¸åŒçš„offset $\\Delta$ ã€‚ å› æ­¤ï¼Œä½¿ç”¨generalized free XORæŠ€æœ¯garble unary gateséœ€è¦$m$ä¸ªciphertextsã€‚ å½“ç„¶å¦‚æžœåŠ ä¸Šrow reductionæŠ€æœ¯ï¼Œåªéœ€è¦ $m-1$ ä¸ªciphertextsã€‚ å› ä¸ºä½œç”¨åœ¨ $\\mathbb{Z}_m$ ä¸Šï¼Œæ‰€ä»¥ä¸ºäº†å‡å°‘evaluatorçš„å¼€é”€ï¼Œä¸€èˆ¬åŒ–point-and-permuteæŠ€æœ¯ï¼Œ color bits $\\in \\mathbb{Z}_m$ ã€‚ Generalized garbling toolé€šè¿‡generalized free xorçš„æ€æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆæ··æ·†æœ‰å¦‚ä¸‹ç‰¹å¾çš„ç”µè·¯ï¼š Each wire has a preferred modulus $\\mathbb{Z}_m$ Wire-label-offset $\\Delta_m$ global to all $\\mathbb{Z}_m$ -wires Addition gates: all wires touching gate have same modulus Garbling cost: free Mult-by-constant gates: input/output wires have same modulus Garbling cost: free Unary gates: $\\mathbb{Z}_m$ input and $\\mathbb{Z}_l$ output Garbling cost: $m-1$ ciphertexts Arithmetic computationsä¸ºä»€ä¹ˆè¦å¼•å…¥arithmetic circuitsï¼Œå› ä¸ºä»–å¾€å¾€ä¼šæ¯”ä¼ ç»Ÿå¸ƒå°”ç”µè·¯é«˜æ•ˆï¼Œä»¥ä¸€ä¸ª32-bitçš„æ•°ä¸ºä¾‹ï¼Œä½¿ç”¨half-gatesæŠ€æœ¯æ¥æ··æ·†ä¸€äº›å¸¸è§è¿ç®—ï¼Œç»“æžœå¦‚ä¸‹ï¼š è€Œå¦‚æžœä½¿ç”¨ä¸Šæ–‡æåˆ°çš„generalized garbling toolï¼ŒåŠ æ³•æ“ä½œå’Œä¹˜ä»¥å¸¸æ•°æ“ä½œçš„å¼€é”€éƒ½é™ä¸º0ï¼Œè€Œæ¨¡ $m$ ä¹˜æ³•æ“ä½œçš„å¼€é”€ä¸º $2m-2$ ciphetextesï¼ˆè®²åº§ä¸­æ²¡æœ‰è¯¦ç»†è®²è§£ï¼Œæ–¹æ³•æ˜¯generalization of half-gates)ï¼ŒåŒæ ·ä»¥32-bitçš„æ•°ä¸ºä¾‹ï¼Œå’Œä¼ ç»Ÿå¸ƒå°”ç”µè·¯æŠ€æœ¯å¯¹æ¯”ï¼š å¯è§å¯¹äºŽ32-bitçš„ä¹˜æ³•ã€å¼€æ–¹æ“ä½œï¼Œä½¿ç”¨generalized garblingæŠ€æœ¯å¼€é”€éžå¸¸å¤§ï¼Œå› ä¸ºè¿™æ˜¯å’Œ $m$ ç›¸å…³çš„ã€‚ CRT formè€Œè¿™ä¸ªå¯ä»¥é€šè¿‡ä¸­å›½å‰©ä½™å®šç†(Chinese remainder theorem, CRT)æ¥è¿›è¡Œä¼˜åŒ–ã€‚ ç›¸è¾ƒäºŽä¹‹å‰åœ¨ $\\mathbb{Z}_{32}=\\mathbb{Z}_{4294967296}$ è¿›è¡Œç®—æ•°è¿ç®—ï¼Œé€šè¿‡CRTå¯ä»¥åœ¨ $\\mathbb{Z}_{2\\cdot 3\\cdot 5\\cdot 7\\cdot 11\\cdots 29}=\\mathbb{Z}_{6469693239}$ ä¸‹è¿›è¡Œé«˜æ•ˆçš„ä¹˜æ³•ã€å¼€æ–¹æ“ä½œã€‚ é€šè¿‡æŠŠä¸€ä¸ª32-bitçš„æ•° $x$ è¡¨ç¤ºä¸º $(x\\%2, x\\%3,x\\%5,\\cdots,x\\%29)$ ï¼Œå¯¹æ¯ä¸ªä½™æ•°å•ç‹¬è¿›è¡Œç®—æ•°è¿ç®—ï¼Œæœ€åŽé€šè¿‡ä¸­å›½å‰©ä½™å®šç†ï¼Œå³å¯æ±‚è§£å‡ºç»“æžœã€‚ å°†ä½¿ç”¨CRTè¡¨ç¤ºæ–¹æ³•çš„ç³»ç»Ÿä¸Žä¸Šè¿°æ–¹æ³•æ¯”è¾ƒï¼š å¯è§ï¼Œç”¨CRTçš„è¡¨ç¤ºæ–¹æ³•èƒ½å¤§å¹…é™ä½Žgarbled circuitsçš„å¼€é”€ã€‚ä½†æ˜¯CRTçš„è¡¨ç¤ºæ–¹æ³•åŒæ ·ä¹Ÿå¸¦æ¥äº†ä¸€äº›é—®é¢˜ï¼š Not so good: Converting from binary to CRT is not so good. Getting CRT valurs into the citcuit via OT is not so good. Kinda bad: (room for improvement) Comparing two CRT-encoded values Converting from CRT to binaty Integer division Modular reduction different than the CRT composite modulus (e.g., garbled RSA) ä¸‹é¢ä¼šä¸»è¦ä»‹ç»å¦‚ä½•æ¯”è¾ƒä¸¤ä¸ªCRTè¡¨ç¤ºæ–¹æ³•çš„æ•°å­—ã€‚ Comparing CRT valuesä¸‹è¡¨æ˜¯CRTçš„è¡¨ç¤ºæ–¹æ³•ï¼Œå¯ä»¥çœ‹å‡ºCRTçš„è¡¨ç¤ºæ–¹æ³•ä¸èƒ½ç›´æŽ¥å¯¹æ•°å­—è¿›è¡Œæ¯”è¾ƒï¼š ä½†å¦‚æžœå°†CRTè½¬æ¢ä¸ºä¸€ç§å«PMR (Primorial Mixed Radix)çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå°±å¯ä»¥ç›´æŽ¥å¯¹æ•°å­—è¿›è¡Œæ¯”è¾ƒäº†ï¼š è½¬æ¢æ–¹æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š è¦å®žçŽ°ä¸Šè¿°çš„è½¬æ¢æ–¹æ³•ï¼Œåªéœ€è¦å®žçŽ° $(x\\%p, x\\%q)\\mapsto \\lfloor \\frac{x}{p}\\rfloor\\%q$ æ¨¡å—çš„garbled circuitsã€‚ å®žçŽ°è¯¥æ¨¡å—çš„å…·ä½“æ­¥éª¤å¯ä»¥å‚è€ƒä¸‹å›¾ï¼š Subtract x%3 âˆ’ x%5 (mod 7 is fine)ã€ç›¸å‡ã€‘ â€œProjectâ€ x%3 and x%5 to $\\mathbb{Z}_7$ wires Subtract mod 7 for free Result has the same â€œconstant segmentsâ€ as what we want Apply unary projectionï¼šã€å†åšä¸€ä¸ªä¸€å…ƒæ˜ å°„ã€‘ æœ€åŽåˆ†æžä¸€ä¸‹CRTè¡¨ç¤ºæ–¹å¼çš„æ¯”è¾ƒæ•ˆçŽ‡ï¼š $(x\\%p, x\\%q)\\mapsto \\lfloor \\frac{x}{p}\\rfloor\\%q$ æ¨¡å—å¤§çº¦éœ€è¦ $2p+2q$ çš„å¼€é”€ã€‚ è€Œå°†CRTè½¬æ¢ä¸ºPMRï¼šå¯¹äºŽæ¯ä¸€ä¸ªè´¨æ•°å¯¹éƒ½éœ€è¦ä¸Šè¿°æ¨¡å—ã€‚ å¯¹äºŽk-bit: CRTæ¯”è¾ƒ $O(k^3)$","link":"/2021/07/04/MPC2-GarbledCircuits/"},{"title":"ã€ŒMPC-Mike Rosulek ã€ï¼šOblivious Transfer and Extension","text":"æœ¬ç³»åˆ—æ˜¯æ€»ç»“Mike Rosulekæ•™æŽˆåœ¨ä¸Šæµ·æœŸæ™ºç ”ç©¶é™¢çš„å¯†ç å­¦å­¦æœ¯è®²åº§ã€‚ è¿™æ˜¯Mikeæ•™æŽˆçš„ç¬¬ä¸‰ä¸ªåˆ†äº«ï¼šOblivious Transfer and Extension Roadmap Precomputation: can compute OTs even before you know your input! OT extension: 128 OTs suffice for everything. OTåœ¨å¤šæ–¹å®‰å…¨è®¡ç®—ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ï¼Œä½†OTçš„å®žé™…å¼€é”€å¾€å¾€å¾ˆå¤§ï¼Œå› ä¸ºä»–ä¸å¯èƒ½ä½¿ç”¨å»‰ä»·çš„åŠ å¯†æ–¹æ³•æ¥å®žçŽ°[ImpagliazzoRudich89]ã€‚å› æ­¤åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä¼šä»‹ç»ä¸€äº›å‰æ²¿çš„æ–¹æ³•æ¥æé«˜OTçš„æ•ˆçŽ‡ï¼šç¦»çº¿é¢„è®¡ç®—å’ŒOTæ‰©å±•ã€‚ Offline PrecomputationRandom OTåœ¨æå‡ºrandom OTæ¦‚å¿µå‰ï¼Œæˆ‘ä»¬å…ˆå›žé¡¾ä¸€ä¸‹standard OT: åœ¨standard OTä¸­ï¼ŒAliceé€‰æ‹©ä¸¤ä¸ªè¾“å…¥ï¼š$m_0, m_1$ï¼ŒBobé€‰æ‹©ä¸€ä¸ªè¾“å…¥ï¼š$c$ã€‚ ç»è¿‡ä¸€æ¬¡OTåŽï¼ŒBobå¯ä»¥å¾—åˆ°Aliceè¾“å…¥ä¸­çš„ä¸€ä¸ªï¼š$m_c$ï¼Œè€Œä¸çŸ¥é“å¦ä¸€ä¸ªè¾“å…¥ $m_{1-c}$ ã€‚è€ŒAliceä¸çŸ¥é“Bobçš„è¾“å…¥ $c$ ï¼Œä¹Ÿå°±ä¸çŸ¥é“Bobé€‰æ‹©çš„å“ªä¸€ä¸ªã€‚ ä¸Šè¿°è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åŒæ–¹é€‰æ‹©ä»–ä»¬å„è‡ªçš„è¾“å…¥ï¼ŒåŒæ–¹çš„è¾“å…¥ç¡®å®šï¼ŒBobå¾—åˆ°çš„ç»“æžœä¹Ÿæ˜¯ç¡®å®šçš„ï¼š$m_c$ ã€‚ ç›¸å¯¹äºŽstandard OTçš„ç¡®å®šæ€§ï¼Œrandom OTä¸éœ€è¦åŒæ–¹é€‰æ‹©è¾“å…¥ï¼Œè€Œæ˜¯ç›´æŽ¥éšæœºé‡‡æ ·è®¸å¤š $m_0, m_1, c$ ä½œä¸ºrandom OT instancesã€‚ Beaver Derandomization Theorem[Beaver91] ï¼š There is a cheap protocol that securely evaluates an instance of satandard OT using an instance of random OT. ã€Beaverçš„åŽ»éšæœºåŒ–ç†è®ºæå‡ºï¼šå­˜åœ¨ä¸€äº›é«˜æ•ˆçš„åè®®èƒ½å°†ä¸€ä¸ªrandom OT è½¬åŒ–ä¸ºä¸€ä¸ªstandard OT instance.ã€‘ æ ¹æ®Beaverçš„åŽ»éšæœºåŒ–ç†è®ºï¼Œåœ¨è¿è¡ŒOTåè®®æ—¶ï¼Œå¯ä»¥å…ˆç¦»çº¿ç”Ÿæˆè®¸å¤šrandom OTsã€‚åœ¨online phaseï¼šå³OTçš„è¾“å…¥æ˜¯ç”±Aliceå’ŒBobçš„è¾“å…¥ç¡®å®šçš„ï¼Œå¯ä»¥ç”¨Beaverçš„æ–¹æ³•é«˜æ•ˆåœ°å°†ç¦»çº¿ç”Ÿæˆçš„random OTåŽ»éšæœºåŒ–ï¼Œå³å˜æˆstandard OTã€‚ Beaver DerandomizationBeaveråŽ»éšæœºåŒ–è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¦»çº¿é˜¶æ®µå’Œåœ¨çº¿é˜¶æ®µï¼ˆstandard OT) åœ¨ç¦»çº¿é˜¶æ®µï¼Œç”Ÿæˆå¤§é‡random OT instancesï¼Œå…¶ä¸­AliceçŸ¥é“ $m_0^\\$, m_1^\\$ $ ï¼ŒBobçŸ¥é“ $c^\\$, m_{c^{\\$}}^\\$$ ã€‚ è€ŒBeaverçš„åŽ»éšæœºåŒ–çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šç”¨ç”Ÿæˆçš„random OT instanceçš„ $m_0^\\$, m_1^\\$ $ ä½œä¸ºone-time padçš„å¯†é’¥ï¼Œåˆ†åˆ«åŠ å¯†Aliceåœ¨çº¿æ—¶çš„è¾“å…¥ $m_0, m_1$ ã€‚è€ŒBobåªèƒ½ç”¨å”¯ä¸€çŸ¥é“çš„å¯†é’¥ $ m_{c^{\\$}}^\\$ $ è§£å¯†å…¶ä¸­ä¸€ä¸ªï¼Œè€Œä¸çŸ¥é“å¦ä¸€ä¸ªã€‚ Bobåœ¨çº¿æ—¶çš„è¾“å…¥ï¼š $c=c^\\$$ Bobå¯ä»¥ç”¨ $ m_{c^{\\$}}^\\$ $ç›´æŽ¥è§£å¯†ï¼š $x_c \\oplus m^\\$_{c^\\$} = x_c \\oplus m^\\$_c = m_c$ ï¼Œå¾—åˆ°æƒ³è¦çš„ $m_c$ ï¼Œè€Œå¯¹ $m_{1-c}$ ä¸€æ— æ‰€çŸ¥ã€‚ Bobåœ¨çº¿æ—¶è¾“å…¥ï¼š $c\\neq c^\\$$ å¦‚æžœBobè¿˜æƒ³ç”¨ $ m_{c^{\\$}}^\\$ $ç›´æŽ¥è§£å¯†å¾—åˆ° $m_c$ï¼š $x_c \\oplus m^\\$_{c^\\$} =x_c \\oplus m^\\$_{1\\oplus c} =m_c$ ï¼ŒAliceåœ¨åŠ å¯†æ—¶ $m_0, m_1$ æ—¶ï¼Œå¿…é¡»äº¤æ¢å…¶å¯†é’¥ $m_0^\\$, m_1^\\$ $ ï¼š ä¸ºäº†Bobèƒ½å¾—åˆ°æ­£ç¡®çš„ $m_c$ ï¼ŒBobåº”è¯¥å‘Šè¯‰Aliceä»€ä¹ˆæ—¶å€™è¦äº¤æ¢å¯†é’¥ï¼Œä»€ä¹ˆæ—¶å€™ä¸ç”¨äº¤æ¢å¯†é’¥ã€‚ æ–¹æ³•å°±æ˜¯ç”¨ä¸€ä¸ª1-bit one-time padï¼ŒBob è®¡ç®—å‡º $d = c\\oplus c^\\$$ ï¼Œç”±æ­¤å‘Šè¯‰Aliceæ˜¯å¦éœ€è¦äº¤æ¢å¯†é’¥ï¼ˆ$d = 0$ å‘Šè¯‰Aliceä¸éœ€è¦äº¤æ¢å¯†é’¥ï¼Œ$d = 1$å‘Šè¯‰Aliceéœ€è¦äº¤æ¢å¯†é’¥ï¼‰ï¼Œè€ŒAliceå¯¹ $c, c^\\$$ éƒ½ä¸€æ— æ‰€çŸ¥ã€‚ å› æ­¤Beaverçš„åŽ»éšæœºåŒ–çš„å®Œæ•´è¿‡ç¨‹ï¼š Bobè®¡ç®—å‡º $d = c\\oplus c^\\$$ ï¼Œå‘é€ç»™Aliceï¼Œä»¥æ­¤å‘Šè¯‰Aliceæ˜¯å¦éœ€è¦äº¤æ¢å¯†é’¥ $m_0^\\$, m_1^\\$ $ ã€‚ Aliceè¾“å…¥ $m_0, m_1$ ï¼Œæ ¹æ® $d$ çš„å€¼é€‰æ‹©å¯†é’¥ $m_0^\\$, m_1^\\$ $ æ¥åˆ†åˆ«åŠ å¯†$m_0, m_1$ : $$ \\begin{align} x_0 &= m_d^\\$ \\oplus m_0 \\\\ x_1 &= m_{1\\oplus d}^\\$ \\oplus m_1 \\end{align} $$ å†å°†å…¶å‘é€ç»™Bob Bobç”¨å”¯ä¸€å·²çŸ¥çš„å¯†é’¥ $ m_{c^{\\$}}^\\$ $æ¥è§£å¯†å¾—åˆ° $m_c$ï¼š $x_c \\oplus m^\\$_{c^\\$} =x_c \\oplus m^\\$_{1\\oplus c} =m_c$ ï¼Œè€Œå¯¹ $m_{1-c}$ ä¸€æ— æ‰€çŸ¥ã€‚ æœ€åŽåˆ†æžä¸€ä¸‹BeaveråŽ»éšæœºæ–¹æ³•çš„å¼€é”€ï¼šç¦»çº¿é˜¶æ®µç”Ÿæˆä¸€ä¸ªrandom OT instanceå’Œä¹‹å‰çš„OTå¼€é”€ç›¸åŒï¼Œä½†åœ¨åœ¨çº¿é˜¶æ®µï¼Œåªéœ€è¦ä¸€äº›ç®€å•çš„å¼‚æˆ–è¿ç®—ã€‚ OT ExtensionAn Analogy from Encryptionå…¬é’¥åŠ å¯†ï¼ˆPublic-key encryption, PKEï¼‰æœ¬è´¨ä¸Šå…·æœ‰ä¸å¯é¿å…çš„é«˜æ˜‚å¼€é”€[ImpagliazzoRudich89] ï¼Œä½†æ˜¯åœ¨çŽ°å®žçš„åŠ å¯†æ–¹æ¡ˆä¸­ï¼Œå¯ä»¥é€šè¿‡æ··åˆå…¬é’¥åŠ å¯†å’Œå¯¹ç§°åŠ å¯†çš„æ–¹å¼ï¼Œå°†å®žçŽ°PKEçš„å¼€é”€é™åˆ°æœ€ä½Žã€‚ å³PKE of Î» bits + cheap SKE = PKE of N bits Use (expensive) PKE to encrypt short s ã€ç”¨PKEæ¥åŠ å¯†å…±äº«çŸ­å¯†é’¥ï¼Œæ¯”å¦‚DHç®—æ³•ã€‘ Use (cheap) symmetric-key encryption with key s to encrypt long M ã€å†å°†çŸ­å¯†é’¥é€šè¿‡å¯†é’¥æ‰©å±•ç®—æ³•èŽ·å¾—é•¿å¯†é’¥ï¼Œä½œä¸ºå¯¹ç§°åŠ å¯†çš„å¯†é’¥ã€‘ OTæœ¬è´¨ä¸Šä¹Ÿå…·æœ‰ä¸å¯é¿å…çš„é«˜æ˜‚å¼€é”€[ImpagliazzoRudich89] ï¼Œæ‰€ä»¥å¦‚æžœå°†OTä¸Žå…¬é’¥åŠ å¯†ç±»æ¯”ï¼Œæ˜¯å¦å­˜åœ¨ä¸€ç§æ··åˆåŠ å¯†æ–¹æ¡ˆï¼Œä¹Ÿèƒ½å®žçŽ°Î» instances of OT + cheap SKE = N instances of OTï¼Ÿ Beaver OT ExtensionBeaveråˆ©ç”¨Yaoçš„ä¸¤æ–¹å®‰å…¨è®¡ç®—ï¼Œæå‡ºäº†ä¸€ç§OTçš„æ‰©å±•åè®®[Beaver96] å®žçŽ°$\\lambda$ instances of OT + cheap SKE = $N$ instances of OT. Beaver OTæ‰©å±•åè®®å…¶å®žæ˜¯ä¸€ä¸ªä¸¤æ–¹å®‰å…¨è®¡ç®—ç”µè·¯ï¼šè¯¥ç”µè·¯å°†Aliceå’ŒBobçš„è¾“å…¥ä½œä¸ºä¼ªéšæœºçš„ç§å­ï¼Œä»¥æ­¤ç”Ÿæˆ$n$ OT instancesï¼ŒåŒ…æ‹¬è¾“å‡ºç»™Aliceçš„random stringså’Œè¾“å‡ºç»™Bobçš„choice bits+choice stringsã€‚ è¯¥ç”µè·¯çš„è¾“å…¥ä½æ˜¯ $\\lambda$ bitï¼Œå› æ­¤Aliceå’ŒBobéœ€è¦åš $\\lambda$ æ¬¡OTsï¼Œè€Œè¯¥ç”µè·¯å¯ä»¥ç”Ÿæˆ $n&gt;&gt;\\lambda$ OTsï¼Œç”±æ­¤å®žçŽ°äº†$\\lambda$ instances of OT + cheap SKE = $N$ instances of OT. ä½†æ˜¯ç”¨Yaoâ€™s garbled circuitsçš„æ–¹æ³•å®žçŽ°Beaver OTæ‰©å±•åè®®å‡ ä¹Žæ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºè¿™å¤ªå¤æ‚äº†ã€‚ IKNP Protocolè€ŒYuval Ishai, Joe Kilian, Kobbi Nissim, Erez Petrankåœ¨Crypto 2003å¹´å‘è¡¨çš„è®ºæ–‡ï¼šExtending Oblivious Transfers Efficientlyï¼Œæå‡ºäº†ä¸€ç§å¯å®žçŽ°ã€æ›´é«˜æ•ˆçš„OTæ‰©å±•åè®®ï¼Œä»¥æ­¤å®žçŽ° $\\lambda$ instances of OT + cheap SKE = $N$ instances of OTã€‚ è¯¥åè®®èƒ½å¤Ÿç”¨æœ‰é™çš„OTsï¼ˆæ¯”å¦‚128æ¬¡OTsï¼‰å®žçŽ°è¶³å¤Ÿå¤šæ¬¡çš„OTsï¼ˆæ¯”å¦‚1 millionï¼‰ï¼Œä¸‹é¢å°†è¯¦ç»†é˜è¿°åè®®æ˜¯å¦‚ä½•å®žçŽ°çš„ã€‚ IKNP Details Bob: has input $r $ $\\Rightarrow$ extend to matrix ã€æŠŠè¾“å…¥ $r$ é‡å¤å¤šæ¬¡æ‰©å±•ä¸ºçŸ©é˜µ $R$ï¼ŒçŸ©é˜µçš„æ¯è¡Œ $R_i$ ï¼ˆith row of $R$ï¼‰è¦ä¹ˆæ˜¯å…¨1æˆ–è€…å…¨0ï¼Œè€Œä¸”è¯¥çŸ©é˜µçš„é«˜æ˜¯millionçº§åˆ«çš„ï¼Œå®½åº¦å¯ä»¥ä¸º128ã€‘ $\\Rightarrow$ secret share as ($T, Tâ€™$) ã€ç„¶åŽå¯¹è¯¥çŸ©é˜µ$R$åšç§˜å¯†åˆ†äº«ï¼Œå³æ‹†åˆ†ä¸ºä¸¤ä¸ªçŸ©é˜µ $T,Tâ€™$ ã€‘ ã€ç§˜å¯†åˆ†äº«çš„ç­–ç•¥æ˜¯éšæœºç”Ÿæˆä¸€ä¸ªçŸ©é˜µ $T$ ï¼Œè€Œ $Tâ€™=T\\oplus R$ ï¼Œè§‚å¯Ÿä¸‹å›¾å¯ä»¥çœ‹å‡º $R_i = T_i \\oplus T_iâ€™$ ï¼Œå³ä¸¤ä¸ªçŸ©é˜µçš„æ¯è¡Œè¦ä¹ˆç›¸åŒ($r=0$)ï¼Œè¦ä¹ˆç›¸å($r=1$) ã€‘ Aliceï¼š chooses random string $s$ OT for each column $\\Rightarrow$ Alice obtains matrix $Q$ ã€ä»Žåˆ—çš„è§’åº¦åšOTï¼ŒAliceæ¯æ¬¡æ ¹æ® $s$ çš„å€¼ä»ŽBobçš„($T,Tâ€™$)é€‰æ‹©ä¸€åˆ—ï¼Œå¦‚æžœ $s_i=0$ ï¼Œä»ŽçŸ©é˜µ$T$é€‰æ‹©ç¬¬iåˆ—ï¼Œå¦‚æžœ $s_i=1$ ï¼Œä»ŽçŸ©é˜µ$Tâ€™$é€‰æ‹©ç¬¬iåˆ—ï¼Œå¾—åˆ°çŸ©é˜µ $Q$ã€‘ ã€è¿™é‡Œçš„OTä¸­ï¼ŒBobæ˜¯senderï¼ŒAliceæ˜¯receiverã€‘ è§‚å¯ŸAliceç»è¿‡128æ¬¡OTå¾—åˆ°çš„çŸ©é˜µ $Q$ ï¼Œå¯ä»¥å‘çŽ°Aliceå¾—åˆ°çš„çŸ©é˜µ $Q$ å’ŒBobæ‹¥æœ‰çš„çŸ©é˜µ $T$ æœ‰å¦‚ä¸‹å…³ç³»ï¼š Whenever $r_i = 0$, Alice row = Bob row Whenever$r_i =1$, Alice row=Bob row $\\oplus s$ For every i: Bob knows $t_i$ ; Alice knows $q_i$ and $q_i\\oplus s$ . ã€å†ä»Žè¡Œçš„è§’åº¦è§‚å¯ŸAliceå’ŒBobå¾—åˆ°çš„ä¿¡æ¯ï¼ŒAliceæ¯æ¬¡æ”¶åˆ° $q_i$ åŽï¼Œéƒ½å¯ä»¥é€šè¿‡å’Œrandom string $s$ å¼‚æˆ–å¾—åˆ° $q_i$ å’Œ $q_i\\oplus s$ ï¼Œè€Œè¿™å…¶ä¸­çš„ä¸€ä¸ªå€¼ä¸ŽBobçŸ¥é“çš„ $t_i$ ç›¸åŒã€‘ ã€Aliceï¼šå› ä¸ºä¸çŸ¥é“Bobçš„ $r_i$ å€¼ï¼Œæ‰€ä»¥ä¸çŸ¥é“Bobé€‰æ‹©çš„ $t_i$ åˆ°åº•æ˜¯å’Œ $q_i$ ç›¸ç­‰è¿˜æ˜¯å’Œ $q_i\\oplus s$ ç›¸ç­‰ã€‘ ã€Bobï¼šå› ä¸ºä¸çŸ¥é“Aliceçš„ $s$ å€¼ï¼Œæ‰€ä»¥åªçŸ¥é“é€‰æ‹©çš„ $t_i$ ç­‰äºŽ$q_i$ å’Œ$q_i\\oplus s$ ä¸­çš„ä¸€ä¸ªï¼Œè€Œä¸çŸ¥é“å¦ä¸€ä¸ªã€‘ ä½†æ˜¯ä»ŽBobè§†è§’é‡å†™Aliceå¾—åˆ°çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬çŸ¥é“å¾—åˆ°ï¼š $r_i=0: q_i=t_i$ $r_i=1:q_i=t_i\\oplus s$ è‡³æ­¤ï¼Œæˆ‘ä»¬å‡ ä¹Žå·²ç»å®žçŽ°äº†è¡Œè§’åº¦çš„OTï¼ŒAliceæœ‰ä¸¤ä¸ªå€¼ï¼š$q_i$ å’Œ $q_i\\oplus s$ ï¼ŒBobæ ¹æ® $r_i$ ä»Žä¸­é€‰æ‹©å¾—åˆ°ä¸€ä¸ªï¼Œè€Œå¯¹å¦ä¸€ä¸ªä¸€æ— æ‰€çŸ¥ã€‚ Break correlations by applying random oracle ä¸Šé¢æˆ‘è¯´æ˜¯â€œå‡ ä¹Žå®žçŽ°äº†è¡Œè§’åº¦çš„OTâ€ï¼Œä¸ºä»€ä¹ˆæ˜¯å‡ ä¹Žå‘¢ï¼Ÿ å› ä¸ºstring sçš„åŽŸå› ï¼Œè¿™äº›OT instanceså…·æœ‰ä¸€å®šçš„çº¿æ€§ç›¸å…³æ€§ã€‚æ‰€ä»¥ä¸ºäº†ç ´åå…¶ä¸­çš„çº¿æ€§ç›¸å…³æ€§ï¼Œå¯¹è¿™äº›stringséƒ½æ‰§è¡Œä¸€ä¸ªä»»æ„çš„å¯†ç å­¦å‡½æ•° $H$ ï¼š $\\Rightarrow$ Random OT instance for each row, using base OT for each column ã€ç”±æ­¤ï¼Œé€šè¿‡ä»Žåˆ—çš„è§’åº¦åšbase OTï¼Œå¾—åˆ°äº†è¶³å¤Ÿå¤šçš„è¡Œè§’åº¦random OT instancesã€‘ æœ€åŽï¼Œæ€»ç»“ä¸€ä¸‹IKNPå¦‚ä½•å®žçŽ°$\\lambda$ instances of OT + cheap SKE = $N$ instances of OTã€‚ Bobæœ‰ä¸€ä¸ªtall matrix($\\lambda$ åˆ—, $n&gt;&gt;\\lambda$ è¡Œ)ï¼ŒAliceä»Žåˆ—çš„è§’åº¦æ ¹æ®ç§˜å¯†å­—ç¬¦ä¸² $s$ åš $\\lambda$ æ¬¡base OTsã€‚ æœ€åŽAliceå’ŒBobå¾—åˆ°äº†è¡Œè§’åº¦çš„ $n$ ä¸ªæ‰©å±•OTsã€‚ Malicious Bob in IKNP Protocol[KellerOrsiniScholl15] ä»‹ç»äº†å¦‚æžœæœ‰æ¶æ„çš„å‚ä¸Žæ–¹ï¼ŒIKNPå—åˆ°çš„å¨èƒ å‡è®¾Bobæ˜¯æ¶æ„çš„ï¼š Bobï¼š æœ‰ä¸€ä¸ªè¾“å…¥ $r $ $\\Rightarrow$ å¯¹å…¶æ‰©å±•ä¸ºçŸ©é˜µ $R$ ï¼Œä½†æ˜¯æ¶æ„åœ°ç¿»è½¬äº† $t_2$ çš„ç¬¬äºŒä½bitï¼š $\\Rightarrow$å†åšç§˜å¯†å…±äº«ä¸º ($T, Tâ€™$) ï¼ŒåŒæ ·çš„ï¼Œ$Tâ€™$ ä¸­ä¹Ÿæœ‰ä¸€ä½è¢«ç¿»è½¬äº†ï¼š Alice: é€‰æ‹©ä¸€ä¸ªéšæœºä¸² $s$ å¯¹æ¯åˆ—åšOT $\\Rightarrow$ Aliceå¾—åˆ°çŸ©é˜µ $Q$ å› ä¸º $r_2=0$ ï¼Œæœ¬åº”è¯¥æœ‰ $q_i = t_i$ ã€‚ä½†å› ä¸ºBobæ¶æ„ç¿»è½¬äº†ä¸€ä½ï¼Œæ‰€ä»¥Aliceå¾—åˆ°çš„ $q_i$ å’Œ $t_i$ åœ¨ç¬¬äºŒä½ä¸åŒ â€¦ æ‰€ä»¥åœ¨Aliceå’ŒBobåŒæ—¶ä½¿ç”¨ $H(row)$ ç ´åçº¿æ€§ç›¸å…³æ€§åŽï¼ŒBobä¼šå‘çŽ°å¾—åˆ°çš„ $H(row#2)$ å€¼ä¸åŒï¼ŒBobå°±çŸ¥é“Aliceé€‰æ‹©çš„éšæœºä¸²ä¸­ $s_2=1$ ã€‚ å¦‚æ­¤ä»¥æ¥ï¼ŒBobæ¯æ¬¡éƒ½æ¶æ„ç¿»è½¬æŸä¸€åˆ—çš„ä¸€ä¸ªbitï¼Œå°±å¯ä»¥å­¦åˆ° $s$ çš„æ‰€æœ‰æ¯”ç‰¹ä¿¡æ¯ã€‚ Consistency Checkæ‰€ä»¥å¦‚ä½•ä¿è¯Bobæ²¡æœ‰æ¶æ„ç¯¡æ”¹çŸ©é˜µ $R_i$ ($R$ çŸ©é˜µçš„ç¬¬iè¡Œ)? [KellerOrsiniScholl15] æå‡ºäº†ä¸€ç§ä¸€è‡´æ€§æ£€éªŒæ–¹æ³•æ¥æ£€æµ‹Bobæ˜¯å¦æ¶æ„ç¯¡æ”¹äº†çŸ©é˜µã€‚ Aliceåœ¨æ‰§è¡ŒIKNPä¸­é€šè¿‡å¤šæ¬¡æ£€éªŒä¸Šè¿°æ–¹ç¨‹çš„ä¸€è‡´æ€§ï¼Œæ¥æ£€æµ‹Bobæ˜¯å¦æ¶æ„ç¯¡æ”¹äº†æ‰©å±•çŸ©é˜µã€‚ åŒæ—¶ï¼Œä¸ºäº†ä¿æŠ¤æ‰©å±•çŸ©é˜µçš„ä½ä¿¡æ¯ï¼ŒBobä¹Ÿä½¿ç”¨ä¸€ä¸ªéšæœºçŸ©é˜µæ¥åŠ å¯† $R^*$ ã€‚ IKNPå¯ä»¥é€šè¿‡ä¸Šè¿°ä¸€è‡´æ€§æ£€éªŒæ¥å®žçŽ°malicious securityï¼Œè€Œå¼•å…¥çš„é¢å¤–å¼€é”€å¾ˆå°ã€‚ Generalizing IKNPIKNPåè®®ç”Ÿæˆäº†è®¸å¤šäºŒé€‰ä¸€çš„OT instancesï¼Œè¿™é‡Œå°†ä»‹ç»å¦‚ä½•å¯¹IKNPåè®®ä¸€èˆ¬åŒ–ï¼Œä»¥ç”Ÿæˆå¤šé€‰ä¸€çš„OT instancesã€‚ å†æ¬¡å›žé¡¾ä¸€ä¸‹IKNPçš„åšæ³•ï¼š Bobæœ‰ä¸€ä¸ªé€‰æ‹©ä¸² $r$ æ‰©å±•ä¸ºçŸ©é˜µ $R$ ï¼ŒçŸ©é˜µçš„æ¯è¡Œæ˜¯å…¨0æˆ–å…¨1 å†å¯¹çŸ©é˜µRåšç§˜å¯†åˆ†äº« ä»Žè¡Œçš„è§’åº¦çœ‹çŸ©é˜µæ‰©å±•ï¼ŒæŠŠ0 ç¼–ç ä¸º 000... ï¼ŒæŠŠ1 ç¼–ç ä¸º111... ã€‚æ‰€ä»¥å¯ä»¥æŠŠçŸ©é˜µæ‰©å±•çœ‹ä½œå¯¹ $r$ çš„bitçš„ä¸€ç§ç¼–ç æ–¹å¼ï¼ˆrepetition codeï¼‰ã€‚ å› æ­¤ï¼Œä¸€èˆ¬åŒ–IKNPçš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯ï¼šç”¨å…·æœ‰å·®é”™æ ¡æ­£çš„ç¼–ç æ–¹å¼å¯¹ $r$ çš„bitè¿›è¡Œç¼–ç ã€‚ Coding view of IKNP çŽ°åœ¨ï¼Œæˆ‘ä»¬ä»Žç¼–ç çš„æ–¹å¼çœ‹IKNPã€‚ Bob: Bob æœ‰ä¸€ä¸ªè¾“å…¥ $r$ å¯¹ $r$ çš„æ¯”ç‰¹è¿›è¡Œç¼–ç ï¼š åœ¨è¿™ç§ç¼–ç æ–¹å¼ä¸‹è¿›è¡Œç§˜å¯†åˆ†äº« Alice å¯¹æ¯åˆ—åšOTï¼Œå¾—åˆ°çŸ©é˜µ$Q$ Bobå¾—åˆ°çš„ $t_i$ å’ŒAliceå¾—åˆ°çš„ $q_i$ æœ‰å¦‚ä¸‹å…³ç³»ï¼š$t_i = q_i\\oplus C(r_i)\\wedge s$ å¦‚æžœæ˜¯åœ¨ç¼–ç $C(0)=000â€¦, C(1)=111â€¦$ ä¸‹ï¼š $r_i=0 \\Rightarrow t_i = q_i\\oplus (000â€¦)\\wedge s=q_i$ $r_i=1 \\Rightarrow t_i=q_i\\oplus (111â€¦)\\wedge s=q_i\\oplus s$ For every i: Bob knows $t_i$; Alice knows $q_i\\oplus C(0)\\wedge s$ and $q_i\\oplus C(1)\\wedge s$ ã€æ­¤æ—¶AliceçŸ¥é“$q_i\\oplus C(0)\\wedge s$ å’Œ $q_i\\oplus C(1)\\wedge s$ ï¼ŒBobçŸ¥é“$t_i$ ã€‘ ä»ŽBobçš„è§†è§’é‡å†™Aliceå¾—åˆ°çš„ $q_i=t_i\\oplus C(r_i)\\wedge s$ å½“$C$æ˜¯ä¸€ç§çº¿æ€§ç¼–ç æ—¶ï¼Œå³æ»¡è¶³: $[C(a)\\wedge s]\\oplus [C(b)\\wedge s]=C(a\\oplus b)\\wedge s$ and $C(0)\\wedge s = 000â€¦$ å¾—åˆ°ï¼š åŒ–ç®€ï¼š Use random oracle to destroy correlations ã€å†ç ´åå…¶çº¿æ€§ç›¸å…³æ€§ã€‘ Generalizing IKNPåˆšåˆšæˆ‘ä»¬è€ƒè™‘çš„æ˜¯åªå¯¹ $\\{0, 1\\}$ ç¼–ç ï¼Œå³ï¼š $C:\\{0, 1\\}\\rightarrow \\{0, 1\\}^k$ ç¼–ç ã€‚ çŽ°åœ¨è€ƒè™‘ç¼–ç æ›´å¤šçš„bitsï¼Œå³ $C:\\{0, 1\\}^3\\rightarrow \\{0, 1\\}^k$ ï¼ŒåŒæ ·åœ°ï¼Œ$C$æ˜¯çº¿æ€§ç¼–ç ã€‚ åŒæ ·çš„è¿è¡ŒIKNPï¼š For every i: Alice can compute 8 (things) ã€ä¹‹å‰Aliceæ˜¯å¯ä»¥è®¡ç®—$q_i\\oplus C(0)\\wedge s$ å’Œ $q_i\\oplus C(1)\\wedge s$ ã€‘ çŽ°åœ¨æ˜¯å¯¹ $\\{0, 1\\}^3$ ç¼–ç ï¼Œå°±å¯ä»¥è®¡ç®—ï¼š $q_i\\oplus C(000)\\wedge s, q_i\\oplus C(001)\\wedge s, \\cdots,q_i\\oplus C(111)\\wedge s$ åŒæ ·çš„ï¼Œä»ŽBobçš„è§’åº¦é‡å†™Aliceå¾—åˆ°çš„å€¼$q_i=t_i\\oplus C(r_i)\\wedge s$ å¹¶ä¸” $C$ æ˜¯çº¿æ€§ç¼–ç ï¼Œæ»¡è¶³$[C(a)\\wedge s]\\oplus [C(b)\\wedge s]=C(a\\oplus b)\\wedge s$ and $C(0)\\wedge s = 000â€¦$ ï¼ŒåŒ–ç®€åŽï¼š å¯¹ä¸Šè¿°Aliceè®¡ç®—å‡ºçš„8ä¸ªä¸²ï¼ŒBobåªçŸ¥é“å…¶ä¸­çš„ä¸€ä¸ª $t_i$ ï¼Œè€Œè¿™ä¸ª $t_i$ ç­‰äºŽAliceçŸ¥é“çš„ç¬¬å‡ ä¸ªä¸²ï¼Œè¿™å–å†³äºŽBobçš„é€‰æ‹©çš„ $r_i$ ï¼Œä½¿å¾— $C(r_i\\oplus \\cdots)=000â€¦$ ã€‚ In the random oracle model: $H(t_1 \\oplus c_1 \\wedge s),â€¦H(t_n \\oplus c_n \\wedge s)$ pseudorandom if all $c_i$ have Hamming weight $\\ge \\lambda$ ã€åœ¨ç ´åçº¿æ€§ç›¸å…³æ€§æ—¶ï¼Œåªæœ‰å½“æ‰€æœ‰ $c_i$ çš„æ±‰æ˜Žé‡é‡ï¼ˆHamming weightï¼‰$\\ge\\lambda$ æ—¶ï¼Œ$H(\\cdot)$ å’Œéšæœºä¸²æ‰å…·æœ‰ä¸å¯åŒºåˆ†æ€§ã€‘ æ±‰æ˜Žé‡é‡ï¼ˆHamming weightï¼‰:æ±‰æ˜Žé‡é‡æ˜¯ä¸€ä¸²ç¬¦å·ä¸­éž0ç¬¦å·çš„ä¸ªæ•°ã€‚ æ±‰æ˜Žè·ç¦»ï¼ˆHamming distanceï¼‰ï¼šä¸¤ä¸ªå­—ç¬¦ä¸²å¯¹åº”ä½ç½®çš„ä¸åŒå­—ç¬¦çš„ä¸ªæ•°ã€‚ å°±æ­¤ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ‰©å±•çš„IKNPåè®®ï¼Œ[KolesnikovKumaresan13] æåˆ°ä½¿ç”¨è¿™æ ·çš„ç¼–ç ï¼š $C:\\{0, 1\\}^l\\rightarrow \\{0, 1\\}^k$ (ç¼–ç å­—ç¬¦ä¸²çš„æ±‰æ˜Žè·ç¦» $\\ge \\lambda$) æ‰§è¡Œ $k$ æ¬¡base OTï¼Œå°±èƒ½å®žçŽ°ä»Ž $2^l$ ä¸­é€‰ä¸€ä¸ªçš„OTæ‰©å±•(1-out-of- $2^l$ OT extension)ã€‚ 1-out-of-256 OT[KolesnikovKumaresan13] : Walsh-Hadamard code $C:\\{0, 1\\}^8\\rightarrow \\{0, 1\\}^k$ (min. dist. 128) 1-out-of- $2^{76}$ OT[OrruOrsiniScholl16] : BCH code $C:\\{0, 1\\}^{76}\\rightarrow \\{0, 1\\}^{512}$ (min. dist. 171) 1-out-of-âˆž OT[KolesnikovKumaresanRosulekTrieu16] è¿™ç¯‡æ–‡ç« æå‡ºï¼Œå¯¹äºŽä»»æ„çš„ä¼ªéšæœºç¼–ç $C:\\{0, 1\\}^*\\rightarrow \\{0, 1\\}^{ï½ž480}$ ï¼Œåªéœ€è¦æ»¡è¶³æœ€å°çš„hamming dist.ï¼Œè€Œä¸ç”¨è¦æ±‚ç¼–ç  $C$ çš„çº¿æ€§æ€§è´¨ï¼Œå°±å¯ä»¥å®žçŽ°1-out-of-âˆž OTã€‚ å°±æ­¤ï¼ŒIKNPåè®®çš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚ä¹Ÿæ­£æ˜¯å› ä¸ºIKNPåè®®ï¼Œä½¿å¾—OTçš„æ•ˆçŽ‡å¤§å¹…æé«˜ã€‚","link":"/2021/07/06/MPC3-OT/"},{"title":"ã€ŒMathã€ï¼šMersenne Prime","text":"åœ¨å¯†ç å­¦ä¸­ï¼Œæœ‰é™åŸŸä¸­çš„è¿ç®—æ€§èƒ½æžå¤§å½±å“å¯†ç åè®®çš„å®žçŽ°ã€‚ å¦‚æžœæœ‰é™åŸŸé€‰æ‹©æ¢…æ£®ç´ æ•°ï¼Œå¾—ç›ŠäºŽå®ƒçš„ä¼˜è‰¯æ€§è´¨ï¼Œå¯ä»¥æžå¤§æé«˜è¿ç®—æ•ˆçŽ‡ï¼Œç‰¹åˆ«æ˜¯æœ‰é™åŸŸä¸‹çš„æ¨¡è¿ç®—ã€ä¹˜æ³•æ“ä½œã€‚ äºŽæ˜¯è¿‘æ—¥å­¦ä¹ äº†æ¢…æ£®ç´ æ•°çš„ç›¸å…³æ€§è´¨ï¼Œä»¥åŠå¦‚ä½•çº¦å‡æ¢…æ£®ç´ æ•°åŸŸä¸‹æ¨¡è¿ç®—å’Œä¹˜æ³•è¿ç®—ã€‚ [Mersenne Prime]","link":"/2021/12/22/Mersenne-Prime/"},{"title":"ã€ŒLeetCodeã€ï¼šArray","text":"8æœˆæŸå¸å®žè®­+å‡†å¤‡å¼€å­¦æœŸæœ«è€ƒï¼Œæˆ‘å¯å¤ªå’•äº†q w qâ€¦dbqï¼Œï¼ˆå¸Œæœ›ï¼‰é«˜äº§åšä¸»æˆ‘.æˆ‘..åˆå›žæ¥äº†ã€‚ LeetCode Arrayä¸“é¢˜ï¼ŒæŒä¹…æ›´æ–°ã€‚ï¼ˆGitHub) Array27-Remove Elements27-Remove Elements Solution 12345678910111213141516from typing import Listclass Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: tot = 0 for element in nums: if element == val: continue else: nums[tot] = element tot = tot + 1 return tot Pythonçš„å‚æ•°ä¼ é€’å’Œå‡½æ•°è¿”å›žå€¼ï¼š 1def removeElement(self, nums:List[int], val:int) -&gt; int: é¢˜ç›®è¦æ±‚ï¼š â€œremove all instances of that value in-place and return the new length.â€ â€œDo not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.â€ â€œConfused why the returned value is an integer but your answer is an array? Note that the input array is passed in by reference, which means modification to the input array will be known to the caller as well.â€ é¢˜ç›®è¦æ±‚æ˜¯åœ¨åŽŸæ•°ç»„ä¸Šåˆ é™¤æ•°å€¼ï¼Œä¸èƒ½é¢å¤–å¼€æ–°çš„ç©ºé—´å­˜å‚¨æ•°ç»„ã€‚ æ„æ€å°±æ˜¯è¯´ï¼Œè™½ç„¶å‡½æ•°è¿”å›žçš„æ˜¯ä¸€ä¸ªæ•°å€¼ï¼Œä½†å®žé™…è¿”å›žç­”æ¡ˆæ˜¯ä¸€ä¸ªæ•°ç»„ã€‚ å› ä¸ºæ•°ç»„çš„ä¼ é€’æ˜¯æŒ‡é’ˆä¼ é€’ï¼Œè¿”å›žçš„æ˜¯æ•°ç»„é•¿åº¦ï¼Œåˆ™ç›¸å½“äºŽè¿”å›žäº†è¿™ä¸ªin-placeçš„æ–°æ•°ç»„ã€‚ 26-Remove Duplicates from Sorted Array26-Remove Duplicates from Sorted Array Solutionï¼š 1234567891011121314151617from typing import Listclass Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: tot = 0 before = None for index in range(len(nums)): if nums[index] != before: nums[tot] = nums[index] before = nums[index] tot = tot + 1 else: continue return tot ç¬¬ä¸€æ¬¡æäº¤çš„æ—¶å€™before = nums[0] - 1 æŠ¥é”™äº†ï¼ŒåŽŸå› æ˜¯ä¼ å…¥æ•°ç»„é•¿åº¦ä¸º0ï¼Œä¸‹æ ‡è¶Šç•Œã€‚ æ³¨æ„ç©ºæ•°ç»„çš„ä¸‹æ ‡è¶Šç•Œé—®é¢˜ã€‚ 80-Remove Duplicates from Sorted Array II80-Remove Duplicates from Sorted Array II Solution: 123456789101112131415161718192021from typing import Listclass Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: before = None before_cnt = 0 length = 0 for index in range(len(nums)): if nums[index] != before: nums[length] = nums[index] before = nums[index] length += 1 before_cnt = 1 else: before_cnt += 1 if before_cnt &lt;= 2: nums[length] = nums[index] length += 1 return length 189-Rotate Arrayï¼ˆS123ï¼‰189-Rotate Array Problem: ç®€è¿°é¢˜ç›®å¤§æ„ï¼Œç»™ä¸€ä¸ªåˆ—è¡¨numsï¼Œä¸€ä¸ª $k$ å€¼ï¼Œè¦æ±‚åŽŸå€è®©åˆ—è¡¨å¾ªçŽ¯å³ç§» $k$ ä½ã€‚ Solution: å…¶å®žä»¥ä¸‹ä¸‰ç§åšæ³•æ—¶é—´ç©ºé—´å¤æ‚åº¦å·®åˆ«ä¸å¤§ï¼Œä¸»è¦çœ‹ä¸ªæ€è·¯å§ã€‚ S Runtime Memory Language S1 64ms 15.2MB pyhon3 S2 64ms 15.1MB python3 S3 116ms 15.1MB python3 S1-ç®€å•åšæ³•ï¼šç©ºé—´æ¢æ—¶é—´æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ï¼Œå¾ªçŽ¯å³ç§»æ—¶å¤šå¼€äº†ä¸€ä¸ªæ•°ç»„ã€‚ 12345678910111213from typing import Listimport queueclass Solution: def rotate(self, nums: List[int], k: int) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; length = len(nums) a = [0] * length for index in range(length): a[(index + k)%length] = nums[index] nums[:] = a S2-åˆ©ç”¨æ•°å­¦åŒä½™å…³ç³»æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ï¼Œå¾ªçŽ¯å³ç§»æ—¶åªå¤šå¼€äº†ä¸€ä¸ªå˜é‡ã€‚ åŽŸç†ï¼š å®šç†[1]ï¼š è®¾ $m$ æ˜¯æ­£æ•´æ•°ï¼Œæ•´æ•° $a$ æ»¡è¶³ $(a,m)=1$ ï¼Œ$b$ æ˜¯ä»»æ„æ•´æ•°ã€‚è‹¥ $x$ éåŽ†æ¨¡ $m$ çš„ä¸€ä¸ªå®Œå…¨å‰©ä½™ç³»ï¼Œåˆ™ $ax+b$ ä¹ŸéåŽ†æ¨¡ $m$ çš„ä¸€ä¸ªå®Œå…¨å‰©ä½™ç³»ã€‚ ç”±ä»¥ä¸Šå®šç†å¯ä»¥å¾—çŸ¥ï¼Œè®¾ $n$ ä¸ºåˆ—è¡¨é•¿åº¦ï¼Œ $x$ æ˜¯åˆ—è¡¨çš„ä¸‹æ ‡ï¼ŒéåŽ† $n$ çš„ä¸€ä¸ªå®Œå…¨å‰©ä½™ç³»ã€‚ å¦‚æžœ $(k,n)=1$ ï¼Œ $kx$ ä¹ŸéåŽ† $n$ çš„ä¸€ä¸ªå®Œå…¨å‰©ä½™ç³»ã€‚è¿™ç§æƒ…å†µï¼Œåˆ—è¡¨ä¸‹æ ‡é€šè¿‡ $k$ çš„å€æ•°çš„é¡ºåºè¿žæˆä¸€ä¸ªçŽ¯ã€‚ ï¼šåªéœ€è¦é¢å¤–ä¸€ä¸ªå˜é‡ $temp$ å­˜å‚¨ç§»åŠ¨å ç”¨çš„å€¼ã€‚ å¦‚æžœ $(k,n)\\neq 1$ ï¼Œé‚£ä¹ˆ $kx$ ä¸ä¼šéåŽ†ä¸€ä¸ª $n$ çš„å®Œå…¨å‰©ä½™ç³»ï¼Œä¼šå‡ºçŽ°ä¸‹å›¾çš„æƒ…å†µï¼ˆå¦‚ç»¿è‰²çš„çº¿çš„å…ƒç´ çš„ $idx = kx+0$ ï¼Œçº¢è‰²çº¿çš„å…ƒç´ éƒ½æ˜¯ $idx=kx+1$ ï¼‰ï¼Œä¼šåœ¨ $k$ çš„æŸä¸ªå‰©ä½™ç±»ä¸€ç›´å¾ªçŽ¯ã€‚ ï¼šéåŽ†æ¯ä¸ª $k$ çš„å‰©ä½™ç±»ã€‚ åœ¨æ¯æ¬¡å¾ªçŽ¯ç§»ä½æ—¶ï¼Œéœ€è¦è®°å½•è¯¥æ¬¡å¾ªçŽ¯çš„èµ·å§‹ä½ï¼Œé˜²æ­¢é‡å¤ã€‚ 123456789101112131415161718class Solution: def rotate(self, nums: List[int], k: int) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; length = len(nums) k %= length start = 0 cnt = 0 while cnt &lt; length: current, prev = start, nums[start] while True: current = (current + k) % length prev, nums[current] = nums[current], prev cnt += 1 if current == start: break start += 1 S3-åˆ©ç”¨åè½¬åˆ—è¡¨çš„æ€è·¯æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ åŽŸç†ï¼š 1234Original List : 1 2 3 4 5 6 7After reversing all numbers : 7 6 5 4 3 2 1After reversing first k numbers : 5 6 7 4 3 2 1After revering last n-k numbers : 5 6 7 1 2 3 4 --&gt; Result 123456789101112131415161718from typing import List# Solution 3: Reverseclass Solution: def reverse(self, nums: list, begin: int, end: int) -&gt; None: while begin &lt; end: nums[begin], nums[end] = nums[end], nums[begin] begin += 1 end -= 1 def rotate(self, nums: List[int], k: int) -&gt; None: n = len(nums) k %= n self.reverse(nums, 0, n-1) self.reverse(nums, 0, k-1) self.reverse(nums, k, n-1) Pythonç”¨å¼•ç”¨ç®¡ç†å¯¹è±¡ã€‚ 12int a1 = 1, *p = &amp;a1;int a2 = 2, &amp;b = a2; æŒ‡é’ˆï¼šæŒ‡é’ˆå˜é‡æ˜¯ä¸€ä¸ªæ–°å˜é‡ï¼Œè¿™ä¸ªå˜é‡å­˜å‚¨çš„æ˜¯ï¼ˆå˜é‡a1çš„ï¼‰åœ°å€ï¼Œè¯¥åœ°å€æŒ‡å‘ä¸€ä¸ªå­˜å‚¨å•å…ƒã€‚ï¼ˆè¯¥å­˜å‚¨å•å…ƒå­˜æ”¾çš„æ˜¯a1çš„å€¼ï¼‰ã€‚ å¼•ç”¨ï¼šå¼•ç”¨çš„å®žè´¨æ˜¯å˜é‡çš„åˆ«åï¼Œæ‰€ä»¥a2å’Œbå®žé™…æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œåœ¨å†…å­˜ä¸­å æœ‰åŒä¸€ä¸ªå­˜å‚¨å•å…ƒã€‚ æ‰€ä»¥pythonä¸­äº¤æ¢å¯¹è±¡å¯ä»¥ç›´æŽ¥a,b = b,a Python åˆ—è¡¨çš„æ“ä½œï¼šåˆ‡ç‰‡ã€‚ 41-First Missing Positive41-First Missing Positive Solution: æŽ’ä¸€ä¸‹åºï¼Œç»´æŠ¤ä¸€ä¸ªexpectå˜é‡å°±è¡Œäº†ã€‚ æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n\\log{n})$ ï¼Œé¢˜ç›®æ²¡æœ‰å¡å¸¸ã€‚ Runtime: 36 ms, faster than 70.96% of Python3 online submissions for First Missing Positive. ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ Memory Usage: 13.8 MB, less than 69.19% of Python3 online submissions for First Missing Positive. 12345678910111213from typing import Listclass Solution: def firstMissingPositive(self, nums: List[int]) -&gt; int: nums.sort() expect = 1 for element in nums: if element == expect: expect += 1 elif element &gt; expect: return expect return expect 299-Bulls and Cows299-Bulls and Cows Problem: é¢˜ç›®å¤§æ„æ˜¯ï¼šç»™å®šä¸¤ä¸ªç›¸åŒé•¿åº¦çš„å­—ç¬¦ä¸²ï¼Œè®¡ç®—è¿™ä¸¤ä¸ªå­—ç¬¦ä¸²æœ‰å¤šå°‘ä¸ªå¯¹åº”ä½æ•°å­—ç›¸åŒï¼Œå’Œå¤šå°‘ä¸ªä½ç½®ä¸å¯¹åº”ä½†æ•°å­—ç›¸åŒçš„ä¸ªæ•°ã€‚ Solution: åº”ç”¨å­—ç¬¦0-9æœ¬èº«æ•°å­—çš„æ€§è´¨ã€‚ æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ 123456789101112131415161718192021class Solution: def getHint(self, secret: str, guess: str) -&gt; str: # 0-9 cnt for guess (expect the same digit) cnt = [0] * 10 bulls = cows = 0 g = lambda a: ord(a) - ord('0') for si, gi in zip(secret, guess): if si == gi: bulls += 1 else: cnt[g(gi)] += 1 for si, gi in zip(secret, guess): if si == gi: continue elif cnt[g(si)] &gt; 0: cows += 1 cnt[g(si)] -= 1 output = &quot;{}A{}B&quot;.format(bulls, cows) return output ord()å‡½æ•°å’Œchr()å‡½æ•° ord()è¿”å›žå­—ç¬¦çš„ASCIIç ï¼Œchrå‡½æ•°è¿”å›žASCIIç å¯¹åº”çš„å­—ç¬¦ã€‚ æµ…æžlambdaè¡¨è¾¾å¼ï¼ŒåŒ¿åå‡½æ•°ï¼Œç±»ä¼¼äºŽCè¯­è¨€çš„å®ã€‚ æ ¼å¼ï¼šlambda [arg1[, arg2,...]] : expression åŒå˜é‡åŒæ—¶éåŽ†ä½¿ç”¨zip()å‡½æ•° 134-Gas Stationï¼ˆS12ï¼‰134-Gas Station Problemï¼š é¢˜ç›®å¤§æ„æ˜¯ï¼šæœ‰Nä¸ªçŽ¯å½¢åŠ æ²¹ç«™ï¼Œæ¯ä¸ªåŠ æ²¹ç«™èƒ½åŠ æ²¹gas[i]ï¼Œä¸€ä¸ªæ±½è½¦èµ·å§‹æ²¹é‡ä¸º0ï¼Œä¸”ä»Žiä¸ªç«™å¼€åˆ°ç¬¬i+1ä¸ªç«™éœ€è¦èŠ±è´¹cost[i]çš„æ²¹é‡ã€‚æ‰¾å‡ºè¿™ä¸ªè½¦èƒ½é¡ºæ—¶é’ˆè·‘å®Œä¸€åœˆçš„èµ·å§‹ç‚¹ï¼ˆå¦‚æžœæœ‰ï¼Œåˆ™å”¯ä¸€ï¼‰ï¼Œå¦‚æžœä¸èƒ½è¿”å›ž-1ã€‚ Solutionï¼š Solution Runtime Memory Language S1-ç®€å•åšæ³• 3244ms 14.9MB python3 S2-åŽŸç†ä¼˜åŒ– 104ms 14.8MB python3 S1-ç®€å•è§£æ³•æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n^2)$ ï¼Œå®žé™…è¿œè¾¾ä¸åˆ° $n^2$ ï¼Œç®—æœ‰ä¸€ç‚¹è´ªå¿ƒå­ã€‚ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ è¯¥æ±½è½¦ä»Žèµ·ç‚¹ièƒ½è·‘å®Œçš„å¿…è¦æ¡ä»¶ï¼š èµ·å§‹ç‚¹ gas[i] - cost[i] &gt;= 0 ã€‚å¹¶ä¸”ç»´æŠ¤ä¸€ä¸ªæ•°ç»„å­˜æ”¾ gas[i] - cost[i] ï¼Œå³è¿™ä¸ªç«™è‡ªç»™è‡ªè¶³çš„æ²¹é‡ä½™é‡ã€‚ å¦‚æžœä»Žæ»¡è¶³æ¡ä»¶1çš„èµ·ç‚¹å¼€å§‹è·‘ä¸€åœˆï¼Œ è¦æ±‚è·¯ç¨‹ä¸­çš„æ²¹é‡å¿…é¡»å¤§äºŽç­‰äºŽ0ã€‚ç»´æŠ¤ä¸€ä¸ªæ±½è½¦å½“å‰æ€»æ²¹é‡ $S$ ï¼ˆç”¨å‰ç¼€å’Œç»´æŠ¤ï¼‰ï¼Œæ¯è·‘è¿‡ä¸€æ®µè·¯ç¨‹ï¼Œéƒ½è¦æ±‚ $S&gt;=0$ ã€‚ 123456789101112131415161718192021from typing import Listclass Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: n = len(gas) remain = [] for g, c in zip(gas, cost): remain.append(g - c) for i in range(n): if remain[i] &lt; 0: continue else: S = 0 for j in range(n): S += remain[(i + j) % n] if S &lt; 0: break if S &gt;= 0: return i return -1 S2-å¯¹é—®é¢˜åˆ†æžè¿›è¡Œå†ç®€åŒ–æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ 12345 i : 0 1 2 3 4g[i]: 1 2 3 4 5c[i]: 3 4 5 1 2g-c :-2-2-2 3 3sum :-2-4-6-3 0 å¦‚æžœ $S=\\sum_{i=0}^{n-1} g[i]-c[i],S&lt;0$ é‚£ä¹ˆä¸€å®šæ— è§£ï¼Œ $S$ ç§°ä¸ºæ€»ç§¯ç´¯æ²¹é‡ã€‚ å¦‚æžœ $S&gt;=0$ï¼Œå¦‚æžœæœ‰æ‰¾å‡ºæœ€ä¼˜è§£çš„æ–¹æ³•ï¼Œåˆ™ä¸€å®šæœ‰è§£ã€‚ èµ·ç‚¹æ»¡è¶³ $g[i]-c[i]&gt;=0$ ï¼ŒæŠŠè¿™äº›ç‚¹ç§°ä¸ºæ­£ä½™é‡ç‚¹ã€‚ ç”¨ $\\mathcal{O}(1)$ ç®—å‡ºä»Žç¬¬ $i$ ä¸ªç‚¹å‡ºå‘åˆ°ç¬¬ $n$ ï¼ˆnå°±æ˜¯ç¬¬0ä¸ªç‚¹ï¼‰ ä¸ªç‚¹æ‰€ç§¯ç´¯çš„æ²¹é‡ï¼š $res[i] =S-sum[i-1]$ .å³ç”¨æ€»ç§¯ç´¯æ²¹é‡å‡åŽ»å‰ $i-1$ æ®µè·¯ç¨‹èƒ½ç§¯ç´¯çš„æ²¹é‡ï¼ˆä¸€èˆ¬ç§¯ç´¯ä¸ºè´Ÿï¼‰ã€‚(sumæ•°ç»„å°±æ˜¯ g-cçš„å‰ç¼€å’Œ) å¯¹äºŽæ»¡è¶³èµ·ç‚¹è¦æ±‚ $g[i]-c[i]&gt;=0$ çš„æ‰€æœ‰ç‚¹ï¼Œè®¡ç®—ä»Žç¬¬ $i$ ä¸ªç‚¹å‡ºå‘åˆ°ç¬¬ $n$ ä¸ªç‚¹åˆ°æ²¹é‡ç§¯ç´¯ã€‚é‚£ä¹ˆæœ‰æœ€å¤§æ²¹é‡ç§¯ç´¯çš„ç‚¹å³ä¸ºæœ€ä¼˜èµ·å§‹ç‚¹ã€‚ï¼ˆé¢˜ç›®è§„å®šå¦‚æžœå­˜åœ¨ï¼Œåˆ™ç‚¹å”¯ä¸€ï¼‰ å› æ­¤ï¼Œå› ä¸º $S$ å›ºå®šï¼Œåªéœ€è¦æ‰¾åˆ° $sum$ æ•°ç»„ä¸­çš„æœ€å°å€¼çš„ä¸‹æ ‡ï¼Œä¸‹æ ‡+1å³æ˜¯ç»“æžœã€‚ è¯æ˜Žå…¶æ­£ç¡®æ€§ï¼š å¦‚æžœæ»¡è¶³ $g[i]-c[i]&gt;=0$ çš„ä¸Šè¿°ç‚¹ $i,jï¼ˆi&lt;j)$ ï¼Œå¦‚æžœ $res[i]&gt;=res[j]$ ï¼Œè¯´æ˜Žä»Ž $i$ åˆ° $j$ æ˜¯æ­£æ²¹é‡ç§¯ç´¯ï¼Œè´ªå¿ƒçš„æ€æƒ³ï¼Œé‚£è‚¯å®šç§¯ç´¯çš„æ²¹é‡è¶Šå¤šè¶Šå¥½ï¼Œ $i$ æ¯” $j$ ä¼˜ã€‚ å¦‚æžœ $res[i]&lt;=res[j]$ ï¼Œè¯´æ˜Žä»Ž $i$ åˆ° $j$ æ˜¯è´Ÿæ²¹é‡ç§¯ç´¯ï¼Œå¦‚æžœä»Ž $i$ ç‚¹å‡ºå‘ï¼Œåˆ° $j$ ç‚¹å°±è´Ÿæ²¹é‡äº†ï¼›å¦‚æžœä»Ž $j$ ç‚¹å‡ºå‘ï¼Œè¯¥è½¦æœ€åŽå†è·‘ $i$ åˆ°$j$ æ®µï¼Œå› ä¸ºä¿è¯äº†æ€»ç§¯ç´¯æ²¹é‡æ˜¯æ­£ï¼Œæ‰€ä»¥æœ€åŽä¸€å®šæœ‰è¶³å¤Ÿçš„æ²¹é‡èƒ½è·‘å®Œ $i$ åˆ° $j$ æ®µã€‚ å†è¯åªè¦ $S&gt;=0$ åˆ™ä¸€å®šæœ‰è§£ã€‚æ˜¯åŠ¨æ€å°è¯•èµ·å§‹ç‚¹ï¼Œ( $i,j$ éƒ½æ»¡è¶³ $g[i]-c[i]&gt;=0$ ï¼‰ä»Žç‚¹ $i$ å¼€å§‹ï¼Œè·‘åˆ°ç‚¹ $j$ æ—¶ï¼Œå¦‚æžœè¯¥é€”ä¸­é€”æœ‰å‡ºçŽ°æ²¹é‡ä¸å¤Ÿï¼Œé‚£å°±æŠŠ $i$ åˆ° $j$ çš„è¿™æ®µè·¯ç¨‹æ”¾åˆ°è·¯é€”çš„åŽé¢æ¥è·‘ï¼Œç­‰æ²¹é‡ç§¯ç´¯å¤Ÿäº†å†è·‘è¿™æ®µã€‚ Code: 123456789101112131415161718from typing import Listclass Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: n = len(gas) sum = 0 remain = [] for g, c in zip(gas, cost): remain.append(g - c) sum += g - c if sum &lt; 0: return -1 for i in range(1, n): remain[i] += remain[i - 1] min_idx = remain.index(min(remain)) return (min_idx + 1) % n å‰ç¼€å’Œ list.index(value) æ‰¾å‡ºlistä¸­å€¼ä¸ºvalue çš„ç¬¬ä¸€ä¸ªä¸‹æ ‡ã€‚ min(list) è¿”å›žlistä¸­çš„æœ€å°å€¼ã€‚ 118-Pascalâ€™s Triangle118-Pascalâ€™s Triangle Problem: ç»™å®šä¸€ä¸ªæ•°å­—ï¼Œè¾“å‡ºå¦‚ä¸‹è§„åˆ™çš„å€¼ã€‚ Solutionï¼š æ³¨æ„è¾¹ç•Œå§ã€‚ï¼ˆä¸å¤ªå–œæ¬¢è¿™ç§é¢˜qwq 123456789101112from typing import Listclass Solution: def generate(self, numRows: int) -&gt; List[List[int]]: ans = [] for i in range(0, numRows): temp = [1] * (i+1) for j in range(1, i): temp[j] = ans[i-1][j-1] + ans[i-1][j] ans.append(temp) return ans Python ä¸­çš„appendä¼šå‡ºçŽ°å€¼è¢«è¦†ç›–çš„æƒ…å†µï¼šå˜é‡åœ¨å¾ªçŽ¯å¤–å®šä¹‰ï¼Œä½†åœ¨å¾ªçŽ¯ä¸­å¯¹è¯¥å˜é‡åšå‡ºä¸€å®šæ”¹å˜ï¼Œç„¶åŽappendåˆ°åˆ—è¡¨ï¼Œæœ€åŽå‘çŽ°åˆ—è¡¨ä¸­çš„å€¼éƒ½æ˜¯ä¸€æ ·çš„ã€‚ å› ä¸ºPythonä¸­å¾ˆå¤šæ—¶å€™éƒ½æ˜¯ä»¥å¯¹è±¡çš„å½¢å¼ç®¡ç†å¯¹è±¡ï¼Œå› æ­¤appendç»™åˆ—è¡¨çš„æ˜¯ä¸€ä¸ªåœ°å€ã€‚ 119-Pascalâ€™s Triangle II119-Pascalâ€™s Triangle II Problemï¼š ç»™å®šä¸€ä¸ªæ•°å­—ï¼Œè¾“å‡ºæŸä¸€è¡Œã€‚ Solutionï¼š 123456789101112from typing import Listclass Solution: def getRow(self, rowIndex: int) -&gt; List[int]: temp = [1] for i in range(0, rowIndex): for j in range(i, 0, -1): temp[j] += temp[j-1] temp.append(1) return temp 169-Majority Element169-Majority Element Problem: ç»™ä¸€ä¸²æ•°å­—ï¼Œæ‰¾åˆ°å‡ºçŽ°æ¬¡æ•°å¤§äºŽ n/2 çš„æ•°å­—ã€‚ Solutionï¼š ç”¨å­—å…¸è®¡æ•°ã€‚ 12345678910111213from typing import Listclass Solution: def majorityElement(self, nums: List[int]) -&gt; int: n = len(nums) cnt = {} for ele in nums: if ele in cnt: cnt[ele] += 1 else: cnt[ele] = 1 return max(cnt, key=cnt.get) è¿”å›žå€¼æœ€å¤§/æœ€å°çš„é”®/ç´¢å¼•ã€‚ åˆ—è¡¨ï¼š æœ€å¤§å€¼çš„ç´¢å¼•ï¼šlist.index(max(list)) æœ€å°å€¼çš„ç´¢å¼•ï¼šlist.index(min(list)) å­—å…¸ï¼š æœ€å¤§å€¼çš„é”®ï¼šmax(dict, key=dict.get) æœ€å°å€¼çš„é”®ï¼šmin(dict, key=dict.get) 229-Majority Element II229-Majority Element II Problem: ç»™ä¸€ä¸²æ•°å­—ï¼Œè¿”å›žå‡ºçŽ°æ¬¡æ•°å¤§äºŽ n/3 çš„æ•°å­—ã€‚ Solutionï¼š 1234567891011121314151617from typing import Listclass Solution: def majorityElement(self, nums: List[int]) -&gt; int: n = len(nums) cnt = {} ans = [] for ele in nums: if ele in cnt: cnt[ele] += 1 else: cnt[ele] = 1 for (k, v) in cnt.items(): if v &gt; n/3: ans.append(k) return ans å­—å…¸çš„å®žç”¨æ–¹æ³•ï¼š æ“ä½œ å®žçŽ°æ–¹æ³• åˆ é™¤å­—å…¸å…ƒç´  del dict['Name'] æ¸…ç©ºå­—å…¸æ‰€æœ‰æ¡ç›® dict.clear() åˆ é™¤å­—å…¸ del dict è¿”å›žæŒ‡å®šé”®çš„å€¼ï¼Œå¦‚æžœå€¼ä¸å­˜åœ¨è¿”å›ždefaultçš„å€¼ dict.get(key, default) å¦‚æžœé”®ä¸å­˜åœ¨å­—å…¸ä¸­ï¼Œæ·»åŠ é”®å¹¶å°†å€¼è®¾ä¸ºdefault,äºŽgetç±»ä¼¼ dict.setdefault(key, default=None) åˆ¤è¯»é”®æ˜¯å¦å­˜åœ¨ 1. if k in dict 2. dict.has_key(key) å­˜åœ¨è¿”å›žtrue ä»¥åˆ—è¡¨è¿”å›žå¯éåŽ†çš„ï¼ˆé”®ï¼Œå€¼ï¼‰å…ƒç¥–æ•°ç»„ dict.items() ä»¥åˆ—è¡¨è¿”å›žä¸€ä¸ªå­—å…¸çš„æ‰€æœ‰é”® dict.keys() ä»¥åˆ—è¡¨è¿”å›žå­—å…¸ä¸­çš„æ‰€æœ‰å€¼ dict.values() è¿”å›žæœ€å¤§å€¼çš„é”®å€¼ max(dict, key=dict.get) è¿”å›žæœ€å°å€¼çš„é”®å€¼ min(dict, key=dict.get) éåŽ†å­—å…¸çš„æ–¹æ³•ï¼š 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-dict={&quot;a&quot;:&quot;Alice&quot;,&quot;b&quot;:&quot;Bruce&quot;,&quot;J&quot;:&quot;Jack&quot;}# å®žä¾‹ä¸€ï¼šé”®å¾ªçŽ¯for i in dict: print &quot;dict[%s]=&quot; % i,dict[i]# ç»“æžœ:# dict[a]= Alice# dict[J]= Jack# dict[b]= Bruce# å®žä¾‹äºŒï¼šé”®å€¼å…ƒç»„å¾ªçŽ¯for i in dict.items(): print i# ç»“æžœ:# ('a', 'Alice')# ('J', 'Jack')# ('b', 'Bruce')# å®žä¾‹ä¸‰ï¼šé”®å€¼å…ƒç»„å¾ªçŽ¯for (k,v) in dict.items(): print &quot;dict[%s]=&quot; % k,v# ç»“æžœ:# dict[a]= Alice# dict[J]= Jack# dict[b]= Bruce 274-H-Index274-H-Index Problem: ç»™å‡ºç ”ç©¶äººå‘˜è®ºæ–‡çš„è®ºæ–‡å¼•ç”¨æ¬¡æ•°ï¼Œè®¡ç®—å®ƒçš„HæŒ‡æ•°ï¼ˆæœ‰hç¯‡è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°è‡³å°‘ä¸ºhï¼Œå‰©ä¸‹N-hç¯‡è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°ä¸è¶…è¿‡hï¼‰ã€‚ Solutionï¼š æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n\\log{n})$ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ æŽ’åºåŽï¼Œå†äºŒåˆ†ã€‚ï¼ˆæ„Ÿè§‰è‡ªå·±çš„äºŒåˆ†å†™çš„æœ‰ç‚¹ä¸‘qwq è¿˜æœ‰ä¸€ç§æ€è·¯æ˜¯ï¼ŒæŽ’åºå®Œï¼Œä»Žæœ€å¤§çš„hå¼€å§‹é€’å‡éåŽ†ï¼Œæ»¡è¶³æ¡ä»¶å°±è¿”å›žã€‚åæ­£æŽ’åºä¹Ÿè¦$\\mathcal{O}(n\\log{n})$ çš„å¤æ‚åº¦â€¦ 12345678910111213141516171819from typing import Listclass Solution: def hIndex(self, citations: List[int]) -&gt; int: n = len(citations) citations.sort() begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 h = n - mid if citations[mid] &gt;= h: end = mid if begin == end: return h else: begin += 1 return n-begin éžé€’å½’å†™äºŒåˆ†ï¼šwhile begin &lt;= end 275-H-Index II275-H-Index II Problem: å’Œ274ä¸€æ ·ï¼Œç»™äº†é€’å¢žçš„è®ºæ–‡å¼•ç”¨æ•°ï¼Œå¸Œæœ›èƒ½ç”¨æŒ‡æ•°æ—¶é—´è¿”å›žHæŒ‡æ•°ã€‚ Solutionï¼š å•Šï¼Œå°±äºŒåˆ†é¸­ã€‚ 123456789101112131415161718from typing import Listclass Solution: def hIndex(self, citations: List[int]) -&gt; int: n = len(citations) begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 h = n - mid if citations[mid] &gt;= h: end = mid if begin == end: return h else: begin += 1 return n - begin 243-Shortest Word Distanceqwq è¿™é“é¢˜è¿˜æ”¶è´¹æ¥ç€ï¼ŒäºŽæ˜¯äºŽæ˜¯å°±å¼€äº†ä¸ªä¸­å›½åŒºçš„ä¼šå‘˜ï¼ˆä¸­å›½åŒºçš„ä¼šå‘˜ä¾¿å®œå¥½å¤šå•Šï¼ï¼ï¼‰ 243-Shortest Word Distance Problemï¼š ç»™å®šä¸€ä¸²å•è¯ï¼Œå•è¯1å’Œå•è¯2ï¼Œè®¡ç®—å•è¯1å•è¯2åœ¨å•è¯åˆ—è¡¨ä¸­çš„è·ç¦»ã€‚ Solutionï¼š Solution Runtime Memory Language S1-äºŒåˆ†æŸ¥æ‰¾ 44ms 15.7MB python3 S2-çº¿æ€§ç»´æŠ¤ 40ms 15.6MB python3 S1-äºŒåˆ†æŸ¥æ‰¾æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n\\log{n})$ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ï¼ˆæœ€å¼€å§‹è¿˜å¾ˆç–‘æƒ‘å•¥æ˜¯å•è¯è·ç¦»â€¦å•è¯1å’Œå•è¯2å¯èƒ½åœ¨å•è¯åˆ—è¡¨ä¸­é‡å¤å‡ºçŽ°ï¼‰ è®¡ç®—å‡ºå•è¯1å’Œå•è¯2åœ¨å•è¯åˆ—è¡¨ä¸­å‡ºçŽ°çš„ç´¢å¼•å€¼åˆ—è¡¨ï¼Œæ˜¯é€’å¢žæœ‰åºçš„ã€‚ å¯¹äºŽå•è¯1ç´¢å¼•åˆ—è¡¨ä¸­çš„æ¯ä¸ªå€¼ï¼Œåœ¨å•è¯2ç´¢å¼•åˆ—è¡¨ä¸­æŸ¥æ‰¾è¯¥å€¼çš„lower_boundï¼Œè®¡ç®—è·ç¦»ã€‚ åŒç†ï¼Œå¯¹äºŽå•è¯2ç´¢å¼•åˆ—è¡¨ä¸­çš„æ¯ä¸ªå€¼ï¼Œä¹ŸåŒæ ·è®¡ç®—è·ç¦»ã€‚ æ‰¾å‡ºæœ€å°è·ç¦»ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from typing import Listdef lower_bound(a: list, x: int) -&gt; int: n = len(a) begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 if a[mid] &gt;= x: end = mid if begin == end: return begin else: begin += 1 return -1class Solution: ans = None def findShortest(self, li1: list, li2: list) -&gt; None: for idx in li1: min_dis_idx = lower_bound(li2, idx) if min_dis_idx == -1: continue else: self.ans = min(self.ans, li2[min_dis_idx] - idx) if self.ans == 1: return def shortestDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) li1 = [] li2 = [] self.ans = n for idx in range(n): # li1 and li2 are ordered if words[idx] == word1: li1.append(idx) if words[idx] == word2: li2.append(idx) self.findShortest(li1, li2) if self.ans &gt; 1: self.findShortest(li2, li1) return self.ans S2-çº¿æ€§ç»´æŠ¤ï¼šæ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ ç©ºé—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ é—®é¢˜è¿˜èƒ½å†ç®€åŒ–ï¼Œçº¿æ€§æ‰«æå•è¯åˆ—è¡¨ï¼Œç»´æŠ¤ä¸¤ä¸ªå˜é‡ï¼Œå•è¯1å‡ºçŽ°çš„æœ€è¿‘ç´¢å¼•ï¼Œå•è¯2å‡ºçŽ°çš„æœ€è¿‘ç´¢å¼•ã€‚æ‰«ææ—¶è®¡ç®—è·ç¦»ï¼Œæ¯å½“å•è¯1æˆ–å•è¯2å‡ºçŽ°æ—¶ï¼Œå°±ç”¨å¦ä¸€ä¸ªå•è¯çš„æœ€è¿‘ç´¢å¼•è®¡ç®—ã€‚ 12345678910111213141516171819from typing import Listclass Solution: def shortestDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) lst1 = None lst2 = None ans = n for idx in range(n): if words[idx] == word1: lst1 = idx if lst2 is not None: ans = min(ans, lst1-lst2) if words[idx] == word2: lst2 = idx if lst1 is not None: ans = min(ans, lst2-lst1) return ans C++ä¸­ï¼š lower_bound(begin, end, num)ï¼šè¿”å›žnumçš„ä¸‹ç•Œï¼Œå³å¤§äºŽç­‰äºŽnumçš„ç¬¬ä¸€ä¸ªç´¢å¼•ä½ç½®ã€‚ upper_bound(begin, end, num)ï¼šè¿”å›žnumçš„ä¸Šç•Œï¼Œå³å¤§äºŽnumçš„ç¬¬ä¸€ä¸ªç´¢å¼•ä½ç½®ã€‚ Pythonä¸­ç”¨äºŒåˆ†å®žçŽ°è¿™ä¸¤ä¸ªå‡½æ•°ã€‚ 244-Shortest Word Distance II244-Shortest Word Distance II Problem: å’Œä¸Šé¢˜é¢˜å¹²ç±»ä¼¼ï¼Œè®¡ç®—å•è¯è·ç¦»ï¼Œä½†æ˜¯æ­¤é—®æ˜¯æ¯ä¸€ä¸ªå•è¯åˆ—è¡¨ï¼Œå¯èƒ½æœ‰å¤šä¸ªè¯¢é—®ã€‚ Solutionï¼š å¯¹æ¯ä¸€ä¸ªå•è¯åˆ—è¡¨ï¼Œéƒ½å¯èƒ½æœ‰å¤šä¸ªè¯¢é—®ã€‚ å› æ­¤ï¼Œä¹‹å‰243çš„è§£æ³•æ¯æ¬¡è¯¢é—®éƒ½ä¼šéåŽ†ä¸€éå•è¯åˆ—è¡¨ã€‚å¦‚æžœå¯¹æ¯ä¸ªå•è¯åˆ—è¡¨è¯¢é—®æ•°ä¸º $M$ ï¼Œé‚£ä¹ˆæ—¶é—´å¤æ‚åº¦ä¸º $\\mathcal{O}(NM)$ ï¼Œä¼šè¶…æ—¶ï¼Œæ‰€ä»¥å¸Œæœ›èƒ½å°†å•è¯åˆ—è¡¨çš„æœ‰å…³ä¿¡æ¯å­˜ä¸‹æ¥ï¼Œå†ç”¨å¸¸æ•°æ—¶é—´å¤„ç†æ¯ä¸€ä¸ªè¯¢é—®ã€‚ è¿™é‡Œçš„è§£æ³•æ˜¯ç”¨ä¸€ä¸ªå­—å…¸æŠŠæ¯ä¸ªå•è¯å‡ºçŽ°çš„indexåˆ—è¡¨å­˜ä¸‹æ¥ï¼Œé”®æ˜¯å•è¯ï¼Œå€¼æ˜¯indexåˆ—è¡¨ã€‚è¿™ä¸ªåˆ—è¡¨ç›¸å¯¹äºŽå•è¯åˆ—è¡¨çš„æ•°ç›®åº”è¯¥æ˜¯è¿œè¿œå°äºŽçš„ï¼Œå› æ­¤ç”¨äºŒé‡å¾ªçŽ¯åº”è¯¥ä¹Ÿèƒ½è¿‡å§ï¼ˆæ²¡æœ‰å°è¯•äºŒé‡å¾ªçŽ¯è§£æ³•ï¼‰ è¿™é‡Œæœ‰ä¸¤ç§æ€è·¯ï¼Œä¸€ç§æ˜¯è‡ªå·±æƒ³çš„å½’å¹¶æ€è·¯ï¼Œè¿˜æœ‰ä¸€ç§æ˜¯å®˜æ–¹è§£ç­”çš„æ€è·¯ï¼Œå®˜æ–¹æ€è·¯æ¯”å½’å¹¶çš„æ€è·¯æ›´ä¼˜é›…ä¸€äº›ï¼Œé—®é¢˜æŠ½è±¡çš„æ›´å¥½ã€‚ï¼ˆä»£ç å·®è·ä¸å¤§ï¼Œæ—¶é—´å·®è·ä¹Ÿä¸å¤ªå¤§ï¼‰ Solution Runtime Memory Language S1-å½’å¹¶æ€è·¯æŸ¥è¯¢ 96ms 20.8MB python3 S2-äº¤å‰è·³è·ƒæŸ¥è¯¢ 80ms 20.4MB python3 åŒæŒ‡é’ˆï¼š$i$ æŒ‡å‘åˆ—è¡¨1ï¼Œ$j$æŒ‡å‘åˆ—è¡¨2. S1-å½’å¹¶æ€è·¯å½’å¹¶æ€è·¯ï¼šåˆ—è¡¨çš„å€¼éƒ½æ˜¯æœ‰åºçš„ï¼Œå†æŠŠä¸¤ä¸ªåˆ—è¡¨çš„å€¼æŒ‰å½’å¹¶çš„æ€æƒ³å†æŽ’åºï¼Œå¯ä»¥æƒ³æˆæŠŠç‚¹ä¸€ä¸ªä¸€ä¸ªæœ‰åºæ”¾åœ¨æ•°è½´ä¸Šã€‚ $i$æŒ‡é’ˆå‰è¿›çš„æƒ…å†µï¼šï¼ˆæŽ’åºæ—¶ï¼Œå–åˆ—è¡¨1çš„ä¸‹ä¸€ä¸ªæ•°å­—ï¼‰ $i+1&lt;len1$ and $li1[i+1] &lt; li[j]$ $i+1&lt;len1$ and $li1[i+1] &lt; li[j+1]$ $i+1 &lt; len1$ and $j+1 == len2$ ($j$ å·²æ— æ³•ç§»åŠ¨) å…¶ä½™æƒ…å†µï¼š$j$ ç§»åŠ¨ã€‚ 1234567891011121314151617181920212223242526272829303132333435from typing import Listclass WordDistance: def __init__(self, words: List[str]): self.words = words self.len = len(words) self.dict = {} for index in range(self.len): word = words[index] if word in self.dict: temp = self.dict[word] temp.append(index) else: temp = [index] self.dict[word] = temp def shortest(self, word1: str, word2: str) -&gt; int: ans = self.len li1 = self.dict[word1] li2 = self.dict[word2] len1 = len(li1) len2 = len(li2) i = j = 0 while i &lt; len1 and j &lt; len2: ans = min(ans, abs(li1[i] - li2[j])) if ans == 1: return ans # i goes ahead if i + 1 &lt; len1 and ((li1[i + 1] &lt; li2[j]) or (j+1 == len2) or (j + 1 &lt; len2 and li1[i + 1] &lt; li2[j + 1])): i += 1 else: j += 1 return ans S2-äº¤å‰æ¯”è¾ƒå¯¹äºŽå½“å‰æŒ‡å‘åˆ—è¡¨1å’Œåˆ—è¡¨2çš„ä¸¤ä¸ªå…ƒç´  $li1[i]$ å’Œ $li2[j]$ ï¼Œå¯¹ $li1[i]$æ¥è¯´ï¼Œåªéœ€è¦å’Œæ—è¾¹çš„å±žäºŽåˆ—è¡¨2çš„å…ƒç´ æ¯”è¾ƒï¼Œå¯¹ $li2[j]$ åŒç†ã€‚ å› æ­¤ï¼Œå½“ $li1[i]&gt;li2[j]$ æ—¶ï¼Œä¸‹ä¸€æ¬¡çš„æ¯”è¾ƒåº”è¯¥è®© $j$ æŒ‡é’ˆå‰ç§»ä¸€ä½ï¼Œç»§ç»­è®¡ç®—æŒ‡é’ˆ $i$ æ‰€æŒ‡å…ƒç´ å’Œå…¶æ—è¾¹çš„åˆ—è¡¨2çš„å…ƒç´ ã€‚åŒç†ï¼Œå½“ $li1[i]&lt;li2[j]$ æ—¶ï¼Œä¸‹ä¸€æ¬¡çš„æ¯”è¾ƒåº”è¯¥è®© $i$ æŒ‡é’ˆå‰ç§»ä¸€ä½ï¼Œç»§ç»­è®¡ç®—æŒ‡é’ˆ $j$ å’Œå…¶æ—è¾¹çš„åˆ—è¡¨1çš„å…ƒç´ ã€‚ å…·ä½“ç§»åŠ¨å¦‚ä¸‹å›¾ã€‚ 1234567891011121314151617181920212223242526272829303132333435from typing import Listclass WordDistance: def __init__(self, words: List[str]): self.words = words self.len = len(words) self.dict = {} for index in range(self.len): word = words[index] if word in self.dict: temp = self.dict[word] temp.append(index) else: temp = [index] self.dict[word] = temp def shortest(self, word1: str, word2: str) -&gt; int: ans = self.len li1 = self.dict[word1] li2 = self.dict[word2] len1 = len(li1) len2 = len(li2) i = j = 0 while i &lt; len1 and j &lt; len2: ans = min(ans, abs(li1[i] - li2[j])) if ans == 1: return ans # i goes ahead if li1[i] &lt; li2[j]: i += 1 else: j += 1 return ans 277-Find the Celebrity277-Find the Celebrity Problem: å·²æœ‰know(i, j) APIï¼Œåˆ¤æ–­iæ˜¯å¦çŸ¥é“jï¼Œiæ˜¯åäººçš„å……è¦æ¡ä»¶æ˜¯å…¶ä»–æ‰€æœ‰äººçŸ¥é“iï¼Œè€Œiä¸çŸ¥é“å…¶ä»–æ‰€æœ‰äººã€‚ Solutionï¼š ä¸¤ç§æ€è·¯ï¼Œç¬¬ä¸€ç§è¾ƒä¸ºç›´è§‚ï¼Œä½¿ç”¨äºŒé‡å¾ªçŽ¯ï¼Œä½†å‰ªæžå¤šï¼Œè¿œè¾¾ä¸åˆ° $\\mathcal{O}(n^2)$ ï¼Œç¬¬äºŒç§ç¨åšä¼˜åŒ–ã€‚å› æ­¤ä¸¤ç§è§£æ³•å·®è·ä¸å¤ªå¤§ã€‚ æäº¤æ—¶é—´ è¿è¡Œæ—¶é—´ å†…å­˜æ¶ˆè€— è¯­è¨€ Så‡ ç§’å‰ 1896ms 13.6MB python3 5 åˆ†é’Ÿå‰ 1772ms 13.6MB python3 S1-ç›´è§‚æ€è·¯æ—¶é—´å¤æ‚åº¦ï¼šè¿œä¸åˆ° $\\mathcal{O}(n^2)$ ç”¨äº†äºŒé‡å¾ªçŽ¯ï¼Œä½†å‰ªæžå¾ˆå¤šï¼Œæ‰€ä»¥è¿œè¾¾ä¸åˆ° $\\mathcal{O}(n^2)$ 12345678910111213141516171819202122232425class Solution: def findCelebrity(self, n: int) -&gt; int: for i in range(n): fg = True # i knows j ? for j in range(n): if i == j: continue if knows(i, j): fg = False break if not fg: continue # j knows i ? for j in range(n): if i == j: continue if not knows(j, i): fg = False break if not fg: continue else: return i return -1 S2-æŽ’é™¤æ³•æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(n)$ æŽ’é™¤iï¼šæ ¹æ®know(i, j)=True å¯ä»¥è®¤ä¸ºiä¸æ˜¯åäººï¼Œjå¯èƒ½æ˜¯åäººã€‚ å¯¹äºŽn-1ä¸ªå…³ç³»ï¼Œæœ€åŽä»Žnä¸ªäººä¸­é€‰å‡ºä¸€ä¸ªå¯èƒ½çš„äººï¼Œå†æ ¹æ®åäººçš„å……è¦æ¡ä»¶åŽ»åˆ¤æ–­ä»–æ˜¯å¦æ˜¯åäººã€‚ 123456789101112131415class Solution: def findCelebrity(self, n: int) -&gt; int: celebrity = 0 for i in range(1, n): if knows(celebrity, i): celebrity = i continue for i in range(n): if celebrity == i: continue if (not knows(celebrity, i)) and knows(i, celebrity): continue else: return -1 return celebrity 245-Shortest Word Distance III245-Shortest Word Distance III Problem: é¢˜æ„å¢žåŠ äº†ä¸¤ä¸ªå•è¯å¯èƒ½ç›¸åŒï¼Œåˆ†ä¸¤ç§æƒ…å†µå°±å¥½äº†ã€‚ Solutionï¼š 123456789101112131415161718192021222324from typing import Listclass Solution: def shortestWordDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) ptr1 = None ptr2 = None ans = n if word1 == word2: for idx in range(n): if words[idx] == word1: ptr1, ptr2 = idx, ptr1 if (ptr1 is not None) and (ptr2 is not None): ans = min(ans, ptr1-ptr2) else: for idx in range(n): if words[idx] == word1: ptr1 = idx if words[idx] == word2: ptr2 = idx if (ptr1 is not None) and (ptr2 is not None): ans = min(ans, abs(ptr1-ptr2)) return ans 217-Contains Duplicate[E]217-Contains Duplicate Problem: åˆ¤æ–­æ•°ç»„ä¸­æœ‰æ— é‡å¤å…ƒç´ å‡ºçŽ°ã€‚ Solutionï¼š 12345678class Solution: def containsDuplicate(self, nums: List[int]) -&gt; bool: S = set() for i in nums: if i in S: return True S.add(i) return False 219-Contains Duplicate II[E]219-Contains Duplicate II Problem: åˆ¤æ–­æ•°ç»„ä¸­æ˜¯å¦æœ‰ä¸¤ä¸ªç›¸åŒçš„å€¼ï¼Œä»–ä»¬çš„ç´¢å¼•ä¹‹å·®å°äºŽç­‰äºŽkã€‚ Solutionï¼š å­˜æ”¾å‡ºçŽ°è¯¥å€¼çš„æœ€è¿‘çš„ç´¢å¼•ï¼Œæ‰«ä¸€éã€‚ 123456789101112class Solution: def containsNearbyDuplicate(self, nums: List[int], k: int) -&gt; bool: dict = {} n = len(nums) for i in range(n): if nums[i] not in dict: dict.setdefault(nums[i], i) else: if i - dict[nums[i]] &lt;= k: return True dict[nums[i]] = i return False 4-Median of Two Sorted Arrays[H] ï¼ˆ2Sï¼‰4-Median of Two Sorted Arrays[H] Problem: ç»™ä¸¤ä¸ªæŽ’å¥½åºçš„æ•°ç»„ï¼Œè¿”å›žä¸€ä¸ªä¸­ä½æ•° Solutionï¼š S è¿è¡Œæ—¶é—´ å†…å­˜æ¶ˆè€— S1 $\\mathcal{O}(m+n)$ ï¼š92ms 14.3MB S2 $\\mathcal{O}(\\log(m+n))$ ï¼š52ms 13.3MB S1:å½’å¹¶æŽ’åºçš„åšæ³•ã€‚ æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(m+n)$ 12345678910111213141516171819202122232425262728293031from typing import Listclass Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: n1 = len(nums1) n2 = len(nums2) median1 = median2 = None i = j = 0 tot = 0 while i &lt; n1 or j &lt; n2: if (i &lt; n1 and j &lt; n2 and nums1[i] &lt;= nums2[j]) or (i &lt; n1 and j &gt;= n2): tot += 1 if tot == (n1 + n2)//2: median1 = nums1[i] if tot == (n1 + n2)//2 + 1: median2 = nums1[i] break i += 1 else: tot += 1 if tot == (n1 + n2) // 2: median1 = nums2[j] if tot == (n1 + n2) // 2 + 1: median2 = nums2[j] break j += 1 if (n1 + n2) % 2 == 1: return median2 else: return (median1 + median2)/2 S2:äºŒåˆ†çš„æ€è·¯ æ—¶é—´å¤æ‚åº¦ï¼š$\\mathcal{O}(\\log(m+n))$ ä¸€ã€é¦–å…ˆè®¨è®ºä¸€ä¸ªæ•°ç»„çš„ä¸­ä½æ•°ï¼Œæ•°ç»„æœ‰nä¸ªå…ƒç´ ï¼Œå¦‚æžœnä¸ºå¥‡æ•°ï¼Œåˆ™ç¬¬(n+1)/2ä¸ªæ˜¯ä¸­ä½æ•°ï¼›å¦‚æžœnä¸ºå¶æ•°ï¼Œåˆ™ç¬¬(n+1)/2å’Œç¬¬(n+2)/2çš„å¹³å‡å€¼ä¸ºä¸­ä½æ•°ã€‚ å›žåˆ°æœ¬é¢˜ï¼Œå› ä¸ºæ˜¯ä¸¤ä¸ªæ•°ç»„ï¼Œå¦‚æžœæ ¹æ®å¥‡å¶æ€§åˆ†ç±»è®¨è®ºå°±è¿‡äºŽéº»çƒ¦äº†ï¼Œæ‰€ä»¥å°†ä¸¤ç§æƒ…å†µç»Ÿä¸€ä»¥ç®€åŒ–è§£é¢˜æ€è·¯ã€‚ å³æ— è®ºnæ˜¯å¥‡æ•°è¿˜æ˜¯å¶æ•°ï¼Œæ•°ç»„çš„ä¸­ä½æ•°éƒ½æ˜¯ç¬¬(n+1)/2å’Œç¬¬(n+2)/2çš„å¹³å‡æ•°ã€‚ å›žåˆ°æœ¬é¢˜ï¼Œè®¾æ•°ç»„1æœ‰nä¸ªå…ƒç´ ï¼Œæ•°ç»„2æœ‰mä¸ªå…ƒç´ ï¼Œåˆ™ä¸­ä½æ•°ä¸ºä¸¤ä¸ªæ•°ç»„çš„æœ‰åºåºåˆ—çš„ç¬¬(n+m+1)/2ä¸ªå’Œç¬¬(n+m+2)/2ä¸ªçš„å¹³å‡æ•°ã€‚ äºŒã€å› æ­¤ï¼Œé¢˜ç›®éœ€è¦æ±‚è§£çš„é—®é¢˜æ”¹ä¸ºæ±‚è¿™ä¸¤ä¸ªæœ‰åºæ•°ç»„çš„æœ‰åºåºåˆ—çš„ç¬¬kä¸ªæ•°ã€‚ äºŒåˆ†æ€æƒ³ï¼šä¸¤ä¸ªæ•°ç»„åˆ†åˆ«æ‰¾ç¬¬k/2ä¸ªæ•°ï¼Œï¼ˆå‡è®¾éƒ½å­˜åœ¨ï¼‰ï¼Œæ¯”è¾ƒï¼Œå¦‚æžœç¬¬ä¸€ä¸ªæ•°ç»„çš„è¿™ä¸ªæ•°å°äºŽç¬¬äºŒä¸ªæ•°ç»„ï¼Œè¯´æ˜Žç¬¬kä¸ªæ•°è‚¯å®šä¸åœ¨ç¬¬ä¸€ä¸ªæ•°ç»„çš„å‰k/2ä¸ªæ•°ä¸­ï¼Œå› æ­¤å°±å¯ä»¥ç›´æŽ¥åŽ»æŽ‰æ•°ç»„1çš„å‰k/2ä¸ªå…ƒç´ ï¼ŒæŸ¥æ‰¾æœ‰åºåºåˆ—çš„ç¬¬k-k/2ä¸ªæ•°ï¼›åŒç†ï¼Œå¦‚æžœå¤§äºŽï¼Œåˆ™è¯´æ˜Žç¬¬kä¸ªæ•°è‚¯å®šä¸åœ¨ç¬¬äºŒä¸ªæ•°ç»„çš„å‰k/2ä¸ªæ•°ä¸­ï¼ŒåŽ»æŽ‰æ•°ç»„2çš„å‰k/2ä¸ªå…ƒç´ ã€‚ ä½¿ç”¨ä¸€ä¸ªæ•°ç»„èµ·å§‹æŒ‡é’ˆl1å’Œl2æ¥å®žçŽ°æ•°ç»„çš„â€œåŽ»æŽ‰â€å‰k/2ä¸ªå…ƒç´ ã€‚ è®¾æ•°ç»„1çš„å…ƒç´ ä¸ªæ•°ä¸ºnï¼Œæ•°ç»„2çš„å…ƒç´ ä¸ªæ•°ä¸ºmã€‚ é€’å½’å‡½æ•°Find(l1, l2, k)ï¼šæŸ¥æ‰¾èµ·å§‹æŒ‡é’ˆä¸ºl1, l2çš„ä¸¤ä¸ªæœ‰åºæ•°ç»„çš„ç¬¬kä¸ªæ•°ã€‚ è®¨è®ºè¾¹ç•Œæƒ…å†µï¼Œæœ‰æ•°ç»„ä¸ºç©ºçš„æƒ…å†µã€‚å³ l1 == n æˆ–è€… l2 == m . å¦‚æžœç¬¬ä¸€ä¸ªæ•°ç»„å·²ä¸ºç©ºï¼Œåˆ™ç›´æŽ¥è¿”å›žç¬¬äºŒä¸ªæ•°ç»„çš„ç¬¬kä¸ªæ•°ï¼› åŒç†ï¼Œå¦‚æžœç¬¬äºŒä¸ªæ•°ç»„ä¸ºç©ºï¼Œåˆ™ç›´æŽ¥è¿”å›žç¬¬ä¸€ä¸ªæ•°ç»„çš„ç¬¬kä¸ªæ•°ã€‚ ä¸¤ä¸ªæ•°ç»„éƒ½ä¸ä¸ºç©ºçš„æƒ…å†µã€‚å³ l1 &lt; n æˆ–è€… l2 &lt; m . é€’å½’è¾¹ç•Œï¼š k == 1 ,å³è¿”å›ž nums1[l1] å’Œ nums2[l2] ä¸­è¾ƒå°çš„é‚£ä¸€ä¸ªã€‚ æ•°ç»„é•¿åº¦è¾¹ç•Œï¼šå³æœ‰æ•°ç»„çš„å‰©ä½™å…ƒç´ ä¸ªæ•°å°äºŽ k/2 ï¼Œé‚£ä¹ˆæ‹¿å‡ºæ¥æ¯”è¾ƒçš„å°±åº”è¯¥æ˜¯æ•°ç»„çš„æœ€åŽä¸€ä¸ªå…ƒç´ ã€‚ ç»´æŠ¤ä¸¤ä¸ªå€¼ k1 å’Œ k2 æ¥åˆ†åˆ«è¡¨ç¤ºç”¨ä¸¤ä¸ªæ•°ç»„çš„ç¬¬ k1å’Œk2ä¸ªæ¥æ¯”è¾ƒã€‚ (k1 k2éƒ½å°äºŽç­‰äºŽk/2) 123# to avoid the rest length of nums1/nums2 is shorter than k//2k1 = k//2 if l1+k//2 &lt;= n else n-l1k2 = k//2 if l2+k//2 &lt;= m else m-l2 æ¯”è¾ƒnums1[l1+k1-1] å’Œ nums2[l2+k2-1] çš„å¤§å°ï¼Œé€’å½’ï¼š ç›¸ç­‰ï¼š å¦‚æžœ k-k1-k2 == 0 è¯´æ˜Žnums1çš„å‰k1ä¸ªå’Œnums2çš„å‰k2ä¸ªå°±æ˜¯æœ‰åºåºåˆ—çš„å‰kä¸ªï¼Œè¿”å›ž nums1[l1+k1-1] ã€‚ å¦åˆ™ï¼Œï¼ˆå³æŸä¸€ä¸ªæ•°ç»„çš„å‰©ä½™é•¿åº¦å°äºŽk/2ï¼‰ï¼Œåˆ†åˆ«åŽ»æŽ‰ä¸¤ä¸ªæ•°ç»„çš„å‰k1å’Œk2ä¸ªæ•°ï¼Œé€’å½’è°ƒç”¨ Find(l1+k1, l2+k2, k-k1-k2) ã€‚ nums1[l1+k1-1] &gt; nums2[l2+k2-1] è¯´æ˜Žå¯ä»¥åŽ»æŽ‰æ•°ç»„2çš„å‰k2ä¸ªæ•°ï¼Œé€’å½’è°ƒç”¨ Find(l1, l2+k2, k-k2) nums1[l1+k1-1] &gt; nums2[l2+k2-1] è¯´æ˜Žå¯ä»¥åŽ»æŽ‰æ•°ç»„1çš„å‰k1ä¸ªæ•°ï¼Œé€’å½’è°ƒç”¨ Find(l1+k1, l2, k-k1) Codeï¼š 123456789101112131415161718192021222324252627282930313233343536class Solution: def __init__(self): self.nums1 = None self.nums2 = None def findKthOfTwo(self, l1: int, l2: int, k: int) -&gt; int: nums1 = self.nums1 nums2 = self.nums2 n = len(nums1) m = len(nums2) # nums1 is empty if l1 == n: return nums2[l2+k-1] # nums2 is empty if l2 == m: return nums1[l1+k-1] # both not empty if k == 1: return nums1[l1] if nums1[l1] &lt;= nums2[l2] else nums2[l2] # to avoid the rest length of nums1/nums2 is shorter than k//2 k1 = k//2 if l1+k//2 &lt;= n else n-l1 k2 = k//2 if l2+k//2 &lt;= m else m-l2 if nums1[l1+k1-1] == nums2[l2+k2-1]: return nums1[l1+k1-1] if k-k1-k2 == 0 else self.findKthOfTwo(l1+k1, l2+k2, k-k1-k2) elif nums1[l1+k1-1] &gt; nums2[l2+k2-1]: return self.findKthOfTwo(l1, l2+k2, k-k2) else: return self.findKthOfTwo(l1+k1, l2, k-k1) def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: n = len(nums1) m = len(nums2) self.nums1 = nums1 self.nums2 = nums2 # median: the average of (n+m+1)//2 th and (n+m+2)//2 th return (self.findKthOfTwo(0, 0, (n+m+1)//2) + self.findKthOfTwo(0, 0, (n+m+2)//2)) / 2 reference ã€Šä¿¡æ¯å®‰å…¨æ•°å­¦åŸºç¡€ã€‹ 2.2åŒä½™ç±»å’Œå‰©ä½™ç³»ã€‚","link":"/2020/09/14/LeetCode_array/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šRegression","text":"åœ¨YouTubeä¸Šçœ‹å°å¤§æŽå®æ¯…è€å¸ˆçš„è¯¾ï¼Œçœ‹å®ŒRegressionè®²åº§çš„æ„Ÿå—å°±æ˜¯ï¼š å¥½æƒ³åŽ»æŠ“Pokemonï¼ï¼ï¼ è¿™ç¯‡æ–‡ç« å°†æ€»ç»“æŽå®æ¯…è€å¸ˆRegressionçš„è®²åº§ï¼Œå¹¶å°è¯•å®žçŽ°å…¶demoã€‚ Regressionï¼ˆå›žå½’ï¼‰DefineRegressionï¼šæ˜¯æ‰¾åˆ°ä¸€ä¸ª$function$ï¼Œè¿›è¡Œé¢„æµ‹ã€‚å¯¹è¾“å…¥çš„featureï¼Œè¾“å‡ºä¸€ä¸ª$Scalar$(æ•°å€¼ï¼Œæ ‡é‡)ã€‚ Example Application Look for a $function$ Stock Market Forecastï¼ˆè‚¡ç¥¨é¢„æµ‹ï¼‰ $input$ï¼šè¿‡åŽ»çš„è‚¡ä»·ä¿¡æ¯ $output$ï¼šæ˜Žå¤©çš„è‚¡ä»·å¹³å‡å€¼ï¼ˆ$Scalar$) Self-Driving Car(è‡ªåŠ¨é©¾é©¶) $input$ï¼šè·¯å†µä¿¡æ¯ $output$ï¼šæ–¹å‘ç›˜è§’åº¦ï¼ˆ$Scalar$) Recommendationï¼ˆæŽ¨èç³»ç»Ÿï¼‰ $input$ï¼šä½¿ç”¨è€…Aã€å•†å“B $output$ï¼šä½¿ç”¨è€…Aè´­ä¹°å•†å“Bçš„å¯èƒ½æ€§ å¯è§ï¼Œ$input$éƒ½æ˜¯ä¸€äº›ç‰¹å¾ä¿¡æ¯ï¼Œ$output$éƒ½æ˜¯ä¸€ä¸ªæ ‡é‡æ•°å€¼ï¼Œè¿™å°±æ˜¯Regressionã€‚ Regression Case: Pokenmon çœ‹å®Œè¿™èŠ‚è¯¾ï¼Œæ„Ÿæƒ³ï¼šå¥½æƒ³åŽ»æŠ“å®å¯æ¢¦QAQ é¢„æµ‹ä¸€ä¸ªpokemonè¿›åŒ–åŽçš„CPï¼ˆCombat Powerï¼Œæˆ˜æ–—åŠ›ï¼‰å€¼ã€‚ ä¸ºä»€ä¹ˆè¦é¢„æµ‹å‘ï¼Ÿ å¦‚æžœè¿›åŒ–åŽçš„CPå€¼é«˜ï¼Œå°±è¿›åŒ–ä»–ï¼Œä¸ç„¶å°±æŠŠä»–å½“ç³–æžœï¼Œå› ä¸ºå®å¯æ¢¦å¾ˆéš¾æŠ“çš„ã€‚ï¼ˆï¼Ÿæ²¡çŽ©è¿‡ï¼Œæˆ‘ä¹Ÿä¸æ‡‚o r zï¼‰ ä¸Šå›¾å¦™è›™ç§å­çš„ä¿¡æ¯(å¯èƒ½çš„$input$)ï¼š $x_{cp}$ï¼šCPå€¼ $x_s$:ç‰©ç§ $x_{hp}$:ç”Ÿå‘½å€¼ $x_w$:é‡é‡ $x_h$:é«˜åº¦ outputï¼šè¿›åŒ–åŽçš„CPå€¼ã€‚ $x_{cp}$ï¼šç”¨ä¸‹æ ‡è¡¨ç¤ºä¸€ä¸ªobjectçš„componentã€‚ $x^1$ï¼šç”¨ä¸Šæ ‡è¡¨ç¤ºä¸€ä¸ªå®Œæ•´çš„objectã€‚ Step 1: æ‰¾ä¸€ä¸ªModelï¼ˆfunction setï¼‰Model ï¼š$y = b + w \\cdot x_{cp}$ å‡è®¾ç”¨ä¸Šå¼ä½œä¸ºæˆ‘ä»¬çš„Modelï¼Œé‚£ä¹ˆè¿™äº›å‡½æ•°ï¼š $ \\begin{aligned} &\\mathrm{f}_{1}: \\mathrm{y}=10.0+9.0 \\cdot \\mathrm{x}_{\\mathrm{cp}}\\\\ &f_{2}: y=9.8+9.2 \\cdot x_{c p}\\\\ &f_{3}: y=-0.8-1.2 \\cdot x_{c p} \\end{aligned} $ ç­‰éƒ½å±žäºŽè¿™ä¸ªé›†åˆï¼Œä½†æ˜¯æ˜¾ç„¶åƒ$f_3$è¿™ç§å‡½æ•°æ˜¯badï¼ŒCPå€¼ä¸å¯èƒ½æ˜¯è´Ÿæ•°ã€‚bad functions å¾ˆå¤šï¼Œæ‰€ä»¥åœ¨ä¸‹é¢çš„æ­¥éª¤ï¼Œä¼šè¯´æ˜Žå¦‚ä½•åˆ¤åˆ«ä¸€ä¸ªå‡½æ•°çš„å¥½åï¼Œè‡ªåŠ¨çš„é€‰å‡ºæœ€å¥½çš„é‚£ä¸ª $function$ã€‚ æŠŠModel 1ä¸€èˆ¬åŒ–ï¼Œå¾—åˆ°çº¿ä»£ä¸­çš„ Linear Modelï¼š$y = b+\\sum w_ix_i$ $x_i$ï¼šxçš„feature $b$ï¼šbias,åç½®å€¼ $w_i$ï¼šweightï¼Œæƒé‡ Step 2: åˆ¤åˆ«Goodness of Function(Training Data)Training Dataå‡å®šä½¿ç”¨Model ï¼š$y = b + w \\cdot x_{cp}$ Training Dataï¼šååªå®å¯æ¢¦ï¼Œç”¨å‘é‡çš„å½¢å¼è¡¨ç¤ºã€‚ ä½¿ç”¨Training dataæ¥judge the goodness of function.ã€‚ Loss Function(æŸå¤±å‡½æ•°)æ¦‚çŽ‡è®ºï¼šåšçº¿æ€§å›žå½’ï¼Œä¸€èˆ¬ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ã€‚ä¸€èˆ¬å›žå½’ï¼Œå¤§å¤šä½¿ç”¨æžå¤§ä¼¼ç„¶ä¼°è®¡ã€‚ Loss function $L$ ï¼š$L(f)=L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ å…¶ä¸­çš„ $\\hat{y}^n-(b+w\\cdot x_{cp}^n)$æ˜¯Estimation error(ä¼°æµ‹è¯¯å·®) Loss Functionçš„æ„ä¹‰ï¼šå®ƒçš„ $input$æ˜¯ä¸€ä¸ª $function$ï¼Œå®ƒçš„ $output$ä½“çŽ°äº†how bad it is,è¿™ä¸ªå‡½æ•°æœ‰å¤šç³Ÿ/å¥½ã€‚ Figure the Result ä¸Šå›¾æ¨ªçºµåæ ‡æ˜¯å‡½æ•° $L$çš„å‚æ•° $w ã€b$ï¼Œå›¾ä¸­çš„æ¯ä¸€ä¸ªpointéƒ½æ˜¯ä¸€ä¸ª $function $ã€‚ colorï¼šä½“çŽ°å‡½æ•°çš„è¾“å‡ºï¼Œè¶Šçº¢è¶Šå¤§ï¼Œè¯´æ˜Žé€‰æ‹©çš„å‡½æ•°è¶Šbadã€‚ æ‰€ä»¥æˆ‘ä»¬è¦é€‰æ‹©ç´«è‰²åŒºåŸŸç»“æžœæœ€å°çš„å‡½æ•°ã€‚ è€Œè¿™ä¸ªå¾—åˆ°best functionçš„è¿‡ç¨‹æ˜¯å¯ä»¥é€šè¿‡æ— æ•°æ¬¡è¿­ä»£å®žçŽ°çš„ã€‚ï¼ˆé‡å¤çš„è¿­ä»£å½“æ—¶æ˜¯äº¤ç»™è®¡ç®—æœºåšäº†ï¼‰ Step 3:è¿­ä»£æ‰¾å‡ºBest Function$L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ æ‰¾åˆ°Best Function: $f^{*}=\\arg \\min _{f} L(f)$ ä¹Ÿå°±æ˜¯æ‰¾åˆ°å‚æ•° $w^{*},b^{*}=\\arg \\min_{w,b} L(w,b)=\\arg \\min_{w,b}\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ arg ï¼šargument,å˜å…ƒ arg minï¼šä½¿ä¹‹æœ€å°çš„å˜å…ƒ arg maxï¼šä½¿ä¹‹æœ€å¤§çš„å˜å…ƒ æ®æ‚‰ï¼Œçº¿æ€§å›žå½’çš„å‚æ•°å¯ä»¥ç”¨çº¿æ€§ä»£æ•°çš„çŸ¥è¯†ï¼Œè§£å‡ºclosed-form solutionï¼ˆè§£æžè§£ï¼‰ï¼Œæˆ‘å…ˆæŒ–ä¸ªå‘QAQï¼Œä»¥åŽæ¥å¡«è¿™å—çŸ¥è¯†ã€‚[1] åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œåªè¦$L$å‡½æ•°å¯å¾®åˆ†ï¼Œ å³å¯ç”¨Gradient Descentï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰çš„æ–¹æ³•æ¥æ±‚è§£ã€‚ Gradient Decentï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰å’Œæ¦‚çŽ‡è®ºä¸­çš„æ¢¯åº¦ä¸‹é™ä¼°è®¡å‚æ•°çš„åŽŸç†ç›¸åŒï¼Œåªæ˜¯è®¡ç®—æœºä¸èƒ½ç›´æŽ¥è§£å‡ºæ–¹ç¨‹çš„è§£ï¼Œæ‰€ä»¥è®¡ç®—æœºçš„æ–¹æ³•æ˜¯è¿­ä»£ã€‚ è€ƒè™‘ä¸€ä¸ªå‚æ•°w*$w^*=\\arg \\min_w L(w)$ æ­¥éª¤ï¼š éšæœºé€‰å–ä¸€ä¸ªåˆå§‹å€¼ $w^0$ è®¡ç®— $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp; &nbsp; $\\begin{equation} w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{0}} \\end{equation}$ è®¡ç®— $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp;&nbsp; $\\begin{equation} w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{1}} \\end{equation}$ â€¦until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$ [![3rgGsP.md.png](https://s2.ax1x.com/2020/02/28/3rgGsP.md.png)](https://imgchr.com/i/3rgGsP) **ä¸Šå›¾è¿­ä»£è¿‡ç¨‹çš„å‡ ç‚¹è¯´æ˜Ž** - $\\begin{equation} \\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}} \\end{equation}$çš„æ­£è´Ÿ å¦‚æžœæ˜¯negativeï¼Œä¹Ÿå°±æ˜¯è¯¥ç‚¹åˆ‡çº¿æ–œçŽ‡æ˜¯è´Ÿçš„ï¼Œé‚£åº”è¯¥Increse wï¼Œä»¥æ‰¾åˆ°æœ€ä½Žç‚¹ã€‚ - Negative $\\rightarrow$ Increase w - Positive $\\rightarrow$ Decrease w - $-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{i}}$ï¼šæ­¥é•¿ - $\\eta$ï¼šlearning rateï¼ˆå­¦ä¹ é€Ÿåº¦ï¼‰ï¼Œäº‹å…ˆè®¾å¥½çš„å€¼ã€‚ - $-$(è´Ÿå·)ï¼šå¦‚æžœ $\\begin{equation} \\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}} \\end{equation}$æ˜¯è´Ÿçš„ï¼Œåº”è¯¥å¢žåŠ wã€‚ - Local optimalï¼šå±€éƒ¨æœ€ä¼˜å’Œå…¨å±€æœ€ä¼˜ - å¦‚æžœæ˜¯ä»¥ä¸Šå›¾åƒï¼Œåˆ™å¾—åˆ°çš„wä¸æ˜¯å…¨å±€æœ€ä¼˜ã€‚ - ä½†çº¿æ€§å›žå½’çš„æŸå¤±å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œå­˜åœ¨ä¸€ä¸ªå…¨å±€æœ€ä¼˜ï¼Œæ²¡æœ‰å±€éƒ¨æœ€ä¼˜ã€‚ ### è€ƒè™‘å¤šä¸ªå‚æ•° $w^{*},b^{*}$ å¾®ç§¯åˆ†çŸ¥è¯†ï¼šgradientï¼ˆæ¢¯åº¦ï¼Œå‘é‡)ï¼š $\\nabla L=\\left[\\begin{array}{l}\\frac{\\partial L}{\\partial w} \\\\frac{\\partial L}{\\partial b}\\end{array}\\right]$ è€ƒè™‘å¤šä¸ªå‚æ•°å’Œè€ƒè™‘ä¸€ä¸ªå‚æ•°æ€è·¯ç›¸åŒï¼Œæ¯æ¬¡è¿­ä»£ï¼Œè¿­ä»£ä¸¤ä¸ªå‚æ•°ã€‚ $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ æ­¥éª¤ï¼š éšæœºé€‰å–åˆå€¼ $w^0,b^0$ è®¡ç®— $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ &nbsp; &nbsp; $w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0}} \\quad b^{1} \\leftarrow b^{0}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ è®¡ç®— $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ &nbsp; &nbsp; &nbsp; $w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1}} \\quad b^{2} \\leftarrow b^{1}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ â€¦until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$, $ \\frac{{\\rm d}L}{{\\rm d}b}|_{b=b^n}=0$ ä¸Šå›¾ï¼Œåæ ‡ä¸º $L(w,b)$å‡½æ•°çš„å‚æ•°ï¼ŒColorä»£è¡¨ $L$çš„å¤§å°ï¼Œè¶Šç´«å€¼è¶Šå°ã€‚ æ¯ä¸€ä¸ªç‚¹éƒ½æ˜¯ä¸€ä¸ª $function$ï¼Œæ²¿ç€æ¢¯åº¦æ–¹å‘ï¼ˆå›¾ä¸­æ³•çº¿æ–¹å‘ï¼‰è¿­ä»£ï¼Œæ‰¾åˆ°å…¨å±€æœ€ä¼˜ç‚¹ã€‚ å†æ¬¡è¯´æ˜Žï¼šçº¿æ€§å›žå½’ä¸­ï¼ŒæŸå¤±å‡½æ•°æ˜¯convexï¼ˆå‡¸å‡½æ•°ï¼‰ï¼Œæ²¡æœ‰å±€éƒ¨æœ€ä¼˜è§£ã€‚ $\\frac{\\partial L}{\\partial w}$å’Œ $\\frac{\\partial L}{\\partial b}$çš„å…¬å¼æŽ¨å¯¼$L(w, b)=\\sum_{n=1}^{10}\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)^{2}$ å¾®ç§¯åˆ†çš„çŸ¥è¯†ï¼Œæ˜¾ç„¶ã€‚ æ•°å­¦çœŸé¦™ã€‚â€”â€”â€”æˆ‘è‡ªå·± $\\frac{\\partial L}{\\partial w}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)ï¼ˆ-x_{cp}^n)$ $\\frac{\\partial L}{\\partial b}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)(-1)$ å®žé™…ç»“æžœåˆ†æžTraining Data Training Dataçš„Error=31.9ï¼Œä½†æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„æ˜¯Testing Dataçš„errorã€‚ Testing Data æ˜¯new Dataï¼šå¦å¤–çš„Pokemonï¼ã€‚ Testing DataModel 1ï¼š $y = b+w\\cdot x_{cp}$ error = 35,æ¯”Training Data erroræ›´å¤§ã€‚ Model 2ï¼š$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2$ Testing error=18.4ï¼Œæ¯”Model 1 å¥½ã€‚ Model 3ï¼š$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3$ Testing error=18.1ï¼Œæ¯”Model 2å¥½ã€‚ Model 4:$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4$ Testing error =28.8,æ¯”Model3æ›´å·®ã€‚ Model 5ï¼š$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4+w_5\\cdot (x_{cp})^5$ Testing error = 232.1,çˆ†ç‚¸äº†ä¸€æ ·çš„å·®ã€‚ Overfitingï¼ˆè¿‡æ‹Ÿåˆäº†ï¼‰ä»Žä¸Šé¢5ä¸ªModelä¸­å¯ä»¥å¾—å‡ºï¼Œè¶Šå¤æ‚çš„å‡½æ•°æ¨¡åž‹ï¼Œåœ¨Testing dataä¸Šä¸ä¸€å®šèƒ½å¾—åˆ°æ›´å¥½çš„ç»“æžœã€‚ï¼ˆè¿‡æ‹Ÿåˆä½¿Training data çš„è¯¯å·®è¶Šæ¥è¶Šå°ï¼‰ æ‰€ä»¥åœ¨é€‰æ‹©Modelæ—¶ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„Modelã€‚ å¯¹æ¨¡åž‹è¿›è¡Œæ”¹è¿›å¦‚æžœæ”¶é›†æ›´å¤šçš„Training Dataï¼Œå¯ä»¥å‘çŽ°ä»–å¥½åƒä¸æ˜¯ä¸€ä¸ªLinear Modelã€‚ Back to step 1:Redesigh the Modelä»Žä¸Šé¢é‚£å¼ å›¾ï¼Œæ„Ÿè§‰ä»–ä¸æ˜¯ä¸€ä¸ªLinear Model,è€Œæ˜¯éœ€è¦if æ˜¯ä¼Šå¸ƒï¼Œæ¨¡åž‹æ˜¯â€¦ï¼Œif æ˜¯â€¦,å¯è§æ˜¯å’Œç‰©ç§æœ‰å…³ç³»ã€‚ ï¼ˆå¾ˆæŠ±æ­‰ï¼Œæˆ‘åªè®¤è¯†å³ä¸Šè§’æ—¶ä¼Šå¸ƒï¼ŒQAQï¼Œæˆ‘ä¹Ÿè¯´ä¸å‡ºåå­—ï¼‰ ä½†ç”¨ $\\delta$(å¾®ç§¯åˆ†å­¦çš„ç‹„æ‹‰å…‹å‡½æ•°)è¡¨ç¤ºæ¡ä»¶è¯­å¥ï¼Œå¯ä»¥å‘çŽ°ï¼Œä»–ä»ç„¶æ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡åž‹ã€‚ $\\delta(x_s= \\text{Pidgey)}\\left\\{\\begin{array}{ll}=1 & \\text { If } x_{s}=\\text { Pidgey } \\\\ =0 & \\text { otherwise }\\end{array}\\right.$ $y = b_1\\cdot \\delta_1+w_1\\cdot \\delta_1+b2\\cdot \\delta_2+w_2\\cdot \\delta_2+â€¦$æ˜¯ä¸€ä¸ªlinear modelã€‚ æ‹Ÿåˆå‡ºæ¥ï¼ŒTraining Data å’ŒTesting Dataçš„erroréƒ½è›®å°çš„ã€‚ å¦‚æžœæƒ³è®©æ‹Ÿåˆè¯¯å·®æ›´å°ï¼Œè¿˜å¯ä»¥è€ƒè™‘å…¶ä»–çš„featureï¼Œé‡é‡ã€é«˜åº¦ã€HPç­‰ã€‚ ä½†åŒæ ·çš„ï¼Œå¦‚æžœå‡½æ•°è¿‡äºŽå¤æ‚ï¼Œä¹Ÿä¼šå‡ºçŽ°Overfittingçš„æƒ…å†µã€‚ Back to Step 2:Regularizationï¼ˆæ­£åˆ™åŒ–ï¼‰å¯¹äºŽLinear Model :$y = b+\\sum w_i x_i$ ä¸ºä»€ä¹ˆè¦æ­£åˆ™åŒ–ï¼Ÿæˆ‘ä»¬å¸Œæœ›å¾—åˆ°çš„å‡½æ•°æ˜¯è¾ƒå¹³æ»‘çš„ï¼Œè¿™æ ·æµ‹è¯•æ—¶ï¼Œå‡½æ•°çš„è¾“å‡ºå¯¹è¾“å…¥çš„noiseä¸sensitiveï¼Œå³è¾“å…¥xçš„ç»†å¾®æ‰°åŠ¨ï¼Œå¹¶ä¸å¤ªä¼šå½±å“è¾“å‡ºçš„ç»“æžœã€‚ æ‰€ä»¥å½“å‚æ•°è¶ŠæŽ¥è¿‘0ï¼Œå‡½æ•°è¶Šå¹³æ»‘ã€‚å› æ­¤åœ¨åŽŸæœ¬çš„loss functionåŽåŠ å…¥ $\\lambda \\sum(w_i)^2$é¡¹ï¼ˆ $\\lambda$éœ€æ‰‹è°ƒï¼‰ï¼Œå¯ä»¥ä¿è¯å‡½æ•°è¾ƒå¹³æ»‘ã€‚ æ­£åˆ™åŒ–ï¼š $L = \\sum_n(\\hat{y}^n-(b+\\sum w_i x_i))^2 + \\lambda\\sum(w_i)^2$ $\\lambda $å¤§å°çš„é€‰æ‹© å¯ä»¥å¾—å‡ºç»“è®ºï¼š $\\lambda $è¶Šå¤§ï¼ŒTraining Errorå˜å¤§äº†ã€‚ å½“ $\\lambda$æ›´å¤§ï¼ŒæŸå¤±å‡½æ•°æ›´è€ƒè™‘wå‚æ•°çš„å–å€¼ï¼Œæ›´å…³å¿ƒå‡½æ•°çš„å¹³æ»‘ç¨‹åº¦ï¼Œè€Œæ›´å°‘çš„å…³å¿ƒæ‹Ÿåˆçš„errorã€‚ $\\lambda $è¶Šå¤§ï¼ŒTesting Errorå˜å°äº†ï¼Œå½“ $\\lambda$è¿‡å¤§æ—¶ï¼Œåˆå˜å¤§ã€‚ $\\lambda $è¾ƒå°æ—¶ï¼Œ$\\lambda $å¢žå¤§ï¼Œå‡½æ•°æ›´å¹³æ»‘ï¼Œèƒ½è‰¯å¥½é€‚åº”æ•°æ®çš„æ‰°åŠ¨ã€‚ $\\lambda $è¾ƒå¤§æ—¶ï¼Œå‡½æ•°è¿‡äºŽå¹³æ»‘ï¼Œå®›å¦‚ç›´çº¿ï¼Œè¿™æ˜¾ç„¶ä¸èƒ½å‡†ç¡®é¢„æµ‹ã€‚ å› æ­¤ï¼Œåœ¨è°ƒèŠ‚$\\lambda $å¤§å°æ—¶ï¼Œä¹Ÿè¦é€‚å½“é€‰æ‹©ã€‚ æ­£åˆ™åŒ–çš„ä¸€ä¸ªæ³¨æ„ç‚¹åœ¨regularizationä¸­ï¼Œæˆ‘ä»¬åªè€ƒè™‘äº†wå‚æ•°ï¼Œæ²¡æœ‰è€ƒè™‘biasåç½®å€¼å‚æ•°ã€‚ å› ä¸ºæ­£åˆ™åŒ–æ˜¯å¯»æ‰¾è¾ƒå¹³æ»‘æ‹Ÿåˆï¼Œè€Œåç½®å‚æ•°åªæ˜¯è®©å‡½æ•°å¹³ç§»ï¼Œä¸Žå¹³æ»‘æ— å…³ã€‚ Againï¼šRegularizationä¸è€ƒè™‘bias Fllowing Gradient descent[2] Overfitting and regularization[3] Validation[4] ç”±äºŽåšä¸»ä¹Ÿæ˜¯åœ¨å­¦ä¹ é˜¶æ®µï¼Œå­¦ä¹ åŽï¼Œä¼špoä¸Šä¸‹é¢å†…å®¹çš„é“¾æŽ¥ã€‚ å¸Œæœ›èƒ½åœ¨å­¦ä¹ ã€å†™åšå®¢çš„è¿‡ç¨‹ä¸­ï¼Œé”»ç‚¼è‡ªå·±çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå°½é‡è®©æ–‡é£Žè¨€ç®€æ„èµ…åˆç§‘å­¦ä¸¥è°¨ã€‚ å†™åšå®¢æ˜¯ä¸ºäº†è®°å½•ä¸Žåˆ†äº«ï¼Œæ„Ÿè°¢æŒ‡æ­£ã€‚ Reference[1] â€œå‘¨å¿—åŽè¥¿ç“œä¹¦p55,å¾…è¡¥å……â€ [2] [3] [4]","link":"/2020/02/28/Regression/"},{"title":"ã€ŒMathã€:Entropy, Cross-Entropy and DL-Divergence","text":"åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¸¸ç”¨cross-entropyæ¥ä½œä¸ºæ¨¡åž‹çš„æŸå¤±å‡½æ•°ï¼Œè¿™ç¯‡æ–‡ç« å°†é˜è¿°ä¿¡æ¯å­¦ä¸­çš„entropyï¼ˆç†µï¼‰æ˜¯ä»€ä¹ˆï¼Œcross-entropyï¼ˆäº¤å‰ç†µï¼‰åˆæ˜¯ä»€ä¹ˆï¼ŒKL-Divergenceå’Œentropyã€cross-entropyçš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ å¦‚ä½•å…·è±¡çš„ç†è§£è¿™äº›æ¦‚å¿µï¼Ÿ åœ¨å¼€å§‹é˜…è¯»è¿™ç¯‡æ–‡ç« ä¹‹å‰ï¼Œå…ˆæåŠä¸€ä¸‹é¦™å†œå¯¹bitçš„å®šä¹‰ï¼Œé¦™å†œè®¤ä¸ºbitæ˜¯ç”¨æ¥æ¶ˆé™¤ä¿¡æ¯çš„ä¸ç¡®å®šæ€§çš„ã€‚ bitï¼šuncertainty divided by 2. åŽŸè§†é¢‘ è®²çš„å¾ˆå¥½ï¼Œæœ¬æ–‡åªæ˜¯åœ¨æ­¤åŸºç¡€ä¸Šå¯¹ä¸€äº›æ€»ç»“ï¼Œæ–¹ä¾¿ç†è§£ç‰©è´¨åŒ–ï¼ˆé©¬åŽŸ.jpgï¼‰ã€‚ å…¬å¼æ€»æ¦‚bitï¼šç”¨æ¥æ¶ˆé™¤ä¿¡æ¯çš„ä¸ç¡®å®šæ€§ Entropyï¼ˆç†µï¼‰ï¼š $H(p)=-\\sum_i p_i\\log(p_i)$ åº¦é‡æ¦‚çŽ‡åˆ†å¸ƒçš„å¹³å‡ä¿¡æ¯é‡ï¼ˆå³ä¸ç¡®å®šæ€§ï¼‰ã€‚å€¼è¶Šå¤§ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§ã€‚ Cross-Entropyï¼ˆäº¤å‰ç†µï¼‰ï¼š $H(p,q)=-\\sum_i p_i\\log(q_i)$ åº¦é‡ä¸¤ä¸ªåˆ†å¸ƒçš„ç›¸ä¼¼ç¨‹åº¦ï¼ˆä¸€èˆ¬ $p$ ä¸ºçœŸå®žåˆ†å¸ƒï¼Œ$q$ä¸ºé¢„æµ‹åˆ†å¸ƒï¼‰ï¼Œå€¼è¶Šå¤§ï¼Œä¸¤ä¸ªåˆ†å¸ƒè¶Šä¸ç›¸ä¼¼ã€‚ KL-Divergenceï¼ˆKLæ•£åº¦ï¼Œä¹Ÿå«ç›¸å¯¹ç†µï¼‰ ï¼š$D_{KL}(p|q)=H(p,q)-H(q)$ åº¦é‡äº¤å‰ç†µè¶…è¿‡ç†µçš„é‚£ä¸€éƒ¨åˆ†ã€‚ Entropy-ç†µå®žä¾‹1ï¼š sunnyå’Œrainyçš„å‘ç”Ÿçš„æ¦‚çŽ‡éƒ½æ˜¯0.5ï¼Œå¤©æ°”é¢„æŠ¥é¢„æµ‹æ˜Žå¤©çš„å¤©æ°”ä¸ºsunnyï¼Œå°†sunnyæ¶ˆæ¯å‘ç»™ç”¨æˆ·ã€‚ è¯¥æ¡æ¶ˆæ¯ä¸ç®¡å¤šé•¿ï¼Œæœ‰ç”¨çš„ä¿¡æ¯å…¶å®žåªæœ‰1ä¸ªæ¯”ç‰¹ï¼Œå³uncertainty divided by 2. å®žä¾‹2ï¼š æœ‰å…«ç§ä¸åŒçš„å¤©æ°”ï¼Œå‘ç”Ÿçš„æ¦‚çŽ‡ç›¸åŒï¼Œå½“å¤©æ°”é¢„æŠ¥å°†é¢„æµ‹æ¶ˆæ¯å‘é€ç»™ç”¨æˆ·æ—¶ã€‚ è¯¥æ¡æ¶ˆæ¯èƒ½ä½¿å¾—uncertainty divided By 8.å³æœ‰ç”¨ä¿¡æ¯ä¸º3ä¸ªæ¯”ç‰¹ã€‚ å®žä¾‹3: sunnyå‘ç”Ÿçš„æ¦‚çŽ‡ä¸º0.75ï¼Œrainyçš„æ¦‚çŽ‡ä¸º0.25ï¼Œå¦‚æžœå¤©æ°”é¢„æµ‹æ˜Žå¤©çš„å¤©æ°”ï¼š å°†è¿™ä¸ªä¾‹å­ç†è§£ä¸ºæŠ½çƒæ¸¸æˆï¼Œç›’å­é‡Œæœ‰3ä¸ªçº¢çƒï¼ˆè¡¨ç¤ºsunnyå¤©æ°”ï¼‰ï¼Œ1ä¸ªç™½çƒï¼ˆrainyå¤©æ°”ï¼‰ã€‚ äº‹ä»¶ $X$ è¡¨ç¤ºä¸ºåœ¨ç›’å­é‡ŒæŠ½ä¸­çƒçš„é¢œè‰²ï¼Œå¯å¾—çŸ¥æŠ½ä¸­çº¢çƒçš„æ¦‚çŽ‡ä¸º0.75ï¼ŒæŠ½ä¸­ç™½çƒçš„æ¦‚çŽ‡æ˜¯0.25ã€‚ æŠ½ä¸­å“ªä¸ªçƒæ˜¯ä¸ç¡®å®šçš„ï¼Œå³uncertainty å¦‚æžœåŽŸæ¥æ˜¯4ï¼Œå³ä¸çŸ¥é“å°†æŠ½ä¸­è¿™å››ä¸ªçƒä¸­çš„å“ªä¸€ä¸ªã€‚ å¦‚æžœæŠ½ä¸­ç™½çƒï¼Œé‚£è¯¥ä¿¡æ¯è¡¨ç¤ºï¼šå°±æ˜¯é‚£4ä¸ªçƒä¸­çš„å”¯ä¸€ä¸€ä¸ªç™½çƒï¼Œuncertainty ä»ŽåŽŸæ¥çš„4å˜ä¸º1ï¼Œå³ uncertainty divided by 4.è¡¨ç¤ºè¯¥ä¿¡æ¯ï¼Œéœ€è¦æœ‰ç”¨æ¯”ç‰¹ï¼Œ $\\log_2(4)=\\log_2(1/0.25)=2$ ä¸ªæ¯”ç‰¹æ¥è¡¨ç¤ºã€‚å³æŠ½æ‰“ç™½çƒçš„æƒ…å†µçš„ä¸ç¡®å®šæ€§æ›´å¤§ï¼Œéœ€è¦æ›´å¤šçš„æ¯”ç‰¹æ¥æ¶ˆé™¤ä¸ç¡®å®šæ€§ï¼Œæ¥è¡¨ç¤ºç™½çƒçš„å‘ç”Ÿã€‚æ‰€ä»¥è¯¥æ¡ä¿¡æ¯ä¸­åªæœ‰2ä¸ªæ¯”ç‰¹æ˜¯useful information. å¦‚æžœæŠ½ä¸­çº¢çƒï¼Œè¯¥ä¿¡æ¯è¡¨ç¤ºä¸ºï¼šæ˜¯é‚£3ä¸ªçº¢çƒä¸­çš„ä¸€ä¸ªï¼Œuncertainty ä»ŽåŽŸæ¥çš„4å˜ä¸º3 ï¼ˆå¦‚æžœå’ŒæŠ½ä¸­ç™½çƒçš„æƒ…å†µç»Ÿä¸€ï¼Œæœ€åŽçš„ç¡®å®šå‘ç”Ÿçš„uncertaintyéƒ½è¡¨ç¤ºä¸º1ï¼Œå³åœ¨æ²¡æœ‰æŠ½ä¹‹å‰ï¼ŒæŠ½åˆ°çº¢çƒçš„uncertaintyä¸º $1/0.75=4/3$ ï¼‰ å³uncertainty divided by 4/3.è¡¨ç¤ºè¯¥ä¿¡æ¯éœ€è¦æœ‰ç”¨ $\\log_2(4/3)=\\log_2(1/0.75)=-\\log_2(0.75)=0.41$ æ¯”ç‰¹æ¥è¡¨ç¤ºã€‚å³æŠ½åˆ°çº¢çƒçš„æƒ…å†µä¸ç¡®å®šæ€§æ²¡æœ‰é‚£ä¹ˆå¤§ï¼Œåªéœ€è¦è¾ƒå°‘æ¯”ç‰¹å³å¯æ¶ˆé™¤ä¸ç¡®å®šæ€§ï¼Œæ¥è¡¨ç¤ºçº¢çƒçš„å‘ç”Ÿã€‚æ‰€ä»¥è¯¥æ¡ä¿¡æ¯ä¸­åªæœ‰0.41ä¸ªæ¯”ç‰¹æ˜¯useful information. è¿™é‡Œä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œå¦‚æžœä¸€ä¸ªäº‹ä»¶çš„å‘ç”Ÿçš„æ¦‚çŽ‡è¶Šå°ï¼ˆè¶Šä¸å¯èƒ½å‘ç”Ÿï¼‰ï¼Œå³å¯¹è¯¥äº‹ä»¶å‘ç”Ÿçš„ä¸ç¡®å®šæ€§è¶Šå¤§ï¼Œä½†ä¸€æ—¦å‘ç”Ÿäº†ï¼Œæ‰€æºå¸¦çš„ä¿¡æ¯é‡å°±ä¼šå¾ˆå¤§ï¼Œå› ä¸ºéœ€è¦ç”¨æ›´å¤šçš„æ¯”ç‰¹æ¥æ¶ˆé™¤ä¸ç¡®å®šæ€§ã€‚ å›žåˆ°æœ¬ä¾‹å­ï¼š å¦‚æžœé¢„æµ‹å¤©æ°”ä¸ºrainyï¼Œå°†é¢„æµ‹æ¶ˆæ¯å‘ç»™ç”¨æˆ·ï¼Œåˆ™è¯¥æ¡æ¶ˆæ¯åŒ…å«2æ¯”ç‰¹ï¼ˆ$\\log_2(1/0.25)=-\\log_2(0.25)=2$ï¼‰çš„æœ‰ç”¨ä¿¡æ¯ï¼Œå³å¯¹rainyå¤©æ°”å‘é€çš„ä¸ç¡®å®šæ€§æ›´å¤§ï¼Œéœ€è¦æ›´å¤šçš„æ¯”ç‰¹æ¥æ¶ˆé™¤ä¸ç¡®å®šæ€§ã€‚ å¦‚æžœé¢„æµ‹å¤©æ°”ä¸ºsunnyï¼Œå› ä¸ºåœ¨é¢„æµ‹ä¹‹å‰ï¼Œç”¨æˆ·å¯¹sunnyå‘ç”Ÿçš„å¯èƒ½æ€§å°±æ²¡æœ‰é‚£ä¹ˆå¤§ï¼Œå› æ­¤åªéœ€è¦0.41æ¯”ç‰¹ï¼ˆ$\\log_2(1/0.75)=-\\log_2(0.75)$)æ¥æ¶ˆé™¤ä¸ç¡®å®šæ€§ã€‚ é‚£å¹³å‡ä¸‹æ¥ï¼Œæ°”è±¡å±€å‘é€çš„å¹³å‡ä¿¡æ¯é‡ä¸º $0.75\\times 0.41+0.25\\times2=0.81$ bits. å› æ­¤æˆ‘ä»¬ç”¨ $\\log_2(1/p)=-\\log_2p$ æ¥è¡¨ç¤ºäº‹ä»¶å‘ç”Ÿæ—¶æ‰€æºå¸¦çš„ä¿¡æ¯é‡ã€‚ï¼ˆæˆ–è€…è¯´éœ€è¦è¿™ä¹ˆå¤šä¿¡æ¯é‡æ¥æ¶ˆé™¤äº‹ä»¶å‘ç”Ÿçš„ä¸ç¡®å®šæ€§ï¼‰ ç”¨ $-\\sum_i{p_i}\\log_2{p_i}$ æ¥è¡¨ç¤ºè¯¥äº‹ä»¶çš„å¹³å‡ä¿¡æ¯é‡ï¼ˆæ¦‚çŽ‡åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§ï¼‰ï¼Œè¿™å°±æ˜¯ä¿¡æ¯ç†µï¼ˆEntropyï¼‰ã€‚ Entropyï¼š$$H(p)=-\\sum_i pi\\log_2(p_i)$$ç†µè¶Šå¤§ï¼Œè¯´æ˜Žæºå¸¦çš„å¹³å‡ä¿¡æ¯é‡è¶Šå¤šï¼Œå³ä¸ç¡®å®šæ€§è¶Šå¼ºï¼Œéœ€è¦è¶Šå¤šçš„æ¯”ç‰¹æ¥æ¶ˆé™¤ä¸ç¡®å®šæ€§ã€‚æ‰€ä»¥ç†µæ˜¯ç”¨æ¥è¡¡é‡ä¸ç¡®å®šæ€§çš„é‡ã€‚ å’ŒåŒ–å­¦ä¸­è¡¡é‡æ··ä¹±ç¨‹åº¦çš„ç†µï¼Œæ˜¯ç±»ä¼¼çš„ã€‚ Cross-Entropy-äº¤å‰ç†µä¾‹1ï¼š ä»Žä¸Šé¢çš„å®žä¾‹2æ¥çœ‹ï¼Œå³8ä¸­å¤©æ°”å‘ç”Ÿæ¦‚çŽ‡ç›¸åŒï¼Œå¯¹å¤©æ°”è¡¨ç¤ºè¿›è¡Œä¿¡æ¯ç¼–ç ï¼Œä¸ºä¸‹å›¾ï¼š entropyä¸º3bitsï¼Œè€Œcross-entropyï¼ˆäº¤å‰ç†µï¼‰ï¼Œä¹Ÿå°±æ˜¯æ¶ˆæ¯ï¼ˆæ¯”ç‰¹æµï¼‰çš„å¹³å‡é•¿åº¦ï¼Œä¸º3bits. ä¾‹2ï¼š ä½†å¦‚æžœ8ç§å¤©æ°”å‘ç”Ÿçš„å¯èƒ½æ€§ä¸ºä¸‹å›¾ï¼š ç®—å‡ºæ¥çš„entropyä¸º2.23bitsï¼Œå³å¹³å‡ä¿¡æ¯é‡ä¸º2.23bitsã€‚ å¦‚æžœä»æ˜¯ç”¨è¿™æ ·çš„ç¼–ç ï¼Œcross-entropyä¸º3bitsï¼Œå°±å¤šå‡ºä¸€äº›å†—ä½™ä¿¡æ¯é‡ã€‚ ä¾‹3: å¦‚æžœæ¢ä¸€ç§ç¼–ç æ–¹å¼ï¼š ç®—å‡ºæ¥çš„cross-entropyä¸º $0.35 \\times2+0.35\\times2+0.1\\times3+â€¦+0.01\\times5=2.42$ bitsï¼Œå°±éžå¸¸æŽ¥è¿‘entropy=2.23bitsã€‚ è¯´æ˜Žè¿™ç§ç¼–ç æ–¹å¼å†—ä½™é‡å¾ˆå°ï¼Œéžå¸¸æŽ¥è¿‘çœŸå®žçš„æ¦‚çŽ‡åˆ†å¸ƒæ‰€åŒ…å«çš„å¹³å‡ä¿¡æ¯ã€‚ ä¾‹4: å¦‚æžœå¤©æ°”çš„æ¦‚çŽ‡åˆ†å¸ƒå˜ä¸ºä¸‹å›¾ï¼Œentropyä¸å˜ä»ç„¶ä¸º2.23bitsï¼š é‚£ä¹ˆç®—å‡ºæ¥çš„cross-entropyä¸º $0.01\\times2+0.01\\times 2+0.04\\times3+â€¦+0.35\\times5=4.58$ bitsï¼Œè¿œå¤§äºŽentropyçš„å€¼ã€‚è¯´æ˜Žè¿™ç§ç¼–ç æ–¹å¼å†—ä½™é‡å¾ˆå¤§ã€‚ æ¢ä¸€ç§è§’åº¦çœ‹ä¾‹4ï¼ŒæŠŠä¿¡æ¯ç¼–ç è®¤ä¸ºæ˜¯é¢„æµ‹çš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œä¾‹4ç‚¹ç¼–ç è¡¨ç¤ºçš„åˆ†å¸ƒå¦‚ä¸‹ï¼š Cross-Entropyï¼š$$H(p,q)=-\\sum_i p_i\\log(q_i)$$æ‰€ä»¥cross-entropyå¯ä»¥ç†è§£ä¸ºä¿¡æ¯/æ¯”ç‰¹æµçš„å¹³å‡é•¿åº¦ã€‚ å¦‚æžœé¢„æµ‹çš„æ¦‚çŽ‡åˆ†å¸ƒéžå¸¸æŽ¥è¿‘çœŸå®žçš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œé‚£æ¯”ç‰¹æµçš„å¹³å‡é•¿åº¦ä¹Ÿä¼šéžå¸¸æŽ¥è¿‘åŽŸåˆ†å¸ƒçš„å¹³å‡ä¿¡æ¯é‡ã€‚ å¦‚æžœé¢„æµ‹çš„æ¦‚çŽ‡åˆ†å¸ƒ $q$ å’ŒçœŸå®žåˆ†å¸ƒ $p$ å®Œå…¨ä¸€æ ·ï¼Œé‚£ä¹ˆcross-entropyç­‰äºŽentropyã€‚ æ‰€ä»¥cross-entropyå¯ä»¥ç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒçš„ç›¸ä¼¼ç¨‹åº¦ã€‚ ç”¨åœ¨æœºå™¨å­¦ä¹ ä¸­ç”¨ä½œè¯„åˆ¤æ¨¡åž‹å¥½åçš„æŸå¤±å‡½æ•°ï¼Œåº¦é‡æ¨¡åž‹é¢„æµ‹åˆ†å¸ƒå’ŒçœŸå®žåˆ†å¸ƒçš„ç›¸ä¼¼ç¨‹åº¦ã€‚ KL-Divergence-KLæ•£åº¦è€Œå¦‚æžœé¢„æµ‹çš„æ¦‚çŽ‡åˆ†å¸ƒå’ŒçœŸå®žåˆ†å¸ƒä¸åŒï¼Œé‚£ä¹ˆcross-entropyçš„å€¼å°±ä¼šå¤§äºŽentropyçš„å€¼ï¼Œè¶…è¿‡çš„éƒ¨åˆ†å°±å«åšrelative entropyï¼ˆç›¸å¯¹ç†µï¼‰ï¼Œä¹Ÿå°±æ˜¯KL-Divergenceï¼ˆKullback-Leibler Divergenceï¼ŒKLæ•£åº¦ï¼‰ å³å¯ä»¥å¾—åˆ°ç­‰å¼ï¼š$\\text{Cross-Entropy = Entropy+KL-Divergence}$ åˆ™KL-Divergenceï¼š$$\\begin{align}D_{KL}(p|q)=H(p,q)-H(p) &amp;= -\\sum_i p_i\\ln q_i - \\sum_i p_i\\ln p_i \\&amp;= -\\sum_i p_i \\ln \\frac{p_i}{q_i} \\&amp;= \\sum_i p_i \\ln \\frac{q_i}{p_i}\\end{align}$$ Reference è§†é¢‘é“¾æŽ¥ï¼šhttps://www.youtube.com/watch?v=ErfnhcEV1O8","link":"/2021/01/15/entropy-and-more/"},{"title":"ã€ŒPaper Readingã€ï¼šAFL++ï¼šCombine Incremental Steps of Fuzzing Research","text":"ä»Šå¤©ç»™å¤§å®¶åˆ†äº«çš„è®ºæ–‡æ˜¯ä¸€ç¯‡åŸºäºŽAFLçš„å·¥ä½œï¼šAFL++ï¼šCombine Incremental Steps of Fuzzing Researchï¼Œå‘è¡¨åœ¨USENIX Workshopã€‚åˆ†äº«æ—¶çš„slides ç›®å½• Introduction State-of-the-Art AFL Smart Scheduling AFLFast MOpt Bypassing Roadblocks LAF-Intel RedQueen Mutate Structured Inputs: AFLSmart AFL++ Evaluation IntroductionFuzzingå·²ç»æˆä¸ºè½¯ä»¶æµ‹è¯•å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œé€šè¿‡å¤§é‡çš„éžé¢„æœŸçš„è¾“å…¥ï¼Œæ£€æµ‹è½¯ä»¶çš„è¿è¡ŒçŠ¶æ€ï¼Œä»¥å‘çŽ°è½¯ä»¶çš„éšè—æ€§æ¼æ´žã€‚ è€ŒAFLæ˜¯æœ€å…·ç››åçš„fuzzingå·¥å…·ï¼ŒåŸºäºŽAFLçš„ç ”ç©¶ä¹Ÿå±‚å‡ºä¸ç©·ï¼Œä½†è¿™äº›æŠ€æœ¯å¾€å¾€æ˜¯æ­£äº¤å¼ã€ç‹¬ç«‹åœ°å‘å±•ã€‚ å› æ­¤ï¼š å°†è¿™äº›å‰æ²¿æ–°é¢–çš„fuzzingæŠ€æœ¯ç»“åˆèµ·æ¥æ˜¯ä¸€ä»¶å¾ˆå›°éš¾çš„äº‹æƒ… è€Œè¯„ä¼°è¿™äº›ä¸åŒç»´åº¦çš„fuzzingæŠ€æœ¯ä¹Ÿæ˜¯ä¸€ä»¶å¾ˆå›°éš¾çš„äº‹ AFL++è¿™ä¸ªå·¥ä½œå°±è‡´åŠ›äºŽè§£å†³è¿™ä¸ªé—®é¢˜ï¼š AFL++æä¾›äº†ä¸€ä¸ªå¯ç”¨çš„fuzzingå·¥å…·ï¼Œç»“åˆäº†è®¸å¤šå‰æ²¿fuzzingæŠ€æœ¯ AFL++è¿˜æä¾›äº†ä¸€ç§æ–°é¢–çš„å¯è‡ªå®šä¹‰çš„å˜å¼‚APIï¼ˆCustom Mutator APIï¼‰ï¼Œç ”ç©¶äººå‘˜èƒ½å¤Ÿè½»æ¾å°†è‡ªå·±è®¾è®¡çš„Mutatoråº”ç”¨åˆ°AFL++ä¸Šï¼Œæˆ–å’Œå…¶ä»–Mutatorsç»“åˆèµ·æ¥ã€‚ è¿™ç¯‡è®ºæ–‡è¿˜è¯„ä¼°äº†è®¸å¤šç»“åˆèµ·æ¥çš„fuzzingæŠ€æœ¯ç»„åˆï¼Œè¯„ä¼°ç»“æžœä½“çŽ°äº†fuzzingæŠ€æœ¯çš„target-dependency. State-of-the-Artåœ¨æ­£å¼ä»‹ç»AFL++ ä¹‹å‰ï¼Œå¾ˆæœ‰å¿…è¦ä»‹ç»ä¸€ä¸‹afl++ç»“åˆçš„å…¶ä»–fuzzingæŠ€æœ¯ã€‚ é™¤äº†AFLçš„ä¸»è¦ç‰¹ç‚¹ï¼Œafl++ä¸»è¦ç»“åˆäº†ä¸‰æ–¹é¢çš„æŠ€æœ¯ï¼Œåˆ†åˆ«æ˜¯ï¼š è°ƒåº¦ä¸Šçš„ã€‚åŒ…æ‹¬AFLFastçš„ç§å­è°ƒåº¦å’ŒMOptçš„å˜å¼‚è°ƒåº¦ã€‚ ç»•è¿‡fuzzingä¸­çš„ä¸€äº›roadblocksï¼ŒåŒ…æ‹¬LAF- Intelå’ŒRed Queenã€‚ é’ˆå¯¹ä¸€äº›å¤æ‚çš„ç»“æž„åŒ–è¾“å…¥çš„å˜å¼‚ã€‚ AFLé¦–å…ˆä»‹ç»ä¸€ä¸‹AFLçš„å·¥ä½œåŽŸç†å’Œä¸»è¦ç‰¹å¾ã€‚ AFLæ˜¯ä¸€ç§åŸºäºŽè¦†ç›–çŽ‡åé¦ˆï¼ˆcoverage- guidedï¼‰çš„fuzzå·¥å…·ã€‚ AFLçš„å·¥ä½œæµç¨‹å¦‚å›¾æ‰€ç¤ºï¼š ç¼–è¯‘æ—¶å¯¹æºç è¿›è¡Œæ’æ¡©ï¼Œä»¥è®°å½•ä»£ç è¦†ç›–çŽ‡ï¼ˆCode Coverageï¼‰ åˆå§‹åŒ–ä¸€äº›è¾“å…¥æ–‡ä»¶ï¼ŒåŠ å…¥åˆ°è¾“å…¥é˜Ÿåˆ—ï¼ˆqueueï¼‰ åœ¨é˜Ÿåˆ—ä¸­æŒ‰ç…§ä¸€å®šç­–ç•¥é€‰æ‹©ç§å­ï¼ˆseedï¼‰ï¼Œå¹¶è¿›è¡Œå¤§é‡çš„çªå˜ï¼ˆmutationï¼‰ï¼Œå¾—åˆ°å¤§é‡çš„å˜å¼‚æ–‡ä»¶ å¦‚æžœè¯¥å˜å¼‚æ–‡ä»¶è§¦å‘äº†æ–°çš„æ‰§è¡Œè·¯å¾„ï¼Œåˆ™å°†å…¶ä¿å­˜ä¸‹æ¥ï¼ŒåŠ å…¥åˆ°é˜Ÿåˆ—ä¸­ã€‚ goto 2 ä¸Šè¿°è¿‡ç¨‹ä¼šä¸€ç›´è¿›è¡Œä¸‹åŽ»ï¼Œå…¶ä¸­è§¦å‘çš„crashä¼šè¢«è®°å½•ä¸‹æ¥ï¼Œä»¥ä¾¿åŽé¢åˆ†æžç¨‹åºæ¼æ´žã€‚ Coverage Guided FeedbackAFLæ˜¯ä¸€ç§ä½¿ç”¨è¾¹è¦†ç›–çŽ‡ä½œä¸ºåé¦ˆçš„ç°ç›’fuzzerã€‚ ä»€ä¹ˆæ˜¯è¾¹è¦†ç›–çŽ‡å‘¢ï¼Ÿ åœ¨ä»‹ç»è¾¹è¦†ç›–çŽ‡ä¹‹å‰ï¼Œå…ˆä»‹ç»ä¸€ä¸‹å—è¦†ç›–çŽ‡ï¼š å¦‚ä¸Šå›¾ï¼Œå°†ä¸€ä¸ªç¨‹åºåˆ’åˆ†ä¸ºä¸€ä¸ªä¸€ä¸ªçš„ç¨‹åºå—ï¼Œä¸€ä¸ªç¨‹åºå—ä¸­çš„æŒ‡ä»¤è¦ä¹ˆéƒ½æ‰§è¡Œï¼Œè¦ä¹ˆéƒ½ä¸æ‰§è¡Œã€‚ å°†ä¸Šè¿°ç¨‹åºæ‹–åˆ°IDAä¸­ï¼Œå¾—åˆ°ä¸‹å›¾ï¼Œå› æ­¤å¯ä»¥ç”¨ç¨‹åºå—ä¹‹é—´çš„è·³è½¬è¡¨ç¤ºè¾¹ã€‚æ‰€ä»¥ A -&gt; B -&gt; C -&gt; D -&gt; E (tuples: AB, BC, CD, DE) A -&gt; B -&gt; D -&gt; C -&gt; E (tuples: AB, BD, DC, CE) è¿™ä¸¤æ¡æ‰§è¡Œè·¯å¾„çš„è¦†ç›–çŽ‡ä¸åŒã€‚ å®žçŽ°ä¸Šï¼Œå°±æ˜¯ç»™æ¯ä¸€ä¸ªå—åˆ†é…ä¸€ä¸ªç¼–è¯‘éšæœºå€¼ï¼Œé€šè¿‡ä¸Šä¸€ä¸ªå—å’Œå½“å‰å—çš„è¿ç®—çš„å€¼æ¥è¡¨ç¤ºè¯¥è¾¹ï¼Œå†è¿›è¡Œç»Ÿè®¡ã€‚ MutationAFLä¸­çš„Mutationä¸»è¦åˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§æ˜¯deterministic stageï¼Œä»Žä¸‹è¡¨çš„å˜å¼‚ä¸­é€‰æ‹©ä¸€ä¸ªmutationè¿›è¡Œè¿žç»­å˜å¼‚ã€‚ è€Œhavoc stage åˆ™æ˜¯æ¯æ¬¡é€‰æ‹©ä¸€å †å˜å¼‚ï¼Œå·¥ä½œä½œç”¨åœ¨seedä¸Šã€‚ Fork Serverç”±äºŽAFLä¼šå¤§é‡è¿è¡Œç›®æ ‡ç¨‹åºï¼Œå› æ­¤ä¸ºäº†å‡å°‘fuzzerçš„æ‰§è¡Œå¼€é”€ï¼ŒAFLä½¿ç”¨äº†ä¸€ç§forkserverçš„æŠ€æœ¯ã€‚ forkserverçš„å·¥ä½œåŽŸç†å¦‚ä¸‹å›¾æ‰€ç¤º: å½“fuzzeråœ¨æ‰§è¡Œç›®æ ‡ç¨‹åºå‰ï¼Œä¼šå…ˆæ‰§è¡Œfork()å‘½ä»¤ï¼Œå¾—åˆ°ä¸€ä¸ªå­è¿›ç¨‹ã€‚ å­è¿›ç¨‹å†é€šè¿‡execv()æŒ‡ä»¤æ‰§è¡Œç›®æ ‡ç¨‹åºï¼Œè€Œexecv()å‘½ä»¤æœ‰ä¸€ä¸ªå¾ˆç‰¹åˆ«çš„åœ°æ–¹ï¼Œä»–ä¼šç”¨è¿™ä¸ªç›®æ ‡ç¨‹åºçš„æ˜ åƒè¦†ç›–æŽ‰å­è¿›ç¨‹çš„æ˜ åƒã€‚ å› æ­¤ï¼Œæ­¤æ—¶çš„fuzzer fork()å‡ºçš„å­è¿›ç¨‹å°±å˜æˆäº†æ’è¿‡æ¡©çš„ç›®æ ‡ç¨‹åºï¼Œä¹Ÿå°±æ˜¯forkserverã€‚ The exec() family of functions replaces the current process image with a new process image. å½“fuzzeræƒ³è¦æ‰§è¡Œç›®æ ‡ç¨‹åºæ—¶ï¼Œå°±å’Œforkserveré€šä¿¡ï¼Œå°†forkç›®æ ‡ç¨‹åºçš„ä»»åŠ¡äº¤ç»™forkserverï¼Œè¿™æ ·å°±æžå¤§çš„æé«˜äº†fuzzerçš„æ‰§è¡Œæ•ˆçŽ‡ã€‚ Persistent ModeAFLå¦ä¸€ä¸ªæé«˜æ•ˆçŽ‡çš„åŠŸèƒ½æ˜¯Persistent Modeã€‚ ä½¿ç”¨Persistent Modeï¼Œåªéœ€è¦å¯¹ç¨‹åºåšä¸€ç‚¹å°å°çš„æ”¹åŠ¨ï¼Œå³å¯¹ç¨‹åºpatchä¸€ä¸ªå¾ªçŽ¯ï¼Œå°±åƒè¿™æ ·ï¼š Persistent Modeå…è®¸å•ä¸ªè¿›ç¨‹é‡å¤è¾“å…¥ï¼Œæžå¤§å‡å°‘äº†forkçš„å¼€é”€ã€‚ Smart SchedulingAFLFaståŸºäºŽAFLçš„å¦ä¸€ä¸ªæ”¹è¿›é¢†åŸŸæ˜¯è°ƒåº¦é—®é¢˜ï¼Œä¸€ä¸ªæ˜¯ç§å­è°ƒåº¦ï¼Œå¦‚ä½•é€‰ç§å­çš„é—®é¢˜ï¼›å¦ä¸€ä¸ªæ˜¯å˜å¼‚è°ƒåº¦ï¼Œå¦‚ä½•é€‰mutationçš„é—®é¢˜ã€‚ AFLFastä¸»è¦æ˜¯ä»Žç§å­è°ƒåº¦çš„æ–¹å‘æ”¹è¿›AFLã€‚ AFLFastè§‚å¯Ÿåˆ°ç§å­å¤§å¤šæ•°ç”Ÿæˆçš„è¾“å…¥éƒ½ç»åŽ†äº†ç›¸åŒçš„é«˜é¢‘è·¯å¾„ï¼Œå› æ­¤æƒ³è¦è®¾è®¡ä¸€äº›ç­–ç•¥èƒ½å¤Ÿfocus onä¸€äº›ä½Žé¢‘è·¯å¾„ã€‚ å› æ­¤ï¼ŒAFLFastè®¾è®¡äº†ä¸¤ç§ç­–ç•¥ï¼š Search Strategyï¼šå…³äºŽåœ¨é˜Ÿåˆ—ä¸­å¦‚ä½•é€‰ç§å­çš„é—®é¢˜ï¼Œå†³å®šç§å­æŒ‘é€‰é¡ºåºã€‚ Power Scheduleï¼šå…³äºŽé€‰å‡ºçš„ç§å­å¯ä»¥è¢«fuzzå¤šå°‘æ¬¡ï¼Œå³å¯ä»¥ç”Ÿæˆå¤šå°‘å˜å¼‚æ–‡ä»¶ï¼Œè€Œè¿™ä¸ªæ•°é‡è¢«å®šä¹‰ä¸ºç§å­çš„energyã€‚ seedâ€™s energy: the amount of generated inputs from each seed AFLå…¶å®žåœ¨AFLä¸­ä¹Ÿæœ‰ç›¸åº”çš„ç­–ç•¥ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹AFLä¸­çš„ç§å­ç­–ç•¥æ˜¯è¿™æ ·çš„ï¼š å¦‚ä½•æŒ‘é€‰ç§å­ï¼š åœ¨AFLä¸­ï¼Œupdate_bitmap_score å‡½æ•°ä¸­ç»´æŠ¤äº†ä¸€ä¸ªå˜é‡fav_factor ï¼Œè¿™ä¸ªå€¼è¶Šå°æ„å‘³å€¼ç§å­è¶Šfavoredï¼Œè€Œè¿™ä¸ªå€¼å…¶å®žæ˜¯ç”±ç¨‹åºçš„æ‰§è¡Œæ—¶é—´(exec_us)å’Œç§å­çš„é•¿åº¦(len)å†³å®šçš„ã€‚ å¦‚ä½•ç»™ç§å­åˆ†é…energyï¼š åœ¨AFLä¸­ï¼Œcalculate_score å‡½æ•°ä¸­ç»´æŠ¤äº†ä¸€ä¸ªå˜é‡perf_score ï¼Œè¿™ä¸ªå€¼è¶Šå¤§æ„å‘³ç€ä¼šç»™ç§å­åˆ†é…æ›´å¤šçš„energyï¼Œè¿™ä¸ªç§å­å°±æœ‰æ›´å¤šçš„æœºä¼šè¢«fuzzï¼Œè€Œè¿™ä¸ªå€¼ä¸»è¦ç”±æ‰§è¡Œæ—¶é—´(exec_us)ã€ç¨‹åºçš„æ‰§è¡Œè·¯(bitmap_size)ã€å‘çŽ°è¯¥ç§å­çš„å›°éš¾åº¦(handicap)ä»¥åŠç§å­çš„æ·±åº¦(depth)å…±åŒå†³å®šçš„ã€‚ å…¶ä¸­handicapï¼Œå›°éš¾åº¦ï¼Œå¯ä»¥è¿™æ ·æ¥ç†è§£ï¼Œè¿™ä¸ªå€¼è¶Šå¤§ï¼Œè¯´æ˜Žå‘çŽ°è¿™ä¸ªç§å­ç»è¿‡äº†å¾ˆé•¿è½®æ•°ï¼Œæ¥ä¹‹ä¸æ˜“ï¼Œæ‰€ä»¥å¸Œæœ›èƒ½æ›´focus onåœ¨è¿™äº›æ¥ä¹‹ä¸æ˜“çš„ç§å­ä¸Šã€‚ AFLFastè€ŒAFLFastä¸­ï¼Œä¸ç®¡æ˜¯å†³å®šå¦‚ä½•æŒ‘é€‰ç§å­ï¼Œè¿˜æ˜¯è§‰å¾—å¦‚ä½•ç»™ç§å­åˆ†é…energyï¼ŒAFLFastéƒ½è¿˜è€ƒè™‘äº†å¦å¤–ä¸¤ä¸ªå˜é‡ã€‚ å¯¹æ¯ä¸ªç§å­ï¼Œå®šä¹‰ä¸¤ä¸ªå˜é‡ï¼š ä¸€ä¸ªæ˜¯f(i) ï¼Œè¡¨ç¤ºè¯¥ç§å­è¢«fuzzçš„æ€»æ¬¡æ•°ï¼Œä¹Ÿå«åšé¢‘çŽ‡ã€‚ å¦ä¸€ä¸ªæ˜¯s(i) ï¼Œè¡¨ç¤ºè¯¥ç§å­åœ¨é˜Ÿåˆ—æŒ‘é€‰ä¸­ï¼Œè¢«æŒ‘é€‰äº†å¤šå°‘æ¬¡ã€‚ AFLFast-Power Scheduleè¿™é‡Œä¸»è¦ä»‹ç»ä¸€ä¸‹AFLFastçš„Power Scheduleï¼š AFLFastæä¾›äº†6ä¸­Power Scheduleï¼š å®šä¹‰p(i)ä¸ºåˆ†é…çš„energyã€‚ EXPLOITï¼šp(i) = AFL EXPLOITæ¨¡å¼ä¸‹çš„power scheduleï¼Œå°±æ˜¯ä¹‹å‰æåˆ°çš„AFLçš„åŽŸç”Ÿç­–ç•¥ã€‚ EXPLOREï¼šp(i) = AFL / const EXPLOREæ¨¡å¼ä¸‹ï¼Œå¯¹AFLä¸­è®¡ç®—å‡ºçš„energyé™¤ä»¥äº†ä¸€ä¸ªå¸¸æ•°ã€‚ çœ‹è¿™æ ·çš„ä¸€ä¸ªä¾‹å­ï¼š ç¨‹åºéœ€è¦ä¾æ¬¡åŒ¹é…åˆ°è¿™äº›å­—ç¬¦ï¼Œæ‰å¯ä»¥æ‰¾åˆ°crashã€‚ å¦‚æžœè§„å®šæ¯ä¸ªç§å­çš„energyæ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå³p = $2^{16}$ ï¼Œé‚£æ€»å…±åˆ†é… $2^{18}$ çš„energyæ‰èƒ½æ‰¾åˆ°crashã€‚ ä½†å¦‚æžœæŠŠè¿™ä¸ªç§å­çš„è½¬ç§»è¿‡ç¨‹ç”¨é©¬å°”å¯å¤«é“¾å»ºæ¨¡ï¼Œå¯ä»¥å‘çŽ°å¦‚æžœä»Žb*** è½¬ç§»åˆ°ba** ï¼Œfuzzçš„è½¬ç§»æ¦‚çŽ‡ä¸º $2^{-10}$ ï¼ˆä»Ž4ä¸ªå­—ç¬¦ä¸­é€‰æ‹©ä¸€ä¸ªå­—èŠ‚ï¼Œæ¯ä¸ªå­—èŠ‚æœ‰ $2^{8}$ ä¸­æƒ…å†µï¼‰ å› æ­¤ï¼Œå¯ä»¥å¾—åˆ°ä»Ži ç§å­è½¬ç§»åˆ°j ç§å­éœ€è¦çš„energyçš„æœŸæœ›æ˜¯ $E[X_{ij}]=\\frac{1}{p_{ij}}$ é‚£ä¹ˆæ‰¾åˆ°bad! çŠ¶æ€ï¼Œæ‰€éœ€è¦çš„æ€»energyçš„æœŸæœ›å’Œä¸º : $E[X_{01}]+E[X_{12}]+E[X_{23}]+E[X_{34}]=4 \\cdot 2^{10}=4k$ å› æ­¤ï¼Œæˆ‘ä»¬æ€»æ˜¯åˆ†é…æ‰€éœ€æœŸæœ›energyæ›´å¤šçš„å€¼ï¼Œæ‰€ä»¥åœ¨AFLFastæ¨¡å¼ï¼Œä¼šå¯¹energyé™¤ä»¥ä¸€ä¸ªå¸¸æ•°ã€‚ å‰©ä¸‹è¿™å››ç§ç­–ç•¥éƒ½æ˜¯ä»Žä¸åŒçš„æ–¹å¼æŠ‘åˆ¶é«˜é¢‘è¾¹è¢«fuzzã€‚ MOptä¸Žç§å­è°ƒåº¦ç›¸å¯¹çš„æ˜¯å˜å¼‚è°ƒåº¦ï¼ŒMOptè¿™ä¸ªå·¥ä½œå°±æ˜¯ä»Žå˜å¼‚è°ƒåº¦çš„è§’åº¦æå‡fuzzçš„æ•ˆçŽ‡ã€‚ MOptå·¥ä½œçš„ä¸»è¦è´¡çŒ®æœ‰ï¼š é¦–å…ˆæ˜¯è§‚å¯Ÿåˆ°ï¼šå¾ˆå¤šæœ‰æ•ˆçš„å˜å¼‚å¦‚bitflipï¼Œæ‰§è¡Œçš„æ—¶é—´å´å¾ˆå°‘ã€‚ è®ºæ–‡ä¸­ç»Ÿè®¡äº†åœ¨deterministic stageä¸åŒå˜å¼‚äº§ç”Ÿçš„interesting test casesçš„æ•°é‡ï¼Œå‘çŽ°bitlipè¡¨çŽ°ä¼˜å¼‚ã€‚ ä½†ä»Žå˜å¼‚æ‰§è¡Œæ—¶é—´çš„è§’åº¦ï¼Œå‘çŽ°è¿™äº›å˜å¼‚æ•ˆçŽ‡é«˜çš„å˜å¼‚ï¼Œæ‰§è¡Œæ—¶é—´æ¯”è¾ƒçŸ­ã€‚ æ‰€ä»¥MOptçš„motivationæ˜¯ï¼šå¸Œæœ›èƒ½èŠ±æ›´å¤šçš„æ—¶é—´åœ¨é‚£äº›å˜å¼‚æ•ˆçŽ‡é«˜çš„å˜å¼‚ä¸Šã€‚ PSOMOptä½¿ç”¨ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•æ¥å¯¹é—®é¢˜å»ºæ¨¡ã€‚ å®šä¹‰ç²’å­ï¼ˆparticleï¼‰ï¼Œå³å˜å¼‚ï¼ˆmutationï¼‰ã€‚ç²’å­çš„ä½ç½®ï¼Œå°±æ˜¯è¯¥å˜å¼‚è¢«é€‰æ‹©çš„æ¦‚çŽ‡ã€‚ æ¯ä¸€ä¸ªç²’å­ç¾¤ï¼ˆswarmï¼‰ï¼Œåˆ™æ˜¯æ‰€æœ‰mutationsçš„æ¦‚çŽ‡åˆ†å¸ƒã€‚ è€ŒMOptä¸ŽåŽŸå§‹PSOç®—æ³•ä¸åŒï¼ŒMOptä½¿ç”¨çš„æ˜¯å¤šä¸ªç¾¤ï¼ˆmultiple swarmsï¼‰ï¼Œæ¯ä¸€ä¸ªç¾¤éƒ½æ˜¯ä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒã€‚ åœ¨MOptçš„å·¥ä½œæµç¨‹ä¸­ï¼Œä¸»è¦æœ‰ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚ ä¸€ä¸ªæ˜¯Pilotæ¨¡å—ï¼Œå¦ä¸€ä¸ªæ˜¯Coreæ¨¡å—ã€‚ Pilotæ¨¡å—ï¼šè¯„ä¼°æ¯ä¸€ä¸ªç²’å­ç¾¤ï¼Œä¹Ÿå°±æ˜¯è¯¥mutationsçš„æ¦‚çŽ‡åˆ†å¸ƒçš„fuzzæ•ˆçŽ‡ã€‚ Coreæ¨¡å—ï¼šä½¿ç”¨Pilotæ¨¡å—è¯„ä¼°å‡ºçš„æ•ˆçŽ‡æœ€é«˜çš„mutationç­–ç•¥ æ¥fuzzã€‚ Bypassing Roadblocksfuzzingä¸­æœ‰æ—¶ä¼šé‡åˆ°ä¸€äº›roadblocksã€‚ LAF- IntelLAF-Intelè§£å†³çš„fuzzingä¸­é‡åˆ°çš„ä¸€äº›å›°éš¾æ¯”è¾ƒè¯­å¥ï¼Œå¦‚ä¸‹å›¾ï¼š å³ä½¿å½“è¾“å…¥ä¸º0xabad1deeæ—¶ï¼Œå·²ç»éžå¸¸æŽ¥è¿‘æ­£ç¡®ç­”æ¡ˆäº†ï¼Œfuzzerä¹Ÿä¼šè®¤ä¸ºä»–æ˜¯é”™è¯¯çš„ã€‚ å› æ­¤LAF- Intelçš„æ€è·¯æ˜¯ï¼ŒæŠŠè¿™äº›æ¯”è¾ƒéš¾çš„ã€æ¯”è¾ƒä¸€è¿žä¸²å­—ç¬¦çš„æ¯”è¾ƒåˆ’åˆ†ä¸º å¤šä¸ªå•å­—èŠ‚çš„æ¯”è¾ƒã€‚ è¿™æ ·å¯ä»¥è®©ç¨‹åºå—çš„åˆ’åˆ†ç²’åº¦æ›´ç»†ï¼Œå½“ä½ æ¯åŒ¹é…åˆ°ä¸€ä¸ªå­—èŠ‚æ—¶ï¼Œå°±è¢«è®¤ä¸ºæ˜¯interestingï¼Œè¢«ä¿å­˜åˆ°é˜Ÿåˆ—ä¸­ï¼Œä»¥åŽå¯ä»¥ç»§ç»­fuzzï¼Œè¿™æ ·ï¼Œfuzzerå°±å¯ä»¥ä¸€æ­¥ä¸€æ­¥çš„è§£å†³è¿™ä¸ªroadblockã€‚ å¦å¤–ï¼ŒLAF-Intelæ˜¯åŸºäºŽLLVMæž¶æž„çš„ï¼Œæ‰€ä»¥LAF- Intelå®žçŽ°äº†ä¸‰ç§Passï¼š The split-compares-passï¼šåˆ’åˆ†ä¸ºå•å­—èŠ‚æ¯”è¾ƒï¼Œå¹¶å…¨éƒ¨è½¬æ¢ä¸º&lt;, &gt;, ==, !=å’Œæ— ç¬¦å·æ•°çš„æ¯”è¾ƒã€‚ The compare-transform-passï¼šé‡å†™äº†strcmpå’Œmemcmpï¼Œå°†å…¶å…¨éƒ¨è½¬æ¢ä¸ºå•å­—èŠ‚æ¯”è¾ƒã€‚ The split-switches-passï¼šå°†switchæ¯”è¾ƒè½¬æ¢ä¸ºå•å­—èŠ‚æ¯”è¾ƒçš„ifä¸²ã€‚ RedQueenRedQueenè§£å†³çš„roadblocksï¼š magic numberï¼šå’Œä¸Šæ–‡LAF-Intelè§£å†³çš„roadblocksç±»ä¼¼ã€‚ nested checksumï¼šè€Œæ ¡éªŒå’Œ/åµŒå¥—æ ¡éªŒå’Œçš„æƒ…å†µå°±åƒä¸‹å›¾æ‰€ç¤ºï¼š ä»£ç å¦‚ä¸‹å›¾æ‰€ç»˜ï¼š RedQueenè¿™ç¯‡å·¥ä½œçš„è´¡çŒ®æ˜¯ï¼š é¦–å…ˆä»–è§‚å¯Ÿåˆ°ç§å­çš„è¾“å…¥ï¼Œæœ‰æ—¶æ˜¯å’Œç¨‹åºçš„è¿è¡ŒçŠ¶æ€ç›´æŽ¥ç›¸å…³çš„ï¼Œè¿™ç§å…³è”å®šä¹‰ä¸ºInput-to-Stateè”ç³»ã€‚ æ¯”å¦‚ä¸‹å›¾ï¼š hookä½cmpæŒ‡ä»¤ï¼Œè¿è¡Œæ—¶ï¼Œè§‚å¯Ÿåˆ°eaxçš„å€¼ä¸ºVALU ï¼Œä¸Žä¹‹æ¯”è¾ƒçš„å€¼ä¸ºABCDï¼ˆéƒ½æ˜¯å°ç«¯åºï¼‰ã€‚ è€ŒVALUåœ¨è¾“å…¥ä¸­ä¹Ÿæœ‰å‡ºçŽ°ï¼Œæ‰€ä»¥è¿™é‡Œè§‚å¯Ÿåˆ°çš„VALUå¤§æ¦‚çŽ‡å°±æ˜¯è¾“å…¥çš„VALUï¼Œå¦‚æžœèƒ½å°†è¾“å…¥çš„VALUæ¢æˆABCDï¼Œå°±æœ‰è¾ƒå¤§å¯èƒ½ç»•è¿‡è¿™ä¸ªroadblockã€‚ RedQueenå°±æ˜¯åˆ©ç”¨è¿™æ ·çš„Input-to-Stateçš„å…³ç³»æ¥è§£å†³è¿™äº›roadblocksã€‚ Magic Bytesè§£å†³Magic Bytesçš„æ–¹æ³•å°±æ˜¯ä¸Šæ–‡æåˆ°çš„é‚£æ ·ï¼Œå¸Œæœ›èƒ½æ‰¾åˆ°ä¸€ç³»åˆ—çš„å¯æ›¿æ¢å¯¹ã€‚ å…·ä½“æµç¨‹ä¸ºï¼š è·Ÿè¸ªï¼šå°†æ‰€æœ‰çš„cmpæŒ‡ä»¤hookä½ï¼Œå°è¯•è¿è¡Œä¸€ä¸‹ï¼ŒæŠŠæŒ‡ä»¤æ¯”è¾ƒçš„æ“ä½œæ•°éƒ½æå–å‡ºæ¥ã€‚ å˜åŒ–ï¼šå¯¹æ¯”è¾ƒæŒ‡ä»¤çš„æ“ä½œæ•°åšå˜å¼‚æ“ä½œï¼Œæ¯”å¦‚åŠ ä¸€æˆ–å‡ä¸€çš„æ“ä½œï¼Œå› ä¸ºä»Žè¯¥æ¡æŒ‡ä»¤å¹¶ä¸èƒ½å¾—åˆ°æºç ä¸­çš„æ¯”è¾ƒå…³ç³»ï¼Œæºç çš„æ¯”è¾ƒé€»è¾‘å¯èƒ½æ˜¯å¤§äºŽã€å°äºŽç­‰ã€‚ ç¼–ç ï¼šå¯¹å¾—åˆ°çš„æ›¿æ¢å¯¹è¿›è¡Œç¼–ç æ“ä½œï¼Œå¦‚å°ç«¯åºã€hexã€base-64ç­‰ï¼Œåƒåˆšåˆšçš„ä¾‹å­å°±æ˜¯å°ç«¯åºçš„ç¼–ç ã€‚ åº”ç”¨ï¼šå°†å¾—åˆ°çš„è¿™äº›æ›¿æ¢å¯¹&lt; pattern -&gt; repl &gt;åº”ç”¨åˆ°è¾“å…¥ä¸­ï¼Œå³åœ¨è¾“å…¥ä¸­æ‰¾patternï¼Œæ›¿æ¢ä¸ºreplï¼Œè¯•è¿è¡Œã€‚ åœ¨æ‰§è¡Œä¸Šè¿°æµç¨‹ä¹‹å‰ï¼ŒRedQueenæ‰§è¡Œäº†ä¸€ä¸ªæ“ä½œï¼Œè¯¥æ“ä½œæžå¤§æé«˜äº†ç»•è¿‡çš„æ•ˆçŽ‡ã€‚ å¦‚æžœæ›¿æ¢å¯¹ä¸º(0x00, 0x04)ï¼Œå¹¶ä¸”è¾“å…¥æ–‡ä»¶åƒä¸‹é¢å·¦å›¾è¿™æ ·ï¼š è¾“å…¥æ–‡ä»¶ä¸­å‡ºçŽ°å¤§é‡çš„0x00ï¼Œå°±åƒäº§ç”Ÿäº†ç¢°æ’žä¸€æ ·ï¼Œå…¶å®žå¾ˆå¤šä½ç½®å¹¶ä¸å’Œé‚£æ¡ç¨‹åºæŒ‡ä»¤ç›¸å…³ï¼Œè¿™æ ·å°±ä¼šèŠ±è´¹å¤§é‡æ—¶é—´ã€‚ å¦‚æžœè¾“å…¥æ˜¯åƒä¸Šå›¾å³è¾¹è¿™æ ·çš„ï¼Œæ¯”è¾ƒcolorfulï¼Œé‚£RedQueençš„æ•ˆçŽ‡å°±ä¼šå¾ˆé«˜ã€‚ æ‰€ä»¥RedQueenåœ¨è¿›è¡Œç»•è¿‡ä¹‹å‰ï¼Œä¼šå¯¹è¾“å…¥åšæŸ“è‰²ï¼ˆColorizationï¼‰çš„æ“ä½œï¼Œåœ¨ä¿è¯ç§å­æ‰§è¡Œè·¯å¾„ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå¢žå¤§è¾“å…¥çš„ç†µå€¼ã€‚ Nested Checksumè€Œå¯¹checksumçš„ç»•è¿‡ï¼Œè¿™å…¶å®žæ˜¯ä¸€ä»¶å¾ˆéš¾çš„äº‹æƒ…ï¼Œå› æ­¤RedQueenä¼šé€‰æ‹©å…ˆå¿½ç•¥æŽ‰è¿™äº›å›°éš¾ï¼ŒåŽé¢å†æ¥ä¿®æ­£ã€‚ å…·ä½“æ“ä½œï¼š å¯¹è¾“å…¥è¿›è¡ŒæŸ“è‰² æ ¹æ®æŒ‡å®šæ¡ä»¶ï¼Œè¯†åˆ«è¿™äº›åƒchecksumæ¯”è¾ƒçš„æŒ‡ä»¤ï¼Œhookä½ã€‚ ç„¶åŽå°±ç”¨cmp al, al patchåŽŸç¨‹åºï¼Œè¿™æ ·å°±è®©è¿™äº›checksumçš„åˆ¤æ–­ä¸€å®šä¸ºæ­£ã€‚ ä½†è¿™æ ·çš„patchå°±ä¼šå¸¦æ¥false positiveï¼Œå³è¿™äº›è¾“å…¥çš„æ‰§è¡Œè·¯å¾„å¯èƒ½å¹¶ä¸æ˜¯è¿™æ ·çš„ã€‚ RedQueenå°±ä¼šåœ¨ä¹‹åŽè¿›è¡Œè¾“å…¥éªŒè¯ï¼Œå¹¶ä¿®å¤ä»–ä»¬ã€‚ åœ¨fixé˜¶æ®µï¼Œå…¶å®žå°±æ˜¯ç”¨magic bytesçš„æ–¹æ³•ï¼Œå¯¹checksumçš„ä½ç½®è¿›è¡Œæ›¿æ¢ã€‚ ä¸è¿‡å¦‚æžœå¯¹äºŽåµŒå¥—çš„æ ¡éªŒå’ŒæŒ‡ä»¤ï¼Œå°±éœ€è¦æŒ‰ç…§æ‹“æ‰‘åºï¼ˆTopological Sortï¼‰ï¼Œä¸€ä¸ªä¸€ä¸ªçš„fixã€‚ Mutate Structure Inputs:AFLSmartå¯¹äºŽè¾“å…¥å¤æ‚çš„ã€ç»“æž„æ€§å¼ºçš„ç¨‹åºï¼Œfuzzeré€šå¸¸ä¼šç”Ÿæˆå¤§é‡çš„æ— æ•ˆè¾“å…¥ã€‚ å› æ­¤AFLSmartå°†AFLå’ŒPeachç»“åˆèµ·æ¥ï¼ŒAFLSmartçš„è¾“å…¥ä¸ºPeach pitsæ ¼å¼ï¼Œä¸€ç§xmlæ–‡ä»¶ã€‚ AFLSmart å°†ç§å­éƒ½è¡¨ç¤ºä¸ºPeach pitsæ ¼å¼ï¼Œè¿™æ ·ï¼Œå°±å¯ä»¥åŸºäºŽè¿™äº›å—è¿›è¡Œå˜å¼‚ï¼Œè€Œä¸éœ€è¦åŸºäºŽæ¯”ç‰¹çº§çš„ å˜å¼‚ã€‚ AFL++AFL++å°±å°†ä¸Šè¿°æåˆ°çš„è¯¸å¤šfuzzingæŠ€æœ¯éƒ½ç»“åˆåœ¨ä¸€èµ·ï¼Œå¹¶æä¾›äº†ä¸€ç§å¯ä¾›æ‰©å±•çš„APIã€‚ Seed SchedulingAFL++çš„Seed Schedulingå°±æ˜¯åŸºäºŽAFLFastçš„ç§å­è°ƒåº¦ã€‚ AFL++çš„Power Schedulesé™¤äº†AFLFastæåˆ°çš„6ç§ï¼Œè¿˜æœ‰å¦å¤–ä¸¤ç§ï¼ŒMmoptå’ŒRareã€‚ Mmoptä¸»è¦å…³æ³¨é‚£äº›æœ€æ–°å‘çŽ°çš„ç§å­ è€ŒRareä¸»è¦å…³æ³¨é‚£äº›å…·æœ‰ç½•è§è¾¹çš„ç§å­ã€‚ MutatorsAFL++é›†æˆäº†è®¸å¤šmutatorï¼ŒåŒ…æ‹¬RedQueençš„Input-to-State mutatorï¼ŒåŒ…æ‹¬Mopt mutatorã€‚ å› æ­¤ï¼ŒAFL++å°±åƒä¸€ä¸ªæ¡†æž¶ï¼Œæä¾›äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„mutatoræŽ¥å£è§„èŒƒï¼Œå®žçŽ°è¿™äº›æŽ¥å£ï¼Œå°±å¯ä»¥å°†è‡ªå·±çš„mutatorç¼åˆåˆ°AFL++ä¸Šï¼Œæˆ–è€…å°†ä¸åŒçš„mutatorç¼åˆåœ¨ä¸€èµ·ã€‚ AFL++é™¤äº†æä¾›äº†mutatorçš„æŽ¥å£è§„èŒƒï¼Œè¿˜æä¾›äº†trimmingçš„å€Ÿå£è§„èŒƒã€‚ Input-To-State Mutatorè¿™ä¸ªmutatoræ˜¯åŸºäºŽRedQueençš„input-to-state. è¿™é‡Œä¸»è¦ä»‹ç»ä»–å’ŒRedQueenä¸åŒçš„åœ°æ–¹ï¼š Colorization RedQueenåœ¨æŸ“è‰²æ—¶æ˜¯ä¿æŒç¨‹åºæ‰§è¡Œè·¯å¾„ä¸å˜ï¼Œå³hash of bitmapä¸å˜ã€‚ è€ŒAFL++é™¤äº†ä¿æŒç¨‹åºçš„æ‰§è¡Œè·¯å¾„ä¸å˜ï¼Œè¿˜å¯¹ç¨‹åºçš„æ‰§è¡Œæ—¶é—´åšäº†ä¸€å®šæŽ§åˆ¶ï¼Œè§„å®šäº†ç¨‹åºçš„æ‰§è¡Œæ—¶é—´ä¸‹ç•Œä¸º2x slowdown Bypass Comparison AFL++é‡‡ç”¨çš„æ˜¯ä¸€ç§probabilistic fuzzingã€‚å³å¦‚æžœè¿™ä¸ªroadblockï¼Œä½¿ç”¨æ›¿æ¢çš„æ–¹æ³•ï¼Œæˆ–è€…ä¿®å¤çš„æ–¹æ³•å¤±è´¥äº†ï¼Œé‚£fuzzerä¸‹ä¸€æ¬¡å°±ä¼šä»¥è¾ƒå°æ¦‚çŽ‡å°è¯•ç»•è¿‡ä»–ã€‚ï¼ˆå½“ä¸‹è§£å†³ä¸äº†çš„å›°éš¾ï¼Œå…ˆæ”¾ä¸€æ”¾zzzzï¼‰ ä¸è¿‡å…¶å®žRedQueenä¸­ä¹Ÿæœ‰ç›¸åº”çš„è®¾è®¡ï¼ŒRedQueenæ˜¯ä½¿ç”¨çš„è™šæ‹Ÿæœºæ–­ç‚¹hook cmpæŒ‡ä»¤ã€‚å› æ­¤ï¼Œå¦‚æžœè¿™ä¸ªæ–­ç”µè¢«hitçš„æ¬¡æ•°æ¯”è¾ƒå°‘ï¼Œå°±å°†è¿™ä¸ªæ–­ç‚¹åŽ»æŽ‰ã€‚ CmpLog Instrumentation RedQueenä¸­é‡‡ç”¨çš„æ˜¯è™šæ‹Ÿæœºæ–­ç‚¹hookçš„cmpæŒ‡ä»¤ï¼Œå½“æ–­ç‚¹è¢«hitæ—¶ï¼Œå†æå–æŒ‡ä»¤æ“ä½œæ•°ã€‚ è€ŒAFL++åˆ™æ˜¯ä½¿ç”¨çš„ä¸€ç§å…±äº«è¡¨ï¼Œæ¯ä¸€ä¸ªæŒ‡ä»¤éƒ½è®°å½•ä»–çš„å‰256æ¬¡æ‰§è¡Œçš„æ“ä½œæ•°ã€‚ MOptAFL++ä¸­ä¹Ÿç¼åˆäº†MOptçš„Pilotå’ŒCoreæ¨¡å—ã€‚ å¹¶ä¸”å¯ä»¥å’ŒInput-to-Stateç»“åˆã€‚ Instrumentationåœ¨æ’æ¡©ä¸Šï¼ŒAFL++é¦–å…ˆè§£å†³äº†è¾¹hit countæº¢å‡ºçš„é—®é¢˜ã€‚ å› ä¸ºåœ¨AFLä¸­ï¼Œè¾¹åªä¼šè®°å½•åˆ°255ã€‚ æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥è§£å†³ï¼š NeverZeroï¼ŒåŠ ä¸€ä¸ªè¿›ä½æ ‡å¿—ã€‚NeverZeroå¯ä»¥æé«˜fuzzerçš„è¡¨çŽ°æ€§èƒ½ã€‚ Saturated Countersï¼šå½“è®¡æ•°è¶…è¿‡255æ—¶ï¼Œå°±åœåœ¨255ã€‚è¿™ä¸ªåšæ³•ï¼Œä¸æŽ¨èï¼Œåè€Œä¼šè®©fuzzerçš„æ€§èƒ½å˜å·®ã€‚ é™¤äº†ä½¿ç”¨NeverZeroï¼ŒAFL++è¿˜ä½¿ç”¨äº†Ngramä¼˜åŒ–è¾¹è¦†ç›–çŽ‡çš„ç»Ÿè®¡ã€‚ AFLåŽŸç”Ÿçš„ç»Ÿè®¡è¾¹è¦†ç›–çŽ‡çš„ä»£ç æ˜¯ï¼š 123cur_location = &lt;COMPILE_TIME_RANDOM&gt;; shared_mem[cur_location ^ prev_location]++; prev_location = cur_location &gt;&gt; 1; AFLåªè€ƒè™‘äº†ä¸Šä¸€ä¸ªåŸºæœ¬å—å’Œå½“å‰åŸºæœ¬å—ï¼Œè¿™æ ·çš„è®¡ç®—é€Ÿåº¦æ›´å¿«ï¼Œä½†ä¼šå¸¦æ¥æ›´å¤šçš„ç¢°æ’žã€‚ è€ŒNgramåˆ™æ˜¯è€ƒè™‘å½“å‰åŸºæœ¬å—å’Œå‰N-1ä¸ªåŸºæœ¬å—è¡¨ç¤ºè¯¥è¾¹ï¼Œè¿™æ ·èƒ½éƒ¨åˆ†ç¢°æ’žï¼Œå®žéªŒç»“æžœä¹Ÿè¡¨æ˜ŽNgramèƒ½æé«˜å®žéªŒçš„æ€§èƒ½ã€‚ AFL++å®žçŽ°äº†å¤šç§åŽç«¯çš„æ’æ¡©ï¼Œå…·ä½“å®žçŽ°çš„åŒºåˆ«å¦‚ä¸‹ï¼š Evaluationç•¥ï¼Œå…·ä½“å¯è§slides ã€‚ Reference AFLï¼šhttps://afl-1.readthedocs.io/en/latest/ AFLFast: https://mboehme.github.io/paper/CCS16.pdf https://github.com/mboehme/aflfast RedQueen:https://react-h2020.eu/m/filer_public/6d/86/6d869f98-f544-49cc-8221-b380c593888f/ndss19-redqueen.pdf https://hexgolems.com/talks/redqueen.pdf MOpt:https://www.usenix.org/system/files/sec19-lyu.pdf AFLSmart: https://thuanpv.github.io/publications/TSE19_aflsmart.pdf AFL++:https://aflplus.plus/papers/ https://github.com/AFLplusplus/AFLplusplus","link":"/2021/04/08/aflpp/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šError","text":"è¿™ç¯‡æ–‡ç« å™è¿°äº†è¿›è¡Œregressionæ—¶ï¼Œwhere dose the error come from?è¿™ç¯‡æ–‡ç« é™¤äº†è§£é‡Šäº†errorä¸ºä»€ä¹ˆæ¥è‡ªbiaså’Œvarianceï¼Œè¿˜ç»™å‡ºäº†å½“erroräº§ç”Ÿæ—¶åº”è¯¥æ€Žä¹ˆåŠžï¼Ÿå¦‚ä½•è®©æ¨¡åž‹åœ¨å®žè·µåº”ç”¨ä¸­ä¹Ÿèƒ½è¡¨çŽ°åœ°å’Œæµ‹è¯•æ—¶å‡ ä¹Žä¸€æ ·çš„å¥½ï¼Ÿ Erroråœ¨ä¸­çš„2.4èŠ‚ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸åŒçš„Modelã€‚ä¸‹å›¾ä¸ºä¸åŒModelä¸‹ï¼Œtesting data errorçš„å˜åŒ–ã€‚ å¯ä»¥å‘çŽ°ï¼Œéšç€æ¨¡åž‹è¶Šæ¥è¶Šå¤æ‚ï¼Œtesting dataçš„errorå˜å°ä¸€äº›åŽï¼Œçˆ†ç‚¸å¢žå¤§ã€‚ è¶Šå¤æ‚çš„æ¨¡åž‹åœ¨testing dataä¸Šä¸ä¸€å®šèƒ½å¾—åˆ°å¥½çš„performanceã€‚ æ‰€ä»¥ï¼Œwhere dose the error come from? ï¼šbias and variance Bias and Variance of Estimatorç”¨æ‰“é¶ä½œæ¯”ï¼Œå¦‚æžœä½ çš„å‡†å¿ƒï¼Œæ²¡æœ‰å¯¹å‡†é¶å¿ƒï¼Œé‚£æ‰“å‡ºçš„å¾ˆå¤šå‘å­å¼¹çš„ä¸­å¿ƒåº”è¯¥ç¦»é¶å¿ƒæœ‰ä¸€æ®µè·ç¦»ï¼Œè¿™å°±æ˜¯biasã€‚ ä½†æŠŠå‡†å¿ƒå¯¹å‡†é¶å¿ƒï¼Œä½ ä¹Ÿä¸ä¸€å®šèƒ½æ‰“ä¸­é¶å¿ƒï¼Œå¯èƒ½ä¼šæœ‰é£Žé€Ÿç­‰ä¸€ç³»åˆ—åŽŸå› ï¼Œè®©å­å¼¹è½åœ¨é¶å¿ƒå‘¨å›´ï¼Œè¿™å°±æ˜¯varianceã€‚ ä¸Šå›¾ä¸­ï¼Œå¯ä»¥ç›´è§‚ä½“çŽ°å‡ºbias å’Œ varianceçš„å½±å“ã€‚ æ¦‚çŽ‡è®ºä¸­ ï¼š ä¸€ä¸ªé€šè¿‡æ ·æœ¬å€¼å¾—åˆ°äº†ä¼°è®¡é‡ï¼Œæœ‰ä¸‰ä¸ªè¯„åˆ¤å‡†åˆ™ï¼šæ— åæ€§ã€æœ‰æ•ˆæ€§å’Œç›¸å’Œæ€§ã€‚ è¿™é‡Œçš„æ— åæ€§çš„åä¹Ÿå°±æ˜¯biasã€‚ æ¦‚çŽ‡è®ºä¸­å®šä¹‰ï¼šè®¾ $\\hat{\\theta}(X_1,X_2,â€¦,X_n)$ æ˜¯æœªçŸ¥å‚æ•° $\\theta$ çš„ä¼°è®¡é‡ï¼Œè‹¥ $E(\\hat{\\theta})=\\theta$ ï¼Œåˆ™ç§° $\\hat{\\theta}$ æ˜¯ $\\theta$ çš„æ— åä¼°è®¡ã€‚ å˜é‡ $x$ ï¼Œå‡è®¾ä»–çš„æœŸæœ›æ˜¯ $\\mu$ ï¼Œä»–çš„æ–¹å·®æ˜¯ $\\sigma^2$. å¯¹äºŽæ ·æœ¬ï¼š $x^1,x^2,â€¦,x^N$ ï¼Œä¼°è®¡ä»–çš„æœŸæœ›å’Œæ–¹å·®ã€‚ æ¦‚çŽ‡è®ºçš„çŸ¥è¯†ï¼š $m=\\frac{1}{N} \\sum_{n} x^{n} \\quad s^{2}=\\frac{1}{N} \\sum_{n}\\left(x^{n}-m\\right)^{2}$ $E(m)=\\mu$ ï¼Œæ‰€ä»¥ç”¨ $m$ æ˜¯ $\\mu$ çš„æ— åä¼°è®¡ã€‚(unbiased) ä½†æ˜¯ $E\\left[s^{2}\\right]=\\frac{N-1}{N} \\sigma^{2} \\quad \\neq \\sigma^{2}$ ï¼Œæ‰€ä»¥è¿™æ ·çš„ä¼°è®¡æ˜¯æœ‰åå·®çš„ã€‚(biased) å› æ­¤ç»Ÿè®¡å­¦ä¸­ç”¨æ ·æœ¬ä¼°ç®—æ€»ä½“æ–¹å·®éƒ½è¿›è¡Œäº†ä¿®æ­£ã€‚ è€Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼ŒBiaså’ŒVarianceé€šå¸¸ä¸Žæ¨¡åž‹ç›¸å…³ã€‚ ä¸Šå›¾ä¸­ï¼Œå‡è®¾é»‘è‰²çš„çº¿æ˜¯ true functionï¼Œçº¢è‰²çš„çº¿æ˜¯è®­ç»ƒå¾—åˆ°çš„å‡½æ•°ï¼Œè“è‰²çš„çº¿æ˜¯ï¼Œè®­ç»ƒå‡½æ•°çš„å¹³å‡å‡½æ•°ã€‚ å¯è§ï¼Œéšç€å‡½æ•°æ¨¡åž‹è¶Šæ¥è¶Šå¤æ‚ï¼Œbiasåœ¨å˜å°ï¼Œä½†varianceä¹Ÿåœ¨å¢žå¤§ã€‚ å³ä¸‹è§’å›¾ä¸­ï¼Œçº¢è‰²çš„çº¿æŽ¥è¿‘é“ºæ»¡äº†ï¼Œvarianceå·²ç»å¾ˆå¤§äº†ï¼Œæ¨¡åž‹è¿‡æ‹Ÿåˆäº†ã€‚ å¯¹æœºå™¨å­¦ä¹ ä¸­æ¨¡åž‹å¯¹biaså½±å“çš„ç›´è§‚è§£é‡Š å·¦å›¾çš„modelç®€å•ï¼Œå³å›¾çš„modelå¤æ‚ã€‚ ç®€å•çš„modelï¼ŒåŒ…å«çš„å‡½æ•°é›†è¾ƒå°ï¼Œå¯èƒ½é›†åˆåœˆæ ¹æœ¬æ²¡æœ‰åŒ…æ‹¬targetï¼ˆtrue functionï¼‰ï¼Œå› æ­¤åœ¨è¿™ä¸ªmodelä¸‹ï¼Œæ— è®ºæ€Žä¹ˆè®­ç»ƒï¼Œå¾—åˆ°çš„å‡½æ•°éƒ½æœ‰ large biasã€‚ è€Œå³å›¾ä¸­ï¼Œå› ä¸ºå‡½æ•°éžå¸¸å¤æ‚ï¼Œæ‰€ä»¥å¤§æ¦‚çŽ‡åŒ…å«äº†targetï¼Œå› æ­¤è®­ç»ƒå‡ºçš„å‡½æ•°å¯èƒ½variacneå¾ˆå¤§ï¼Œä½†æœ‰ small biasã€‚ what to do with large bias/variance ä¸Šå›¾ä¸­ï¼Œçº¢è‰²çš„çº¿è¡¨ç¤ºbiasçš„è¯¯å·®ï¼Œç»¿è‰²çš„çº¿è¡¨ç¤ºvarianceçš„è¯¯å·®ï¼Œè“è‰²çš„çº¿è¡¨ç¤ºè§‚æµ‹çš„è¯¯å·®ã€‚ å½“æ¨¡åž‹è¿‡äºŽç®€å•æ—¶ï¼šæ¥è‡ªbiasçš„è¯¯å·®ä¼šè¾ƒå¤§ï¼Œæ¥è‡ªvaianceçš„è¯¯å·®è¾ƒå°ï¼Œä¹Ÿå°±æ˜¯ Large Bias Small Variance å½“æ¨¡åž‹è¿‡é›¨å¤æ‚æ—¶ï¼šæ¥è‡ªbiasçš„è¯¯å·®ä¼šè¾ƒå°ï¼Œæ¥è‡ªvarianceçš„è¯¯å·®ä¼šå¾ˆå¤§ï¼Œä¹Ÿå°±æ˜¯ Small Bias Large Variance 2 case : Underfitting ï¼šIf your model cannot even fit the training examples, then you have large bias. Overfitting : If you can fit the traning data, but large error on testing data , then you probably have large variance. With Large BiasFor bias, redesign your model. Add more features as input. A more complex model. è€ƒè™‘æ›´å¤šçš„featureï¼›ä½¿ç”¨ç¨å¾®å¤æ‚äº›çš„æ¨¡åž‹ã€‚ With Large Variance More data Regularization (åœ¨è¿™ç¯‡2.5.2æ–‡ç« ä¸­æœ‰å™è¿°ä»€ä¹ˆæ˜¯regularization) Model Selection There is usually a trade-off beween bias and variance. Select a model that balances two kinds of error to minimize total error. é€‰æ‹©æ¨¡åž‹éœ€è¦åœ¨biaså’Œvarianceä¸­å¹³è¡¡ï¼Œå°½é‡ä½¿å¾—æ€»erroræœ€å°ã€‚ What you should NOT do: ä»¥ä¸Šï¼Œæè¿°çš„æ˜¯è¿™æ ·çš„ä¸€ä¸ªæƒ…å½¢ï¼šåœ¨traning dataä¸­ï¼Œå¾—åˆ°äº†ä¸‰ä¸ªè‡ªè®¤ä¸é”™çš„æ¨¡åž‹ï¼Œkaggleçš„å…¬å¼€çš„testing dataæµ‹è¯•ï¼Œåˆ†åˆ«å¾—åˆ°ä¸‰ä¸ªæ¨¡åž‹çš„errorï¼Œè®¤ä¸ºç¬¬ä¸‰ä¸ªæ¨¡åž‹æœ€å¥½ï¼ ä½†æ˜¯ï¼Œå½“æŠŠkaggleç”¨privateçš„testing data è¿›è¡Œæµ‹è¯•æ—¶ï¼Œerrorè‚¯å®šæ˜¯å¤§äºŽ0.5çš„ï¼Œæœ€å¥½çš„modelä¹Ÿä¸ä¸€å®šæ˜¯ç¬¬ä¸‰ä¸ªã€‚ åŒç†ï¼Œå½“æŠŠæˆ‘ä»¬è®­ç»ƒå‡ºçš„modelæ‹¿æ¥å®žé™…åº”ç”¨æ—¶ï¼Œå¯èƒ½ä¼šå‘çŽ°æƒ…å†µå¾ˆç³Ÿï¼Œå¹¶ä¸”ï¼Œè¿™ä¸ªmodelå¯èƒ½é€‰çš„æ˜¯æµ‹è¯•ä¸­æœ€å¥½çš„ï¼Œä½†åœ¨åº”ç”¨ä¸­å¹¶ä¸æ˜¯æœ€å¥½çš„ã€‚ Cross Validationä»€ä¹ˆæ˜¯Cross Validation(äº¤å‰éªŒè¯)ï¼Ÿ åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå°±æ˜¯ä¸‹å›¾è¿‡ç¨‹ï¼š æŠŠTraning Set åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼šTraining Setå’ŒValidation Setã€‚ åœ¨Training Setéƒ¨åˆ†é€‰å‡ºæ¨¡åž‹ã€‚ ç”¨Validation Setæ¥åˆ¤æ–­å“ªä¸ªæ¨¡åž‹å¥½ï¼šè®¡ç®—æ¨¡åž‹åœ¨Validate Setçš„errorã€‚ å†ç”¨æ¨¡åž‹é¢„æµ‹Testing Set(public)ï¼Œå¾—åˆ°çš„errorä¸€å®šæ˜¯æ¯”Validation Setä¸­å¤§çš„ã€‚ Not recommend : Notç”¨public testing dataçš„è¯¯å·®ç»“æžœåŽ»è°ƒæ•´ä½ çš„æ¨¡åž‹ã€‚ è¿™æ ·ä¼šè®©æ¨¡åž‹åœ¨publicçš„performanceæ¯”privateçš„å¥½ã€‚ ä½†æ¨¡åž‹åœ¨private testing dataçš„performanceæ‰æ˜¯æˆ‘ä»¬çœŸæ­£å…³æ³¨çš„ã€‚ é‚£ä¹ˆå½“æ¨¡åž‹é¢„æµ‹private testing setæ—¶ï¼ˆæŠ•å…¥åº”ç”¨æ—¶ï¼‰ï¼Œèƒ½å°½æœ€å¤§å¯èƒ½çš„ä¿è¯æ¨¡åž‹å’Œåœ¨é¢„æµ‹public testing dataç›¸è¿‘ã€‚ N-fold Cross ValidationN-fold Cross Validationï¼ˆN-æŠ˜äº¤å‰éªŒè¯ï¼‰çš„è¿‡ç¨‹å¦‚ä¸‹ï¼š æŠŠTraining Set åˆ†ä¸º3ï¼ˆ3-foldï¼‰ä»½ï¼Œæ¯ä¸€æ¬¡æ‹¿å…¶ä¸­ä¸€ä»½å½“Validation Setï¼Œå¦å¤–ä¸¤ä»½å½“ä½œTraining Setã€‚ æ¯ä¸€æ¬¡ç”¨Train Setæ¥è®­ç»ƒã€‚å¾—åˆ°äº†ä¸‰ä¸ªModelã€‚ è¦åˆ¤æ–­å“ªä¸€ä¸ªModelå¥½ï¼Ÿ æ¯ä¸€ä¸ªModeléƒ½è®¡ç®—å‡ºä¸åŒValidation Setçš„errorã€‚ å¾—åˆ°ä¸€ä¸ªAverage Errorã€‚ æœ€åŽé€‰è¿™ä¸ªaverage erroræœ€å°çš„modelã€‚ æœ€åŽåº”ç”¨åœ¨public traning setï¼Œæ¥è¯„ä¼°æ¨¡åž‹åº”ç”¨åœ¨private training setçš„performanceã€‚","link":"/2020/03/14/error/"},{"title":"ã€ŒWebã€:HTML and CSS","text":"æ¸©æ•…çŸ¥æ–°ï¼šå¯¹WebåŸºç¡€çŸ¥è¯†â€”â€”HTMLå’ŒCSSçš„æŒç»­æ›´æ–°ã€‚ è¯´åœ¨å‰é¢B/S è½¯ä»¶ç»“æž„C/Sï¼š Client Serverï¼ˆJavaSEï¼‰ B/Sï¼šBrowser Serverï¼ˆJavaEEï¼‰ å‰ç«¯å¼€å‘æµç¨‹ ç¾Žæœ¯å®žçŽ°ï¼šç½‘é¡µè®¾è®¡ å‰ç«¯å·¥ç¨‹å¸ˆï¼šè®¾è®¡ä¸ºé™æ€ç½‘é¡µ Javaç¨‹åºå‘˜ï¼šåŽç«¯å·¥ç¨‹å¸ˆä¿®æ”¹ä¸ºåŠ¨æ€é¡µé¢ ç½‘é¡µç«¯ç»„æˆéƒ¨åˆ†å†…å®¹ï¼šé¡µé¢ä¸­å¯ä»¥çœ‹åˆ°çš„æ•°æ®ã€‚ä¸€èˆ¬ä½¿ç”¨htmlæŠ€æœ¯ã€‚ è¡¨çŽ°ï¼šå†…å®¹åœ¨é¡µé¢ä¸Šçš„å±•ç¤ºå½¢å¼ã€‚ä¸€èˆ¬ä½¿ç”¨CSSã€‚ è¡Œä¸ºï¼šé¡µé¢ä¸­çš„å…ƒç´ ä¸Žè¾“å…¥è®¾å¤‡äº¤äº’ã€‚ä¸€èˆ¬ä½¿ç”¨javascriptæŠ€æœ¯ã€‚ HTMLåˆ›å»ºHTMLæ–‡ä»¶ åˆ›å»ºä¸€ä¸ªWebé™æ€å·¥ç¨‹ åœ¨å·¥ç¨‹ä¸‹åˆ›å»ºhtmlé¡µé¢ 12345678910&lt;!DOCTYPE html&gt;&lt;!--å£°æ˜Ž--&gt;&lt;html lang=&quot;zh_CN&quot;&gt;&lt;!--htmlä¸­åŒ…å«ä¸¤éƒ¨åˆ†ï¼šheadå’Œbody--&gt;&lt;head&gt;&lt;!--headä¸­åŒ…å«ï¼štitleæ ‡ç­¾ã€CSSæ ·å¼ã€jsä»£ç --&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;Hello World!&lt;/body&gt;&lt;/html&gt; HTMLæ ‡ç­¾ æ ‡ç­¾åå¤§å°å†™ä¸æ•æ„Ÿ æ ‡ç­¾æœ‰è‡ªå·±çš„å±žæ€§ åŸºæœ¬å±žæ€§ï¼šä¿®æ”¹ç®€å•æ ·å¼ äº‹ä»¶å±žæ€§ï¼šè®¾ç½®äº‹ä»¶å“åº”åŽçš„ä»£ç  æ ‡ç­¾åˆ†ä¸ºå•æ ‡ç­¾&lt;æ ‡ç­¾/&gt;å’ŒåŒæ ‡ç­¾&lt;æ ‡ç­¾&gt;&lt;/æ ‡ç­¾&gt; æ ‡ç­¾çš„å±žæ€§å¿…é¡»è¦æœ‰å€¼ï¼Œå±žæ€§å€¼åŠ åŒå¼•å·ã€‚ æ˜¾ç¤ºç‰¹æ®Šæ ‡ç­¾ï¼š&lt; &gt; ç©ºæ ¼ç­‰ç­‰ï¼Œå»ºè®®æŸ¥é˜…æ–‡æ¡£ã€‚ å­—ä½“æ ‡ç­¾12345&lt;body&gt; &lt;font color=&quot;red&quot; size=&quot;7&quot;&gt; å“’å“’å“’ã€‚ &lt;/font&gt;&lt;/body&gt; æ ‡é¢˜æ ‡ç­¾ï¼šh1 åˆ° h612&lt;h1 align=&quot;center&quot;&gt;æ ‡é¢˜1&lt;/h1&gt;&lt;h2 align=&quot;left&quot;&gt;æ ‡é¢˜2&lt;/h2&gt;&lt;!--alignï¼šæ˜¾ç¤ºä½ç½®,é»˜è®¤å·¦--&gt; è¶…é“¾æŽ¥123&lt;a href=&quot;https://baidu.com&quot; target=&quot;_self&quot;&gt;ç™¾åº¦&lt;/a&gt;&lt;!--_selfå±žæ€§ï¼šå½“å‰çª—å£è·³è½¬--&gt;&lt;br/&gt;&lt;a href=&quot;https://baidu.com&quot; target=&quot;_blank&quot;&gt;ç™¾åº¦&lt;/a&gt;&lt;!--_blankå±žæ€§ï¼šæ‰“å¼€æ–°çª—å£è·³è½¬--&gt; åˆ—è¡¨æ ‡ç­¾12345678910111213&lt;ul type=&quot;none&quot;&gt;&lt;!--æ— åºåˆ—è¡¨--&gt;&lt;!--typeå±žæ€§å¯ä»¥æ›´æ”¹åˆ—è¡¨å‰çš„ç¬¦å·--&gt; &lt;li&gt;ç™¾åº¦&lt;/li&gt; &lt;li&gt;ç™¾åº¦&lt;/li&gt; &lt;li&gt;ç™¾åº¦&lt;/li&gt; &lt;li&gt;ç™¾åº¦&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;!--æœ‰åºè¡¨æ ¼--&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt;&lt;/ol&gt; imgæ ‡ç­¾ å±žæ€§src:å›¾ç‰‡ç­‰è·¯å¾„ä½ç½® JavaSEä¸­è·¯å¾„ ç›¸å¯¹è·¯å¾„ï¼šä»Žå·¥ç¨‹åå­—å¼€å§‹ç®— ç»å¯¹è·¯å¾„ï¼šç¡¬ç›˜ä¸­çš„è·¯å¾„ Webä¸­çš„è·¯å¾„ ç›¸å¯¹è·¯å¾„ï¼š . ï¼šè¡¨ç¤ºå½“å‰æ–‡ä»¶æ‰€åœ¨çš„ç›®å½• .. ï¼šè¡¨ç¤ºå½“å‰æ–‡ä»¶æ‰€åœ¨çš„ä¸Šçº§ç›®å½• æ–‡ä»¶åï¼šè¡¨ç¤ºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„æ–‡ä»¶ï¼Œç›¸å½“äºŽ./æ–‡ä»¶å ç»å¯¹è·¯å¾„ï¼šhttp://ip:port/å·¥ç¨‹å/èµ„æºè·¯å¾„ å±žæ€§ï¼šweight; height; borderï¼šè®¾ç½®å›¾ç‰‡è¾¹æ¡†å¤§å°ã€‚ altï¼šå½“æŒ‡å®šè·¯å¾„æ‰¾ä¸åˆ°å›¾ç‰‡æ—¶ï¼Œç”¨æ¥ä»£æ›¿æ˜¾ç¤ºçš„æ–‡æœ¬å†…å®¹ã€‚ è¡¨æ ¼æ ‡ç­¾ï¼šå®žçŽ°è·¨è¡Œè·¨åˆ—1234567891011121314151617181920212223&lt;table border=&quot;1&quot; width=&quot;300&quot;&gt;&lt;!--è¡¨æ ¼æ ‡ç­¾--&gt; &lt;!--borderï¼šè®¾ç½®è¾¹æ¡†ã€widthï¼šè®¾ç½®å®½åº¦ã€heightï¼šè®¾ç½®é«˜åº¦--&gt; &lt;!--alignï¼šè®¾ç½®è¡¨æ ¼å¯¹é½æ–¹å¼--&gt; &lt;!--cellspacing:å•å…ƒæ ¼é—´è·--&gt; &lt;tr&gt;&lt;!--è¡Œæ ‡ç­¾--&gt; &lt;th&gt;h1&lt;/th&gt;&lt;!--è¡¨å¤´æ ‡ç­¾--&gt; &lt;th&gt;h2&lt;/th&gt; &lt;th&gt;h3&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt;&lt;!--å•å…ƒæ ¼æ ‡ç­¾--&gt; &lt;td align=&quot;center&quot;&gt;1.2&lt;/td&gt; &lt;!--alignï¼šè®¾ç½®å•å…ƒæ ¼æ–‡æœ¬å¯¹é½æ–¹å¼--&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;2.1&lt;/td&gt;&lt;!--colspan:åˆ—çš„å®½åº¦,å®žçŽ°å•å…ƒæ ¼è·¨åˆ—--&gt; &lt;td rowspan=&quot;2&quot;&gt;2.2&lt;/td&gt;&lt;!--rowspan:è¡Œçš„å®½åº¦ï¼Œå®žçŽ°å•å…ƒæ ¼è·¨è¡Œ--&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;3.1&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; iframeæ¡†æž¶æ ‡ç­¾å¯ä»¥åœ¨htmlé¡µé¢ä¸Šå¼€è¾Ÿä¸€ä¸ªå°åŒºåŸŸåŠ è½½å•ç‹¬çš„é¡µé¢ï¼Œå®žçŽ°å†…åµŒçª—å£ã€‚ 12345&lt;iframe src=&quot;hello.html&quot; width=&quot;400&quot; height=&quot;600&quot; name=&quot;abc&quot;&gt;&lt;/iframe&gt;&lt;!--nameï¼šè¡¨ç¤ºè¯¥åŒºåŸŸçš„åå­—--&gt;&lt;a href=&quot;welcome.html&quot; target=&quot;abc&quot;&gt;æ¬¢è¿Ž&lt;/a&gt;&lt;!--targetï¼šæ‰“å¼€çª—å£æ˜¾ç¤ºçš„ä½ç½®--&gt;&lt;!--aæ ‡ç­¾çš„targetå±žæ€§è®¾ç½®ä¸ºiframeçš„nameå±žæ€§ï¼Œå°±åœ¨å¼€è¾Ÿçš„åŒºåŸŸæ‰“å¼€é“¾æŽ¥çª—å£--&gt; è¡¨å•æ ‡ç­¾è¡¨å•ï¼šhtmlä¸­ç”¨æ¥æ”¶é›†ç”¨æˆ·ä¿¡æ¯çš„å…ƒç´ é›†åˆï¼Œå°†è¿™äº›ä¿¡æ¯å‘é€ç»™æœåŠ¡å™¨å¤„ç†ã€‚ 1234567891011121314151617181920212223242526272829303132333435&lt;form&gt;&lt;!--è¡¨å•æ ‡ç­¾--&gt; ç”¨æˆ·åç§°ï¼š&lt;input type=&quot;text&quot; value=&quot;User&quot;/&gt;&lt;br/&gt;&lt;!--inputè¾“å…¥æ¡†æ ‡ç­¾--&gt; &lt;!--typeï¼šè¾“å…¥ç±»åž‹ valueï¼šé»˜è®¤å€¼--&gt; &lt;!--textï¼šæ–‡æœ¬ç±»åž‹--&gt; ç”¨æˆ·å¯†ç ï¼š&lt;input type=&quot;password&quot; /&gt;&lt;br/&gt; &lt;!--passwordï¼šå¯†ç ç±»åž‹--&gt; ç¡®è®¤å¯†ç ï¼š&lt;input type=&quot;password&quot;/&gt;&lt;br/&gt; æ€§åˆ«ï¼š&lt;input type=&quot;radio&quot; name=&quot;sex&quot; checked=&quot;checked&quot;/&gt;ç”· &lt;input type=&quot;radio&quot; name=&quot;sex&quot;/&gt;å¥³&lt;br/&gt; &lt;!--radioï¼šå•é€‰æ¡†; nameå±žæ€§ï¼šå¯å¯¹å…¶åˆ†ç»„; checkedï¼šé»˜è®¤é€‰é¡¹--&gt; å…´è¶£çˆ±å¥½ï¼š&lt;input type=&quot;checkbox&quot; checked=&quot;checked&quot;/&gt;Java &lt;input type=&quot;checkbox&quot;/&gt;JavaScript&lt;br/&gt; &lt;!--checkboxï¼šå¤é€‰æ¡†; checked:é»˜è®¤é€‰é¡¹--&gt; å›½ç±ï¼š &lt;select&gt;&lt;!--ä¸‹æ‹‰åˆ—è¡¨æ¡†æ ‡ç­¾--&gt; &lt;option&gt;--è¯·é€‰æ‹©å›½ç±--&lt;/option&gt;&lt;!--é€‰é¡¹æ ‡ç­¾--&gt; &lt;option selected=&quot;selected&quot;&gt;ä¸­å›½&lt;/option&gt; &lt;!--selectedï¼šé»˜è®¤é€‰æ‹©--&gt; &lt;option&gt;ç¾Žå›½&lt;/option&gt; &lt;option&gt;æ—¥æœ¬&lt;/option&gt; &lt;/select&gt;&lt;br/&gt; è‡ªæˆ‘è¯„ä»·ï¼š&lt;textarea rows=&quot;10&quot; cols=&quot;30&quot;&gt;é»˜è®¤å€¼&lt;/textarea&gt;&lt;br/&gt; &lt;!--textareaæ ‡ç­¾ï¼šå¤šè¡Œæ–‡æœ¬è¾“å…¥æ¡†ï¼›å±žæ€§ rows:è¡Œæ•°; å±žæ€§ colsï¼šåˆ—æ•°--&gt; &lt;!--textareaèµ·å§‹æ ‡ç­¾å’Œç»“æŸæ ‡ç­¾ä¸­çš„å†…å®¹æ˜¯é»˜è®¤å€¼--&gt; &lt;input type=&quot;reset&quot; value=&quot;é‡æ–°è¾“å…¥&quot;/&gt;&lt;br/&gt; &lt;!--resetï¼šé‡ç½®æŒ‰é’®; valueå±žæ€§ï¼šæ›´æ”¹æŒ‰é’®æ–‡æœ¬--&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt;&lt;br/&gt; &lt;!--submitï¼šæäº¤æŒ‰é’®--&gt; &lt;input type=&quot;button&quot; value=&quot;æŒ‰é’®&quot;/&gt;&lt;br/&gt; &lt;!--button:æŒ‰é’®--&gt; &lt;input type=&quot;file&quot;/&gt;&lt;br/&gt; &lt;!--file:æ–‡ä»¶ä¸Šä¼ --&gt; &lt;input type=&quot;hidden&quot;/&gt;&lt;br/&gt; &lt;!--hidden:éšè—åŸŸï¼Œéœ€è¦å‘é€ä¸€äº›ä¸éœ€è¦ç”¨æˆ·å‚ä¸Žçš„ä¿¡æ¯è‡³æœåŠ¡å™¨ï¼Œå¯ä½¿ç”¨éšè—åŸŸ--&gt;&lt;/form&gt; è¡¨å•æ ¼å¼åŒ–æŠŠè¡¨å•æ”¾å…¥è¡¨æ ¼ï¼Œä½¿è¡¨å•æŽ’åˆ—æ•´é½ã€‚ 12345678910111213141516171819202122&lt;form&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;ç”¨æˆ·åç§°ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; value=&quot;User&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ç”¨æˆ·å¯†ç ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ç¡®è®¤å¯†ç ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;æ€§åˆ«ï¼š&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; checked=&quot;checked&quot;/&gt;ç”· &lt;input type=&quot;radio&quot; name=&quot;sex&quot;/&gt;å¥³&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; è¡¨å•æäº¤çš„ç»†èŠ‚ä»¥ä¸‹æ ¼å¼åŒ–çš„è¡¨å•ï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;form action=&quot;https://localhost:8080&quot; method=&quot;get&quot;&gt; &lt;!--formæ ‡ç­¾å±žæ€§--&gt; &lt;!--actionï¼šè®¾ç½®æäº¤çš„æœåŠ¡å™¨åœ°å€--&gt; &lt;!--methodï¼šè®¾ç½®æäº¤çš„æ–¹å¼ï¼Œé»˜è®¤GETï¼ˆä¹Ÿå¯ä»¥æ˜¯POSTï¼‰--&gt; &lt;input type=&quot;hidden&quot; name=&quot;action&quot; value=&quot;login&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;ç”¨æˆ·åç§°ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; value=&quot;User&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ç”¨æˆ·å¯†ç ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ç¡®è®¤å¯†ç ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;æ€§åˆ«ï¼š&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; checked=&quot;checked&quot;/&gt;ç”· &lt;input type=&quot;radio&quot; name=&quot;sex&quot;/&gt;å¥³&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;å…´è¶£çˆ±å¥½ï¼š&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;checkbox&quot; checked=&quot;checked&quot;/&gt;Java &lt;input type=&quot;checkbox&quot;/&gt;JavaScript &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;å›½ç±ï¼š&lt;/td&gt; &lt;td&gt; &lt;select&gt; &lt;option&gt;--è¯·é€‰æ‹©å›½ç±--&lt;/option&gt; &lt;option selected=&quot;selected&quot;&gt;ä¸­å›½&lt;/option&gt; &lt;option&gt;ç¾Žå›½&lt;/option&gt; &lt;option&gt;æ—¥æœ¬&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;è‡ªæˆ‘è¯„ä»·:&lt;/td&gt; &lt;td&gt;&lt;textarea rows=&quot;10&quot; cols=&quot;30&quot;&gt;é»˜è®¤å€¼&lt;/textarea&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;reset&quot; value=&quot;é‡æ–°è¾“å…¥&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; æ ¼å¼åŒ–åŽçš„è¡¨å•æ˜¾ç¤ºä¸ºï¼š è¡¨å•æäº¤åŽï¼Œurlæ˜¾ç¤ºä¸ºï¼šhttps://localhost:8080/?action=login&amp;sex=on è¯¥urlä½“çŽ°äº†ä¸‰éƒ¨åˆ† æäº¤è¡¨å•çš„æœåŠ¡å™¨åœ°å€/actionå±žæ€§çš„å€¼ï¼šlocalhost:8080/ åˆ†éš”ç¬¦ï¼š? è¯·æ±‚å‚æ•°/è¡¨å•ä¿¡æ¯ï¼šaction=login; sex=on è¡¨å•æäº¤çš„æ—¶å€™ï¼Œæ•°æ®æ²¡æœ‰å‘é€ç»™æœåŠ¡å™¨çš„ä¸‰ç§æƒ…å†µï¼š è¡¨å•é¡¹inputæ ‡ç­¾æ²¡æœ‰nameå±žæ€§å€¼ã€‚ å•é€‰ã€å¤é€‰è¾“å…¥æ ‡ç­¾ä»¥åŠä¸‹æ‹‰åˆ—è¡¨çš„optionæ ‡ç­¾ï¼Œè¿˜éœ€è¦åŠ valueå±žæ€§å€¼ï¼Œä»¥ä¾¿å‘é€ç»™æœåŠ¡å™¨å…·ä½“å€¼ï¼Œè€Œä¸æ˜¯onã€‚ è¡¨å•é¡¹ä¸åœ¨æäº¤çš„formæ ‡ç­¾ä¸­ã€‚ ä¿®æ”¹åŽçš„è¡¨å•ä»£ç å¦‚ä¸‹ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;form action=&quot;https://localhost:8080&quot; method=&quot;get&quot;&gt; &lt;!--actionï¼šè®¾ç½®æäº¤çš„æœåŠ¡å™¨åœ°å€--&gt; &lt;!--methodï¼šè®¾ç½®æäº¤çš„æ–¹å¼ï¼Œé»˜è®¤GET--&gt; &lt;input type=&quot;hidden&quot; name=&quot;action&quot; value=&quot;login&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;ç”¨æˆ·åç§°ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;user&quot; value=&quot;User&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ç”¨æˆ·å¯†ç ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; name=&quot;password&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ç¡®è®¤å¯†ç ï¼š&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; name=&quot;password&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;æ€§åˆ«ï¼š&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; checked=&quot;checked&quot; value=&quot;boy&quot;/&gt;ç”· &lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;girl&quot;/&gt;å¥³&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;å…´è¶£çˆ±å¥½ï¼š&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;checkbox&quot; checked=&quot;checked&quot; name=&quot;hobby&quot; value=&quot;Java&quot;/&gt;Java &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;js&quot;/&gt;JavaScript &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;å›½ç±ï¼š&lt;/td&gt; &lt;td&gt; &lt;select name=&quot;country&quot;&gt; &lt;option value=&quot;none&quot;&gt;--è¯·é€‰æ‹©å›½ç±--&lt;/option&gt; &lt;option selected=&quot;selected&quot; value=&quot;ä¸­å›½&quot;&gt;ä¸­å›½&lt;/option&gt; &lt;option value=&quot;ç¾Žå›½&quot;&gt;ç¾Žå›½&lt;/option&gt; &lt;option value=&quot;æ—¥æœ¬&quot;&gt;æ—¥æœ¬&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;è‡ªæˆ‘è¯„ä»·:&lt;/td&gt; &lt;td&gt;&lt;textarea rows=&quot;10&quot; cols=&quot;30&quot;&gt;é»˜è®¤å€¼&lt;/textarea&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=&quot;reset&quot; value=&quot;é‡æ–°è¾“å…¥&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; è¡¨å•æäº¤åŽçš„url: https://localhost:8080/?action=login&amp;user=fred&amp;password=123&amp;password=123&amp;sex=girl&amp;hobby=Java&amp;hobby=js&amp;country=ä¸­å›½ è¡¨å•æ ‡ç­¾methodå±žæ€§å‚æ•°çš„åŒºåˆ« GETï¼š æµè§ˆå™¨çš„åœ°å€æ ä¸ºï¼šactionå±žæ€§å€¼ + ? + è¯·æ±‚å‚æ•° è¯·æ±‚å‚æ•°æ ¼å¼ä¸ºï¼šname=value&amp;name=value ä¸å®‰å…¨ æœ‰æ•°æ®é•¿åº¦é™åˆ¶ POSTè¯·æ±‚çš„ç‰¹ç‚¹ï¼š æµè§ˆå™¨ä¸Šçš„åœ°å€æ ä¸ºï¼šactionå±žæ€§å€¼ï¼ˆæ²¡æœ‰è¯·æ±‚å‚æ•°ï¼‰ ç›¸å½“äºŽGETè¯·æ±‚æ›´å®‰å…¨ ç†è®ºä¸Šæ²¡æœ‰æ•°æ®é•¿åº¦é™åˆ¶ divå’Œspan div æ ‡ç­¾ï¼šé»˜è®¤ç‹¬å ä¸€è¡Œ span æ ‡ç­¾ï¼šé•¿åº¦æ˜¯å°è£…æ•°æ®é•¿åº¦ p æ ‡ç­¾ï¼šé»˜è®¤åœ¨æ®µè½çš„ä¸Šæ–¹æˆ–ä¸‹æ–¹å„ç©ºå‡ºä¸€è¡Œï¼ˆå¦‚æžœå·²æœ‰ç©ºè¡Œåˆ™ä¸ç©ºï¼‰ labelæ ‡ç­¾labelæ ‡ç­¾ä¸ºinputå…ƒç´ å®šä¹‰æ ‡æ³¨ã€‚ è¯¥æ ‡ç­¾ä¸ä¼šä¸ºç”¨æˆ·å‘ˆçŽ°ç‰¹æ®Šçš„æ•ˆæžœï¼Œä½†ä¸ºé¼ æ ‡ç”¨æˆ·æ”¹è¿›äº†å¯ç”¨æ€§ï¼Œå³åœ¨labelå…ƒç´ å†…ç‚¹å‡»æ–‡æœ¬ï¼Œå°±ä¼šè§¦å‘è¯¥æŽ§ä»¶ã€‚å³å½“ç”¨æˆ·é€‰æ‹©è¯¥æ ‡ç­¾æ—¶ï¼Œæµè§ˆå™¨ä¼šè‡ªåŠ¨å°†ç„¦ç‚¹è½¬åˆ°å’Œlabelæ ‡ç­¾ç»‘å®šçš„è¡¨å•é¡¹ä¸Šã€‚ å¸¸è§çš„åº”ç”¨æƒ…å†µæ˜¯ï¼šå•é€‰æ¡†/å¤é€‰æ¡†ï¼Œç‚¹å‡»æ–‡æœ¬å³å¯å‹¾é€‰ï¼Œè€Œä¸éœ€è¦åŽ»ç‚¹é‚£ä¸ªæ¡†ã€‚ for : è¡¨ç¤ºè¯¥labelæ˜¯ä¸ºè¡¨å•ä¸­å“ªä¸ªæŽ§ä»¶æœåŠ¡ï¼Œforå±žæ€§ç‚¹å€¼è®¾ç½®ä¸ºè¯¥å…ƒç´ çš„idå±žæ€§å€¼ CSSCSSç®€ä»‹CSSï¼šå±‚å æ ·å¼è¡¨å•ï¼Œç”¨äºŽå¢žå¼º/æŽ§åˆ¶ç½‘é¡µæ ·å¼ï¼Œä¸”å…è®¸å°†æ ·å¼ä¿¡æ¯å’Œç½‘é¡µå†…å®¹åˆ†ç¦»çš„ä¸€ç§æ ‡è®°æ€§è¯­è¨€ã€‚ è¯­æ³•è§„åˆ™ï¼š é€‰æ‹©å™¨ï¼šæµè§ˆå™¨æ ¹æ®é€‰æ‹©å™¨å†³å®šå—CSSæ ·å¼å½±å“åˆ°HTMLå…ƒç´ /æ ‡ç­¾ã€‚ å±žæ€§ï¼šå±žæ€§:å€¼; å½¢æˆä¸€ä¸ªå®Œæˆçš„declarationã€‚ CSSä¸­çš„æ³¨é‡Šï¼š/**/ CSSä¸ŽHTMLç»“åˆæ–¹å¼æ ‡ç­¾ä¸­çš„styleåœ¨æ ‡ç­¾çš„styleå±žæ€§è®¾ç½®style=&quot;key: value1 value2;&quot; è¿™ç§æ–¹å¼å¯è¯»æ€§å·®ï¼Œä¸”æ²¡æœ‰å¤ç”¨æ€§ã€‚ headæ ‡ç­¾ä¸­ä½¿ç”¨styleæ ‡ç­¾åœ¨headæ ‡ç­¾ä¸­ï¼Œç”¨styleæ ‡ç­¾å®šä¹‰éœ€è¦çš„cssæ ·å¼ã€‚ styleæ ‡ç­¾ä¸­çš„è¯­å¥æ˜¯CSSè¯­æ³•ã€‚ 123456789&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style&gt; div{ border: 1px solid red; } &lt;/style&gt;&lt;/head&gt; å¯ä»¥åœ¨åŒä¸€é¡µé¢å¤ç”¨ä»£ç ï¼Œä¸èƒ½åœ¨å¤šä¸ªé¡µé¢å¤ç”¨CSSä»£ç ï¼Œä¸”ç»´æŠ¤ä¸æ–¹ä¾¿ï¼Œéœ€è¦ä¿®æ”¹æ¯ä¸ªé¡µé¢ã€‚ CSSæ–‡ä»¶æŠŠCSSæ ·å¼å†™æˆCSSæ–‡ä»¶ï¼Œåœ¨htmlæ–‡ä»¶çš„headæ ‡ç­¾ä¸­é€šè¿‡linkæ ‡ç­¾å¼•ç”¨ã€‚ style.css 123456div{ border: 1px red solid;}span{ border: 1px red solid;} div.html 123456789&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--linkæ ‡ç­¾ä¸“é—¨åœ¨headä¸­ç”¨æ¥å¼•å…¥CSSæ ·å¼ä»£ç --&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;style.css&quot;/&gt; &lt;!--rel:æ–‡æ¡£é—´çš„å…³ç³»--&gt; &lt;!--type:ç›®æ ‡URLçš„ç±»åž‹--&gt; &lt;!--href:URL--&gt;&lt;/head&gt; å¯ä»¥åœ¨å¤šä¸ªé¡µé¢ä¸­å¤ç”¨CSSæ ·å¼ï¼Œä¸”ç»´æŠ¤æ–¹ä¾¿ã€‚ CSSé€‰æ‹©å™¨æ ‡ç­¾åé€‰æ‹©å™¨1234æ ‡ç­¾å{ å±žæ€§:å€¼; å±žæ€§:å€¼;} æ ‡ç­¾åé€‰æ‹©å™¨å†³å®šå“ªäº›æ ‡ç­¾è¢«åŠ¨çš„ä½¿ç”¨è¿™ä¸ªæ ·å¼ã€‚ idé€‰æ‹©å™¨1234#idé€‰æ‹©å™¨{ å±žæ€§:å€¼; å±žæ€§:å€¼;} idé€‰æ‹©å™¨é€šè¿‡idå±žæ€§é€‰æ‹©æ€§çš„ä½¿ç”¨è¿™ä¸ªæ ·å¼ã€‚ htmlæ–‡ä»¶ 12&lt;div id=&quot;id001&quot;&gt;div1&lt;/div&gt;&lt;!--æ ‡ç­¾çš„idå±žæ€§--&gt; &lt;div id=&quot;id002&quot;&gt;div2&lt;/div&gt; CSSæ–‡ä»¶ï¼š 123456789101112&lt;style&gt; #id001{ border: yellow 1px solid; font-size: 30px; color: blue; } #id001{ border: 5px blue dotted; font-size: 20px; color: red; }&lt;/style&gt; class é€‰æ‹©å™¨1234.classå±žæ€§å€¼{ å±žæ€§:å€¼; å±žæ€§:å€¼;} classå±žæ€§å¤šç”¨æ¥åˆ†ç»„å®šä¹‰CSSæ ·å¼ã€‚ classé€‰æ‹©å™¨é€šè¿‡classå±žæ€§å€¼é€‰æ‹©æ€§ä½¿ç”¨è¿™ä¸ªæ ·å¼ã€‚ htmlæ–‡ä»¶ 12&lt;div class=&quot;class0&quot;&gt;div1&lt;/div&gt;&lt;!--æ ‡ç­¾çš„classå±žæ€§--&gt; &lt;div class=&quot;class0&quot;&gt;div2&lt;/div&gt; CSSæ–‡ä»¶ï¼š 1234567&lt;style&gt; .div{ color: blue; font-size: 30px; border: 1px yellow solid; }&lt;/style&gt; ç»„åˆé€‰æ‹©å™¨12345.class0, #id001{ color: blue; font-size: 30px; border: 1px yellow solid;} ç»„åˆé€‰æ‹©å™¨å¯ä»¥è®©å¤šä¸ªé€‰æ‹©å™¨å…±ç”¨åŒæ ·çš„CSSæ ·å¼ã€‚ å¸¸ç”¨æ ·å¼å…·ä½“å¯æŸ¥é˜… å­—ä½“é¢œè‰² color : red; color : rgb(33,33,13); color : #00F666; å®½åº¦ width : 19px; width : 20%; é«˜åº¦ height : 19px; height : 20%; èƒŒæ™¯é¢œè‰² background-color : #0F2222; å­—ä½“å¤§å° font-size : 20px; è¾¹æ¡† border : 1px solid red; DIVå±…ä¸­ï¼ˆç›¸å½“äºŽé¡µé¢çš„å±…ä¸­ï¼‰ 12margin-left : auto;margin -right : auto; æ–‡æœ¬å±…ä¸­ text-align : center; è¶…é“¾æŽ¥åŽ»ä¸‹åˆ’çº¿ text-decoration : none; è¡¨æ ¼ç»†çº¿ 1234567table{ border : 1px solid black; border-collapse : collapse;/*åˆå¹¶è¡¨æ ¼è¾¹æ¡†*/}td,th{ border : 1px, solid black;} åˆ—è¡¨åŽ»ä¿®é¥°ç¬¦ list-style : none","link":"/2020/07/21/html-css/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 1","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Everything youâ€™ve ever wanted is on the other side of fear. Topics: Introduction to cryptography Secure Communication and Shannonâ€™s definitions of perfect secrecy Perfect Indistinguishability definitions. The One-time pad construction Shannonâ€™s lower bound. Introduction6.875 is about Crypto, not Cryptocurrencies. 6.875 is about foundations, including Digital Signatures, Zero-knowledge Proofs, Public-key Encryption, Homomorphic Encryption, Threshold Cryptography, Pseudorandomness and so on. The Intellectual OriginsClaude E. Shannon The paper, â€œCommunication Theory of Secrecy Systemsâ€(1945), gave a way to define some terms in Secure Communication. The paper, â€œA mathematical Theory of Communicationâ€(1948), founded Information Theory. Alan M. Turing He was engaged on Cryptanalysis of Enigma Machine in 1938-39. The paper, â€œOn Computable Numbers, with an Application to the Entscheidungsproblemâ€(1936), give birth to Computer Science. Modern CryptographyModern Cryptography is a Practice to Theory and Back. The Definitions in CRYPTO are important. It solves the problems of Security, Privacy, Integrity(etc.) using Encryption, Digital Signatures, Pseudorandom Functions(etc.). It makes use of the ideas in Math and Theoretical CS to construct some tools, such as Interactive Proofs, Probabilistically checkable Proofs, Locally decodable Codes etc. 6.875 Themes The Omnipresent, Worst-case, Adversary. The central idea is to model the adversary.What they know? What they can do? What their goals are? Definitions will be our friend.If you cannot define something, you cannot achieve it. A key takeaway from 6.875.Cryptographic(or, adversarial) thinking.ã€è¿™é—¨è¯¾æœ€é‡è¦çš„æ˜¯åŸ¹å…»ä¸€ç§ä»Žæ”»å‡»è€…è§’åº¦çš„æ€è€ƒæ–¹å¼ï¼Œå¦‚ä½•æ‰èƒ½æŠµæŠ—å“ªäº›æœªçŸ¥çš„æ”»å‡»ã€‘ Computational Hardness will be our enabler. The central theme is to use cryptographic leash.Use computational hardness to â€œtameâ€ the adversary.ã€è®¡ç®—éš¾åº¦å°†æ˜¯æœ‰åˆ©çš„å·¥å…·ã€‘ Number theory is a classic source of hard problem.ã€æ•°è®ºæ˜¯å›°éš¾é—®é¢˜çš„ç»å…¸æ¥æºã€‘ [G.H.Hardy, â€œA Mathematicianâ€™s Apologyâ€] â€Both Gauss and lesser mathematicians may be justified in rejoicing that there is one such science [number theory] at any rate, whose very remoteness from ordinary human activities should keep it gentle and cleanâ€ã€é«˜æ–¯å’Œä¸€äº›æ•°å­¦å®¶éƒ½ä¼šè®¤ä¸ºåº”è¯¥ä¿æŒæ•°è®ºçš„çº¯æ´æ€§ï¼Œå³ä½¿å®ƒè¿œç¦»ä¸€èˆ¬çš„äººç±»æ´»åŠ¨ã€‚ã€‘ More recently, people get inspiration from geometry, coding theory, combinatorics.ã€çŽ°åœ¨ï¼Œä¹Ÿä¼šä»Žå‡ ä½•å­¦ã€ç¼–ç ç†è®ºã€ç»„åˆå­¦ä¸­èŽ·å¾—å›°éš¾é—®é¢˜ã€‘ Security Proofs via Reductions. The proof usually comes in a from of reduction.ã€å®‰å…¨è¯æ˜Žå¾€å¾€ä¼šè§„çº¦ä¸ºä¸€ç§å›°éš¾é—®é¢˜çš„æ±‚è§£ã€‘â€ If there is an (efficient) adversary that breaks scheme A w.r.t definition D, then there is an (efficient) adversary that factors large numberâ€ The reductions in 6.875 will be probabilistic and significantly more involved than the NP-hardness reductions.ã€è¿™é—¨è¯¾æ‰€æ¶‰åŠçš„è§„çº¦å°†æ˜¯æ¦‚çŽ‡æ€§çš„ï¼Œæ¯”NPéš¾åº¦ä¸­çš„è§„çº¦æ›´å¤æ‚ã€‘ 6.875 Topics6.875 will cover these topics. Pseudorandomnessã€ä¼ªéšæœºæ€§ã€‘ Secret-key Encryption and Authenticationã€å¯¹ç§°åŠ å¯†å’Œè®¤è¯ã€‘ Public-key Encryption and Digital Signaturesã€å…¬é’¥åŠ å¯†å’Œæ•°å­—ç­¾åã€‘ Cryptographic Hashingã€å“ˆå¸Œã€‘ Zero-knowledge Proofsã€é›¶çŸ¥è¯†è¯æ˜Žã€‘ Secure Multiparty Computationã€å®‰å…¨å¤šæ–¹è®¡ç®—ã€‘ Private Information Retrievalã€éšç§ä¿¡æ¯æ£€ç´¢ã€‘ Homomorphic Encryptionã€åŒæ€åŠ å¯†ã€‘ Advanced topicsï¼š Threshold Cryptography, Differential Privacy, â€¦ã€å‰æ²¿è¯¾é¢˜ï¼šé—¨é™å¯†ç ï¼Œå·®åˆ†éšç§ç­‰ã€‘ Secure CommunicationThe scenario in Secure Communication is that Alice wants to send a message M to Bob without revealing it to Eve. Itâ€™s actually very difficult. However, it can be achievable in such a setting which Alice and Bob meet beforehand to agree on a secret key k. Symmetric-key EncryptionIn order to achieve Symmetric-key Encryption, it is necessary to design three (possibly probabilistic) polynomial-time algorithm. Key Generation Algorithm Gen: $k\\leftarrow \\mathrm{Gen}(1^n)$ It has to be probabilistic (or, random).n is the desired output length.w Encryption Algorithm Enc: $c\\leftarrow \\mathrm{Enc}(k,m)$ Decryption Algorithm Dec: $m\\leftarrow \\mathrm{Dec}(k,c)$ The Worst-case AdversaryWhat dose the worst-case adversary looks like? Itâ€™s actually an arbitrary computationally unbounded algorithm EVE.ã€å®žé™…ä¸Šæ˜¯ä¸€ä¸ªè®¡ç®—èƒ½åŠ›æ— é™çš„ä»»æ„ç®—æ³•ã€‘ It knows Alice and Bobâ€™s algorithms Gen, Enc and Dec but does not know the key nor their internal randomness.ã€ç®—æ³•å…¬å¼€ï¼Œåªæœ‰å¯†é’¥æ˜¯æœªçŸ¥çš„ã€‘ Kerckhoffâ€™s principle or Shannonâ€™s maxim: - Gen, Enc and Dec are public algorithms. - The key is private. It can see the ciphertexts going through the channel.ã€èƒ½å¤Ÿçœ‹åˆ°ä¿¡é“é‡Œçš„å¯†æ–‡ã€‘(but cannot modify themâ€¦) Consequently, Security Definition actually defines what the adversary is trying to learn? What dose the security definition looks like? (intuitive)EVE should learning nothing about m from ciphertext c. $$ \\forall \\mathrm{EVE}, \\mathrm{Pr}[\\mathrm{EVE(Dec}(k,c)=m)]\\le \\frac{1}{\\mathcal{M}} $$ where k is generated by $k\\leftarrow \\mathrm{Gen}(1^n)$ and m is sampled in $m\\leftarrow \\mathcal{M}$ supposing $\\mathcal{M}$ is probabilistic uniform distribution. Itâ€™s important to note that $\\mathcal{M}$ is an arbitrary distribution as a matter of fact. The only thing we can control is the distribution of k.ã€Mçš„åˆ†å¸ƒå®žé™…ä¸Šæ˜¯ä»»æ„çš„ï¼Œä½†æˆ‘ä»¬èƒ½æŽ§åˆ¶kçš„åˆ†å¸ƒã€‘ Two Equiv. Def.s of SecurityIn this section, I will introduce two equivalent definitions of security, Shannonâ€™s perfect Secrecy Definition and Perfect Indistinguishable Definition. It will be easier to prove one of them in some cases. Shannonâ€™s Perfect Secrecy Def.The main idea in Shannonâ€™s perfect secrecy definition is the posteriori of attacker is equal to the priori of the attacker, i.e. A-posteriori= A-priori. ã€é¦™å†œå®‰å…¨å®šä¹‰çš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œæ”»å‡»è€…çš„åŽéªŒæ¦‚çŽ‡ç­‰äºŽå…ˆéªŒæ¦‚çŽ‡ï¼Œä¹Ÿå°±æ˜¯è¯´æ”»å‡»è€…çœ‹åˆ°å¯†æ–‡åŽèŽ·å¾—çš„ä¿¡æ¯ï¼ˆåŽéªŒï¼‰å’Œçœ‹åˆ°å¯†æ–‡å‰å¾—åˆ°çš„ä¿¡æ¯ï¼ˆå…ˆéªŒï¼‰æ˜¯ä¸€æ ·çš„ã€‘ ** Shannonâ€™s Perfect Secrecy Definition: ** $$ \\begin{array}{l}\\forall\\mathcal{M}, \\forall m\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\\\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]=\\operatorname{Pr}[\\mathcal{M}=m]\\end{array} $$ where $\\mathcal{M,K,C}$ are variables in message space, key space and ciphertext space respectively. There are two worlds, real world and ideal world. A-posteriori represents the real world.In real world, the attacker can see the ciphertext in the channel.So the probability that the attacker knows the message is m when the ciphertext c is known is $\\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]$ . A-priori represents the ideal world.In ideal world, the attacker can only guess the message when the ciphertext is unknown. So the probability is $\\operatorname{Pr}[\\mathcal{M}=m]$ . Perfect Indistinguishability Def.Perfect indistinguishability is a formalizaton of a turing test. There are two worlds, world 0 and world 1. The two worlds both sample the key $k$ from the key space $\\mathcal{K}$, and the world 0 uses $k$ to encrypt $m$ while world 1 uses $k$ to encrypt $mâ€™$. The attacker is a distinguisher that gets the $c$ and tries to guess which world sheâ€™s in. ã€æœ‰ä¸¤ä¸ªä¸–ç•Œï¼Œä¸€ä¸ªä¸–ç•Œç”¨éšæœºå¯†é’¥åŠ å¯†$m$ï¼Œå¦ä¸€ä¸ªç”¨éšæœºå¯†é’¥åŠ å¯†$mâ€™$ï¼Œæ”»å‡»è€…å¸Œæœ›èƒ½æ ¹æ®å¯†æ–‡åŒºåˆ†å¥¹åœ¨å“ªä¸€ä¸ªä¸–ç•Œã€‘ ** Perfect Indistinguishability Definition: ** $$ \\begin{array}{l}\\forall\\mathcal{M}, \\forall m,m'\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\\\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c\\right]=\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m')=c\\right]\\end{array} $$ where $\\mathcal{M,K,C}$ are variables in message space, key space and ciphertext space respectively. The Two Def.s are Equiv.The two definitions are equivalent. ** Theorem: ** An encryption scheme (Gen, Enc, Dec) satisfies secrecy IFF it satisfies perfect indistinguishability. The proof is simple use of Bayesâ€™ Theorem. Indistinguishability â†’ SecrecyHere we prove that if it satisfies perfect indistinguishability then it satisfies perfect secrecy. We know indistinguishability(IND) $$ \\begin{array}{l}\\forall\\mathcal{M}, \\forall m,m'\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\\\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c\\right]=\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m')=c\\right]\\end{array} $$ We want secrecy(SEC) $$ \\begin{array}{l}\\forall\\mathcal{M}, \\forall m\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\\\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]=\\operatorname{Pr}[\\mathcal{M}=m]\\end{array} $$ ** Proof: ** Suppose tha the Probability is equal to $\\alpha$ in IND. There is a key observation.$\\forall m \\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K,M})=c]=\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]$ The key and the message are both random in left while the message is fixed in right. Proof: $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K,M})=c]$ = $\\sum\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K,M})=c\\mid \\mathcal{M}=m]; \\operatorname{Pr}[\\mathcal{M}=m]$ = $\\sum\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]; \\operatorname{Pr}[\\mathcal{M}=m]$ = $\\alpha \\sum\\operatorname{Pr}[\\mathcal{M}=m]$ = $\\alpha$ We can use Bayesâ€™ theorem and the key observation to deduce the SEC. $\\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]$ = $\\frac{\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\mid \\mathcal{M}=m \\right] \\operatorname{Pr}[\\mathcal{M}=m]}{\\operatorname{Pr}\\left[ \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]}$ (Bayes) = $\\frac{\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c \\right] \\operatorname{Pr}[\\mathcal{M}=m]}{\\operatorname{Pr}\\left[ \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]}$ = $\\frac{\\alpha \\operatorname{Pr}[\\mathcal{M}=m]}{\\alpha}$ ï¼ˆkey observation) = $\\operatorname{Pr}[\\mathcal{M}=m]$ Secrecy â†’ IndistinguishabilityHere we prove that if it satisfies perfect secrecy then it satisfies perfect indistinguishability. We know secrecy(SEC) $$\\begin{array}{l}\\forall\\mathcal{M}, \\forall m\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]=\\operatorname{Pr}[\\mathcal{M}=m]\\end{array}$$ We want indistinguishability(IND) $$\\begin{array}{l}\\forall\\mathcal{M}, \\forall m,mâ€™\\in \\operatorname{Supp}(\\mathcal{M}),\\forall c\\in \\operatorname{Supp}(\\mathcal{C}) , \\ \\qquad \\qquad \\qquad \\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c\\right]=\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, mâ€™)=c\\right]\\end{array}$$ ** Proof: ** Just prove the above key observation. $\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, m)=c\\right]$ = $\\operatorname{Pr}\\left[\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\mid \\mathcal{M}=m \\right]$ = $\\frac{\\operatorname{Pr}\\left[ \\mathcal{M}=m\\mid\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c \\right]\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c ]}{\\operatorname{Pr}[\\mathcal{M}=m]}$ (Bayes) From SEC, we know:$\\operatorname{Pr}\\left[\\mathcal{M}=m \\mid \\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c\\right]=\\operatorname{Pr}[\\mathcal{M}=m]$ = $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K}, \\mathcal{M})=c ]$ = $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K}, mâ€™)=c ]$ One-time PadPerfect Secrecy is achievable using one-time pad. Perfect Secrecy is AchievableThe One-time Pad Construction: Gen: choose an n-bit string k at random, i.e. $k\\leftarrow {0,1}^n$ Enc(k,m), where M is an n-bit message: Output $c=m\\oplus k$ Dec(k, c): Output $m=c\\oplus k$ xor property: uniformityX is a uniform variable and $Y$ is a random variable.Then $Z=X\\oplus Y$ is a uniform variable. randomness$X$ is a fixed value and $Y$ is a random variable.The $Z=X\\oplus Y$ is a random variable. Correctness can be easily verified. $c\\oplus k=(m\\oplus k)\\oplus k=m$. ** Claim: ** One-time Pad achieves Perfect Indistinguishability (and therefore perfect secrecy) ** Proof: ** It requires that for any $m,mâ€™$, $c\\in{0,1}^n$, $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]=\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},mâ€™)=c]$ $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]$ = $\\operatorname{Pr}[\\mathcal{K}\\oplus m=c]$ = $\\operatorname{Pr}[\\mathcal{K}= m\\oplus c]$ = $1/2^n$ $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},mâ€™)=c]$ = $\\operatorname{Pr}[\\mathcal{K}\\oplus mâ€™=c]$ = $\\operatorname{Pr}[\\mathcal{K}= mâ€™\\oplus c]$ = $1/2^n$ QED. Reusing a One-time Pad?Is it secure when we reuse a one-time Pad? The answer is absolutely no. ** Claim: ** One-time Pad does not achieve Perfect Indistinguishability (and therefore not perfect secrecy). ** Proof: ** It requires that for all pairs $(m_0,m_1),(m_0â€™,m_1â€™),(c_0,c_1)\\in{0,1}^{2n}$,$\\operatorname{Pr}[\\operatorname{Enc}(k, m _0)=c_0 \\text { and } \\operatorname{Enc}(k, m_1)=c_1]=\\operatorname{Pr}[\\operatorname{Enc}(k, m _0â€™)=c_0 \\text { and } \\operatorname{Enc}(k, m_1â€™)=c_1]$ We want to pick $(m_0,m_1),(m_0â€™,m_1â€™),(c_0,c_1)\\in{0,1}^{2n}$ which makes the left of the equation not equal to the right. Pick $m_0=m_1=m,m_0â€™\\ne m_1â€™$ and $c_0=c_1=c$ The left side $\\operatorname{Pr}[\\operatorname{Enc}(k, m _0)=c_0 \\text { and } \\operatorname{Enc}(k, m_1)=c_1]$ = $\\operatorname{Pr}[\\operatorname{Enc}(k, m )=c]$ = $1/2^n$ The right side $\\operatorname{Pr}[\\operatorname{Enc}(k, m _0â€™)=c_0 \\text { and } \\operatorname{Enc}(k, m_1â€™)=c_1]$ = 0 QED. A Serious LimitationPerfect secrecy has its price. A serious limitation of perfect secrecy is that the key space must be greater than or equal to the message space. ** Theorem: ** For any perfectly secure encryption scheme, $\\mathcal{|K|\\ge|M|}$. ** Proof(by picture): ** Assume for contradiction that $\\mathcal{|K|&lt;|M|}$. If we pick any $c\\in \\mathcal{C}$, there are at most $|\\mathcal{K}|$ possible messages using distinct keys.Suppose the possible message space is the bluer part in the left ellipse which the size is at most $|\\mathcal{K}|$ and less than $\\mathcal{|M|}$ . Choose the message $m$ inside the bluer part. $m$ is the possible message.So the probability is $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},m)=c]&gt;0$. Choose the message $\\tilde{m}$ outside the bluer part. $\\tilde{m}$ canâ€™t be the possible message. So the probability is $\\operatorname{Pr}[\\operatorname{Enc}(\\mathcal{K},\\tilde{m})=c]=0$ There is a contradiction. SolutionSo, what are we to do? The solution is to RELAX the definition. The adversary we mentioned above is an arbitrary computationally unbounded algorithm EVE. Now, we relax the adversary to an arbitrary computationally bounded algorithm.","link":"/2022/06/29/mit6875-lec1/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 10","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Todayâ€™s topic is Digital Signatures. Topics Covered: Motivation for Digital Signatures. Definition: EUF-CMA Security One-time signatures: Lamportâ€™s Scheme. how to sign a single bit, once how to sign $n$ bits, once Collision-resistant hashing how to sign polynomially many bits, once. Digital SignaturesDigital Signatures vs. MACsMACsIn Lecture 5, we mentioned the applications of PRF and one of them is authentication. Message Authentication Codes(MAC) can be evaluated by PRF, the message taken as input. MAC gives the Authenticity that Bob is able to ensure the messages came from Alice. Yet it needs Alice and Bob to share a secret key beforehand. Then Alice uses the $sk$ to produce the MAC. Digital SignaturesDigital Signatures is the Public-key analog of MACs. The goal: Only Alice can produce signatures. Bob (or anyone else) can verify them. There is a pair of keys, secret key $sk$ and the corresponding (public) verification key $vk$. (Public) verification keys are stored in a â€œdictionaryâ€. Comparison Signatures MACs 1 n uses require n key-pairs $n$ user require $n^2$ keys 2 Publicly Verifiable Privately Verifiable 3 Transferable Not transferable 4 Provides Non-Repudiation Dose not provide Non-Repudiation Signatures have public verification keys and private secret keys, so $n$ uses require $n$ key-pairs.But MAC have (private) shared keys beforehand, so $n$ users require $n^2$ keys. Anyone can verify the signatures using the public $vk$.But only Bob who has the secret key $sk$ can verify the MACs. Signatures are Transferable. That is, you can take signatures and show it to others. Signatures provides Non-Repudiation. That is, you cannot claim that you didnâ€™t sign it.[*ä¸å¯æŠµèµ–] Transferability and Non-Repudiation whether they are good properties or bad properties depends on the scenario. It is double edged sword. ApplicationsThere are abundant applications of signatures. Certificates, or a public-key directory in practice. Trusted Certificate Authority(CA), e.g. Verisign, Letâ€™s Encrypt. When Alice (=www.google.com) wants to register her public (encryption and signing) keys $pk$ and $vk$, CA first checks that she is Alice. CA issues a â€œcertificateâ€ $\\sigma\\leftarrow Sign(SK_{Verisign},Alice||pk||vk)$.The certificate is essentially the signature.The certificate indicates that $pk$ and $sk$ are indeed Aliceâ€™s. CA produces the signature using its sign key $SK_{Verisign}$. Alice can later stores this certificate to prove she â€œownsâ€ $pk$ and $sk$. Browsers store $VK_{Verisign}$ and check the certificate.$VK_{Verisign}$ is the public verification key of CA. Bitcoin and other cryptocurrencies. I am identified by my verification key $vk$. When I pay you (your verification key = $vkâ€™$), I sign â€œ$x$ paid to $vkâ€™$â€ with my $sk$. DefinitionThe definition of Digital Signatures consists of a triple of PPT algorithms $(Gen,Sign,Verify)$. $Gen(1^n)\\rightarrow (vk,sk)$ running by Alice(signer)PPT Key Generation algorithm generates a public-private key pair. $Sign(sk,m)\\rightarrow \\sigma$ running by Alice (signer)(possible probabilistic) Signing algorithm uses the secret signing key to produce a signature $\\sigma$.Whether deterministic or probabilistic Signing algorithm makes sense. $Verify(vk,m,\\sigma)\\rightarrow Acc(1)/Rej(0)$ running by Bob (any verifier)Verification algorithm uses the public verification key to check the signature $\\sigma$ against a message $m$. Correctness: For all $vk,sk,m$: $Verify(vk,m,Sign(sk,m))=accept$. EUF-CMA SecurityDefine the adversary first. The adversary after seeing signatures of many messages, should not be able to produce a signature of any new message. Adversary: Power of adversary: Chosen-message attackThe adversary can request for, and obtain, signatures of (poly. many) messages $m_1,m_2,\\dots$ Goal of adversary: Existential ForgeryThe adversary wins if she produces a signature of any new message $m^*\\notin \\{m_1,m_2,\\dots \\}$. Then we give Existentially Unforgeable against a Chosen Message Attack (EUF-CMA) security by a game. Game : The Challenger generates a public-private key pair $(vk,sk)$ and sends the public verification key $vk$ to Eve. Eve can request for poly. many messages $\\{m_1,m_2,\\dots\\}$ and obtain the corresponding signatures $\\{\\sigma_1,\\sigma_2,\\dots\\}$. Eve produce a signature $\\sigma^*$ against a new message $m^*\\notin \\{m_1,m_2,\\dots\\}$. EUF-CMA Definition: Eve wins if $Verify(vk,m^*,\\sigma^*)=1$ and $m^*\\notin\\{m_1,m_2,\\dots\\}$. The signature scheme is EUF-CMA-secure if no PPT Eve can win with probability better than $negl(n)$. The challenger gives Eve a lot of signatures. Now Eve wants to forge a signature of other message right. We say that she wins if she produces a signature right of even one message that was not signed already. We call this an existential forgery because there exists a $m^*$ not in the set for which she produces a signature. Itâ€™s a very strong definition. But the definition dose not prevent the adversary from producing a new signature for the same message. Yet she dose not win. In other words, itâ€™s consistent with the definition to allow the adversary to produce a new signature for the same message. We can make the job easier for adversary. She wins as well if she produces a new signature for the same message. Then we can get strong EUF-CMA definition. So the stronger adversary, the stronger security by definition. Lamport (One-time) SignaturesIn this section, we introduce a beautiful signature, Lamport Signature. Itâ€™s One-time signature sort of like One-time Pads. The one-time signatures means that the adversary gets a signature of some message once, a single message, and she should not be able to produce a signature of any other different message. How to sign a bitWe only use the signing key to sign once and we are really signing a bit. $Gen(1^n)\\rightarrow (SK,VK)$ Signing Key $SK:[x_0,x_1]$where $x_0,x_1$ are both $n$-bit string. Verification Key $VK:[y_0=f(x_0),y_1=f(x_1)]$where $f$ is a OWF. $Sign(SK,b)\\rightarrow \\sigma$ where $b$ is a bit The signature is $\\sigma=x_b$. $Verify(VK,b,\\sigma)$ Check if $f(\\sigma)\\overset ? = y_b$ The signing keys $SK$ are two random $n$-bit string and the verification keys $VK$ are the corresponding values of OWF, i.e. the signing keys are images of verification keys. The game in Lamport (One-time) Signatures differs from that Eve only gets a signature of a single message. If $b=0$, the challenger gives Eve $x_0$, the signature of $0$, and Eve wants to forge the signature of $1$. So the task of adversary is producing a signature of $1$. Thatâ€™s $x_1$. She cannot do it because $x_1$ is random. Claim : Assuming $f$ is a OWF, no PPT adversary can produce a signature of $\\bar{b}$ given a signature of $b$. The intuition of proof is easy. Thereâ€™s no way Eve can produce $x_1$ unless she can invert the $y_1$ she have. How to sign n bitsWe can use the signing keys to sign $n$ bits, once. Just repeat it. $Gen(1^n)\\rightarrow (SK,VK)$ Signing Key $ SK:\\left[\\begin{array}{c}x_{1,0},x_{2,0},\\dots, x_{n,0}\\\\ x_{1,1},x_{2,1} ,\\dots, x_{n,1}\\end{array} \\right]$ where $x_{\\cdot,\\cdot}$ is $n$-bit random string and each column is a pair-key for one bit. Verification Key $VK:\\left[\\begin{array}{c}y_{1,0},y_{2,0},\\dots, y_{n,0}\\\\ y_{1,1},y_{2,1} ,\\dots, y_{n,1}\\end{array} \\right]$ where $f$ is a OWF and $y_{i,c}=f(x_{i,c})$. $Sign(SK,\\vec m)\\rightarrow \\vec\\sigma$ where $m$ is a $n$-bit message $(m_1,\\dots, m_n)$. The signature is $\\vec \\sigma=(x_{1,m_1},\\dots, x_{n,m_n})$. $Verify(VK,\\vec m,\\vec\\sigma)$ Check if $\\forall i:f(\\sigma_i)\\overset ? = y_{i,m_i}$. In the game, Eve can request for a signature once of any $n$-bit message $m$ and she wants to forge the signature of a different message $mâ€™\\ne m$. There are two claims. Claim 1: Assuming $f$ is a OWF, no PPT adversary can produce a signature of $mâ€™$ given a signature of a single message $m\\ne mâ€™$. Claim 2: The adversary can forge signature on any message given the signatures on (some) two messages. We only give the proof of Claim 1. Claim 2 can be comprehended easily after proving Claim1. Proof for Claim 1: Suppose for the contradiction that there is a adversary forging a signature of $mâ€™$ given a signature of a single message $m$ s.t. $mâ€™\\ne m$. We want to construct a OWF Inverter for $y$ so we need to interact with the forger. Interaction with the forger: Inverter gives the verification keys $VK$ to the forgery. The forger request for a signature of a single message $m$. Inverter produces the signature $\\sigma$ she wants. Then the forger promises to give a signature $\\sigmaâ€™$ against a new message $mâ€™$ The key idea is we can take $y$ into $VK$ and plop it into one of the two slots in one column. The thing to notice is that there is at least one different bit between $m$ and $mâ€™$. Note: The message $m$ is what the forger wants to obtain its signature The message $mâ€™$ is what the forger wants to forge its signature. For simplicity, we suppose there is only one different bit between $m$ and $mâ€™$. messages $m$ and $mâ€™$ are known in advance: Suppose the Inverter knows $m$ and $mâ€™$in advance.For simplicity, suppose $m=00\\dots 0$ and $mâ€™=10\\dots 0$. The Inverter wants to get the inverse of $y$ against the OWF $f$. So the Inverter interacts with the forger. The Inverter samples $SK$ and generates the verification keys (put $y$ into one slot as follows) $$ VK=\\left[\\begin{array}{c}f(x_{1,0}) &,f(x_{2,0}),\\dots, f(x_{n,0})\\\\ y &,f(x_{2,1}) ,\\dots, f(x_{n,1})\\end{array} \\right] $$ where $x_{\\cdot,\\cdot}$ is known to the Inverter. The forger requests for the signature of $m=00\\dots 0$. The Inverter produces the signature $\\sigma=(x_{1,0},\\dots,x_{n,0})$. The forger promises to produce the signature on $mâ€™=10\\dots0$ from the contradiction which has the inverse of $y$. Done. However, the Inverter dosenâ€™t know the two messages in advance.The Invert only knows there is one different bit between $m$ and $mâ€™$. So the Inverter could guess it. messages $m$ and $mâ€™$ are unknown in advance: The Inverter guesses the different bit locates in $i$-th bit right w.p. $1/n$. Suppose the Inverter plants the trap $y$ into the slot $(i,0)$, $i$-th column and $0$-th row.Then the generated verification keys is as follows. $$ VK=\\left[\\begin{array}{c}f(x_{1,0}) ,\\dots ,&y&,\\dots, f(x_{n,0})\\\\ f(x_{1,1}) ,\\dots ,&f(x_{i,1})& ,\\dots, f(x_{n,1})\\end{array} \\right] $$ where $x_{\\cdot,\\cdot}$ is known to the Inverter. In the forgerâ€™s point of view, when she looks at the verification keys $VK$, she has no information about where the trap is. There are two events that could happen even if $m$ differs from $mâ€™$ in $i$-th bit. The message $m$ hits the trap while the $mâ€™$ dose not hit the trap. The message $m$ dosenâ€™t hit the trap while the $mâ€™$ hits the trap. The Inverter can only handle with the second event.The message $m$ hits the opposite location to the trap while the $mâ€™$ hits the trap. The Inverter is able to give the signature of $m$, i.e. $(\\dots,x_{i,1},\\dots)$ The forger promises to produce the signature of $mâ€™$from the contradiction, which have the inverse of $y$. Otherwise, the forger gives something that the Inverter already has known. So the probability of inverting right is $\\varepsilon/2n$ if the advantage of forging is non-negligibly $\\varepsilon$.The $1/n$ is the probability of guessing the location of different bit and the $1/2$ is the probability of guessing whether $m$ hits the trap. Done. Besides, Claim 2 says that once I give you the two signatures, I am actually giving you all the inverses and you can produce signatures of any message you want. This violates one-wayness. So far, the length of message is limited by the size of verification keys. Before proceeding to the next question that how to sign poly. many bits, we take a detour in collision-resistant hash function. Detour: Collision-Resistant Hash FunctionsA compressing function $h:\\{0,1\\}^m\\rightarrow \\{0,1\\}^n$ (where $m&gt;n$ ) for which it is computationally hard to find collisions. It compresses the bits sort of the opposite of PRG. Definition: $h$ is collision-resistant if for every PPT algorithm $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[A(1^n)=(x,y):x\\ne y, h(x,y)]=\\mu(n) $$ In theory, we like to talk about families of functions to handle non-uniform adversaries (who could have hardcoded collision for the fixed function $h$). A compressing family of functions $\\mathcal{H}=\\{\\{0,1\\}^m\\rightarrow \\{0,1\\}^n\\}$ (where $m&gt;n$) for which it is computationally hard to find collisions. Collision-Resistant Definition: $\\mathcal{H}$ is collision-resistant if for every PPT algorithm $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}_{h\\gets \\mathcal{H}}[A(1^n)=(x,y):x\\ne y, h(x,y)]=\\mu(n) $$ How to sign poly. many bitsBack to the question that how to sign poly. many bits with a fixed verification key. The key idea is hashing the message into $n$ bits and sign the hash. $Gen(1^n)\\rightarrow (SK,VK)$ Signing Key $SK:\\left[\\begin{array}{c}x_{1,0},x_{2,0},\\dots, x_{n,0}\\\\ x_{1,1},x_{2,1} ,\\dots, x_{n,1}\\end{array} \\right]$ where $x_{\\cdot,\\cdot}$ is $n$-bit random string and each column is a pair-key for one bit. Verification Key $VK:\\left[\\begin{array}{c}y_{1,0},y_{2,0},\\dots, y_{n,0}\\\\ y_{1,1},y_{2,1} ,\\dots, y_{n,1}\\end{array} \\right]$ where $f$ is a OWF and $y_{i,c}=f(x_{i,c})$. Sample $h\\gets \\mathcal{H}$. $Sign(SK,\\vec m)\\rightarrow \\vec\\sigma$ where $m$ is a $n$-bit message $(m_1,\\dots, m_n)$. Compute the hash $z=h(m)$. The signature is $\\vec \\sigma=(x_{1,z1},\\dots, x_{n,z_n})$. $Verify(VK,\\vec m,\\vec\\sigma)$ Recompute the hash $z=h(m)$ Check if $\\forall i:f(\\sigma_i)\\overset ? = y_{i,z_i}$. Claim : Assuming $f$ is a OWF and $\\mathcal{H}$ s a collision-resistant family, no PPT adversary can produce a signature of $mâ€™$ given a signature of a single $m\\ne mâ€™$. We only give the idea of proof. Intuition of Proof: Suppose for the contradiction. There are two possibilities. Either the adversary picked $mâ€™$ s.t. $h(mâ€™)=h(m)$, in which case she violated collision-resistance of $\\mathcal{H}$. Or she produced a Lamport signature on a â€œmessageâ€ $zâ€™\\ne z$, in which case she violated one-time security of Lamport, and therefore the one-wayness of $f$.","link":"/2022/07/28/mit6875-lec10/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 11","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Todayâ€™s topic is Many-time Digital Signatures. Topics Covered: Many-time, stateful, signature schemes. Naor-Yung construction: stateless EUF-CMA-secure signature schemes. In last blog was introduced the definition of Digital Signatures and the EUF-CMA Security. Moreover, we introduced Lamport (One-time) Digital Signatures. We can use it to sign polynomially many bits with a fixed verification key. The main idea is hashing the message into $n$ bits and signing the hash. So far, itâ€™s one-time security. The adversary can forge signature on any message given the signatures on (some) two messages. How to achieve Many-time Signature Scheme ? Itâ€™s todayâ€™s gist. We will achieve many-time signature scheme in four+ steps. Stateful, Growing Signatures.Idea: Signature Chains How to Shrink the signatures.Idea: Signature Trees How to Shrink Aliceâ€™s storage.Idea: Pseudorandom Trees How to make Alice stateless.Idea: Randomization (optional). How to make Alice stateless and deterministic.Idea: PRFs. S1: Stateful Many-time SignaturesThe first step is to achieve stateful many-time signatures. The main idea is Signature Chains. sign m1 Alice starts with a secret signing Key $SK_0$.Her public verification Key $VK_0$ is stored in the (public) â€œdirectoryâ€. When signing a message $m_1$: Generate a new pair $(VK_1,SK_1)$. Produce signature $\\sigma_1\\leftarrow Sign(SK_0,m_1||VK_1)$. Output $VK_1||\\sigma_1$ Remember $VK_1||m_1||\\sigma_1$ as well as $SK_1$. Alice is going to sign not only the message, but the message together with the next verification key. She uses it to authenticate a new verification. Itâ€™s the concatenation of two strings and we can sign polynomially many bits. To verify a signature $VK_1||\\sigma_1$ for message $m_1$: Run $Verify(VK_0,m_1||VK_1,\\sigma_1)$. We use $VK_0$ to verify the signature $\\sigma_1$ that $m_1$is sent from Alice and $VK_1$ is authenticated from Alice. sign m2But how about the next message $m_2$ ? Can we just output $VK_2||\\sigma_2$ as follows ? (NO!) When signing the next message $m_2$: Generate a new pair $(VK_2,SK_2)$. Produce signature $\\sigma_2\\leftarrow Sign(SK_1,m_2||VK_2)$. Output $VK_2||\\sigma_2$ The thing to point is that each signing is independent and everyone only knows the verification key $VK_0$ in that â€œdirectoryâ€. The verifier dosenâ€™t know the verification key $VK_1$ for the signature $\\sigma_2$ nor the authentication for $VK_1$ when he receives $VK_2||\\sigma_2$. So Alice needs to send $VK_1$ as well as the authentication for $VK_1$. We should output $VK_1||m_1||\\sigma_1||VK_2||\\sigma_2$ as follows. When signing the next message $m_2$: Generate a new pair $(VK_2,SK_2)$. Produce signature $\\sigma_2\\leftarrow Sign(SK_1,m_2||VK_2)$. Output $VK_1||m_1||\\sigma_1||VK_2||\\sigma_2$. (additionally) Remember $VK_2||m_2||\\sigma_2$ as well as $SK_2$. The first part $VK_1||m_1||\\sigma_1$ is to authenticate the verification key $VK_1$. The verify uses $VK_1$ to verify the message $m_2$ together with the next verification $VK_2$. To verify a signature $VK_1||m_1||\\sigma_1||VK_2||\\sigma_2$ for message $m_2$: Run $Verify(VK_0,m_1||VK_1,\\sigma_1)$ to authenticate $VK_1$. Run $Verify(VK_1,m_2||VK_2,\\sigma_2)$ to authenticate the message $m_2$ together with the next verification key $VK_2$. Itâ€™s growing signatures since Alice needs remember the $VK_i||m_i||\\sigma_i$ as well as $SK_i$. And the signature chains is as follows. An optimizationIn fact, Alice stores the $m_i$ just to use $\\sigma_i$ to authenticate $VK_i$. So there is an optimization that need to remember only the past verification keys, not the past messages. Suppose we can split the verification into two halves. We use part of $VK_i$ to sign $m_{i+1}$ and the rest to sign $VK_{i+1}$. The signature chains is as follows. The verifier only needs to verify the past verification keys, not the past messages. ProblemsThere are still two major problems. Alice is stateful.Alice needs to remember a whole lot of things, $\\mathcal{O}(T)$ information after $T$ steps. The signatures grow.Length of the signature of the $T$-th message is $\\mathcal{O}(T)$. S2: How to Shrink the signatures ?The next step is to shrink the signature. The main idea is Signature Trees. Alice starts with a secret signing Key $SK_\\epsilon$.Her public verification Key $VK_\\epsilon$ is stored in the (public) â€œdirectoryâ€. Alice generates many random $(VK,SK)$ pairs and arrange them in a tree of depth = security parameter $\\lambda$.There are $2^\\lambda$ leaves and Alice only uses the leaf to sign the message. When signing the first message $m_0$ Alice only uses the leaf to sign the message while use the parent node to sign both children nodes. Use $VK_{000}$ to sign $m_0$. $\\tau_0\\gets Sign(SK_{000},m_0)$ â€œAuthenticate â€ $VK_{000}$ using the â€œsignature pathâ€.Alice produces the authentication path for $VK_{000}$ : $(\\sigma_\\epsilon,\\sigma_0,\\sigma_{00})$. Authenticate $VK_0$: use $VK_\\epsilon$ to sign both $VK_0$ and $VK_1$ $\\sigma_\\epsilon\\gets Sign(SK_\\epsilon,VK_{0},VK_{1})$ Authenticate $VK_{00}$: use $VK_{0}$ to sign both $VK_{00}$ and $VK_{01}$$\\sigma_0\\gets Sign(SK_0,VK_{00},VK_{01})$ Authenticate $VK_{000}$: use $VK_{00}$ to sign both $VK_{000}$ and $VK_{001}$ $\\sigma_{00}\\gets Sign(SK_{00},VK_{000},VK_{001})$ Signatures of $m_0$: (Authentication path for $VK_{000}$, $\\tau_0\\gets Sign(SK_{000},m_0)$) When signing the next message $m_1$ Use $VK_{001}$ to sign $m_1$. $\\tau_1\\gets Sign(SK_{001},m_1) $ â€œAuthenticate â€ $VK_{001}$ using the â€œsignature pathâ€.Alice produces the authentication path for $VK_{001}$ : $(\\sigma_\\epsilon,\\sigma_0,\\sigma_{00})$. Authenticate $VK_0$: $\\sigma_\\epsilon\\gets Sign(SK_\\epsilon,VK_{0},VK_{1})$ Authenticate $VK_{00}$: $\\sigma_0\\gets Sign(SK_0,VK_{00},VK_{01})$ Authenticate $VK_{000}$: $\\sigma_{00}\\gets Sign(SK_{00},VK_{000},VK_{001})$ Signatures of $m_1$: (Authentication path for $VK_{001}$, $\\tau_1\\gets Sign(SK_{001},m_1)$) The good news is the signatures consist of $\\lambda$ one-time signatures and do not grow with time. But the bad news is the signer generates and keeps the entire ($\\approx 2^\\lambda$-size) signature tree in memory. Besides, the signer also needs to remember the state that what is the last leaf used for signing. S3: How to Shrink Aliceâ€™s storage.The main idea is Pseudorandom Trees. Instead of truly random signature trees, Alice uses PRF to build a pseudorandom signature trees. Alice keeps a secret PRF key $K$. Alice populates the nodes with $r_x=PRF(K,x)$ Use $r_x$ to derive the key pair $(VK_x,SK_x)\\gets Gen(1^\\lambda;r_x)$. The thing to notice is that Alice only registers the verification key $VK_\\epsilon$.So the verifier only knows $VK_\\epsilon$, not the PRF key. We can use the pseudorandom signature tree to sign many-time signatures same as above. As a matter of fact, the signer can do lazy evaluation instead of evaluating every node beforehand. So the signer can achieve short signatures and small storage at the same time. However, itâ€™s still stateful. The signer still needs to keep a counter indicating which leaf (which tells her which secret key) to use next. It proceeds to the next step. S4: How to make Alice stateless.The main idea is randomization. We can achieve stateless via randomization. When signing a message $m$ Pick a random leaf $r$. Use $VK_r$ to sign $m$.$\\sigma_r\\gets Sign(SK_r,m)$ Output $(r,\\sigma_r,\\text{authentication path for }VK_r)$. The good news is itâ€™s stateless. But we cannot pick the same leaf twice since we are using the one-time signature scheme. The key idea of security analysis is birthday attack. If the signer produces $q$ signatures, the probability she picks the same leaf twice is $\\le q^2/2^\\lambda$, which is negligible. S5: How to make Alice stateless and deterministic.The key idea is generating $r$ pseudo-randomly. Have another PRF key $Kâ€™$ and let $r=PRF(Kâ€™,m)$. When signing a message $m$ Pick a pseudorandom leaf $r=PRF(Kâ€™,m)$. Use $VK_r$ to sign $m$.$\\sigma_r\\gets Sign(SK_r,m) $ Output $(r,\\sigma_r,\\text{authentication path for }VK_r)$. Security AnalysisFor simplicity, we analyze the randomization stateless scheme. (S4) The many-time digital signature scheme is EUF-CMA secure if the one-time digital signature is one-time secure. Assume for the contradiction that there is an adversary breaking the EUF-CMA security, then we can construct a one-time forger. We have the adversary $\\mathcal{A}$ for EUF-CMA security. Get the verification key $VK$. Request for signatures of $q$ messages. ($q$-time) Request for message $m_i$ Obtain the signature $\\sigma_i$ for $m_i$. Produce $(m^*,\\sigma^*)$ that a signature against a new message $m^*\\notin \\{m_1,m_2,\\dots m_q\\}$ with non-negligible advantage. We want to construct a forger $\\mathcal{B}$ for one-time security. Get the one-time verification key $OVK$. Request for the signatures $\\sigma$ of a single message $m$. (only one-time) Produce $(mâ€™,\\sigmaâ€™)$ that a signature against a new message $mâ€™\\ne m$. For simplicity, we condition on the event $E$ where all our random $r$â€™s are distinct. $\\operatorname{Pr}[\\mathcal{A}\\text{ wins }\\mid E]\\ge q^2/2^\\lambda$ Suppose the probability above dosenâ€™t change very much. $\\ge \\operatorname{Pr}[\\mathcal{A}\\text{ wins }]-\\operatorname{Pr}[E]=1/poly(\\lambda)-negl(\\lambda)$ $\\ge 1/poly(\\lambda)$. So the advantage of $\\mathcal{A}$ is non-negligible even on the condition. So We need the forger $\\mathcal{B}$ to interact with the adversary $\\mathcal{A}$ to win the game. Construction of One-time Forger $\\mathcal{B}$: Plop $OVK$ into a random leaf $r$. $\\mathcal{B}$ get the one-time verification key $OVK$. $\\mathcal{B}$ generates the pseudorandom signature trees and plop the $OVK$ into a random leaf $r_{OVK}$. $\\mathcal{B}$ sends the root verification key $VK_\\epsilon$ to $\\mathcal{A}$. $\\mathcal{A}$ requests for the signatures for $q$ times.For each message $m_i$, there are two cases. If $\\mathcal{B}$ picks the random leaf $r\\ne r_{OVK}$, $\\mathcal{B}$ is able to produce the signature. If $\\mathcal{B}$ picks the random leaf $r= r_{OVK}$, $\\mathcal{B}$ cannot produce the signature. But $\\mathcal{B}$ can request the signature for a single message. $\\mathcal{B}$ requests for the message $m_i$. $\\mathcal{B}$ obtains the signature $\\sigma_i$ for the single message $m_i$. $\\mathcal{B}$ passes the signature $\\sigma_i$ to the $\\mathcal{A}$ as the response. Note: $\\mathcal{B}$ can only picks $r_{OVK}$ once. Besides, itâ€™s necessary to pick it. $\\mathcal{A}$ promises to produce the signature for a new message $m^*\\notin \\{m_1,m_2,\\dots m_q\\}$ The signature consists of $(r^*,\\sigma^*,\\text{authentication path for }VK_{r^*})$ If $r^*= r_{OVK}$ and $\\sigma ^*$ is different from the previous $\\sigma_i$ generated by $\\mathcal{B}$. Then $\\mathcal{B}$ wins. $\\mathcal{B}$ just passes the $(m^*,\\sigma^*)$ as the forgery signature. But it could happen only if $\\mathcal{A}$ picks $r^*$ from one of the leaves $\\{r_1,r_2,\\dots r_q\\}$ given by $\\mathcal{B}$. Claim : If $\\mathcal{A}$ picks one of the leaves $\\{r_1,r_2,\\dots r_q\\}$ when forging, then $\\mathcal{B}$ can produce the one-time forgery w.p. $1/q$. But there is no guarantee that $\\mathcal{A}$ could pick one of the leaves $\\{r_1,r_2,\\dots r_q\\}$ given from $\\mathcal{B}$. Instead, we plop $OVK$ into a node. Plop $OVK$ into the $VK_\\epsilon$ location as follows. $\\mathcal{B}$ get the one-time verification key $OVK$. $\\mathcal{B}$ generates the pseudorandom signature trees and plop the $OVK$ into the root node. So $\\mathcal{B}$ knows all secret key except $SK_\\epsilon$. $\\mathcal{B}$ sends the root verification key $VK_\\epsilon$ to $\\mathcal{A}$. $\\mathcal{A}$ requests for the signatures for $q$ times.For each message $m_i$, there are two cases. Pick a random leaf $r$. Use $VK_r$ to sign $m$.$\\tau_i\\gets Sign(SK_r,m) $ Produce authentication path for $VK_r$.Signatures for message $m_1$: $(r,\\tau_1,(\\sigma_\\epsilon,\\epsilon_0,\\epsilon_{01}))$. $\\mathcal{B}$ cannot produce the signature of $\\sigma_\\epsilon$ since she dosenâ€™t know the $SK_\\epsilon$. But $\\mathcal{B}$ can request the signature $\\sigma_\\epsilon$ for a single message $(VK_0||VK_1)$. $\\mathcal{A}$ promises to produce the signature for a new message $m^*\\notin \\{m_1,m_2,\\dots m_q\\}$ The signature consists of $(r^*,\\tau^*,\\text{authentication path for }VK_{r^*})$ Suppose $\\mathcal{A}$ picks $r^*=1$ as above figure.The authentication path for $VK_{001}$: $(\\sigma_\\epsilonâ€™,\\sigma_0â€™,\\sigma_{00}â€™)$. In fact, the output of forgery consists $VK_0â€™,VK_1â€™,VK_{00}â€™,VK_{01}â€™,VK_{000}â€™,VK_{001}â€™$, which could be different from the tree built by $\\mathcal{B}$. If $VK_0||VK_1 \\ne VK_0â€™||VK_1â€™$, $\\mathcal{A}$ wins. Plop $OVK$ into the $VK_0$ location.The analysis is the same.If $VK_{00}||VK_{01} \\ne VK_{00}â€™||VK_{01}â€™$, $\\mathcal{A}$ wins. The main idea is as above.","link":"/2022/08/01/mit6875-lec11/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 12","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Construction of CRHF from Discrete Log Digital Signatures only from OWF Direct Constructions:Trapdoor Permutation and the Hash-and-Sign Paradigm. Random Oracles. Digital Signature from CRHFWe showed the theorem about digital signature in Lecture 10. Theorem: Assuming the existence of one-way functions and collision-resistant hash function families, there are digital signature schemes. CRHF DefinitionRecall the definition of Collision-Resistant Hash Functions. A compressing family of functions $\\mathcal{H}=\\{h:\\{0,1\\}^m\\rightarrow \\{0,1\\}^n\\}$ (where $m&gt;n$ ) for which it is computationally hard to find collisions. Definition: $\\mathcal{H}$ is collision-resistant if for every PPT algorithm $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}_\\mathcal{h\\gets H}[A(1^n,h)=(x,y):x\\ne y,h(x)=h(y)]=\\mu(n) $$ The function $h$ is given to the adversary. And the advantage of finding a collision is negligible. How can we construct the CRHF ? Construction of CRHF from Discrete LogConstruction: Let $p=2q+1$ be a â€œsafeâ€ prime. Let $\\mathcal{H}=\\{h:(\\mathbb{Z}_q)^2\\rightarrow QR_p\\}$ $\\mathcal{H}$ maps two element in $\\mathbb{Z}_q$ to one element in $QR_p$, the subgroup of quadratic residues in $\\mathbb{Z}_p^*$ with order $q$. Each function $h_{g_1,g_2}\\in \\mathcal{H}$ is parameterized by two generators $g_1$ and $g_2$ of $QR_p$. Define $h_{g_1,g_2}(x_1,x_2)=g_1^{x_1}g_2^{x_2} \\mod p$. This compresses $2\\log q$ bits into $\\log q\\approx \\log {q+1}$ bits. Prove $h_{g_1,g_2}$ is collision-resistant. Proof: Suppose for contradiction that there is an adversary that finds a collision $(x_1,x_2)$ and $(y_1,y_2)$. $g_1^{x_1}g_2^{x_2}= g_1^{y_1}g_2^{y_2}\\mod p$ $g_1^{x_1-y_1}= g_2^{y_2-x_2}\\mod p$ $g_1=g_2^{(y_2-x_2)(x_1-y_1)^{-1}}\\mod p$ (assuming $x_1-y_1\\ne 0$) This turns to a discrete log problem of $DLOG_{g_2}(g_1)$. Turns out to another theorem of digital signature scheme. Theorem: Assuming the hardness of the discrete logarithm problem, there are digital signature schemes. Other Constructions of CRHFSimilarly, we can construct CRHF from the hardness of factoring, lattice problems etc. Itâ€™s not known to follow from the existence of one-way functions or even one-way permutations. Itâ€™s still a big open problem. â€œBlack-box separationsâ€: Certain ways of constructing CRHF from OWF/OWP cannot work.â€Finding collisions on a one-way streetâ€, Daniel Simon, Eurocrypt 1998. Digital Signature from OWFBut it turns out that collision-resistant hashing is not necessary; something weaker called universal one-way hashing (UOWHF) suffices. Furthermore, UOWHFs can be constructed from one-way functions alone. The challenge is different between CRHF and UOWHF. CRHF Give $\\mathcal{A}$ the function $h$ Itâ€™s computationally hard for $\\mathcal{A}$ to gives $(x,y)$ such that $h(x)=h(y)$ s.t. $x\\ne y$. UOWHF $\\mathcal{A}$ requests for the hash of $x$. Give $\\mathcal{A}$ the hash $h(x)$ Itâ€™s computationally hard for $\\mathcal{A}$ to give $y$ such that $h(x)=h(y)$ s.t. $x\\ne y$. So we can construct Digital Signature only from OWF. Theorem: Digital Signature schemes exist if and only if one-way functions exist. We can construct Digital Signatures from two routes. OWF â†’ UOWHF â†’ Digital Signatures CRHF(+OWF) â†’ Digital Signatures Now we catch the sight of words in crypto. Direct ConstructionsWe will show that â€œHash-and-Signâ€ is secure in random oracle model. â€œVanillaâ€ RSA SignaturesWe can construct Digital Signature scheme directly from any trapdoor permutation, e.g. RSA. Vanilla RSA Signatures: $Gen(1^\\lambda)$ Pick primes $(P,Q)$ and let $N=PQ$. Pick $e$ relatively prime to $\\phi(N)$ and let $d=e^{-1} \\pmod {\\phi(N)}$. $SK=(N,d)$ and $VK=(N,e)$ $Sign(SK,m)$ Output signature $\\sigma=m^d \\pmod N$ $Verify(VK,m,\\sigma)$ Check if $\\sigma^e=m\\pmod N$ But it is existentially forgeable and malleable. Problems: Existentially forgeable Attack1: Pick a random $\\sigma$ and output $(m=\\sigma^e,\\sigma)$ as the forgery. Malleable Attack2: Given a signature of $m$, you can produce a signature of $2m,3m,\\dots$ Fundamental issues under the problems: Can â€œreverse-engineerâ€ the message starting from the signature. (Attack 1) Algebraic structure allows malleability. (Attack 2) How to fix Vanilla RSA ? Fixed Vanilla RSA Signature: $Gen(1^\\lambda)$ Pick primes $(P,Q)$ and let $N=PQ$. Pick $e$ relatively prime to $\\phi(N)$ and let $d=e^{-1} \\pmod {\\phi(N)}$. $SK=(N,d)$ and $VK=(N,e,\\color{blue}{H})$ $Sign(SK,m)$ Output signature: $\\sigma= \\color{blue}{H(m)}^d \\pmod N$ $Verify(VK,m,\\sigma)$ Check if $\\sigma^e=\\color{blue}{H(m)}\\pmod N$ What is $H$ ? $H$ is some very complicated â€œhashâ€ function. $H$ should be at least one-way. ( to prevent Attack 1) $H$ should be hard to â€œalgebraically manipulateâ€ $H(m)$ into $H(\\text{related } mâ€™)$.(to prevent Attack 2) Collision-resistance dose not seem to be enough. Given a CRHF $H(m)$, you may be able to produce $H(mâ€™)$ for related $mâ€™$. The Random Oracle HeuristicWe want a public $H$ that is â€œnon-malleableâ€. Given $H(m)$, it is hard to produce $H(mâ€™)$ for any non-trivially related $mâ€™$. Random Oracle Definition: For every PPT adversary $A$ and â€œevery non-trivial relationâ€ $R$, $$\\operatorname{Pr}[A\\left(H(m)\\right)=H(mâ€™):R(m,mâ€™)=1]=negl(\\lambda)$$ The goal of adversary is to come up with the relation $R$ such that you can somehow manipulate $H(m)$ into $H(mâ€™)$. How about the relation $R$ where $R(x,y)=1$ if and only if $y=H(x)$ ? A public $H$ that â€œbehaves like a random functionâ€. We can consider it as a proxy to a random function. (A PRF also behaves like a random function, but $PRF_K$ is not publicly computable. ) The adversary $\\mathcal{A}$ can get the public function $H$ in reality. But in the Random Oracle Heuristic world, the only way to compute $H$, virtually a black box, is by calling the oracle. Claim: The hashed RSA is EUF-CMA secure in the random oracle model. Proof: Assume there is a PPT adversary $\\mathcal{A}$ that breaks the EUF-CMA security of hashed RSA in the random oracle model. Given $\\mathcal{A}$ the verification key. $\\mathcal{A}$ asks the Hash Query for poly. times.(We can model it to split the hash queries and sign queries.) $\\mathcal{A}$ asks the Sign Query for poly. times. $\\mathcal{A}$ gives a forgery $(m^*, \\sigma^*)$. Recall the RSA assumption:given $N,e$ and $y=x^e\\mod N$, hard to compute $x$. Then, there is an algorithm $\\mathcal{B}$ that solves the RSA problem. The task of $\\mathcal{B}$ is to compute $x$ given the $(N,e,y)$. $\\mathcal{B}$ needs to interact with the adversary $\\mathcal{A}$ $\\mathcal{B}$ gives the verification key $VK=(N,e)$ to $\\mathcal{A}$. $\\mathcal{A}$ asks polynomially many Hash Queries. For all hash queries, $\\mathcal{B}$ picks a random $\\tilde{m}$ as the trap. For the trap $\\tilde{m}$, $\\mathcal{B}$ sets the hash $H(\\tilde{m})=y$. For other normal $m$, $\\mathcal{B}$ picks a random $x$ and sets the hash $H(m)=x^e$. $\\mathcal{A}$ asks polynomially many Sign Queries. For each Sign Query for $m$: If $m=\\tilde{m}$, i.e. hits the trap, $\\mathcal{B}$ aborts.Because $\\mathcal{B}$ cannot produce the signature. Otherwise, $\\mathcal{B}$ is able to produce the signature $\\sigma=x$. $\\mathcal{A}$ promises to produce the forgery $(m^*,\\sigma^*)$. The thing to notice is that the message $m^*$ is new to all the messages in Sign Query, not in Hash Query. If $m^*=\\tilde{m}$, hits the trap, then the signature $\\sigma^*=x$ is what she wants. Claim:To produce a successful forgery, $\\mathcal{A}$ must have queried the hash oracle on $m^*$. With probability $1/q$, $m^*$ is the trap. (where $q$ is the number of hash queries) Bottomline: Hashed RSA (SHA-3)In practice, we let $H$ be the SHA-3 hash function. And we believe that SHA-3 acts like a random function. Thatâ€™s the heuristic. On the one hand, it doesnâ€™t make any sense, but one the other hand, it has served us well so far. There are no attacks against RSA+SHA-3, for example.","link":"/2022/08/03/mit6875-lec12/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 13","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Zero knowledge, definitions and example ZK Proof for QR Language. honest-verifier ZK malicious-verifier ZK In the following lectures, we will introduce much more than communicating securely. Complex Interactions: proofs, computations, games. Complex Adversaries: Alice or Bob, adaptively chosen. Complex Properties: Correctness, Privacy, Fairness. Many Parties: this class, MIT, the Internet. Todayâ€™s gist is the proof. Itâ€™s very different from the classic proofs. NP ProofsIn classic proofs, prover writes down a string(proof) and verifier checks. Little formally, it can be formalized as below. There is a claim(or theorem), e.g. 15627 is a prime. There are two parties, a prover and a verifier. The prover provide a proof to the verifier. The verifier can accept or reject the proof. NP Language DefinitionThe proofs of $\\mathcal{NP}$ are efficiently verifiable proofs. Nondeterministic Polynomial-time (NP) In computational complexity theory, $\\mathcal{NP}$ (nondeterministic polynomial time) is a complexity class used to classify decision problems.$\\mathcal{NP}$ is the set of decision problems for which the problem instances, where the answer is â€œyesâ€, have proofs verifiable in polynomial time by a deterministic Turing machine, or alternatively the set of problems that can be solved in polynomial time by a nondeterministic Turing machine. The prover has no computational bounce and needs to work hard to produce a proof. But the verifier can efficiently verify it in polynomial time. The $\\mathcal{NP}$ proof of a theorem is actually a set of strings which can be written down. Definition of Language Procedure: A language/decision procedure $\\mathcal{L}$ is simply a set of strings. So $\\mathcal{L}\\subseteq \\{0,1\\}^*$. The language is actually a set of strings which represent the true statements. Definition of $\\mathcal{NP}$-language: $\\mathcal{L}$ is an $\\mathcal{NP}$-language if there is a poly-time verifier $V$ where Completeness: True theorems have (short) proofs.For all $x\\in \\mathcal{L}$, there is a $\\texttt{poly}(|x|)\\texttt{-long}$ witness (proof) $w\\in\\{0,1\\}^*$ s.t. $V(x,w)=1$. Soundness: False theorems have no short proofs.For all $x\\notin \\mathcal{L}$, there is no witness. That is, for all polynomially long $w\\in\\{0,1\\}^*$ s.t. $V(x,w)=0$. Look at some examples. e.g.1 N=PQ LanguageTheorem: $N$ is a produce of two prime numbers. Prover: Give the two prime factors as proof. $=(P,Q)$. Verifier: Accept if and only if $N=PQ$ and $P,Q$ are primes. After interaction, Bob, the Verifier knows $N$ is a produce of two primes. Also, the two factors of $N$. e.g.2 QR LanguageTheorem: $y$ is a quadratic residue $\\mod N$ where $N=PQ$. Prover: Give the square root as proof. $=x$ Verifier: Accept if and only if $y=x^2\\mod N$. After interaction, Bob, the Verifier knows $y$ is a quadratic residue $\\mod N$. Also, the square root of $y$. e.g.3 Graph Isomorphism Language Graph Isomorphism Problem Two graphs $G_0$ and $G_1$ are isomorphic graphs if they have the same number of vertices, edges and also the same edge connectivity. The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic.The problem is not known to be solvable in polynomial time nor to be NP-complete, and therefore may be in the computational complexity class NP-intermediate. Theorem: Graphs $G_0$ and $G_1$ are isomorphic. Prover: Give the map of nodes as proof. $=\\pi$ Verifier: Accept iff the edge connectivity is retained after mapping. After interaction, Bob, the Verifier knows $G_0$ and $G_1$ are isomorphic. Also, the isomorphism. e.g.4 Hamiltonian Cycle Language Hamiltonian Path Problem and Hamiltonian Cycle Problem A Hamiltonian path is a path that visits each vertex exactly once.A Hamiltonian cycle is a closed loop on a graph where every vertex is visited exactly once. In the mathematical field of graph theory the Hamiltonian path problem and the Hamiltonian cycle problem are problems of determining whether a Hamiltonian path or a Hamiltonian cycle exists in a given graph. Both problems are NP-complete. Theorem: Graph $G$ has a Hamiltonian cycle. Prover: Give the Hamiltonian circle as proof. Verifier: Accept iff every edge exists. After interaction, Bob, the Verifier knows $G$ has a Hamiltonian cycle. Also, the Hamiltonian cycle itself. Every one of the above can be reduced to $\\mathcal{NP}$-Complete Problem. Besides, every proof above leaks some valuable information more than the proof itself. Is there any other way that the prover only tells the verifier whether $y$ is quadratic residue without telling the square root?(Example 2) The point is that we want to reveal the minimal information. Interactive ProofsBefore proceeding to Zero Knowledge Proof, letâ€™s start with Interactive Proofs. Interactive Proof is not a monologue but a dialogue. There are two necessary new ingredients in Interactive Proofs. Two (Necessary) New Ingredients: Interaction: Rather than passively reading the proof, the verifier engages in a conversation with the prover. Randomness: The verifier is randomized and can make a mistake with a (exponentially small) probability. Note: The randomness of the verifier is necessary. If the verifier is completely deterministic as a function, the prover, since the first message she sends, she knows what the verifier is going to send back. So she can prepare her second message beforehand, then she can prepare her third messageâ€¦ Itâ€™s not interactive. The thing to point is that for the prover, she can do no more powerful thing than sending a single message. IdeaLetâ€™s start with solving the Rubikâ€™s Cube. Theorem: There is an $\\le k$ move solution to this cube. The idea is split the process of resolving in half. Interactive Proof: Prover: sends a â€œrandomâ€ cube. Verify: sends a Challenge (0 or 1) Prover If Challenge 0: show $k/2$ moves If Challenge 1: show $k/2$ moves The point is that if the prover can do both challenges consistently (or many and many times), then there exists $k$ moves. We do not take it seriously since there are many flaws in the above protocol. We just get some inspiration from it. Definition of IPThe Interactive Proofs for a Language $\\mathcal{L}$ is as follows. There is a claim or a theorem. Two parties are interacting for a language $\\mathcal{L}$. Prover: Computational Unbounded Verifier: Probabilistic Polynomial-time Definition of $\\mathcal{IP}$-Language: $\\mathcal{L}$ is an $\\mathcal{IP}$-language if there is a probabilistic poly-time verifier $V$ where(There are three similar definitions) Definition in words. Completeness: If $x\\in \\mathcal{L}$, $V$ always accepts. Soundness: If $x\\notin \\mathcal{L}$, regardless of the cheating prover strategy, $V$ accepts with negligible probability. Rewrite with probability. Completeness: If $x\\in \\mathcal{L}$, $\\operatorname{Pr}[(P,V)(x)=\\texttt{accept}]=1. $ Soundness: If $x\\notin \\mathcal{L}$, there is a negligible function $negl$ s.t. for every $P^*$, $\\operatorname{Pr}[(P^*,V)(x)=\\texttt{accept}]=negl(\\lambda).$ Relax the probability: Equivalent as long as $c-s\\ge 1/poly(\\lambda)$ Completeness: If $x\\in \\mathcal{L}$, $\\operatorname{Pr}[(P,V)(x)=\\texttt{accept}]\\color{blue}{\\ge c}.$ Soundness: If $x\\notin \\mathcal{L}$, there is a negligible function $negl$ s.t. for every $P^*$, $\\operatorname{Pr}[(P^*,V)(x)=\\texttt{accept}]\\color{blue}{\\le s}$. Completeness is a property of the protocol when $P$ and $V$ are both honest. Soundness is a property of the protocol when $P$ is dishonest. So soundness is also a property of the verifier.The verifier has to be sound because it has to work against an arbitrary $P$. IP for QR LanguageWe can give the interactive proof for QR. $\\mathcal{L}=\\{(N,y): y \\textrm{ is a quadratic residue}\\mod N\\}$ Interactive Proof: Prover: send a random square $s$ Verifier: flip a coin $b$ (randomness) Prover If $b=0$, send $z=r$ If $b=1$, send $z=rx$ where $y=x^2 \\mod N$. Verifier: Accept iff $z^2=sy^b \\mod N$. Completeness: Claim: If $(N,y)\\in \\mathcal{L}$, then the verifier accepts the proof with probability 1. Proof: $z^2=(rx^b)^2=r^2(x^2)^b=sy^b$. So the verifierâ€™s check passes and he accepts. Soundness: Claim: If $(N,y)\\notin \\mathcal{L}$, then for every cheating prover $P^*$, the verifier accepts with probability at most $1/2$. Proof: The challenge is random and the randomness is over the coin. If the probability equals $1/2$, it means that the cheating prover $P^*$ can only solve one of the challenges, Challenge 0. Suppose the verifier accepts with probability $\\ge1/2$. Then, there is some $s\\in \\mathbb{Z}_N^*$, the prover is able to pass both challenges. So the prover can produce both $z_0$ and $z_1$ beforehand. $z_0:z_0^2=s\\mod N$ $z_1:z_1^2=sy\\mod N$ This means $(z_1/z_0)^2=y\\mod N$, which tells us that $(N,y)\\in \\mathcal{L}$. (Contradiction) Moreover, we can make the probability of soundness negligible. Just *repeat the procedure sequentially $\\lambda$ times. * Claim: If $(N,y)\\notin \\mathcal{L}$, then for every cheating prover $P^*$, the verifier accepts with probability at most $(\\frac{1}{2})^\\lambda$. Zero Knowledge ProofActually, the interactive proof for QR language is Zero-Knowledge. But what dose zero-knowledge mean? After the interaction, $V$ knows: The theorem is true; and A view of the interaction (=transcript + coins of $V$)The transcript is all the messages going back and forth. $P$ gives zero knowledge to $V$:When the theorem is true, the view gives $V$ nothing that he couldnâ€™t have obtained on his own without interacting with $P$. That means the view is not going to give any new knowledge that $V$ couldnâ€™t have it on his own. We can consider the knowledge as some stuff that we cannot generate in probabilistic poly-time on our own.If we are given something we can generate by ourselves, itâ€™s zero-knowledge. $(P,V)$ is zero-knowledge if $V$ can generate his view of the interaction all by himself in probabilistic polynomial time. $(P,V)$ is zero-knowledge if $V$ can â€œsimulateâ€ his view of the interaction all by himself in probabilistic polynomial time. We formalize it by the Simulation Paradigm. The Simulation ParadigmThere are two indistinguishable distributions. The real view of the interaction: $\\texttt{view}_V(P,V)=(s,b,z$).It consists of Transcript: $(s,b,z)$ Coins: $b$ The simulated view: $\\texttt{sim}_S(s,b,z)$ We can give the zero-knowledge definition by the simulation paradigm. Zero-knowledge Definition (for honest V) (Honest-verifier) Zero-knowledge Definition: An Interactive Protocol $(P,V)$ is zero-knowledge for a language $\\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\\in \\mathcal{L}$, the following two distributions are indistinguishable. $\\texttt{view}_V(P,V)$ $\\texttt{sim}_S(x,1^\\lambda) $ $(P,V)$ is zero-knowledge interactive protocol if it is complete, sound and zero-knowledge. Completeness is a property of the protocol when $P$ and $V$ are both honest. Soundness is a property of the protocol when $P$ is dishonest. So itâ€™s also a property of the verifier.The verifier has to be sound because it has to work against an arbitrary $P$. Zero-knowledge is a property against the verifier. Actually, we give the definition of zero-knowledge against a honest verifier. Weâ€™ll refine it for any arbitrary verifier in the next section. There are some analogous definitions of honest-verifier zero-knowledge. Perfect (Honest-verifier) Zero-knowledge: An Interactive Protocol $(P,V)$ is perfect (honest-verifier) zero-knowledge for a language $\\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\\in \\mathcal{L}$, the following two distributions are identical. $\\texttt{view}_V(P,V) $ $\\texttt{sim}_S(x,1^\\lambda) $ Statistical (Honest-verifier) Zero-knowledge: An Interactive Protocol $(P,V)$ is statistical (honest-verifier) zero-knowledge for a language $\\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\\in \\mathcal{L}$, the following two distributions are statistically indistinguishable. $\\texttt{view}_V(P,V)$ $\\texttt{sim}_S(x,1^\\lambda) $ Computational (Honest-verifier) Zero-knowledge: An Interactive Protocol $(P,V)$ is computational (honest-verifier) zero-knowledge for a language $\\mathcal{L}$ if there exists a PPT algorithm $S$ (a simulator) such that for every $x\\in \\mathcal{L}$, the following two distributions are computationally indistinguishable. $\\texttt{view}_V(P,V) $ $\\texttt{sim}_S(x,1^\\lambda)$ QR Protocol is (honest-V) ZKClaim: The QR protocol is (honest-verifier) perfect zero knowledge. Simulator S works as follows: First pick a random bit $b$. Pick a random $z\\in \\mathbb{Z}_N^*$. Compute $s=z^2/y^b$. Output $(s,b,z)$. The simulator can sort of permute things which is offline. And the verifier is online in the real world. Lemma: The simulated transcript is identically distributed as the real transcript in the interaction $(P,V)$. Proof of Lemma: The thing we need to prove is that the distribution of every component in the transcript is identical. Real transcript: $\\texttt{view}_{V}(P,V)=(s,b,z) $ $s$ is square as same as random $b$ is random coin(since the honest verifier) $z=\\sqrt{sy^b}$ is random Simulated transcript: $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,z) $ $s=z^2/y^b$ is square as same as random ($(N,y)\\in \\mathcal{L}$ so $y$ is square)Besides, the distribution of $s$ hides $b$ perfectly. $b$ is random $z$ is random QED Actually we only prove the QR protocol is honest-verifier Zero-knowledge. When the theorem is true, the view gives the honest verifier $V$ nothing that $V$ couldnâ€™t have it on his own. What if $V$ is NOT honest ? Note: The following malicious part is actually lectured in the Lecture 14.I put it here for the continuous and complete description. Zero-knowledge Definition (for malicious V)In real world, the verifier could be malicious that he can do anything he wants. We hope the view also gives zero-knowledge to the malicious verifier $V^*$. The definition for honest verifier undermines the definition for malicious verifier. Recap: A language $\\mathcal{L}$ is actually a set of strings which represent true statements.The view of $V^*$ is the transcripts and the coins, which contains all the messages going back and forth. Refine the definitions for malicious verifier $V^*$. Perfect Zero-knowledge Definition: An interactive Protocol $(P,V)$ is perfect zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are identical: $\\texttt{view}_{V^*}(P,V^*) $ $\\texttt{sim}_S(x,1^\\lambda)$ Statistical Zero-knowledge Definition: An interactive Protocol $(P,V)$ is statistical zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are statistical indistinguishable: $\\texttt{view}_{V^*}(P,V^*) $ $\\texttt{sim}_S(x,1^\\lambda) $ Computational Zero-knowledge Definition: An interactive Protocol $(P,V)$ is computational zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are computationally indistinguishable: $\\texttt{view}_{V^*}(P,V^*)$ $\\texttt{sim}_S(x,1^\\lambda) $ QR Protocol is (malicious-V) ZKThe ZK proof for QR language we gave above is actually honest-verifier zero-knowledge. In this section, we consider the malicious-verifier zero-knowledge for QR Protocol. Recap the zero-knowledge proof for QR Language. $\\mathcal{L}=\\{(N,y):y \\textrm{ is a quadratic residue }\\mod N\\}$. The view of a malicious verifier $V^*$ is $\\texttt{view}_{V^*}(P,V^*)=(s,b,z)$. When $V^*$ obtains the $s$, the only power that $V^*$ has is to choose the $b$ in a bizarre fashion rather than random. So the distribution of $b$ is not random. Let $b=V^*(s)$ denote the bizarre thing generated by $V^*$ after receiving $s$. The simulator $S$ only gets an instance of $(N,y)$ and wants to generate $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,z)$. In order to produce the same distribution of $b$, the simulator $S$ needs to interact with the malicious verifier $V^*$. Itâ€™s sort of a prover which also needs to interact with the verifier. The only distinction is that the prover $P$ has to be online to answer the challenge from the verifier $V^*$ and the simulator $S$ can be offline with the goal of generating the view. Claim: The QR protocol is (malicious-verifier) perfect zero knowledge. Simulator S works as follows: First set $s=z^2/y^b$ for a random $z$ and a random $b$ and â€œfeedâ€ $s$ to $V^*$ Let $b'=V^*(s)$. (generated by $V^*$ in any bizarre fashion rather than random) If $b=bâ€™$, output $(s,b,z)$ and stop. Otherwise, go back to step 1 and repeat. (also called â€œrewindingâ€) Lemma: $S$ runs in expected polynomial-time. When $S$ outputs a view, it is identical to the view of $V^*$ in a real execution. Lemma 1 is well proven. The probability of terminating in one iteration is $1/2$ since $b$ is random. So the expected iterations is $2$. Proof of Lemma 2: We need to prove that the distribution of every component is identical. Real transcript: $\\texttt{view}_{V^*}(P,V^*)=(s,b,z) $ $s$ is square as same as random $b$ is generated in any bizarre way, i.e. $b=V^*(s)$. $z=\\sqrt{sy^b}$ is random Simulated transcript: $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,y) $ $s=z^2/y^b$ is square as same as random ($(N,y)\\in \\mathcal{L}$ so $y$ is square)Besides, the distribution of $s$ hides $b$ perfectly. $b$ has the same distribution with $bâ€™=V^*(s)$. $z$ is random QED So far we have proven the QR protocol is (malicious-verifier) zero-knowledge. What made ZK Proof possible ? Each statement had multiple proofs of which the prover chooses one at random.(They are $(s,\\sqrt{s})$ and $(s,\\sqrt{sy})$ as for the QR protocol.) Each such proof is made of two parts (as shown above): seeing either one on its own gives the verifier no knowledge ; seeing both imply 100% correctness. (Thatâ€™s the completeness) Verifier choose to see either part, at random.(Verifier can choose to see $\\sqrt{s}$ or $\\sqrt{sy}$ at random)The proverâ€™s ability to provide either part on demand convinces the verify. (Thatâ€™s the soundness)The prover has to prepare both part correctly, otherwise there is a half chance that heâ€™ll get caught.","link":"/2022/08/08/mit6875-lec13/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 14","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Perfect ZK Proof for QR Language Perfect ZK Proof for Graph Isomorphism Comp. ZK Proof for 3Coloring All NP Languages have Comp. ZK Proofs Commitment Schemes Note: The first two sections are also posted at the end of last Lecture, just for the continuous and complete description. ZK DefinitionIn the previous Lecture is introduced the definition of honest-verifier Zero-knowledge. When the theorem is true, the view gives the honest verifier $V$ nothing that $V$ couldnâ€™t have it on his own. In real world, the verifier could be malicious that he can do anything he wants. Refine the ZK definitions for any verifier $V^*$. Recap: A language $\\mathcal{L}$ is actually a set of strings which represent true statements.The view of $V^*$ is the transcripts and the coins, which contains all the messages going back and forth. Perfect Zero-knowledge Definition: An interactive Protocol $(P,V)$ is perfect zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are identical: $\\texttt{view}_{V^*}(P,V^*) $ $\\texttt{sim}_S(x,1^\\lambda)$ Statistical Zero-knowledge Definition: An interactive Protocol $(P,V)$ is statistical zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are statistical indistinguishable: $\\texttt{view}_{V^*}(P,V^*) $ $\\texttt{sim}_S(x,1^\\lambda) $ Computational Zero-knowledge Definition: An interactive Protocol $(P,V)$ is computational zero-knowledge for a language $\\mathcal{L}$ if for every PPT $V^*$, there exists a (expected) poly time simulator $S$ s.t. for every $x\\in \\mathcal{L}$, the following two distributions are computationally indistinguishable: $\\texttt{view}_{V^*}(P,V^*)$ $\\texttt{sim}_S(x,1^\\lambda) $ Perfect ZK Proof for QR LanguageRecap the zero-knowledge proof for QR Language. $\\mathcal{L}=\\{(N,y):y \\textrm{ is a quadratic residue }\\mod N\\}$. We proved in last Lecture that the QR protocol is honest-verifier zero knowledge. Now we only consider the malicious-verifier zero knowledge. The QR protocol is malicious-verifier zero knowledge. The view of a malicious verifier $V^*$ is $\\texttt{view}_{V^*}(P,V^*)=(s,b,z)$. When $V^*$ obtains the $s$, the only power that $V^*$ has is to choose the $b$ in a bizarre fashion rather than random. So the distribution of $b$ is not random. Let $b=V^*(s)$ denote the bizarre thing generated by $V^*$ after receiving $s$. The simulator $S$ only gets an instance of $(N,y)$ and wants to generate $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,y)$. In order to produce the same distribution of $b$, the simulator $S$ needs to interact with the malicious verifier $V^*$. Itâ€™s sort of a prover which also needs to interact with the verifier. The only distinction is that the prover $P$ has to be online to answer the challenge from the verifier $V^*$ and the simulator $S$ can be offline with the goal of generating the view. Claim: The QR protocol is (malicious-verifier) perfect zero knowledge. Simulator S works as follows: First set $s=z^2/y^b$ for a random $z$ and a random $b$ and â€œfeedâ€ $s$ to $V^*$ Let $b'=V^*(s)$. (generated by $V^*$ in any bizarre fashion rather than random) If $b=bâ€™$, output $(s,b,z)$ and stop. Otherwise, go back to step 1 and repeat. (also called â€œrewindingâ€) Lemma: $S$ runs in expected polynomial-time. When $S$ outputs a view, it is identical to the view of $V^*$ in a real execution. Lemma 1 is well proven. The probability of terminating in one iteration is $1/2$ since $b$ is random. So the expected iterations is $2$. Proof of Lemma 2: We need to prove that the distribution of every component is identical. Real transcript: $\\texttt{view}_{V^*}(P,V^*)=(s,b,z)$ $s$ is square as same as random $b$ is generated in any bizarre way, i.e. $b=V^*(s)$. $z=\\sqrt{sy^b}$ is random Simulated transcript: $\\texttt{sim}_S((N,y),1^\\lambda)=(s,b,y)$ $s=z^2/y^b$ is square as same as random ($(N,y)\\in \\mathcal{L}$ so $y$ is square)Besides, the distribution of $s$ hides $b$ perfectly. $b$ has the same distribution with $bâ€™=V^*(s)$. $z$ is random QED So far we have proven the QR protocol is (malicious verifier) zero-knowledge. ZK Proof for Graph Isomorphism Recap the Graph Isomorphism Problem:Two graphs $G_0$ and $G_1$ are **isomorphic graphs** if they have the same number of vertices, edges and also the same edge connectivity. The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. Now we describe the zero-knowledge proof for the claim that $G$ and $H$ are isomorphism graphs. The interaction protocol is as shown below: Prover has the knowledge that she knows a map $\\pi$ such that $H=\\pi(G)$. ZK Proof for Graph-iso: Prover: choose a random permutation $\\rho$ generate a new random graph $K$ such that $K=\\rho(G)$. send the graph $K$ to verifier Verifier: generate a random challenge bit $b$ Prover: has to answer the challenge bit If $b=0$: Prover sends the map $\\pi_0=\\rho$ such that $K=\\pi_0(G)$(prove that she can map $G$ â†’ $K$) If $b=1$: Prover sends the map $\\pi_1=\\pi\\circ \\rho^{-1}$ such that $H=\\pi_1(K)$(prove that she can map $K$ â†’ $H$) Completeness: Completeness is a property of the protocol when $P$ and $V$ are both honest. We prove that the verifier will pass the equations and accept it. If $b=0$: check $K=\\pi_0(G)=\\rho(G)=K$. If $b=1$: check $H=\\pi_1(K)=\\pi\\rho^{-1}(K)=\\pi(G)=H$ Soundness: Soundness is a property of the protocol when $P$ is malicious. The verifier has to be sound because it has to work against an arbitrary $P$. Suppose $G$ and $H$ are non-isomorphic, and the prover could answer both the verifier challenges. Then the prover both prepares the $\\pi_0$ and $\\pi_1$ such that $K=\\pi_0(G)$ and $H=\\pi_1(K)$.(Otherwise, sheâ€™ll be caught in half chance.) In other words, the prover can get $H=\\pi_0\\circ\\pi_1(G)$, a contradiction. QED. Perfect Zero Knowledge: The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. The view of an arbitrary verifier $V^*$ is $\\texttt{view}_{V^*}(P,V^*)=(K,b,\\pi_b)$. When $V^*$ receives the graph $K$, the only power that $V^*$ has is to choose the $b$ in a bizarre fashion rather than random. So the distribution of $b$ is not random. Let $b=V^*(K)$ denote the bizarre thing generated by $V^*$ after receiving $K$. The simulator $S$ only gets an instance of $(G,H)$ and wants to generate $\\texttt{sim}_S((G,H),1^\\lambda)=(K,b,\\pi_b)$. Simulator S works as follows: Pick a random permutation $\\rho$ and a random $b$. Generate a graph $K$ such that If $b=0$: $K=\\rho(G)$.Let $\\pi_0=\\rho$. If $b=1$: $K=\\rho(H)$, i.e. $H=\\rho^{-1}(K)$.Let $\\pi_1=\\rho^{-1}$. Feed the graph $K$ to verifier $V^*$ Let $bâ€™=V^*(K)$ If $b=bâ€™$, output $(K, b, \\pi_b)$ and stop. Otherwise, go back to step 1 and repeat. Intuition of my proof: It sort of split the $K$ into two intermediate points, $K_0$ and $K_1$.The prover has the knowledge that could map $G$ to $K$ and map $K$ to $H$.The point is that the simulator can simulate the knowledge that could map $G$ to $K_0$ and map $K_1$ to $H$. Lemma: $S$ runs in expected polynomial-time. When $S$ outputs a view, it is identical to the view of $V^*$ in a real execution. Lemma 1 is well proven. The probability of terminating in one iteration is $1/2$ since $b$ is random. So the expected iterations is $2$. Proof of Lemma 2: We need to prove that the distribution of every component is identical. Real transcript: $\\texttt{view}_{V^*}(P,V^*)=(K,b,\\pi_b)$ $K$ is a random graph since the random $\\rho$. $b$ is generated in any bizarre way, i.e. $b=V^*(s)$. $\\pi_b$ is a random map If $b=0$: $\\pi_0=\\rho$. â†’ random map If $b=1$: $\\pi_1=\\pi\\circ \\rho^{-1}$. â†’ random map. Simulated transcript: $\\texttt{sim}_S((G,H),1^\\lambda)=(K,b,\\pi_b)$ $\\pi_b$ is a random map since $\\rho$ is random. If $b=0$: $\\pi_0=\\rho$. If $b=1$: $\\pi_1=\\rho^{-1}$. $K$ is a random graph. If $b=0$, $K=\\pi_0(G)=\\rho(G)$ If $b=1$, $K=\\pi_1(H)=\\rho^{-1}(H)$ $b$ has the same distribution with $bâ€™=V^*(s)$. QED Efficient Prover (given a Witness)So far we keep saying that the prover is unbounded and the verifier is ppt. But there are no unbounded people. In both these protocols above, the (honest) prover is actually polynomial-time given the NP witness ï¼ˆthe square root of $y$ in the case of QR, and the isomorphism in the case of graph-iso). Therefore, the prover and the verifier can both be polynomial-time. The only difference between the (honest) prover and the verifier is that the prover knows some privileged knowledge, a witness or a solution to a problem, that the verifier dose not know. Thatâ€™s the common way to reduce the zero-knowledge proofs. The thing to point is that soundness is nevertheless against any, even computationally unbounded, prover $P^*$. All NP languages have Comp. ZK ProofsWe shows two languages with perfect ZK proofs, the QR protocol and the Graph-iso protocol. Do all NP languages have perfect ZK proofs ? The theorem[Fortnowâ€™89, Aiello-Hastadâ€™s 87] answered NO, unless bizarre stuff happens in complexity theory. Technically, the polynomial hierarchy collapses. That is NP = P. Nevertheless, we can relax the question. Do all NP languages have ZK proofs ? Theorem: [Goldreich-Micali-Wigdersonâ€™87] Assuming one-way permutations exist, all of NP has computational zero-knowledge proofs. It means that every language of NP problem has computational zero-knowledge proof given a witness. Moreover, the assumption can be relaxed to one-way functions. This theorem is amazing and it tells us that everything can be proved (in the sense of Euclid) can be proven in zero knowledge! How to prove the theorem ? We cannot prove every NP problem one by one. Luckily, we can prove NP-Complete Problem to which every other problem in NP can be reduced. It turns out that there are a whole of complete problems. There is a list of 20 odd problems that came up with in the 70s already and this list keeps increasing. So NP-complete problems is sort of a wealth. We are going to pick the Graph Coloring Problem and prove it has Comp. ZK Proofs. ZK Proof for 3ColoringHere is the Graph 3Coloring Problem. Given a graph and three colors, red blue and green, youâ€™re supposed to assign colors to every vertex such that no two adjacent vertexes have the same color. (or every two adjacent vertexes have different colors) Before proceeding to the zero-knowledge proof of the three-coloring, letâ€™s introduce the lead-box model. The Lead-box ModelThe lead-box model is as shown below. Lead-box Model: The sender Alice has a bit $b$. Commit to $b$: put $b$ in a lead-box and locks it, and send the box to receiver. Open $b$: send $b$ together with the key. Then the receiver Bob can check the thing in box is what Alice claims. The lead-box above should be hiding and binding $b$. Properties: Hiding means that the lead-box should completely hide $b$. Blinding means that the sender shouldnâ€™t be able to open to $1-b$. Once the Alice sends the box to Bob, she should not be able to change her mind about whatâ€™s inside the box. Thatâ€™ blinding. Thatâ€™s a commitment. It can be used for computational zero-knowledge. It can also be used to ensure fairness. We will later show how to implement such a lead-box (as a commitment protocol) using one-way permutations. ZK Proof with Lead-box: Part IThe language $\\mathcal{L}$ is that the graph $G$ is 3-colorable. Given a 3-colorable witness (solution), it can be proven in computational zero-knowledge. The prover is given the graph $G$ and the 3-colorable witness. The verifier is given the graph $G$. The interaction of ZK proof is as shown below. Interactive Protocol for 3COL: Prover: come up with a random permutation of the colors, $\\rho:V\\rightarrow \\{R,G,B\\}$. The color of every vertex is masked by the random permutation. Prover: commit to (the color of) every vertex. Verifier: pick a random edge $(i,j)$ Prover: open $\\rho(i)$ and $\\rho(j)$ Verifier: check Check the openings that $\\rho(i)$ and $\\rho(j)$ are what Alice claims. Chek the $\\rho(i),\\rho(j)\\in \\{R,G,B\\}$ Check: $\\rho(i)\\ne \\rho(j)$ The completeness is well proven. Soundness: Soundness is the property of the protocol against dishonest prover $P$. If the graph is not 3COL, in every 3-coloring (that $P$ commits to), there is some edge whose end-points have the same color. $V$ will catch this edge and reject with probability $\\ge 1/|E|$. In one time: the verifier accepts with probability $\\le1-1/|E|$. Repeat $|E|\\cdot \\lambda$ times: he verifier accepts with probability $\\le (1-1/|E|)^{|E|\\cdot \\lambda}\\le 2^{-\\lambda}$. which is negligible. QED Moreover, the proof is Computational Zero-knowledge. The key reason of zero-knowledge is the prover commits to all colors (of the vertices) but only open two colors (of the vertexes). It leaks nothing to the verifier since the colors have been randomly permuted. So the prover gives zero-knowledge to the verifier. We will elaborate the Comp. Zero-knowledge in the following Part II. More analysis into the first message(the message in the lead-box). If the first message dose not exist, the proof is not sound. The malicious prover can always answer â€œredâ€ and â€œblueâ€ because the verifier cannot check what Alice claims without the commitment. If the first message are not in the lead-box, the proof is not zero-knowledge. Commitment SchemesThe lead-box is indeed a commitment protocol. The Commitment Protocol $(S,R)$ works as follows. Commitment Protocol $(S,R)$: There are two parties, sender $S$ and receiver $R$. Sender $S$ commits to a bit $b$, so the protocol is instanced to $(S(b,1^\\lambda),R(1^\\lambda))$. Let $\\texttt{dec}$ be the senderâ€™s output, decommitment. Let $\\texttt{com}$ be the receiverâ€™s output, commitment. Sender $S$ opens $b$ $S$ sends $b$ together with $\\texttt{dec}$. Receiver $R$ checks $b$ using $\\texttt{dec}$. Properties of Commitment Protocol: Completeness: $R$ always accepts in an honest execution. Computational Hiding: For every possibly malicious (PPT) $R^*$, $\\texttt{view}_{R^*}(S(0),R^*)\\approx_c\\texttt{view}_{R^*}(S(1),R^*)$ (the view of $R^*$ is $(\\texttt{com},b,\\texttt{dec})$) Perfect Binding: For every possibly malicious $S^*$, let $\\texttt{com}$ be the receiverâ€™s output in an execution of $(S^*, R)$. There is no pair of decommitments $(\\texttt{dec}_0,\\texttt{dec}_1)$ s.t. $R$ accepts both $(\\texttt{com},0,\\texttt{dec}_0)$ and $(\\texttt{com},1,\\texttt{dec}_1)$. Completeness is the property of the commitment protocol when $S$ and $R$ are honest. Computational Hiding is the property of commitment protocol against malicious $R^*$. Perfect Binding is the property of commitment protocol against malicious $S^*$. A Commitment Scheme from any OWPThere is a commitment scheme starting from any OWP as follows. Commitment Protocol $(S,R)$ from OWP: Sender $S$ commits to bit $b$ Pick a random $r$ as the decommitment, $\\texttt{dec}=r$. Compute the commitment $\\texttt{com}=(f(r),HCB(r)\\oplus b)$. Send the commitment to $R$. Sender $S$ opens $b$ Send $(b,r)$ to $R$, $r$ as the $\\texttt{dec}$. Receiver $R$ checks $b$ using $\\texttt{dec}$. Let $\\texttt{com}=(x,y)$ Check $f(r)=x$. Check $HCB(r)\\oplus b=y$. This commitment scheme has completeness, comp. hiding and perfect binding. Properties of Commitment Scheme: Completeness is well proven. Computational Hiding can be proven by the hardcore bit property. As for any arbitrary receiver $R^*$, it is hard to compute $HCB(r)$ given $f(r)$ since $f$ is OWP. We say that the hardcore bit of $f(r)$ computational hides the bit $b$. The point in Perfect Binding is that $f$ is a permutation. If $x=f(r)$ is fixed, then $r$ is fixed. There is no other $râ€™$ such that $f(râ€™)=f(r)$. Then $HCB(r)$ is fixed, and the bit $b$ is fixed since $y$ is fixed. Thatâ€™s perfect binding. ZK Proof with Commitment: Part IIReplace the lead-box with the commitment protocol. Commitment to $\\rho(k)$ is $\\texttt{com}(\\rho(k);r_k)$ where $r_k$ is the random. Decommitment to $\\rho(k)$ is $r_k$. Why is this protocol zero-knowledge? Comp. Zero-knowledge: We dive into a malicious-verifier zero-knowledge. What can an arbitrary verifier $V^*$ do ? He can see all these commitments to He can pick an arbitrary edge in bizarre fashion. Real transcript of malicious $V^*$: $\\texttt{view}_{V^*}(P,V^*)=\\left(\\{\\texttt{com}_k(\\rho(k);r_k)\\}_{k=1}^{n},(i,j),(\\texttt{dec}_i,\\texttt{dec}_j)\\right)$. The commitment to every color (of the vertex): $\\texttt{com}_k(\\rho(k);r_k)$ The edge chosen in bizarre fashion: $(i,j)=V^*(\\{\\texttt{com}_k\\})$ The decommitment: $(\\texttt{dec}_i,\\texttt{dec}_j)$. Simulator S works as follows: First pick a random edge $(i^*,j^*)$.Color this edge with random, different colors.Color all other edges red. Feed the commitments of the colors to $V^*$ and get edge $(i,j)=V^*(\\{\\texttt{com}_k\\})$. If $(i,j)=(i^*,j^*)$, output the commitments and the openings $r_i$ and $r_j$ as the simulated transcript. If $(i,j)\\ne(i^*,j^*)$, go back to step 1 and repeat. The key reason why it works is that the prover commits to all colors (of the vertices) but only open two colors (of the vertexes).It leaks nothing to the verifier since the colors have been randomly permuted. So the prover gives zero-knowledge to the verifier. Lemma: Assuming the commitments is hiding, $S$ runs in expected polynomial-time. When $S$ outputs a view, it is computationally indistinguishable to the view of $V^*$ in a real execution. Proof of Lemma 2: Analysis the distribution of real transcript and simulated transcript. Real transcript: $\\texttt{view}_{V^*}(P,V^*)=\\left(\\{\\texttt{com}_k\\}_{k=1}^{n},(i,j),(\\texttt{dec}_i,\\texttt{dec}_j)\\right)$ The commitments are computationally random since the computational hiding property. The distribution of $(i,j)$ is in bizarre fashion. The decommitments are random. Simulated transcript: $\\texttt{sim}_{S}(G)=\\left(\\{\\texttt{com}_k\\}_{k=1}^{n},(i^*,j^*),(\\texttt{dec}_i,\\texttt{dec}_j)\\right)$ The commitments are computationally random since the computational hiding property. The distribution of $(i^*,j^*)$ is same as $(i,j)=V^*(\\{\\texttt{com}_k\\})$. The decommitments are random. Examples of NP Assertions My public key is well-formed.e.g. in RSA, prove the public key is $N$, a product of two primes together with an $e$ that is relatively to $\\varphi(N)$. Encrypted bitcoin (or Zcash): â€œI have enough money to pay you.â€e.g. I will publish an encryption of my bank account and prove to you that my balance is $\\ge \\$X$. Running programs on encrypted inputs: Given $Enc(x)$ and $y$, prove that $y=\\textrm{PROG}(x)$.","link":"/2022/08/10/mit6875-lec14/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 15","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Sequential vs Parallel Repetition: reduce soundness error Proof of Knowledge PoK of DLOG Non-Interactive ZK(NIZK) NIZK in The Random Oracle Model NIZK for 3COL NIZK in The Common Random String Model (Lecture 16) RecapRecap NP Proofs. Give NP Proofs for the NP-complete problem of graph 3-coloring. Prover $P$: has a witness, the 3-coloring of $G$. $P$ gives the proof, the solution to 3-coloring of $G$, to $V$. Verifier $V$ checks: only 3 colors are used any two vertices connected by an edge are colored differently. The verify learned the graph $G$ is 3-colorable and the 3-coloring solution. So NP proofs reveal too much information. With Zero-knowledge (Interactive) Proofs, the verifier can only learns the graph $G$ is 3-colorable without knowing the solution. Prover $P$: permute the colors commit to each color send all the commitments to the verifier. Verifier $V$: pick a random edge Prover $P$: open the vertices of the edge. Verifier $V$ checks the openings &amp; the colorings of two vertices are different. Besides, we proved the 3COL Protocol is completeness, soundness and zero-knowledge in previous blog. Completeness: For every $G\\in 3COL$, $V$ accepts $P$â€™s proof. Soundness: For every $G\\notin 3COL$ and any cheating $P^*$, $V$ rejects $P^*$â€™s proof with probability $\\ge 1-neg(n)$. Zero-knowledge: For every cheating $V^*$, there is a PPT simulator $S$ such that for every $G\\in 3COL$, $S$ simulates the view of $V^*$. Sequential vs Parallel RepetitionThe 3COL protocol has a large soundness error of $1-1/|E|$, the probability that $V$ accepts even though $G\\notin 3COL$. Reducing Soundness Error Theorem: Sequential Repetition reduces soundness error for interactive proofs, and preserves the ZK property. But it brings about the problem that it costs a lot of rounds. An alternative way is parallel repetition. Theorem [Goldreich-Krawczykâ€™90]: Parallel Repetition also reduces soundness error for interactive proofs. It is also honest-verifier ZK, but dose not, in general, preserve the ZK property. Note: Preserving the ZK property in general means that it is ZK against malicious verifier. There is an intuitive interpretation to the theorem.[Goldreich-Krawczykâ€™90] The interaction in parallel repetition is $P$ sends all first message in parallel and $V$ response at once with all second messages â€¦ If $V$ is honest verifier, he indeed dose not look at the commitments, and just picks the random edges independently, which is the same with the sequential repetition. But when $V^*$ is malicious verifier, there is no reason that $V^*$ picks the edge independently.$V^*$ can apply a giant hash function and do some bizarre thing to pick these dependent edges. Intuitively, itâ€™s harder to simulate such a thing. The simulatorâ€™s strategy in parallel repetition: $S$ feeds some made up first messages to $V^*$. $V^*$ picks the edges in bizarre manners. $S$ only can answer exactly one challenge. The key reason is the challenge space is exponentially large and the probability of hitting that made up challenge is negligible.So this simulation strategy goes down the drain. This theorem tells that some protocols in parallel repetition is not zero-knowledge against malicious verifier. And the following theorem tells us that the parallel repetition of 3COL protocol is not zero-knowledge if we run it in many and many times in parallel. Theorem [Holmgren-Lombardi-Rothblumâ€™s21]: Parallel Repetition of the (Goldreich-Micali-Wigderson) 3COL protocol is not zero-knowledge. Fortunately, we have zero-knowledge protocols in const rounds with exponentially small soundness error, rather in a million rounds. Theorem [Goldreich-Kahanâ€™95]: There is a constant-round ZK proof system for 3COL (will exponentially small soundness error), assuming discrete logarithms are hard. Proofs of KnowledgeSo far, we focus on the decision problem: $y\\in \\mathcal{L}$ or $y\\notin \\mathcal{L}$. (e.g. $y$ is quadratic residue $\\mod N$ or it is not.) Here is a different scenario that Alice has the knowledge, the discrete log of $y$ assuming $g$ is a generator. And Alice wants to convince Bob the discrete log of $y$ always exists. In this scenario the prover wants to convince the verifier that she knows a solution to a problem, e.g. that she knows the discrete log of $y$. It is difficult to formulate it as the decision problem. It is Proof of Knowledge. Likewise, we can define the completeness, soundness and zero-knowledge. Completeness: When Alice and Bob run the protocol where Alice has input $x$, Bob outputs accept. Soundness: How to define soundness that Alice dose not have the knowledge ?It is difficult to formulate the leak of knowledge. Zero-knowledge: There is a simulator that, given only $y$, outputs a view of Bob that is indistinguishable from his view in an interaction with Alice. ExtractorThe main idea of Goldreich is that if Alice knows $x$, there must be a way to â€œextract it from herâ€. Itâ€™s not about putting diodes on her brain. [*diode äºŒæžç®¡] Itâ€™s sort of talking to Alice. Definition of Proof of Knowledge: For any cheating$P^*$, if the prover can convince the verifier that the discrete log of $y$ always exist such that $\\operatorname{Pr}[\\langle P^*,V\\rangle(y)=\\textrm{accept}]\\ge \\varepsilon$, then there exists an extractor $E$ such that $\\operatorname{Pr}[E^{P^*}(y)=x \\text{ s.t. }y=g^x]\\ge \\varepsilon'\\approx \\varepsilon$. The extractor is indeed the expected ppt adversary. The definition of PoK is proposed in On Defining Proofs of Knowledge by Mihir Bellare and Oded Goldreich. We will not dig into the definition but give an example of PoK. ZK Proof of Knowledge of Discrete Log.The protocol is as follows. ZK Proof of Knowledge of DLOG: Prover: Pick a random $r$ and send $z=g^r$ to Verifier. Verifier: Pick a random challenge $c$ Prover: Answer the challenge If $c=0$: send $s=r$ If $c=1$: send $s=r+x$ Verifier: Accept iff $g^s=z\\cdot y^c$. The above protocol is completeness, soundness and zero-knowledge. The completeness and zero-knowledge is well proven. Completeness: If the prover has the discrete log of $y$, the verifier accepts with probability 1. $g^s=g^{r+cx}=g^r\\cdot (g^{x})^c=z\\cdot y^c$ Zero-knowledge: The real view of $V^*$ is $\\texttt{view}_{V^*}=(z,c,s)$ The simulator works as follows Generate $z=g^s/y^c$ for a random $s$ and a random $c$. Feed $z$ to verifier and get the challenge $c^*=V^*(z)$. If $c^*=c$, output as the simulated transcript. If $c^*\\ne c$, back to step 1 and repeat. The simulated view is identical to the view in real execution. Soundness: The key is to construct an extractor by the contradiction. If the protocol is of soundness, the cheating prover $P^*$ can convince the verifier with probability $1/2$. Assume for the contradiction that $P^*$ convinces the verifier with probability $\\ge 1/2+1/poly$. Then the prover $P^*$ should prepare for both challenges. Itâ€™s easy to extract the discrete log of $y$ form $P^*$. Runs $P^*$ with $c=0$ and gets $s_0$. Rewind $P^*$ to the first message. Runs $P^*$ with $c=1$ and gets $s_1$. By contradiction, $g^{s_0}=z$ and $g^{s_1}=zy$ with probability $1/poly$. That is $g^{s_1-s_0}=y$ w.p. $1/poly$. So $s_1-s_0$ is the discrete log of $y$ w.p. $1/poly$. Itâ€™s known as Schnorr proof, or Schnorr Signature. Efficient Signature Generation by Smart Cards Non-Interactive ZKLetâ€™s proceed to the next topic. Can we make proofs non-interactive again ? The advantages of Non-Interactive ZK (NIZK): $V$ dose not need to be online during the proof process. Proofs are not ephemeral, can stay into the future.[*ephemeral çŸ­æš‚çš„] NIZK is ImpossibleFirstly, we claim that NIZK is impossible. Suppose there were an NIZK proof system for 3COL. The NIZK proof is of completeness and zero-knowledge, but NOT sound. Proof: Completeness: When $G$ is in 3COL, $V$ accepts the proof $\\pi$. Zero Knowledge: PPT simulator $S$, given only Gâ€‹ in 3COL, produces an indistinguishable proof $\\tilde{\\pi}$.In particular, $V$ accepts $\\tilde{\\pi}$. Imagine running the Simulator $S$ on a $\\underline{G\\notin 3COL}$.It produces a proof $\\tilde{\\pi}$ which the verifier still accepts! Because $S$ and $V$ are PPT. They together cannot tell if the input graph is 3COL or not without the witness. Therefore, $S$ is indeed a cheating prover!It can produce a proof for a $G\\notin 3COL$ that the verifier nevertheless accepts. Ergo, the proof system is NOT SOUND. Two Roads to NIZKBut we can achieve NIZK under some models. There are two roads to NIZK. Random Oracle Model &amp; Fiat-Shamir Transform Common Random String Model NIZK: Random Oracle ModelAs discussed before randomness of verifier for ZK proofs is necessary. Otherwise, it is not interactive. More discussion about randomness.Give an example in ZK proof of knowledge for discrete log.The protocol is not sound if $P^*$ knows the random challenge $c$ beforehand. If $P^*$ knows $c=0$ beforehand, itâ€™s useless. If $P^*$ knows $c=1$ beforehand, Send $z=g^s/y$ for random $s$. Send $s$. NIZK for 3COLConsider NIZK Proof for 3COL. Start with the parallel repetition of 3COL protocol. It is complete, has exponentially small soundness error, and is hones-verifier ZK. Similarly, the randomness is necessary. Otherwise, the cheating prover can make up message $a$, $z$ beforehand. However, the protocol can be non-interactive in the random oracle model. Recap Random Oracle Model [Lecture 12] In random oracle model, the only way to compute $H$ is by calling the oracle.We can consider it as a very complicated public function, e.g. SHA-3.Moreover, we can consider the public function as a proxy to a random function. But in the Random Oracle Heuristic world, the only way to compute $H$, virtually a black box, is by calling the oracle. Fiat and Shamir (1986): Let $c=H(a)$. Now the prover can compute the challenge herself! It is potentially harmful for soundness. But in random oracle model for $H$, it can prove soundness.","link":"/2022/08/15/mit6875-lec15/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 16","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: IP for Quadratic Non-Residuosity Non-interactive ZK NIZK in The Common Random String(CRS) Model Construction in CRS Model: Blum-Feldman-Micaliâ€™88 (quadratic residuosity) NIZK for QNR NIZK for 3SAT Proofs vs. Argument At the end of last blog, we said that Non-Interactive Zero-Knowledge (NIZK) is achievable in random oracle model. Today, we move to the NIZK in the common random string model. The Power of Interactive ProofsBefore proceeding to NIZK, letâ€™s focus on the power of Interaction Proofs(IP). Is IP more powerful than NP ? We have been using interaction to get zero knowledge proofs for NP. Indeed, interaction is necessary. But, never mind zero knowledge for a moment. Can you prove more stuff with interactive proofs than with traditional (i.e. NP) proofs ? The thing to point is that we know there is NP proof for quadratic residues, i.e. proof = the square root, but there is no NP proof for quadratic non-residues. In Lecture 14, we gave interactive proofs for quadratic residuosity. Indeed, we can also give an (honest-verifier perfect ZK) interactive for quadratic non-residuosity. IP for QNRThe prover wants to convince the verifier that the $y$ is a quadratic non-residuosity. The interactive protocol is as shown below. ZK Proof for Quadratic Non-residue: Verifier: pick a random $r$ pick a random $b\\gets{0,1}$ send $s=r^2y^b$ Prover: guess $bâ€™$ Verifier Check: $b=bâ€™$ The protocol is complete, sound and (honest-verifier perfect) zero-knowledge. Completeness: Recall that the completeness is the property of the protocol when the prover and the verifier are both honest. If $y$ is non-square, the prover should be able to win. The thing to point is that the prover knows $y$ is non-square, so maybe the she is unbounded or she knows the factorization of $N$ since there is no NP proof for QNR. Hence, if $y$ is non-square, then the prover can tell $s$ is square or non-square to answer $bâ€™$. Soundness: Recall that the soundness is the property of the protocol against malicious prover. If $y$ is a square, then $r^2y^b$ is also a square. Hence, the prover cannot cheat the verifier with probability better than $1/2$. Then we can use sequential repetition or parallel repetition to reduce the soundness error. Honest-Verifier Perfect Zero Knowledge: Recall that the zero-knowledge is the property of the protocol against verifier. The view of $V$ is $(r,b,bâ€™)$, so the simulator can do exactly what $V$ dose. The simulated view is $(r,b,bâ€™=b)$ for a random $r$ and a random $b$. Similarly, although there is no NP proof for graph non-isomorphism, there is an (honest-verifier perfect) interactive proof for graph non-isomorphism. It turns out that IP can prove more stuff than NP. Indeed, the IP = PSPACE &gt;&gt; NP. PSPACE is the set of things that we can decide if we have polynomial memory to work with but potential exponential time. Itâ€™s surprising that we can prove such a language, i.e. QNR, using interactive proofs. IP = PSPACE >> NP[Lund-Fortnow-Karloof-Nisanâ€™89, Shamirâ€™90] https://www.cs.princeton.edu/courses/archive/spring09/cos522/BabaiEmail.pdf NIZK: The Common Random String ModelIn last blog is introduced that we can achieve NIZK in the random oracle model. In this section, we will introduce another way, the common random string model. In this model, there is an angle coming up with random sequence of polynomial bits. The angle could be the sunspots. Completeness: For every $G\\in 3COL$, $V$ accepts $P$â€™s proof. Soundness: For every $G\\notin3COL$ and any â€œproofâ€ $\\pi^\\star$, $V(CRS,\\pi^\\star)$ accepts with probability $\\le neg(n)$. Zero Knowledge: There is a PPT simulator such that for every $G\\in 3COL$, $S$ simulates the view of the verifier $V$. $$ S(G)\\approx(CRS\\gets D, \\pi \\gets P(G,\\text{colors}) $$ The view of verifier is $(CRS, \\pi)$. The simulator has to produce the simulated view without knowing the witness and the CRS. So the simulator has to fake potentially the common random and the proof. If we sort of change the definition that the simulator gets a common random string and has to produce the view with a fixed CRS, then this definition is impossible to achieve.For the same reason that we prove the NIZK is impossible (Lecture 15), it dose not satisfy the soundness. Similarly, the Common Reference String Model is that there is an angle coming up the random product of two primes, not a sequence of random bits. And it cannot be achieved by the sunspots. Construct NIZK in the CRS ModelThere are several constructions of NIZK in the CRS model. Blum-Feldman-Micaliâ€™88 (quadratic residuosity) Feige-Lapidot-Shamirâ€™90 (factoring) Groth-Ostrovsky-Sahaiâ€™06 (bilinear maps) Canetti-Chen-Holmgren-Lombardi-Rothblum^2-Wichsâ€™19 and Peikert-Shiehianâ€™19 (learning with errors) In this Lecture, we will introduce the first construction. Blum-Feldman-Micaliâ€™88 (quadratic residuosity) Review our number theory hammers &amp; polish them. Construct NIZK for a special NP language, namely quadratic non-residuosity. Bootstrap to NIZK for 3SAT, an NP-complete language. Quadratic Residuosity More introduction is referred in Lecture 9 Let $N=pq$ be a product of two large primes. Jacobi symbol ($Jac$) divides $\\mathbb{Z}_N^*$ evenly unless $N$ is a perfect square. $Jac_{-1}$: non-squares $Jac_{+1}$: pseudo-squares A surprising fact is Jacobi symbol $\\left(\\frac{x}{N}\\right)=\\left(\\frac{x}{P}\\right) \\left(\\frac{x}{Q}\\right)$ is computable in poly. time without knowing $p$ and $q$. (using Law of Quadratic Reciprocity.[* äºŒæ¬¡äº’åå®šç†]) $x$ is square mod $N$ iff $x$ is square mod $p$ and square mod $q$, so we can even $Jac_{+1}$. $QR_N$: the set of squares mod $N$. $QNR_N$: the set of non-squares mod $N$. (but with Jacobi symbol $+1$) Note: The following are new claims: We call $N$ good if exactly half the elements of $\\mathbb{Z}_N^*$ with Jacobi symbol $+1$ are squares. Exactly half residues even iff $N=p^iq^j$ is odd, and $i,j\\ge 1$, not both even. If $N$ is good, there is an important property is if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$. (This property will be used in the following NIZK for QNR) But if $N$ has three or more prime factors, the fraction of residues is smaller. Analysis of $N=pqr$: $x$ is square mod $N$ iff $x$ is square mod $p$, mod $q$ and mod $r$. If $N=pqr$, the fraction of residues in $Jac_{+1}$ is $1/4$ as shown in table below. Jac (x/p) (x/q) (x/r) 1 1 1 1 1 1 -1 -1 1 -1 1 -1 1 -1 -1 1 -1 1 1 -1 -1 1 -1 1 -1 1 1 -1 -1 -1 -1 -1 Besides, the property, â€œ if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$â€, is no longer satisfied. Quadratic Residuosity Assumption (QRA): Let $N=pq$ be a product of two large primes. No PPT algorithm can distinguish between a random element of $QR_N$ from a random element of $QNR_N$ given only $N$. NIZK for QNRDefine the NP language GOOD with instance $(N,y)$ where $N$ is good; and $y\\in QNR_N$ (that is, $y$ has Jacobi symbol $+1$ but is not a square mod $N$) The non-interactive protocol is as follows. The prover wants to convince the verifier that $(N,y)$ is good. NIZK for QNR: $CRS = (r_1,r_2,\\dots,r_m)$ where each $r_i$ is sampled from $Jac_N^{+1}$. The proof is $\\forall i: \\sqrt{r_i}$ or $\\sqrt{yr_i}$ The verifier check $N$ is odd $N$ is not a prime power $N$ is not a perfect square (Fact: If the preceding three passes, then at most half of $Jac_N^{+1}$ are squares.) I received either a mod-$N$ square root of $r_i$ or $yr_i$. Completeness: If $N$ is good and $y\\in QNR_N$, the prover can compute either $\\sqrt{r_i}$ or $\\sqrt{yr_i}$. The prover has a knowledge that $y$ is non-residuosity. So, maybe she is unbounded or knows the factorization of $N$. If $r_i\\in QR_N$: the prover can compute $\\sqrt{r_i}$. If $r_i\\in QNR_N$: $yr_i$ is square by the property above so the prover can compute $\\sqrt{yr_i}$. Soundness: What if $N$ has more than $2$ prime factors ? No matter what $y$ is, for half the $r_i$ , both $r_i$ and $yr_i$ are not quadratic residues. So the prover cannot compute either $\\sqrt{r_i}$ or $\\sqrt{yr_i}$ for half the $r_i$. Suppose $N=pqr$ The fraction of quadratic residues in $Jac_{+1}$ is $1/4$. Besides, the property, â€œ if $y_1$ and $y_2$ are both in $QNR$, then their product $y_1y_2$ is in $QR$â€, is no longer satisfied. The thing to notice is that the proof only proves $N$ is good, not $N=pq$. Perfect Zero Knowledge Simulator S: First pick the proof $\\pi_i$ to be random in $\\mathbb{Z}_N^*$. Then reverse-engineer the CRS, letting $r_i=\\pi_i^2$ or $r_i=\\pi_i^2/y$ randomly. The distribution of simulated view is identical to the real view. Warning:We define $CRS = (r_1,r_2,\\dots,r_m)$ where each $r_i$ is sampled from $Jac_N^{+1}$. The CRS depends on the instance $N$. Not good.An alternative solution is Let CRS be random numbers. Interpret them as elements of $\\mathbb{Z}_N^*$ and both the prover and the verifier filter out $Jac_N^{-1}$. NIZK for 3SAT From Wiki: SAT:In logic and computer science, the Boolean satisfiability problem (sometimes called propositional satisfiability problem and abbreviated SATISFIABILITY, SAT or B-SAT) is the problem of determining if there exists an interpretation that satisfies a given Boolean formula.SAT is the first problem that was proven to be NP-complete.This means that all problems in the complexity class NP, which includes a wide range of natural decision and optimization problems, are at most as difficult to solve as SAT. 3SAT:Like the satisfiability problem for arbitrary formulas, determining the satisfiability of a formula in conjunctive normal form where each clause is limited to at most three literals is NP-complete also; this problem is called 3-SAT, 3CNFSAT, or 3-satisfiability. Literal:In mathematical logic, a literal is an atomic formula (atom) or its negation. Boolean Variables: $x_i$ can be either true(1) or false(0). A Literal is either $x_i$ or $\\overline{x_i}$ A Clause is a disjunction of literals. A Clause is true if any one of the literals is true. E.g. $x_1\\vee x_2\\vee \\overline{x_5}$ is true as long as $(x_1,x_2,x_5)\\ne(0,0,1)$ A 3-SAT formula is a conjunction of many 3-clauses. A 3-SAT formula $\\Psi$ is satisfiable if there is an assignment of values to the variables $x_i$ that makes all its clauses true. $\\Psi=\\left(x_{1} \\vee x_{2} \\vee \\overline{x_{5}}\\right) \\wedge\\left(x_{1} \\vee x_{3} \\vee x_{4}\\right)\\wedge\\left(\\overline{x_{2}} \\vee x_{3} \\vee \\overline{x_{5}}\\right)$ Cook-Levin Theorem: It is NP-complete to decide whether a 3-SAT formula $\\Psi$ is satisfiable. Recall NIZK for QNR. We saw a way to show that a pair $(N,y)$ is GOOD. That is: the following is the picture of $\\mathbb{Z}_N^*$ and for every $r\\in Jac_{+1}$, either $r$ or $ry$ is a quadratic residue. The prover wants to convince the verifier that $\\Psi$ is satisfiable. The input $\\Psi=\\left(x_{1} \\vee x_{2} \\vee \\overline{x_{5}}\\right) \\wedge\\left(x_{1} \\vee x_{3} \\vee x_{4}\\right)\\wedge\\left(\\overline{x_{2}} \\vee x_{3} \\vee \\overline{x_{5}}\\right)â€¦$ with $n$ variables and $m$ clauses. The non-interactive protocol is as follows. NIZK for 3SAT: $CRS=(r_1,r_2,â€¦)$ where each $r_i$ is sampled from $Jac_N^{+1}$.Note: Similarly, we can Let CRS be random numbers and interpret them as elements of $\\mathbb{Z}_N^*$, and both the prover and the verifier filter out $Jac_N^{-1}$. Prover picks an $(N,y)$ and proves that it is GOOD. (as defined above) Prover encodes the satisfying assignment and sends the encode variables $(y_1,\\dots,y_n)$ to $V$.We should hide the values to achieve zero knowledge so the encoding is indeed a commitment. $y_i\\gets QR_N$ if $x_i$ is false (encryption of $0$ is a residue) $y_i \\gets QNR_N$ if $x_i$ is true (encryption of $1$ a non-residue) For the literal $x_i$, $Enc(x_i)=y_i$ $Enc(\\overline{x_i})=yy_i$ exactly one of the $Enc(x_i)$ or $Enc(\\overline{x_i})$ is a non-residue. A residue is indeed the encryption of 0(false) A non-residue is indeed the encryption of 1(true) Prove that (encoded) assignment satisfies each clause. For each clause, say $x_1\\vee x_2\\vee \\overline{x_5}$, we WANT to SHOW: $x_1$ OR $x_2$ OR $\\overline{x_5}$ is true. Let $(a_1=y_1,b_1=y_2,c_1=yy_5)$ denote the encoded variables. So, each of them is either $y_i$ (if the literal is a var) or $yy_i$ (if the literal is a negated var). Now, we WANT to SHOW: $a_1$ OR $b_1$ OR $c_1$ is a non-residue. A terrible way is to prove $a_1$ is non-residue or $b_1$ is non-residue. If the prover convinces the verifier that $a_1$ is non-residue, $P$ indeed tells $V$ that $x_1$ is true(1). Hence, we want to prove one of them is a non-residue without telling the verifier that which one is non-residue. WANT to SHOW: $a_1$ OR $b_1$ OR $c_1$ is a non-residue Equivalently, the signature of $(a_1,b_1,c_1)$ is NOT (QR, QR, QR). A clever idea is to generate seven additional triples. The original triple is $(a_1,b_1,c_1)$. This 8 triples span all possible QR signatures. There is one triple is (QR, QR, QR), e.g. the second triple, and reveal the square roots of this triple. Then we can prove the origin triple is NOT (QR, QR, QR). Proof consists of two parts Proof of Coverage : show that the 8 triples span all possible QR signatures. For each of poly. many triples $(r,s,t)$ from CRS, show one of the 8 triples has the same signature. That is, there is a triple $(a_i,b_i,c_i)$ s.t. $(ra_i,sb_i,tc_i)$ is (QR, QR, QR) Show one triple (except the original triple) is (QR, QR, QR) and reveal the square roots. Hence, the full proof is as follows. Prover picks an $(N,y)$ and proves that it is GOOD. Prover encodes the satisfying assignment and sends the encode variables $(y_1,\\dots,y_n)$ to $V$. Prove that (encoded) assignments satisfy each clause. For each clause, construct the proof $\\rho$ = (7 additional triples, square root of the second triples, proof of coverage). Completeness and Soundness can be inherited from NIZK for QNR. Computational Zero Knowledge Simulator S: Simulator picks $(N,y)$ where $y$ is a quadratic residue. The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. Simulated view of $(N,y, \\pi)$ Pick the proof $\\pi_i$ to be random in $\\mathbb{Z}_N^*$. Reverse-engineer the CRS, letting $r_i=\\pi_i^2$ or $r_i=\\pi_i^2/y$ randomly. Then $r_i=\\pi_i^2$ and $r_i=\\pi_i^2/y$ are both residues. The distribution of simulated view is computationally indistinguishable to the real view. Simulated view of encoded assignments $(y_1,\\dots, y_n)$ $y_i \\gets QNR_N$ whether $x_i$ is true or false (encryptions are both non-residues) $Enc(x_i)=y_i$ (encryption of $1$ is a non-residue) $Enc(\\overline{x_i})=yy_i$ (encryption of $0$ is a non-residue) Encoding of ALL literals can be set to true. Proofs vs. ArgumentBefore proceeding to the evolution of proofs, letâ€™s discuss the difference between proofs and arguments. The main difference is the definition in soundness. In Lecture 14, we gave the definition of soundness in Proofs that nobody can convince you a false statement. We mentioned that the soundness holds agains unbounded provers. Arguments is where the soundness holds against only computational bounded provers. Itâ€™s actually weakening the notion of proof. Why do we still introduce the the argument ? It allows us to get perfect zero knowledge for all of NP.Recall perfect ZK proofs do not exist for NP-complete languages, unless the polynomial hierarchy collapses. It turns out that if we weaken the notion of proofs, then we can construct the perfect zero knowledge systems. It allows us to get very short proof = succinct arguments or SNARGs.SNARKs : succinct arguments of knowledge. The Evolution of ProofsProofs have been evolving over generations. CLASSIC Proofs (Complexity class: NP) Prover writes down a string (proof); And Verifier checks. INTERACTIVE Proofs (Complexity class: IP = PSPACE &gt;&gt; NP) Prover and verifier talk back and forth. In last blog, we introduced the zk proof for quadratic non-residue. PROBABILISTICALLY CHECKABLE Proofs (Complexity class: NEXP &gt;&gt; PSPACE) Non-interactive, but verifier only looks at 3 bits of proof. MULTIPROVER interactive proofs Proofs in the wild: ZCash","link":"/2022/08/22/mit6875-lec16/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 17","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Definition of IND-CCA Security Application of NIZK: Construction of CCA-secure encryption scheme In last post, we saw a NIZK for 3SAT, and then we can get a NIZK for all of NP in the CRS model. Moreover, we mentioned that if we weaken the notion of proofs into arguments, then we can construct perfect zk for all of NP. Today, we will discuss the application of NIZK, non-malleable and chosen ciphertext secure encryption scheme. Active Attacks against CPA-secure EncryptionRecall the public encryption schemes. Bob generate a pair of keys, a public key $pk$, and a private (or secret) key $sk$. Bob â€œpublishesâ€ $pk$ and keeps $sk$ to himself. Alice encrypts $m$ to Bob using $pk$. Bob decrypts using $sk$. In Lecture 8, we gave the definition of IND-CPA security for public encryption scheme and mentioned the IND-CPA-secure is achievable with randomness. But there are two active attacks against the public encryption scheme above. MalleabilityThe first active attack is malleability. Consider the scenario for bidding. Alice wants to bid Â¥100 and she encrypts her bid with public key. There is a malicious attacker that wants to win the bidding and he can modify the encryption of Â¥100â€‹ into an encryption of Â¥101 as his bidding. The attack can always modify the encryption, which is always one more dollar than Aliceâ€™s although the attack dose not know what Aliceâ€™s bid is. So the adversary could modify (â€œmaulâ€) an encryption of $m$ into an encryption of a related message $mâ€™$. Chosen-Ciphertext AttackAnother active attack is chosen-ciphertext attack. If the first bit of the message is 0, we define itâ€™s the encryption of a valid message. If the first bit of the message is 1, we define itâ€™s the encryption of an invalid message. Then the adversary may have access to a decryption â€œoracleâ€ and can use it to break security of a â€œtargetâ€ ciphertext $c^*$ or even extract the secret key! In fact, Bleichenbacher showed how to extract the entire secret key given only a â€œciphertext verificationâ€ oracle. Bleichenbacher IND-CCA SecurityAfter defining the stronger active attackers, we can define the Indistinguishable Chosen-Ciphertext Attack Security, or IND-CCA. Recall the game in IND-CPA secure definition. Game in IND-CPA (one-message): The Challenger generates a pair of key $(pk, sk)$ and publishes the $pk$. Eve sends two single messages, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$. The challenger samples $b$ from ${0,1}$ and encrypts the message $m_b$ using $pk$.And send the ciphertext to Eve. Eve guesses which message is encrypted and output $bâ€™$. Eve wins if $bâ€™=b$. Letâ€™s move to the game in IND-CCA. As has been said, the adversary has the power of accessing to a decryption â€œoracleâ€. The adversary can query for the decryption in polynomial many times. It can happen both before and after the challenge. Note that the adversary can query for the decryption, before the challenge, of any ciphertext. But after the challenge, he cannot query for the decryption of the challenge. Otherwise, the challenge is meaningless. So there are additional two phases in the game of IND-CCA. Game in IND-CCA: The Challenger generates a pair of key $(pk, sk)$ and publishes the $pk$. Eve asks for the decryption of any ciphertext in poly. many times. Eve sends two single messages, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$. The challenger samples $b$ from ${0,1}$ and encrypts the message $m_b$ using $pk$.And send the ciphertext to Eve. Eve asks for the decryption of any ciphertext (except the challenge $c^\\star$) in poly. many times. Eve guesses which message is encrypted and output $bâ€™$. Eve wins if $bâ€™=b$. IND-CCA Security Definition The encryption scheme is IND-CCA secure if no PPT Eve can win with probability $&gt;1/2+negl(\\lambda)$. Constructing CCA-Secure Encryption In the light of the CCA-secure definition, we can construct the CCA-secure encryption scheme. Our goal is that the adversary is hard to modify an encryption of $m$ into an encryption of a related message, say $m+1$. Intuitionally, the proof of knowledge and the signatures should help against the malleability for CAP-secure encryption. With NIZK proofs of knowledge, the idea is that the encryption party attaches an NIZK proof of knowledge of the underlying message to the ciphertext. Therefore, the encryption consists of CPA-encryption and the NIZK proof of knowledge of the message. $C:(c=\\text{CPAEnc}(m;r),\\text{ proof }\\pi \\text{ that â€œI know }m\\text{ and }r\\text{ â€œ})$. This idea will turns out to be useful, but NIZK proofs themselves can be malleable. So the active attack turns to create the CPA-encryption of $(m+1,r)$ and the NIZK proof of $(m+1,r)$. Start with Digital SignaturesLetâ€™s start with digital signatures. We construct the CCA-secure encryption, which contains the signature of the CPA-secure encryption. Note that it is malleable. $C:(c=\\text{CPAEnc}(pk,m;r),\\text{Sign}_{sgk}(c),vk)$ where the encryption produces a signing/verification key pair by running $(sgk,vk)\\gets \\text{Sign.Gen}(1^n)$. It is actually not CCA-secure. (or itâ€™s malleable) If the adversary changes $vk$, all bets are off. The picture below explains the reason vividly. Consequently, the lesson is that we need to â€œtieâ€ the ciphertext $c$ to $vk$ in a â€œmeaningfulâ€ way. IND-CPA â†’ â€œDifferent-Key Non-malleabilityâ€One observation is we can reduce IND-CPA to Different-Key Non-malleability(NM). The Different-Key NM predicates that given (independent) $pk,pkâ€™$ and the encryption $\\text{CPAEnc}(pk,m;r)$, the adversary cannot produce $\\text{CPAEnc}(pkâ€™,m+1;r)$. Itâ€™s a reduction that suppose the adversary could produce the different-key encryption, then she can break the IND-CPA security of $\\text{CPAEnc}(pk,m;r)$. Proof: Suppose for contradiction that the adversary has the power of producing the different-key encryption, $\\text{CPAEnc}(pkâ€™,m+1;r)$. The interaction with the Diff-Key NM adversary is as follows. Interaction with Diff-Key NM Adv. Pick a random pair $(pkâ€™, skâ€™)$ and give the two public keys $pk,pkâ€™$ . Give the CPA-secure encryption of $m$ using $pk$. The Diff-Key NM adv. promises to produce the CPA-secure encryption of $m+1$ using the different key $pkâ€™$. (w.r.t. contradiction) Then we can construct a CPA adversary by using the Diff-Key NM adversary, as shown below. Break CPA-secure Encryption The challenge is to decrypt the message given CPA-secure encryption.(as shown on the left of the picture) Given $pk$, then we pick a random pair $(pkâ€™,skâ€™)$ and send $pk,pkâ€™$ to the adversary. Give the CPA-secure encryption from CPA challenge. The adversary promises to produce an encryption using the different key $pkâ€™$. Then we can decrypt it with $skâ€™$ and subtract 1 to get $m$. Hence, if the adversary can break the Different-Key NM game, then she can break CPA security. CCA-Secure Encryption SchemeWe can get non-malleable and CCA-secure encryption putting CPA-secure encryption, digital signature and NIZK proofs together. As has been said, we need to tie the ciphertext $c$ to the verification key $vk$. NM Encryption Scheme CCA Encryption (only non-malleable): We define $2n$ public keys of the CPA scheme as the CCA public key. CCA Public Key: $\\left[\\begin{array}{llll}p k_{1,0} &amp; p k_{2,0} &amp; \\dots &amp; p k_{n, 0} \\ p k_{1,1} &amp; p k_{2,1} &amp; \\dots &amp; p k_{n, 1}\\end{array}\\right]$ where $n=|vk|$. First, pick a signing/verification key pair $(sgk, vk)$. Then use the CCA public key, based on the bits of $vk$, to produce the ciphertext. $CT=[ct_{1,vk_1},ct_{2,vk_2},\\dots,ct_{n,vk_n}]$ where $ct_{i,j}\\gets \\text{CPAEnc}(pk_{i,j},m)$. This ties the ciphertext $CT$ to the verification key $vk$ in meaningful way that the encryption is under the public key indexed by the bits of $vk$. For each ciphertext in slot $i$ of $CT$: If the $i$-th bit of $vk$ is 0, then use $pk_{i,0}$ to produce the ciphertext. If the $i$-th bit of $vk$ is 1, then use $pk_{i,1}$ to produce the ciphertext. Output $(CT,vk,\\sigma=\\text{Sign}_{sgk}(CT))$ The encryption scheme above is non-malleable. Non-malleability rationale: If the adversary keeps the $vk$ the same, she needs to produce $(CTâ€™,vk,\\sigma_{sgk}(CTâ€™))$ for the related message $mâ€™$, which has to break the signature scheme. $CTâ€™$ is encrypted under the same public key as $CT$. If the adversary changes the $vk$, she has to break the Different-Key Non-malleability game, and therefore CPA security. The adversary needs the produce $(CTâ€™,vkâ€™,\\sigma_{sgkâ€™}(CTâ€™))$ for the related message $mâ€™$. $CTâ€™$ is encrypted under the different public key, which is indexed by $vkâ€™$. Hence, for each different bit of $vkâ€™$: The original $ct_{i,j}\\gets \\text{CPAEnc}(pk_{i,j},m)$. The adversary needs to produce $ct_{i,j}â€™\\gets \\text{CPAEnc}(pk_{i,1\\oplus j},mâ€™)$. Turns out the Different-Key NM, which can be reduced to CPA security. CCA-Secure Encryption SchemeWe are not done!!Adversary could create ill-formed ciphertexts, e.g. different $ct$s encrypt different messages, and uses it for Bleichenbacher-like attack. Hence, it has to prove that the ciphertext is well-formed. CCA Encryption (non-malleable and CCA-secure): We define $2n$ public keys of the CPA scheme as the CCA public key. CCA Keys: PK = $\\left[\\begin{array}{llll}p k_{1,0} &amp; p k_{2,0} &amp; \\dots &amp; p k_{n, 0} \\ p k_{1,1} &amp; p k_{2,1} &amp; \\dots &amp; p k_{n, 1}\\end{array}\\right]$, CRS where $n=|vk|$. SK = $\\left[\\begin{array}{c}sk_{1,0} \\ sk_{1,1}\\end{array}\\right]$ To achieve NIZK proof, it has to have (public) CRS. The secret key contains only one pair since it needs to be proven well-formed. First, pick a signing/verification key pair $(sgk, vk)$. Then use the CCA public key, based on the bits of $vk$, to produce the ciphertext. $CT=[ct_{1,vk_1},ct_{2,vk_2},\\dots,ct_{n,vk_n}]$ where $ct_{i,j}\\gets \\text{CPAEnc}(pk_{i,j},m)$. Generate NIZK proof $\\pi$ = â€œCT is well-formedâ€. Output $(CT,{\\color{blue}\\pi},vk,\\sigma=\\text{Sign}_{sgk}({\\color{blue}{CT,\\pi}}))$ CCA Decryption: Check the signature Check the NIZK proof Decrypt with $sk_{1,vk_1}$ Now, this encryption scheme is CCA-secure and non-malleable. Proof of CCA-security: Proof Sketch Suppose for contradiction that there is an CCA adversary. Play the CCA game with the adversary.Note that we will define several hybrid distributions to argument and play the CCA game in one Hybrid. Then we can use her to break either the NIZK soundness/ZK, the signature scheme or the CPA-secure scheme. CCA game is as follows. I drew this picture on my own. since the proof is omitted in the lecture. Corrections and advice are welcome. Hybrid 0: Play the CCA game as prescribed. Hybrid 1: Observe that $vk_i\\ne vk^*$. (Otherwise break signature. ) Observe that this means each query ciphertext-tuple involves a different public-key from the challenge ciphertext. Then we can use the â€œdifferent private-keyâ€ to decrypt. (If the adversary sees a difference, she broke NIZK soundness.) It means that the adversary produces a ill-formed ciphertext and she cheats successfuly. Hybrid 2: Now change the CRS/$\\pi$ into simulated CRS/$\\pi$. (Itâ€™s OK by ZK) The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. If we want to use the adversary to break CPA security, it has to win the CPA game as follows. We can plop $pk$, given in CPA game, into one slot of $PK$. We plop ciphertext, given in CPA game, into $c^*$ with the corresponding slot. For other slots in $c^*$, we can generate the ciphertext for random message. But we cannot generate NIZK proof $\\pi$ since we donâ€™t have the witness.It says that the $c^*$ is ill-formed. But we cannot since we donâ€™t have the witness , that is $m^*$, the challenge of CPA game. Hence, in Hybrid 2, we change the CRS/$\\pi$ into simulated CRS/$\\pi$. Itâ€™s zero-knowledge. But more importantly, we can generate simulated proof. Consequently, if the adversary wins in this hybrid, she breaks IND-CPA as shown below. I drew this picture on my own since the proof is omitted in the lecture. Corrections and advice are welcome.","link":"/2022/09/02/mit6875-lec17/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 2","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Science wins either way. Topics: How to circumvent Shannonâ€™s lower bound: the computational adversary Definition of computational security The definition of pseudorandom generators(PRG) RecapIn the previous blog, we define two equivalent definitions of security, Perfect Secrecy and Perfect Indistinguishability, with the setting of an arbitrary computationally unbounded adversary. Perfect Secrecy: A posteriori = A prioriFor all $m, c$: $\\operatorname{Pr}[\\mathcal{M}=m\\mid \\operatorname{E}(\\mathcal{K,M)}=c] = \\operatorname{Pr}[\\mathcal{M}=m]$ Perfect Indistinguishability:For all $m_0, m_1, c$: $\\operatorname{Pr}[ \\operatorname{E}(\\mathcal{K,m_0)}=c] = \\operatorname{Pr}[\\operatorname{E}(\\mathcal{K,m_1)}=c]$ Although the definitions above are equivalent, sometime one is more convenient to work with than the other. We mentioned that the perfect secrecy is achievable using one-time pad. However, the keys are as long as messages in one-time pad. Worse still, in Shannonâ€™s theorem, for any perfectly secure scheme, the key space must be larger than or equal to the message space, i.e. $\\mathcal{|K|\\ge|M|}$. This is known as Shannonâ€™s conundrum. Therefore, the gist of this blog is how we can overcome Shannonâ€™s conundrum. Computational SecurityBefore that, Iâ€™ll introduce two mathematical definitions to instantiate Perfect Indistinguishability. Perfect IndistinguishabilityAs mentioned in the previous blog, Perfect Indistinguishability is substantially a formalization of the Turing test. There are two worlds, world 0 and world 1. The two worlds both sample the key $k$ from the key space $\\mathcal{K}$, and the world 0 uses $k$ to encrypt $m$ while world 1 uses $k$ to encrypt $mâ€™$. The adversary called EVE is a distinguisher that gets the $c$ and tries to guess which world sheâ€™s in. Perfect Indistinguishability: For all $m_0, m_1, c$: $\\operatorname{Pr}[ \\operatorname{E}(\\mathcal{K,m_0)}=c] = \\operatorname{Pr}[\\operatorname{E}(\\mathcal{K,m_1)}=c]$ There are two mathematical definitions to instantiate Perfect Indistinguishability. The First Def: The probability that EVE think she is in World 0 is the same. $\\begin{array}{l}\\text{For all EVE and all }m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; c=E(k,m_0):\\mathrm{EVE}(c)=0] = \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; c=E(k,m_1):\\mathrm{EVE}(c)=0]\\end{array}$ The Second Def: The probability of EVE guessing correctly which world she is in is exactly half. $\\begin{array}{l}\\text{For all EVE and all }m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\};c=E(k,m_b):\\mathrm{EVE}(c)=b] = 1/2\\end{array}$ Key Idea: Computationally Bounded AdversariesNow, back to the question of how to overcome Shannonâ€™s conundrum. The key idea is we relax the adversary to an arbitrary computationally bounded adversary. Computational Indistinguishability (take 1)In order to define the computationally bounded adversary (or, Computational Indistinguishability), Iâ€™ll introduce the axiom of modern crypto that Feasible Computation = Probabilistic Polynomial-time. The Axiom of Modern Crypto: p.p.tFeasible Computation = Probabilistic polynomial-time p.p.t. is the abbreviation for Probabilistic Polynomial-time Itâ€™s a polynomial in a security parameter n (in next subsection). So, in the scenario of Secure Communication mentioned above, Alice and Bob are fixed p.p.t. algorithms while Eve is any p.p.t. algorithm. Toy Definition with p.p.t.Letâ€™s attempt to define Computational Indistinguishability with p.p.t.. There are two worlds again, world 0 and world 1.The only difference from the unbounded case is that the adversary is a p.p.t. distinguisher. Write it as a mathematical definition. Toy Def: $\\begin{array}{l}\\text{For all p.p.t EVE and all }m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; c=E(k,m_0):\\mathrm{EVE}(c)=0] = \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; c=E(k,m_1):\\mathrm{EVE}(c)=0]\\end{array}$ Not FeasibleHowever, itâ€™s still subject to Shannonâ€™s impossibility which means the key space must be larger than or equal to the message space for any scheme s.t. the Toy Def. Prove it by contradiction same with the proof of Shannonâ€™s Theorem. Proof: Assume the contradiction that the key is n bits and the message is n+1 bits, i.e. $\\mathcal{|K|&lt;|M|}$. If we pick any $c\\in \\mathcal{C}$ and decrypt $c$ with all distinct keys, there are at most $|\\mathcal{K}|$ possible messages.The possible message space is the blue part in the left ellipse which the size is at most $|\\mathcal{K}|$ and less than $\\mathcal{|M|}$. Assume $m_0$ is inside the possible message space (inside the blue part) while $m_1$ is outside the possible message space. Consider Eve that picks a random key k and output 0 if $\\mathrm{D}(k,c)=m_0$ (w.p $\\ge 1/2^n$)Itâ€™s possible because $m_0$ is inside the blue part. output 1 if $\\mathrm{D}(k,c)=m_1$ (w.p $=0$)Itâ€™s impossible. output a random bit if neither holds. (w.p $=1/2$) Hence, the probability of EVE guessing correctly which world she is in is more than half. $\\begin{array}{l}\\text{There exist EVE and }m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\};c=E(k,m_b):\\mathrm{EVE}(c)=b] = 1/2 + 1/2^n\\end{array}$ Computational Indistinguishability (take 2)For the purpose of circumventing the Shannonâ€™s lower bound, Iâ€™ll introduce a new notion. Negligible FunctionsInformally, Negligible Functions are functions that grow slower than $1/p(n)$ for any polynomial $p$. Definition of Negligible Functions: A function $\\mu :\\mathbb{N} \\rightarrow \\mathbb{R}$ is negligible if for every polynomial function $p$, there exists an $n_0$ s.t. for all $n&gt;n_0$: $$\\mu(n) &lt; 1/p(n)$$ The key property of negligible function is that events that occur with negligible probability look to poly-time algorithms like they never occur. Examples:Is $\\mu$ negligible ? $\\mu(n)=1/n^{100}$ if $n$ is prime.No, choose $p(n)=n^{200}$. $\\mu(n)=1/2^n$Yes. Let $\\mu(n)$ be a negligible function and $q(n)$ a polynomial function.Then $\\mu(n)q(n)$ is negligible function. ( Run the event $q(n)$ timesï¼‰ Security Parameter: nSecurity Parameter $n$ (sometimes $\\lambda$) always appears in papers. What the hell is it ? Security Parameter is used to measure how secure your system is. Security Parameter is sort of an input to all these algorithms to instantiate them. So runtimes &amp; success probabilities are measured as a function of $n$. We want honest parties run in time (fixed) polynomial in $n$. We want adversaries run in time (arbitrary) polynomial in $n$, and should have success probability negligible in $n$. DefinitionNow we can define Computational Indistinguishability with p.p.t and negligible function. Definition: $ \\begin{array}{l}\\text { For all p.p.t EVE, there is a negligible function } \\mu \\text { s.t. for all } m_{0}, m_{1}: \\\\ \\operatorname{Pr}\\left[k \\leftarrow \\mathcal{K} ; b \\leftarrow\\{0,1\\} ; c=E\\left(k, m_{b}\\right): \\operatorname{EVE}(c)=b\\right] \\leq 1 / 2+\\mu(n)\\end{array} $ Hence, the probability of EVE guessing correctly which world she is in is substantially half, which is half plus a negligible function of security parameter. Pseudorandom Generators (PRG)In the section, Iâ€™ll introduce our first crypto tool: Pseudorandom Generators (PRG). Before that, I want to ask a question. Can you take 10 random bits as input and deterministically generate 20 bits of randomness as output ? No, you cannot generate randomness out of nothing. Informally, Pseudo-random Generators are deterministic programs that stretch a â€œtruly randomâ€ seed into a (much) longer sequence of â€œseeming randomâ€ bits. There are more questions after having the informal definition. How to define â€œseeming randomâ€ ? Can such a G exist ? 3 Equiv. Definitions of PRGFor the first question that how to define a strong pseudo-random number generator, there are three equivalent definitions. Def 1 [Indistinguishability] â€œNo polynomial-time algorithm can distinguish between the output of a PRG on a random seed vs. a truly random string.â€ = â€œas good asâ€ a truly random string for all practical purpose. Def 2 [Next-bit Unpredictability] â€œNo polynomial-time algorithm can predict the (i+1)th bit of the output of a PRG given the first i bits, better than chance 1/2.â€ Def 3 [Incompressibility] â€œNo polynomial-time algorithm can compress the output of the PRG into a shorter stringâ€ All three definitions are equivalent! In this blog, Iâ€™ll just elaborate[*è¯¦å°½è¯´æ˜Ž; explain] the first definition of indistinguishability. The other two definitions and the proof will be elaborated in following blog. Def 1: IndistinguishabilityDef 1 [Indistinguishability] A deterministic polynomial-time computable function ${G}:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ is a PRG if: It is expanding: $m&gt;n$ and for every p.p.t. algorithm D (called a distinguisher or a statistical test) if there is a negligible function ${\\mu}$ such that: $$ |\\operatorname{Pr}[D({G(U_n)})=1]-\\operatorname{Pr}[D({U_m})=1]|={\\mu(n)} $$ Notation: $U_n$(resp. $U_m$) denotes the random distribution on $n$-bit (resp. $m$-bit) strings; $m$ is shorthand for $m(n)$. First of all, PRG is a deterministic polynomial-time computable function which can expand $n$ bits to $m$ bits. $m$ is shorthand for $m(n$) and $m&gt;n$. There are two worlds, world 1 and world 2. World 1 is the pseudorandom world which samples a truly random $n$-bit string from $U_n$ and expands it to a $m$-bit string using PRG $G$, and output the expanded $m$-bit string as $y$. World 0 is the truly random world which sample a truly random $m$-bit string from $U_m$, and output the sampled $m$-bit string as $y$. The adversary is a p.p.t distinguisher that gets the $y$ and tries to tell which world sheâ€™s in. Why good ?Why is this a good definition ? Itâ€™s good for all applications. As long as we can find truly random seeds, can replace true randomness by the output of PRG(seed) in any (polynomial-time) application. If the application behaves differently, then it constitutes a (polynomial-time) statistical test between PRG(seed) and a truly random strings. PRG can Overcome Shannonâ€™s ConundrumPRG can overcome Shannonâ€™s conundrum. That is, we can encrypt $n+1$ bits using an $n$-bit key. How to achieve it ?The scheme consists of three algorithms. $Gen(1^n)$: generate a random n-bit key $k$ $Enc(k,m)$ where $m$ is a $m(n)$-bit message: Expand $k$ into a $(n+1)$-bit pseudorandom string $kâ€™=G(k)$ One time pad with $kâ€™$: $c = kâ€™\\oplus m$ $Dec(k, c)$: output $G(k)\\oplus c$ $G$ is a public and deterministic p.p.t. function, so Alice and Bob can both calculate the one-time pad key $kâ€™=G(k)$. Correctness: $Dec(k,c)=G(k)\\oplus c = G(k)\\oplus G(k)\\oplus m = m$ The correctness is easily proved. The scheme is under the assumption that $G$ is a pseudorandom generator. Hence, if there exists a PRG, we can move beyond Shannonâ€™s. Security AnalysisThe most important thing is to prove the scheme is computational secure. Recall the definitions of computational indistinguishability and pseudorandom generator. Def of Computational Indistinguishability: $\\begin{array}{l}\\text{For all {p.p.t} EVE, {there is a negligible function} }{\\mu }\\\\ \\text{s.t. for all } m_0, m_1:\\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\};c=E(k,m_b):\\mathrm{EVE}(c)=b] \\le 1/2 + \\mu(n)\\end{array}$ Def of PRG [Indistinguishability]: A deterministic polynomial-time computable function ${G}:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ is a PRG if: $\\begin{array}{l}\\text{For all {p.p.t} EVE, {there is a negligible function} }{\\mu }\\text{ s.t. for all } m_0, m_1:\\\\ |\\operatorname{Pr}[D({G(U_n)})=1]-\\operatorname{Pr}[D({U_m})=1]|={\\mu(n)}\\quad ,m>n\\end{array}$ The following proof is maybe your first reduction. The scheme is under the assumption that $G$ is a pseudorandom generator. Hence, the thing we want to prove is if $G$ is a pseudorandom generator, then the scheme is computational secure. Proof: We want to prove the contradiction for simplicity that if the scheme doesnâ€™t satisfy the computational indistinguishability, then the assumption doesnâ€™t hold, i.e. $G$ is not a PRG. Suppose for contradiction that there is a p.p.t. $\\operatorname{EVE}$, a polynomial function $p$ and $m_0, m_1$ s.t. $\\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\};c=Enc(k,m_b):\\mathrm{EVE}(c)=b] \\ge 1/2 + p(n)$ Case 1: If $\\operatorname{EVE}$ is a distinguisher for the scheme, instantiate it. $\\rho = \\operatorname{Pr}[k\\leftarrow \\{0,1\\}^n; b\\leftarrow \\{0,1\\};c=G(k)\\oplus m_b:\\mathrm{EVE}(c)=b] \\ge 1/2 + p(n)$ The probability of $\\operatorname{EVE}$ guessing correctly which world she is in is more than half. As a result, $\\operatorname{EVE}$ is able to distinguish case 1. Case 2: If EVE is a distinguisher for one-time pad, instantiate it. $\\rho' = \\operatorname{Pr}[k'\\leftarrow \\{0,1\\}^{n+1}; b\\leftarrow \\{0,1\\};c=k'\\oplus m_b:\\mathrm{EVE}(c)=b] = 1/2 \\;(+\\mu(n))$ The probability of $\\operatorname{EVE}$ guessing correctly which world she is in is substantially half. As a result, $\\operatorname{EVE}$ cannot distinguish case 2. Then we can construct a distinguisher $\\operatorname{EVEâ€™}$ for $G$ using distinguisher $\\operatorname{EVE}$. A distinguisher for PRG is to get $y$ and try to tell whether $y$ is from the pseudorandom world and the truly random world. Construct $\\operatorname{EVE}â€™$:Get as input a string $y$, run $\\operatorname{EVE}(y\\oplus m_b)$ for a random $b$, and let $\\operatorname{EVE}$â€™s output be $bâ€™$. Output â€œPRGâ€ if $b=bâ€™$ and â€œRANDOMâ€ otherwise. Run $\\operatorname{EVE}(y\\oplus m_b)$ for a random $b$ Let $bâ€™:=\\operatorname{EVE}(y\\oplus m_b)$ $\\operatorname{EVE}'(y)=\\begin{cases} \\text{PRG},& b=b'\\\\ \\text{RANDOM},& \\text{otherwise}\\end{cases}$ If $y$ is from the pseudorandom world, $\\operatorname{EVE}$ is trying to distinguish the Case 1 when running $\\operatorname{EVE}(y\\oplus m_b)$ for a random $b$. The probability of $\\operatorname{EVE}(y\\oplus m_b)$ guessing $b$ correctly, i.e. $bâ€™=b$, is more than half. $\\operatorname{Pr}[\\operatorname{EVEâ€™}\\text{ outputs PRG}\\mid y\\text{ is pseudorandom}]=\\rho \\ge 1/2 + 1/p(n)$ If $y$ is from the truly random world, $\\operatorname{EVE}$ is trying to distinguish the Case 2 when running $\\operatorname{EVE}(y\\oplus m_b)$ for a random $b$. The probability of $\\operatorname{EVE}(y\\oplus m_b)$ guessing $b$ correctly, i.e. $bâ€™=b$, is substantially half. $\\operatorname{Pr}[\\operatorname{EVEâ€™}\\text{ outputs RANDOM}\\mid y\\text{ is random}]=\\rhoâ€™ = 1/2$ Hence, the advantage of $\\operatorname{EVE}â€™$ distinguishing for $G$ is $\\operatorname{Pr}[\\operatorname{EVE'}\\text{ outputs PRG}\\mid y\\text{ is pseudorandom}]-\\operatorname{Pr}[\\operatorname{EVE'}\\text{ outputs RANDOM}\\mid y\\text{ is random}]\\ge 1/p(n)$ , which proves $G$ is not a PRG. QED. So far we have proven it that $G$ is not a PRG if the scheme is not (computational) secure which also means the scheme is (computational) secure if there exists a PRG $G$. We make a reduction from whether the scheme is (computational) secure to whether PRGs exist. Itâ€™s a simple but typical reduction. The reductions will be more intricate but the principle remains same. Two Questions to PRGAs a consequence, the proof of security comes back to PRGs. There are two questions to PRGs Q1: Do PRGs exist ? Q2: How do we encrypt longer messages or many messages with a fixed key ? The next subsection will answer the first question Q1 while Q2 will be answered in following blog. Q1: Do PRGs Exist ?Do PRGs exist ? If P=NP, PRGs do not exist. I believe P$\\ne$NP so I believe PRGs do exist. There are two methodologies to construct PRGs, the practical methodology and the foundational mehodology. Rigndael (AES) is designed by the practical methodology, but this course is more about the foundational methodology. The Practical Methodology Start from a design framework.A practical truth is appropriately chosen functions composed appropriately many times look random. For example, if you repeat the knot many and many times, then you will get a random line ball. Come up with a candidate construction.e.g. the construction of Rijndael (AES) . Do extensive cryptanalysis. The Foundational MethodologyThe maxim is reducing to simpler primitives. Construct one-way function (OWF) from well-studied and average-case hard problems. Then create the thriving world of cryptography based on OWF. â€œScience wins either wayâ€ â€”â€”Silvio Micali There is a PRG candidate from the average-case hardness of subset-sum. A PRG Candidate from the average-case hardness of Subset-sum: $G(a_1,\\dots,a_n,x_1,\\dots,x_n)=(a_1,\\dots,a_n,\\sum_{i=1}^n x_i a_i \\mod{2^{n+1}})$ where $a_i$ are random $(n+1)$-bit numbers, and $x_i$ are random bits. Nevertheless, there exists a better way to construct PRG. Beautiful Function: If $G$ is one-way function, then $G$ is a PRG. If lattice problems are hard on the worst-case, $G$ is a PRG.","link":"/2022/07/01/mit6875-lec2/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 4","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics: Stateless Secret-key Encryption with PRF. The Goldreich-Goldwasser-Micali (GGM) PRF construction. RecapIn the previous blog, we introduced the second definition of PRG, Next-bit Unpredictability. Using hybrid argument, we achieved the predicting-to-distinguishing reduction to prove Next-bit Unpredictability = Indistinguishability for PRGs. We elaborated the theorem of PRG Length Extension and used it to construct stateful secret-key encryption scheme. For the purpose of stateless encryption, we brought about a new notion, Pseudorandom Functions (PRFs). AgendaIn the first place, weâ€™ll finish up the the stateless secret-key encryption scheme. The last blog elaborated the theorem of PRG Length Extension, if there is a PRG that stretched by one bit, there is one that stretched by poly. many bits. In this blog, weâ€™ll introduce the theorem of PRF, if there is a PRG, then there is a PRF. In addition, we will elaborate the Goldreich-Goldwasser-Micali (GGM) construction. PRFDefinitionRecall the definition of PRF. Collection of the Pseudorandom Functions: Consider the collection of pseudorandom functions $\\mathcal{F}_l=\\{f_k:\\{0,1\\}^l\\rightarrow\\{0,1\\}^m\\}_{k\\in\\{0,1\\}^n}$, each of which maps $l$ bits to $m$ bits. indexed by a key $k$. $n$ : key length, $l$: input length, $m$: output length. Independent parameters, all poly(sec-param)=poly(n). #functions in $\\mathcal{F}_l\\le 2^n$ (single exponential in $n$) Collection of ALL Functions: Consider the collection of ALL functions $ALL_l=\\{f:\\{\\ 0,1\\}^l\\rightarrow \\{0,1\\}^m\\}$, each of which is maps $l$ bits to $m$ bits. #functions in $ALL_l\\le 2^{m2^l}$ (doubly exponential in $l$) The #functions in the pseudorandom collection is much less than that in the random collection. But the pseudorandom functions should be â€œindistinguishableâ€ from random. There are two worlds, the pseudorandom world and the random world. The pseudorandom world Sample a function $f$ from $\\mathcal{F}_l$.The function $f$ is picked one and stays fixed for all. The oracle is instantiated once the $f$ is picked. The oracle responses $f(x)$ for each query $x$.You can think there is a truth table of $f$ in the oracle.It responses the same $f(x)$ for the same query $x$ since the function $f$ is fixed. The random world sample a function $f$ from $ALL_l$.Likewise, the function $f$ is picked one and stays fixed for all. The oracle is instantiated once the $f$ is picked. The oracle responses $f(x)$ for each query $x$.Likewise, it responses the same $f(x)$ for the same query $x$ since the function $f$ is fixed. The Distinguisher $D$ has the power of querying the oracle many poly. times and try to guess which world she is in, the pseudorandom world or the random world. Definition of PRF: For all p.p.t. $D$, there is a negligible function $\\mu$ s.t. $$|\\operatorname{Pr}[f\\leftarrow \\mathcal{F}_l:D^f(1^n)=1]-\\operatorname{Pr}[f\\leftarrow ALL_l:D^f(1^n)=1]|\\le \\mu(n)$$ Notation: Actually, $D$ dose not have a challenge since she gets nothing as input except the secure parameter, but she has the power of querying the oracle many poly. times. The Relation in PRG and PRFPonder over the relation in PRG and PRF. They both expand a few random bits into many pseudorandom bit. PRG expands $n$ bits into $p(n)$ bits while PRF expands $n$ bits into $2^l$ bits that $l=p(n)$. With a PRG, accessing $2^l$-th bit takes time $2^l$ time. With a PRF, this can be done in time $l$. So, a PRF = locally accessible (or, random-access) PRG. Stateless-key EncryptionWe can design stateless secret-key encryption from PRF. $Gen(1^n)$: Generate a random $n$-bit key $k$ that defines $f_k:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m$.The domain size of $f_k$, $2^l$, had better be super-polynomially large in $n$. $Enc(k,m)$: Pick a random $x$ and let the ciphertext $c$ be the pair $(x,y=f_k(x)\\oplus m)$.Itâ€™s a polynomial time to evaluate $f_k(x)$ since $f_k$ are random accessible. $Dec(k,c=(x,y))$: Output $f_k(x)\\oplus y$. Instead of remembering the whole truth table of $f$, we remember just the key $k$. Notation: We cannot pick up the same $x$ for the different messages since one-time pad cannot be reused. Therefore, the domain size of $f_k$ is supposed be super-polynomially large in $n$ to prevent collision of $x$. Security of Secret-Key Enc. (for one msg)In [the second blog] of this series, we defined the computationally indistinguishability of the secret-key encryption scheme. There are two mathematical forms of Computational Indistinguishability. We can use them to define the security of the secret-key encryption from PRF respectively. There are two worlds, the left world and the right world. They both sampled a key $k$ using $Gen(1^n)$. The left world uses the key to encrypt $m_0$ while the right world uses the key to encrypt $m_1$ using $Enc(k,m)$. The distinguisher $D$ gets the ciphertext $c=Enc(k,m)$ and tries to guess which world she is in. We can get the definitions. Definition 1 $\\begin{array}{l}\\text{For all }m_0,m_1,\\text{and all p.p.t. }D,\\text{there is a negligible function }\\mu \\text{ s.t.} \\\\ \\operatorname{Pr}[k\\leftarrow \\mathcal{K}; b\\leftarrow \\{0,1\\}:D(Enc(k,m_b))=b] \\le 1/2 + \\mu(n)\\end{array}$ Definition 2 $\\begin{array}{l}\\text{For all }m_0,m_1,\\text{and all p.p.t. }D,\\text{there is a negligible function }\\mu \\text{ s.t.} \\\\ \\mid \\operatorname{Pr}[k\\leftarrow \\mathcal{K}:D(Enc(k,m_0))=1] -\\operatorname{Pr}[k\\leftarrow \\mathcal{K}:D(Enc(k,m_1))=1]\\mid \\le \\mu(n)\\end{array}$ The two definitions are substantially equivalent. The first emphasizes the probability of $D$ guessing correctly while the second emphasized the probability of $D$ guessing she is the world 1. However, the definition indicates that the distinguisher only sees the ciphertext of one message, $m_0$ or $m_1$, which contradicts the setting that distinguisher has the power of querying the oracle poly. many times. So this definition only defines the secret-key encryption scheme for one message, which encrypts different messages with different keys, such as one-time pad. Security of Secret-Ket Enc. (for many msgs)We need to modify it for many messages since the oracle is an interactive oracle. There are two oracles. The Left Oracle $Left(\\cdot,\\cdot)$ Sample a key $k$ .The oracle is instantiated once is key is picked. For each query $(m_L, m_R)$, the left oracle uses the key $k$ to encrypt the left message $m_L$ by $c:=Enc(k,m_L)$ and responses with $c$. The Right Oracle $Right(\\cdot,\\cdot)$ Sample a key $k$.The oracle is instantiated once is key is picked. For each query $(m_L,m_R)$, the right oracle uses the key $k$ to encrypt the right message $m_R$ by $c:=Enc(k,m_R)$ and responses with $c$. The distinguisher $D$ has the power of asking the oracle many poly. times.For each query, $D$ gets the ciphertext and tries to guess with which oracle she is interacting (or which message is encrypted). We can get the secure definitions of the secret-key encryption for many messages. Definition : For all p.p.t. $D$, there is a negligible function $\\mu$ s.t.$$\\mid \\operatorname{Pr}[k\\leftarrow \\mathcal{K}:D^{Left(\\cdot,\\cdot)}(1^n)=1] -\\operatorname{Pr}[k\\leftarrow \\mathcal{K}:D^{Left(\\cdot,\\cdot)}(1^n)=1]\\mid\\le \\mu(n)$$ Crucial Supplement: Actually, the definitions of security we gave above are not achievable including for one messages and for many messages. How can we attack them ? Length Attack. We can use messages of different lengths. For the secret-key encryption for one message, if $|m_0|\\ne|m_1|$, the adversary can distinguish easily only from the length of ciphertext. For the secret-key encryption for many messages, if $|m_L\\ne m_R|$, the adversary can distinguish it easily as well. Itâ€™s a crucial point that the scheme is secure only if it hides all possible information, which many people often overlook in practice. Therefore, the supplements for these definitions are secure definition for one message: $|m_0|=|m_1|$. secure definition for many messages: $|m_L|=|m_R|$ Security AnalysisWeâ€™ll prove the definition by hybrid argument. Proof: We know two oracles. Left oracle: $c=(x,y=f_k(x)\\oplus m_L)$ Right oracle: $c=(x,y=f_k(x)\\oplus m_R)$ The thing we want to prove is $D$ gets $c$ and cannot distinguish with which oracle she is interacting. We want to change the oracle a little bit to define a sequence of hybrid distributions.Consider the ciphertext as the distribution. Hybrid 0: $D$ gets access to the Left oracle.$c=(x,y=f_k(x)\\oplus m_L)$ Hybrid 1: Replace $f_k$ by a random function.$c=(x,y=r_x\\oplus m_L)$ Hybrid 2: Replace $f_k$ by a random function.$c=(x,y=r_x)$ Hybrid 3: Replace $f_k$ by a random function (like Hybrid 1).$c=(x,y=r_x\\oplus m_R)$ Hybrid 4: $D$ gets access to the Right oracle (like Hybrid 0).$c=(x,y=f_k(x)\\oplus m_R)$ The thing we want to prove is that Hybrid 0 and Hybrid 4 are indistinguishable. Prove Hybrid 0 = Hybrid 1 (and Hybrid 4 = Hybrid 3). It can be proved by PRF security. The definition of PRF indicates $D$ cannot distinguish from the pseudorandom world and the random world. Prove Hybrid 1 = Hybrid 2 (and Hybrid 3 = Hybrid 2). It can be proven by birthday paradox. Suppose $D$ has asked the oracle for $q$ times and gets $q$ ciphertexts. The oracle in Hybrid 2 only outputs the function value $r_x$ without $\\oplus$ operation.If $D$ gets two same ciphertexts, $D$ knows that she is interacting with the oracle in Hybrid 2. The oracle in Hybrid 1 is one-time pad.If $D$ gets two ciphertexts with the same $x$ but different $y$, $D$ knows that she is interacting with the oracle in Hybrid 1. The probability of $D$ distinguishing from Hybrid 1 and Hybrid 2 is up to the collision probability of $x$. The domain size of $x$ is $2^l$ that $l$ is super-polynomially large in $n$. The probability of picking the same $x$ is at most $q^2/2^l$, which is negligible.There are $q^2$ possible pairs to match any value in the domain size of $x$. The adjacent hybrid distributions are all indistinguishable, so Hybrid 0 and Hybrid 4 are indistinguishable. QED. Theorem of PRFFinally, we can get into the theorem of PRF. (Tired but Happy ^ - ^) PRG Length ExtensionBefore start, letâ€™s look back at PRG Length Extension. Theorem Let $G:\\{0,1\\}^n \\rightarrow \\{0,1\\}^{n+1}$ be a PRG. Then, for every polynomial $m(n)$, there is a PRG $G':\\{0,1\\}^n\\rightarrow\\{0,1\\}^{m(n)}$.Recall the construction of the last blog. Construction: $G(s)=G_0(s)||G_1(s)$ where $G_0(s)$ is 1 bit and $G_1(s)$ is $n$ bits. We parse the output of PRG as 1 bit and $n$ bits. The 1 bit is as the output and the $n$ bits are as the seed of next call. Write the process as a tree. The problem is accessing the $i$-th output bit takes time linear in $\\approx i$. Goldreich-Goldwasser-Micali PRF Theorem: Let $G$ be a PRF.Then, for every polynomials $l=l(n),m=m(n)$, there exists a PRF family $\\mathcal{F}_l=\\{f_s:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m\\}_{s\\{0,1\\}^n}$. Note: We will focus on $m=l$. The output length could be made smaller (by truncation) or larger (by expansion with a PRG). ConstructionConstruction: Let $G(s)=G_0(s)||G_1(s)$ where $G_0(s)$ and $G_1(s)$ are both $n$ bits each. So $G:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{2n}$ is a PRG that stretches $n$ bits into $2n$ bits and parses the output in half. Notation: $G_0(s),G_1(s)$ are not functions in $s$. $G_0(s)$ represents the first $n$ bits of $G(s)$â€™s output and $G_1(s)$ represents the second $n$ bits of $G(s)$â€™s output. Write it as a tree as well. The depth is $l$ so there are $2^l$ leaves or paths. So each path/leaf labeled by $x\\in\\{0,1\\}^l$ corresponds to $f_s(x)$. The pseudorandom function family $\\mathcal{F}_l$ is defined by a collection of functions $f_s$ where: $$f_s(x_1x_2\\dots x_l)=G_{x_l}(G_{x_{l-1}}(\\dots G_{x_1}(s)))$$ If $G:\\{0,1\\}\\rightarrow \\{0,1\\}^2$ $f_s$ define $2^l$ presudorandom bits. The $x$-th bit can be computed using $l$ evaluations of the PRG $G$.(as opposed to $x \\approx 2^l$ evaluations as before) Security Analysis of GGM PRFPRG Repetition LemmaBefore proving the security of GGM PRF, letâ€™s introduce a PRG Repetition Lemma, which is the component of the following proof. Lemma: Let $G$ be a PRG. Then, for every polynomials $L=L(n)$, the following two distributions are computationally indistinguishable: $$(G(s_1),G(s_2),\\dots,G(s_L)\\approx (u_1,u_2,\\dots,u_L)$$ This lemma can be proved easily by hybrid argument since the hybrid distributions are easy to construct. If there is a p.p.t. distinguisher between the two distributions with distinguishing advantage $\\varepsilon$, then there is a p.p.t. distinguisher for $G$ with advantage $\\ge \\varepsilon/L$. By hybrid argument, we can achieve the PRG reduction. Step 1-Hybrid ArgumentProve it by contradiction. Assume that there is a p.p.t. $D$ and a poly. function $p$ s.t.$|\\operatorname{Pr}[f\\leftarrow \\mathcal{F}_l:D^f(1^n)=1]-\\operatorname{Pr}[f\\leftarrow ALL_l:D^f(1^n)=1]|\\ge 1/p(n)$ Under the assumption, we can derive a contradiction to the security of PRG. The key idea is argument by levels of the tree. Very ingenious! For simplicity, we consider the PRG of GGM PRF is $G:\\{0,1\\}\\rightarrow \\{0,1\\}^2$, so the GGM PRF generate $2^l$ pseudorandom bits. Consider the hybrid distributions of the $2^l$ bits generated by a tree. Hybrid 0 (The Pseudorandom World): In the pseudorandom world, we consider the distribution of $2^l$ bits generated by a GGM tree as Hybrid 0. For each query $x$, the oracle responses with $b_x$ by $l$ evaluations of PRG. Hybrid 1: We can get Hybrid 1 by changing a little to the GGM tree in Hybrid 0. The difference is the first level of the tree in Hybrid 1, $s_0$ and $s_1$, are random while they are generated by PRG in Hybrid 0. Hybrid 2: We can get Hybrid 2 by changing Hybrid 1 a little. The second level of tree in Hybrid 2, $s_{00}, s_{01}, s_{10}, s_{11}$, are random while they are generated by PRG in Hybrid 1. â€¦ Hybrid $l$ (The Random World): We can define a sequence of hybrids slowly by changing a level of tree each time. At last, we get the hybrid $l$, the random world, the $l$-th level of which are all random bits. These hybrids can be efficiently computable using lazy evaluation. Lazy evaluation means that there is no need to evaluation all bits $b_1b_2\\dots b_{2^l}$, the oracle only evaluates $b_x$ for each query $x$. Let $p_i=\\operatorname{Pr}[f\\leftarrow H_i:D^f(1^n)=1]$. We know $p_0-p_l\\ge \\varepsilon$. By a hybrid argument, there is some $i$ s.t. $p_i-p_{i+1}\\ge \\varepsilon/l$. Step 2-ReductionNow weâ€™ll prove that if a distinguisher with advantage $\\varepsilon/l$ between hybrids implies a distinguisher with advantage $\\ge \\varepsilon/ql$ for the PRG. (where $q$ is the number of queries that $D$ makes) Assume there is a p.p.t. $D$ distinguishing from Hybrid $i$ and Hybrid $i+1$ with advantage $\\varepsilon/l$. We will construct a Repeated Repetition Breaker $Dâ€™$ from the distinguisher $D$ so as to get a PRG Breaker from the PRG Repetition Lemma. The challenge of $Dâ€™$ : She gets a tuple of $(y_1,y_2,\\dots, y_q)$ and tries to guess whether they are all pseudorandom or all random. $q$ is the upper bound number of queries that $D$ makes and $q=q(n)$. $(y_1,y_2,\\dots, y_q)$ are either all pseudorandom or all random. Suppose each $y_i$ are $2n$ bits. Construction of Repeated PRG Breaker $Dâ€™$ Parse each $y_i$ in half: $y_i=(y_{i,L},y_{i,R})$. Construct a oracle by putting these $y_i$ into the $i$-th level of the tree in order.Itâ€™s a GGM tree only starting from the $i$-th level. If $(y_1,y_2,\\dots, y_q)$ are all pseudorandom, this oracle implies Hybrid $i$.Each pair of $y_i$, $(y_{i,L},y_{i,R})$, actually has an implicit parent in the tree.We donâ€™t know it but it exists. If $(y_1,y_2,\\dots, y_q)$ are all random, this oracle implies Hybrid $i+1$. This oracle can use lazy evaluation as well.It only responses with $f(x)$ for the each coming query $x$. Let the distinguisher $D$ interact with the oracle we build.$D$ can make at most $q$ queries. If $D$ distinguishes it as Hybrid $i$, $Dâ€™$ outputs â€˜Pseudorandomâ€™. If $D$ distinguishes it as Hybrid $i+1$, $Dâ€™$ outputs â€˜Randomâ€™. $Dâ€™$ has advantage with $\\varepsilon/l$ for distinguishing repeated PRG. There is a distinguisher with advantage $\\varepsilon/lq$ for PRG from the PRG Repetition Lemma. Nits of GGM PRFAlthough the construction of GGM PRF is elegant, there are some nits. Expensive: $l$ invocations of a PRG. Sequential: bit by bit, $l$ sequential invocations of a PRG. Loss in security reduction: break PRF with advantage $\\varepsilon$ â†’ break PRG with advantage $\\varepsilon/ql$, where $q$ is an arbitrary polynomial. $q =$ # queries of the PRF distinguisher. So some new questions come into mind. Is there tighter reduction ? How to avoid the loss ?","link":"/2022/07/07/mit6875-lec4/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 3","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics: The Hybrid Argument. An application: PRG length extension. The notion of pseudorandom functions: Definition, motivation, discussion and comparison with PRGs. PRG implies (stateful) secret-key encryption. PRFs imply (stateless) secret-key encryption. RecapIn the previous blog, we defined Computational Indistinguishability that a new definition of security for secret-key encryption, which can overcome Shannonâ€™s impossibility. Then we brought about Pseudorandom Generator which can encrypt a single message longer than the key. We gave the definition of PRG from the Indistinguishability and proved the secret-key encryption scheme by contradiction. At the end, we saw a construction of PRG based on subset sum, and answered the question of whether there is a PRG. AgendaWe already know PRG can encrypt a single message longer than the key. The last blog left the second question to PRG. Q2: How to encrypt poly. many messages with a fixed key ? This is the gist of this blog. Weâ€™ll introduce two methods. PRG length extension.We can achieve stateful encryption of poly. many messages. Theorem: If there is a PRG that stretched by one bit, there is one that stretched by poly. many bits. Another new notion: Pseudorandom Functions (PRF)We can achieve stateless encryption of poly many messages. Theorem(next blog): If there is a PRG, then there is a PRF. More importantly, weâ€™ll introduce a new proof technique, Hybrid Arguments. PRGIn the last blog, we mentioned that there are three equivalent definitions of PRG. Def 1 [Indistinguishability] â€œNo polynomial-time algorithm can distinguish between the output of a PRG on a random seed vs. a truly random string.â€ = â€œas good asâ€ a truly random string for all practical purpose. Def 2 [Next-bit Unpredictability] â€œNo polynomial-time algorithm can predict the (i+1)th bit of the output of a PRG given the first i bits, better than chance 1/2.â€ Def 3 [ Incompressibility] â€œNo polynomial-time algorithm can compress the output of the PRG into a shorter stringâ€ Def 1 [Indistinguishability]We elaborated the first definition of Indistinguishability last blog. Definition 1 [Indistinguishability]: A deterministic polynomial-time computable function ${G}:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ is a indistinguishable (or, secure against any statistical test) if: for every p.p.t. algorithm D (called a distinguisher or a statistical test) if there is a negligible function ${\\mu}$ such that: $$|\\operatorname{Pr}[D({G(U_n)})=1]-\\operatorname{Pr}[D({U_m})=1]|={\\mu(n)}$$ Notation: $U_n$(resp. $U_m$) denotes the random uniform distribution on $n$-bit (resp. $m$-bit) strings; $m$ is shorthand for $m(n)$. So $G$ is a PRG if $D$ cannot distinguish from the output of the $G$ and a truly random string. Def 2 [Next-bit Unpredictability]Today I will introduce the second definition of Next-bit Unpredictability. Definition 2 [Next-bit Unpredictability]: A deterministic polynomial-time computable function ${G}:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ is a next-bit unpredictable if: for every p.p.t. algorithm D (called a next-bit predictor) and every $i\\in\\{0,\\dots,m\\}$ , if there is a negligible function ${\\mu}$ such that: $$\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_1y_2\\dots y_{i-1})=y_i]=\\frac{1}{2}+\\mu(n)$$ Notation: $y_1,y_2,\\dots,y_m$ are the bits of the m-bit string $y$. So $G$ is a PRG if the probability of $P$ predicting the right $i$th bit of $G$ is substantially half given the first $(i-1)$ bits of $G$. Ind. = NBUThere are two theorems that state the equivalence of Def 1( Indistinguishability) and Def 2(NBU). Theorem: A PRG $G$ is distinguishable if and only if it is next-bit unpredictable. Theorem: A PRG $G$ passes all (poly-time) statistical tests if and only if it passes (poly-time) next-bit tests. Next-bit Unpredictability(NBU) is seemingly a much weaker requirement. It only says that the next bit predictors, a particular type of distinguishers which takes a prefix of a string and try yo predict the next bit, cannot succeed. Yet, surprisingly, Next-bit Unpredictability (NBU) = Indistinguishability (Ind.). In addition, NBU is much more easier to use. Ind. â†’ NBUThe first and foremost, the thing we want to prove is that if a PRG $G$ is indistinguishable, then it is next-bit unpredictable, i.e. Ind. â†’ NBU. Proof: The main idea of contradiction is that if there exists a p.p.t. predictor $P$, then we can construct a p.p.t distinguisher $D$ from $P$, i.e. NOT NBU â†’ NOT Ind. So we can Suppose for contradiction $G$ is NOT Next-bit Unpredictable, i.e. there is a p.p.t. predictor $P$, a polynomial function $p$ and an $i\\in\\{1,\\dots, m\\}$ s.t. $$ \\operatorname{Pr}[y\\leftarrow G(U_n):P(y_1y_2\\dots y_{i-1})=y_i]\\ge \\frac{1}{2}+1/p(n) $$ Then, I claim that $P$ essentially gives us a distinguisher $D$. The probability of $P$ predicting the right bit is more than half. Consequently, the PRG $G$ is NOT Next-bit Unpredictable. Construct a distinguisher $D$ from predictor $P$.Consider $D$ which gets an $m$-bit string $y$ and does the following:(If $P$ is p.p.t. so is $D$.) Run $P$ on the $(i-1)$-bit prefix $y_1y_2\\dots y_{i-1}$ If $P$ returns the $i$-th bit $y_iâ€™$, then output 1 (=PRG) else output 0 (=Random). $D(y)=\\begin{cases} \\text{1(=PRG)},& y_i'=y_i\\\\ \\text{0(=Random)},& \\text{otherwise}\\end{cases}$ The task of $D$ is to guess which world she is in, the pseudorandom world or the truly random world. The task of $P$ is to predict the next-bit given the first $(i-1)$ bits of $y$. By the construction of $D$, the probability of $D$ guessing correctly is equal to the probability of $P$ predicting the right bit regardless the distribution of $y$, i.e. $\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1]=\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_1y_2\\dots y_{i-1})=y_i]$ $\\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]=\\operatorname{Pr}[y\\leftarrow U_m:P(y_1y_2\\dots y_{i-1})=y_i]$ Therefore, we want to show that $D$ is able to distinguish the PRG $G$ which means $G$ is NOT Indistinguishable, i.e. there is a polynomial $pâ€™$ s.t. $|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/pâ€™(n)$ If $y$ is from the pseudorandom world, the probability of $D$ guessing correctly is equal to the probability of $P$ predicting the right bit by the construction of $D$. $|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1]|$ = $\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_1y_2\\dots y_{i-1})=y_i]\\ge \\frac{1}{2}+1/p(n)$ (by assumption) If $y$ is from the truly world, the probability of $P$ predicting the right bit is 1/2 since $y$ is random. $|\\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|$ =$\\operatorname{Pr}[y\\leftarrow U_m:P(y_1y_2\\dots y_{i-1})=y_i]=\\frac{1}{2}$ ($y$ is random) Hence, the advantage of the distinguisher $D$ guessing correctly is$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n)$ QED. NBU â†’ Ind.Furthermore, we want to prove that if a PRG $G$ is next-bit unpredictable, then it is indistinguishable, i.e. NBU â†’ Ind.. Equally, suppose for contradiction that there is a distinguisher $D$, and a polynomial function $p$ s.t. $$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n)$$ But how to construct a next-bit predictor $P$ out of the distinguisher $D$ ? It is imperative that we need a new proof technique, HYBRID ARGUMENT. Using the technique, we can prove it by the following steps: Hybrid Argument From Distinguishing to Predicting Proof of NBU â†’ Ind.Step 1: Hybrid Argument Hybrid Argument is a proof technique used to show that two distributions are computationally indistinguishable. â€”â€”wiki The contradiction is that there is a distinguisher $D$, and a polynomial function $p$ s.t.$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n)$ We define the advantage of $D$ guessing correctly is $\\varepsilon:=1/p(n)$. A PuzzleBefore that, letâ€™s discuss a simple puzzle. Lemma: Let $p_0,p_1,p_2,\\dots,p_m$ be real numbers s.t. $\\color{blue}{p_m-p_0\\ge \\varepsilon}$ Then, there is an index $i$ such that $\\color{blue}{p_i-p_{i-1}\\ge \\varepsilon/m}$. Proof: Write it $p_{m}-p_{0}=\\left(p_{m}-p_{m-1}\\right)+\\left(p_{m-1}-p_{m-2}\\right)+\\cdots+\\left(p_{1}-p_{0}\\right) \\ge \\varepsilon$ At least one of the $m$ terms has to be at least $\\varepsilon/m$ (averaging). Hybrid DistributionsWe define a sequence of hybrid distributions $H_0,H_1,\\dots,H_m$, which $H_0:=U_m$ and $H_m:=G(U_n)$. $H_0$ is the distribution that all bits are random while $H_m$ is the distribution that all bits are pseudorandom. Others are the distributions that some bits are random and others are pseudorandom. In addition, there is only one different bit between the adjacent hybrid distributions as shown the figure above. According to the assumption, $D$ distinguishes the $y$ between the pseudorandom world and the truly random world with advantages $\\varepsilon$, which means $D$ distinguishes the distribution between $H_m$ and $H_0$ with advantage $\\varepsilon$, i.e. $$\\operatorname{Pr}[D(H_m)=1] - \\operatorname{Pr}[D(H_0)=1]\\ge \\varepsilon$$ With reference to the puzzle, there exists $i$ such that $D$ distinguishes the distribution between $H_{i}$ and $H_{i-1}$ with advantage $\\ge \\varepsilon/m$, i.e. $\\exists i$ s.t. $$\\operatorname{Pr}[D(H_i)=1] - \\operatorname{Pr}[D(H_{i-1})=1]\\ge \\varepsilon/m$$ Random bit v.s. Pseudorandom bitBy the hybrid argument, what information can we get from $D$ distinguishing between $H_{i}$ and $H_{i-1}$with advantage $\\ge \\varepsilon/m$ ? Letâ€™s summarize it: Define $p_i = \\operatorname{Pr}[D(H_i)=1]$.$p_0 = \\operatorname{Pr}[D(U_m)=1]$ and $p_m = \\operatorname{Pr}[D(G(U_n))=1]$. By the hybrid argument, we have $p_i-p_{i-1}\\ge \\varepsilon /m$. The key intuition is $D$ output â€˜1â€™ more often given a pseudorandom $i$-th bit than a random $i$-th bit. Therefore, $D$ gives us a â€œsignalâ€ as to whether a given bit is the correct $i$-th bit or not. Right bit v.s. Wrong bitLetâ€™s dig a bit more. $H_{i-1}$ is the hybrid distribution the $i$-th bit of which is a random bit $u$. $H_i$ is the hybrid distribution the $i$-th bit of which is the $i$-th pseudorandom bit $y_i$ generated by $G$. We define a new hybrid distribution $\\overline{H_i}$, the $i$-th bit $\\overline{y_i}$ of which is the opposite bit $y_i$ in $H_i$. With reference to $p_i=\\operatorname{Pr}[D(H_i)=1]$, we define $\\overline{p_i}=\\operatorname{Pr}[D(\\overline{H_i})=1]$. We know: $p_i-p_{i-1}\\ge \\varepsilon/m$. Claim: $p_{i-1}=(p_i+\\overline{p_i})/2$.Proof: The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. In hybrid distribution $H_{i-1}$, $u$ is a random bit. So, $\\operatorname{Pr}[u=0]=0.5$ and $\\operatorname{Pr}[u=1]=0.5$. $\\operatorname{Pr}[D(H_{i-1})=1]$ = $\\operatorname{Pr}[D(H_{i-1})=1\\wedge u=0]+[D(H_{i-1})=1\\wedge u=1]$ = $\\operatorname{Pr}[D(H_{i-1})=1\\mid u=0]\\operatorname{Pr}[u=0]+[D(H_{i-1})=1\\mid u=1]\\operatorname{Pr}[u=1]$ =$(\\operatorname{Pr}[D(H_{i-1})=1\\mid u=0]+[D(H_{i-1})=1\\mid u=1])/2$ If $u=0$ in $H_{i-1}$, we can use $H_i$ with $y_i=0$ and $\\overline{H_{i}}$ with $\\overline{y_i}=0$ to express $H_i$. $\\operatorname{Pr}[D(H_{i-1})=1\\mid u=0]$ = $\\operatorname{Pr}[D(H_{i-1})=1\\wedge y_i=0]+\\operatorname{Pr}[D(\\overline{H_i})=1\\wedge \\overline{y_i}=0]$ If $u=1$ in $H_{i-1}$, we can use $H_i$ with $y_i=1$ and $\\overline{H_{i}}$ with $\\overline{y_i}=1$ to express $H_i$. $\\operatorname{Pr}[D(H_{i-1})=1\\mid u=1]$ = $\\operatorname{Pr}[D(H_{i-1})=1\\wedge y_i=1]+\\operatorname{Pr}[D(\\overline{H_i})=1\\wedge \\overline{y_i}=1]$ Sum it up. $\\operatorname{Pr}[D(H_{i-1})=1]$ = $(\\operatorname{Pr}[D(H_i)=1\\wedge y_i=0] +\\operatorname{Pr}[D(H_i)=1\\wedge y_i=1] \\ + \\operatorname{Pr}[D(\\overline{H_i})=1\\wedge \\overline{y_i}=0] +\\operatorname{Pr}[D(\\overline{H_i})=1\\wedge \\overline{y_i}=1])/2$ = $(\\operatorname{Pr}[D(H_i)=1]+\\operatorname{Pr}[D(\\overline{H_i})=1])/2$ QED. We illustrate the claim as the following figure.$p_{i-1}$ is the midpoint of $p_i$ and $\\overline{p_i}$. Corollary: $p_i-\\overline{p_i}\\ge 2\\varepsilon/m$ (w.r.t. the claim) What can we learn from $p_i-\\overline{p_i}\\ge 2\\varepsilon/m$ ? The takeaway is that $D$ says â€˜1â€™more often when fed with the â€˜right bitâ€™ than the â€˜wrong bitâ€™. Step 2: From Distinguishing to Predicting By the hybrid argument, we get the takeaway: $p_i-\\overline{p_i}\\ge 2\\varepsilon/m$. ( $p_i=\\operatorname{Pr}[D(H_i)=1]$ ) The distinguisher $D$ outputs â€˜1â€™ more often when fed with the â€˜right bitâ€™ than the â€˜wrong bitâ€™. The Predictor $P$The predictor is given the first $i-1$ pseudorandom bits (call it $y_1y_2\\dots y_{i-1}$) and needs to guess the $i$-th bit. The Predictor $P$ works as follows: $$ P(y_1y_2\\dots y_{i-1})=\\begin{cases} b,& \\text{if }D(y_1y_2\\dots y_{i-1}|b| u_{i+1}\\dots u_m)=1\\\\ \\overline{b},& \\text{otherwise}\\end{cases},b\\leftarrow\\{0,1\\} $$ Pick a random bit $b$ Feed $D$ with input $y_1y_2\\dots y_{i-1}|b| u_{i+1}\\dots u_m$ ($u$â€™s are random). If D says â€˜1â€™, output $b$ as the prediction for $y_i$ and if $D$ says â€˜0â€™, output $\\overline{b}$ as the prediction for $y_i$. Analysis of Predictor $P$ The probability of $P$ predicting the right bit$\\operatorname{Pr}[x\\leftarrow {0,1}^n;y=G(x):P(y_1y_2\\dots\\ y_{i-1})=y_i]$ consists of two cases where $D$ outputs 1 and $D$ outputs 0. = $\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}b\\dots)=1 \\wedge b=y_i]+\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}b\\dots)=0\\wedge b\\ne y_i]$ ($y_i=b \\text{ or } y_i=\\overline{b}$) = $\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}b\\dots)=1 \\mid b=y_i]\\operatorname{Pr}[b=y_i] \\ +\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}b\\dots)=0\\mid b\\ne y_i]\\operatorname{Pr}[b\\ne y_i]$ = $\\frac{1}{2}(\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}y_i\\dots)=1]+\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}\\overline{y_i}\\dots)=0)$ (since b is random) = $\\frac{1}{2}(\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}y_i\\dots)=1]+1-\\operatorname{Pr}[D(y_1y_2\\dots y_{i-1}\\overline{y_i}\\dots)=1)$ = $\\frac{1}{2}(1+\\varepsilon/m)\\ge \\frac{1}{2} + 1/p(n)$ (since $m$ is also a polynomial in $n$) QED. SUMMARIZE: We want to prove that if the $G$ is next-bit predictable, then $G$ is indistinguishable. Hybrid Argument Suppose the contradiction that there is a $D$ distinguishing from the pseudorandom world and the truly random world with advantage $\\varepsilon$.$\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]\\ge \\varepsilon$ By defining a sequence of hybrid distributions, we know that $D$ outputs â€˜1â€™ more often when fed with a pseudorandom bit than fed a random bit.$p_i-p_{i-1}\\ge \\varepsilon/m \\qquad(p_i=\\operatorname{Pr}[D(H_i)=1])$ Dig it more. We get the takeaway that $D$ says â€˜1â€™ more often when fed a right bit than a wrong bit.$p_i-\\overline{p_i}\\ge 2\\varepsilon/m \\qquad(\\overline{p_i}=\\operatorname{Pr}[D(\\overline{H_i})=1])$ From distinguishing to predicting Construct a predictor $P$ from the distinguisher $D$ $P(y_1y_2\\dots y_{i-1})=\\begin{cases} b,& \\text{if }D(y_1y_2\\dots y_{i-1}|b| u_{i+1}\\dots u_m)=1\\\\ \\overline{b},& \\text{otherwise}\\end{cases},b\\leftarrow\\{0,1\\}$ Analysis the probability of $P$ predicting the right bit. By the hybrid argument, we can deduce $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):P(y_1y_2\\dots\\ y_{i-1})=y_i]\\ge1/2+1/p(n)$. So the probability of $P$ predicting the right bit is more than half. Proof of PBU = Ind.We have proven that Next-bit Unpredictability = Indistinguishability. Actually, Previous-bit Unpredictability(PBU) is equal to Indistinguishability, i.e. PUB = Ind.. I write it down just to exercise. The proof is similar with the proof of NBU=IND, so you can skip reading it. Proof: Prove PUB â†’ Ind. Suppose for the contradiction that there is a p.p.t. predictor $P$, a polynomial function $p$ and an $i\\in\\{1,2,\\dots,m\\}$ s.t.$\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_{i+1}y_{i+2}\\dots y_m)=y_i]\\ge 1/2+p(n)$ Construct a distinguisher $D$ from $P$ $D(y)=\\begin{cases} \\text{1(=PRG)}&, P(y_{i+1}y_{i+2}\\dots y_m)=y_i\\\\ \\text{0(=Random)}&, \\text{otherwise}\\end{cases}$ Analysis the probability of $D$ distinguishing $y$. If $y$ is from pseudorandom world$\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] \\ = \\operatorname{Pr}[y\\leftarrow G(U_n):P(y_{i+1}y_{i+2}\\dots y_m)=y_i]\\ge 1/2+p(n)$ If $y$ is from the truly random world$|\\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]| \\ =\\operatorname{Pr}[y\\leftarrow U_m:P(y_{i+1}y_{i+2}\\dots y_m)=y_i]= 1/2$ The advantage of $D$ distinguishing $y$ is non-negligible.$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n)$ QED. Prove Ind. â†’ PUB Suppose for the contradiction that there is a p.p.t distinguisher $D$, a polynomial function $p$ s.t.$|\\operatorname{Pr}[y\\leftarrow G(U_n):D(y)=1] - \\operatorname{Pr}[y\\leftarrow U_m:D(y)=1]|\\ge 1/p(n):=\\varepsilon$ Hybrid Argument Define a sequence of hybrid distributions $H_0:=U_m,H_1,\\dots,H_m=G(U_n)$Define $p_i=\\operatorname{Pr}[D(H_i)=1]$ We know $p_m-p_0\\ge \\varepsilon$ from the contradiction. Define $\\overline{p_i}=\\operatorname{Pr}[D(\\overline{H_i})=1]$ where the $i$-th bit in $\\overline{H_i}$ is the opposite bit of the $i$-th bit in $H_i$. We claim $p_{i-1}=(p_i+\\overline{p_i})/2$ . (The proof is same with the above) Get the corollary $p_i-\\overline{p_i}\\ge 2\\varepsilon/m$.Takeaway: $D$ output â€˜1â€™ more often when fed with a right bit than a wrong bit. From distinguishing to predicting Construct a predictor $P$ from the $D$ $P(y_{i+1}y_{i+2}\\dots y_m)\\begin{cases} b,& \\text{if }D(u_1u_2\\dots u_{i-1}|b| y_{i+1}y_{i+2}\\dots y_m)=1\\\\ \\overline{b},& \\text{otherwise}\\end{cases}\\\\b\\leftarrow\\{0,1\\},u\\text{ is random.}$ Analyze the probability of $P$ predicting. $\\operatorname{Pr}[y\\leftarrow G(U_n):P(y_{i+1}y_{i+2}\\dots y_m)=y_i]$ = $\\operatorname{Pr}[D(\\dots b y_{i+1}y_{i+2}\\dots y_m)=1 \\wedge b=y_i]+\\operatorname{Pr}[D(\\dots b y_{i+1}y_{i+2}\\dots y_m)=0\\wedge b\\ne y_i]$($y_i=b \\text{ or } y_i=\\overline{b}$) =$\\frac{1}{2}(\\operatorname{Pr}[D(\\dots y_i y_{i+1}y_{i+2}\\dots y_m)=1]+\\operatorname{Pr}[D(\\dots \\overline{y_i} y_{i+1}y_{i+2}\\dots y_m)=0)$ (since b is random) =$\\frac{1}{2}(\\operatorname{Pr}[D(\\dots y_i y_{i+1}y_{i+2}\\dots y_m)=1]+1-\\operatorname{Pr}[D(\\dots \\overline{y_i} y_{i+1}y_{i+2}\\dots y_m)=1)$ =$\\frac{1}{2}(1+\\varepsilon/m)\\ge \\frac{1}{2} + 1/p(n)$ QED. PRG Length ExtensionLet $G:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{n+1}$ be a pseudorandom generator. The goal is to used $G$ to generate poly. many pseudorandom bits. The construction $Gâ€™$ consists of poly. many calls of $G$. The input of $Gâ€™$ is $s_0$ ($n$-bit). The output of $G$ is parsed by $b_i||s_i$, which $b_i$ is 1-bit and $s_i$ is $n$-bit. The output of $Gâ€™$ are poly. many pseudorandom bit. Itâ€™s also called a stream cipher by the practitioners. Security AnalysisTheorem: If there is a PRG that stretched by one bit, there is one that stretched by poly. many bits. The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. $G:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{n+1}$ is a PRG that stretched by one bit. Define $Gâ€™(s_0)=b_1b_2\\dots b_L$ with the above construction, which is a PRG that stretched by poly. many bits.$s_0$ is $n$-bit random string and $L$ is a polynomial in $n$. The thing we want to prove is that if $G:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{n+1}$ is a secure PRG, then $G':\\{0,1\\}^n\\rightarrow \\{0,1\\}^{n+L}$ is a secure PRG. There are many definitions of PRG, but itâ€™s easy to work with Previous-bit Unpredictability here. Proof: Suppose for the contradiction that there is a p.p.t. predictor $P$, who can predict the previous-bit, and a polynomial function $p$ and an $i\\in\\{1,2,\\dots,L\\}$ s.t.$\\operatorname{Pr}[b\\leftarrow Gâ€™(U_n):P(b_{i+1}b_{i+2}\\dots b_L)=b_i] \\ge 1/p(n):=\\varepsilon$ Then the predictor $P$ essentially gives us an adversary $A$ against $G$. The task of $A$ is to predict $b_i$ given $s_i$ that $b_i$ is the first bit of $G(s_{i-1})=b_i||s_i$. The construction of $A$ Run $Gâ€™$ on the seed $s_i$.We can get $b_{i+1},b_{i+2},\\dots,b_L$ from the output of $Gâ€™(s_i)$. Feed $P$ with $b_{i+1},b_{i+2},\\dots,b_L$. $A$ returns the output of $P$. Analysis of the adversary $A$ predicting the previous bit. $\\operatorname{Pr}[b_i||s_i\\leftarrow G(U_n):A(s_i)=b_i]$ = $\\operatorname{Pr}[(b_{i+1},b_{i+2},\\dots,b_L)\\leftarrow Gâ€™(s_i):P(b_{i+1},b_{i+2},\\dots,b_L)=b_i]$ = $\\operatorname{Pr}[b\\leftarrow Gâ€™(U_n):P(b_{i+1},b_{i+2},\\dots,b_L)=b_i]\\ge 1/p(n)$ QED. Stateful Secret-key EncryptionFrom the PRG length extension, we can encrypt poly. many messages with a fixed key. The procedure is as follows. Alice and Bob have an agreed key $s_0$ as the initial state of $Gâ€™$ Alice wants to encrypt a 1-bit $m$.Then Alice and Bob uses $Gâ€™(s_0)$ to generate 1 bit $b_1$ as the one-time pad key, and their states convert to $s_1$. Alice wants to encrypt a 3-bit $mâ€™$.Then Alice and Bob uses $Gâ€™(s_1)$ to generate 3 bits $b_2b_3b_4$ as the one-time pad key, and their states convert to $s_4$. Now Bob wants to encrypt a 1-bit $mâ€™â€™$.Then Alice and Bob uses $Gâ€™(s_4)$ to generate 1 bit $b_5$ as the one-time pad key, and their states convert to $s_5$. Itâ€™s achievable that Alice and Bob can keep encrypting as many bits as they wish. However, Alice and Bob have to keep their states in perfect synchrony. They cannot transmit simultaneously. Otherwise, correctness goes down the drain, so does security. PRFItâ€™s stateful encryption using PRG length extension. How to be stateless ? There is an idea that Alice and Bob can generate (poly.) many and many bits, such as $n^{100}$ bits. If Alice want to encrypt 1-bit $m$, she can use a random key $b_x$ indexed by the random number $x$ she picks up and send $(x,m\\oplus b_x$) to Bob. Yet, it dose not work. Because there is a collision of Alice using the same one-time key with non-negligible probability.$\\operatorname{Pr}[\\text{Aliceâ€™s first two indices collide}]\\ge 1/n^{100}$ To prevent the collision, there is another idea that Alice and Bob can generate $2^n$ bits and the probability of collision is negligible.$\\operatorname{Pr}[\\exists \\text{ collision in }t=p(n)\\text{ indices}]\\le t^2/2^n=neg(n)$ But it brings about another problem that Alice and Bob are not poly-time. Although it dose not work, there is some inspiration. The goal is never compute the exponentially long string explicitly since itâ€™s not poly-time. Instead, we want a function $f_k(x)=b_x$, that $b_x$ is the $x$-th bit in the implicitly defined (pseudorandom) string. It is computable in polynomial time $p(|x|)=p(n)$ that $|x|$ is the length of $x$. And $f_k(x_1),f_k(x_2)\\dots$ are computationally indistinguishable from random bits for random $x_1,x_2,\\dots$ Hence, there is no need to store the state after each encryption since you can get the encryption key directly by computing the function. Consequently, it is the stateless encryption of poly. many messages. The functions are called Pseudorandom Functions. DefinitionConsider these two collections of functions. Collection of the Pseudorandom Functions: Consider the collection of pseudorandom functions $\\mathcal{F}_l=\\{f_k:\\{0,1\\}^l\\rightarrow\\{0,1\\}^m\\}_{k\\in\\{0,1\\}^n}$, each of which maps $l$ bits to $m$ bits. indexed by a key $k$. $n$ : key length, $l$: input length, $m$: output length. Independent parameters, all poly(sec-param)=poly(n). #functions in $\\mathcal{F}_l\\le 2^n$ (single exponential in $n$) Every (pseudorandom) function is indexed by the key $k$, so if the $k$ is fixed, the function $f_k$ is fixed. So the number of functions in the pseudorandom world is up to the number of the keys, that is $2^n$. Collection of ALL Functions: Consider the collection of ALL functions $ALL_l=\\{f:\\{\\ 0,1\\}^l\\rightarrow \\{0,1\\}^m\\}$, each of which is maps $l$ bits to $m$ bits. #functions in $ALL_l\\le 2^{m2^l}$ (doubly exponential in $l$) For a fixed $m$-bit string in the output space, there are at most $2^l$ possible inputs with different functions. So the number of the functions in the random world is at most $(2^m)^{2^l}=2^{m2^l}$, which is doubly exponential. The #functions in the pseudorandom world is much less than that in the random world. But the pseudorandom functions should be â€œindistinguishableâ€ from random. There are two worlds, the pseudorandom world and the random world. The pseudorandom world Sample a function $f$ from $\\mathcal{F}_l$.The function $f$ is picked one and stays fixed for all. The oracle is instantiated once the $f$ is picked. The oracle responses $f(x)$ for each query $x$.You can think there is a truth table of $f$ in the oracle.It responses the same $f(x)$ for the same query $x$ since the function $f$ is fixed. The random world sample a function $f$ from $ALL_l$.Likewise, the function $f$ is picked one and stays fixed for all. The oracle is instantiated once the $f$ is picked. The oracle responses $f(x)$ for each query $x$.Likewise, it responses the same $f(x)$ for the same query $x$ since the function $f$ is fixed. The Distinguisher $D$ has the power of querying the oracle many poly. times and try to guess which world she is in, the pseudorandom world or the random world. Definition: For all p.p.t. $D$, there is a negligible function $\\mu$ s.t. $$|\\operatorname{Pr}[f\\leftarrow \\mathcal{F}_l:D^f(1^n)=1]-\\operatorname{Pr}[f\\leftarrow ALL_l:D^f(1^n)=1]|\\le \\mu(n)$$ Notation: You can consider the subscript $f$ as an interactive C program, which fed with an input and outputs the result. The distinguisher $D$ actually gets nothing as input except the secure parameter. Stateless Secret-key EncryptionWe can define the stateless secret-key encryption scheme from PRF. $Gen(1^n)$: Generate a random $n$-bit key $k$ that defines $f_k:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m$ The domain size of $f_k$, $2^l$, had better be super-polynomially large in $n$. $Enc(k,m)$: Pick a random $x$ and let the ciphertext $c$ be the pair $(x,y=f_k(x)\\oplus m)$. $Dec(k,c=(x,y))$: Output $f_k(x)\\oplus y$ Correctness $Dec(k,c)=f_k(x)\\oplus y=f_k(x)\\oplus f_k(x)\\oplus m=m$. Alice and Bob agree with the key $k$, which defines the pseudorandom function $f_k$. When decrypting, Bob computes the one-time key directly through $f_k(x)$. They donâ€™t need to store $f_k$( or, the giant truth table), the only thing they need to store is the key $k$. In the next blog, weâ€™ll introduce the theorem that if there is a PRG, then there is a PRF.","link":"/2022/07/05/mit6875-lec3/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 5","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics covered: Applications of PRFs Note: The second half of Lecture 5, Number Theory, is contained in Lecture 6. RecapIn last blog, we introduced the security definition of the stateless secret-key encryption. Then we elaborated on the theorem of PRF, if there are PRGs, then there are PRFs, and gave the Goldreich-Goldwasser-Micali (GGM) construction. Applications of PRFsIn this blog, we will show some applications of PRF. Identification Protocols Authentication Application to Learning Theory. Identification ProtocolsPRF is widely used in identification protocols. Think about the situation that you take your ID card and put it on a RFID to pay the money. The ID card has a secret embedded in it that is specific to your cost right. So itâ€™s sort of a key that the card contains. The device has access to a database which stores all the students keys. The student need the ID card to authenticate to the device. However, there is an eavesdropper in the channel, who is listening to the communications and wants to impersonate Tim. PRFs can prevent the adversary from impersonating. Unpredictability of PRFBefore that, letâ€™s introduce a simple lemma about unpredictability of PRF. Let $f_s:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m$ be a pseudorandom function. Consider an adversary who requests and obtains $f_s(x_1),\\dots,f_s(x_q)$ for a polynomial $q=q(n)$. Can she predict $f_s(x^*)$ for some $x^*$ of her choosing where $x^*\\notin \\{x_1,\\dots, x_q\\}$? Lemma: If she succeeds with probability $\\frac{1}{2^m}+1/p(n)$, then she broke PRF security.This $(\\frac{1}{2^m})$ is negligible in $n$ is $m$ is large enough. Itâ€™s easy to comprehend that the adversary cannot even distinguish it and certainly she cannot predict it. So indistinguishability implies unpredictability and it turns out the lemma is right. In [the lecture 3], we proved that the Unpredictability = Indistinguishability for bits (or, for PRG). However, for PRF, Indistinguishability â†’ Unpredictability, but not vice versa. Challenge-Response ProtocolThe secret in the ID card is the PRF key $s$. The device has access to the database which stores all the studentsâ€™ mapping relation, (ID number $ID$, PRF Key $s$). Whenever Tim wants to authenticate to the device, the protocol is as follows: The device sends a random $r$ to the ID card. The ID card use the embedded key $s$ to evaluate $f_s(r)$, and sends the pair $(ID, f_s(r))$ to the device. According to the lemma of unpredictability, itâ€™s easy to prove the security of the protocol. The adversary collects $(r_i, f_s(r_i))$ for poly. many $r_i$(potentially of her choosing). She eventually has to produce $f_s(r^*)$ for a fresh random $r^*$ when she is trying to impersonate. This is hard as long as the input and output lengths of the PRF are long enough. AuthenticationAnother scenario is to use PRF as the Message Authentication Code(MAC). Consider the initial secure communication. Alice and Bob have an agreed key $k$, and they use the one-time pad to encrypt the message. The adversary can learning nothing from the ciphertext according to Perfect Secrecy. But one-time pad (and encryption schemes in general) are malleable. The adversary can toggle between $m$ and $mâ€™$ easily. She can change the valid ciphertext $m\\oplus k$ to a totally different ciphertext $mâ€™\\oplus k$. Likewise, the adversary can change the valid ciphertext $(r, f_k(r)\\oplus m)$ to a totally different ciphertext $(r,f_k(r)\\oplus mâ€™)$ in the stateless secret-key encryption. MACIt is of importance to use Message Authentication Codes. Alice and Bob have an agreed key $k$ to specify a pseudorandom function $f_k$. MAC is evaluated by the pseudorandom function $f_k$, the message $m$ taken as input. People can see a bunch of messages and tags (MAC) and cannot come up with the new message and the corresponding tag. MACs give us integrity, but not privacy. There is a solution to guarantee the privacy, Encrypt, then MAC. Note that there are two difference PRF $f_k$ and $f_{kâ€™}$ since the input length is different. Learning TheoryThere is a negative results in learning theory. Theorem [Kearns and Valiant 1994]: Assuming PRFs exist, there are hypothesis classes that cannot be learned by polynomial-time algorithms. Itâ€™s showed that the function cannot be learned if PRFs exist. But it gives a counterpoint to machine learning that says everything can be learned. You just throw the samples to a deep neural network.","link":"/2022/07/11/mit6875-lec5/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 8","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Public-key Encryption and Key exchange. Definitions: IND-Security (IND-CPA) Trapdoor permutations. Key Agreement ProblemThere is a remaining problem in secret-key encryption (or symmetric encryption), the Key Agreement Problem. How did Alice and Bob get the same $sk$ to begin with ? Can Alice and Bob, who never previously met, exchange messages securely ? Merkleâ€™s [1974]The Merkleâ€™s idea is based on one-way function. Assume that $H:[n^2]\\rightarrow [n^2]$ is an injective OWF. (or one-to-one). Injective Function: In mathematics, an injective function (also known as injection, or one-to-one function) is a function $f$ that maps distinct elements to distinct elements. That is, $f(x_1) = f(x_2)$ implies $x_1 = x_2$. Equivalently, $x_1\\ne x_2$ implies $f(x_1) \\ne f(x_2)$ in the equivalent contrapositive statement. Alice picks $n$ random numbers $x_1,\\dots,x_n$ Bob picks $n$ random numbers $y_1,\\dots, y_n$ There is a common number since birthday paradox.That says $x_i=y_j$ with high probability. Alice and Bob can detect it in time $\\mathcal{O}(n)$, and they set it as their shared key. How long dose it take Eve, the adversary to compute the shared key ? She knows $i$ and $j$ since she can see the ciphertexts in the channel, but she needs to inver the OWF. Assuming the OWF is very strong, that is $\\Omega(n^2)$. But the Merkleâ€™s only protects against quadratic-time Eves although itâ€™s still an excellent idea. Fascinating HistoryThe Public-key Encryption has a fascinating history. Merkle (1974):â€œSecure Communications Over Insecure Channels.â€ Only protects against quadratic-time adversary. Diffie &amp; Hellman (1976):â€œNew Direction in Cryptographyâ€ Turing Award 2015. Marked the birth of public-key cryptography. Invented the Diffie-Hellman key exchange.(conjectured to be secure against all poly-time attackers unlike Merkle) Used to this day (e.g., TLS 1.3) albeit with different groups than what DH had in mind. Rivest, Shamir &amp; Adleman (1987):â€œA Method for Obtaining Digital Signatures and Public-Key Cryptosystemsâ€ Turing Award 2002. Invented the RSA trapdoor permutation, public-key encryption and digital signatures. RSA Signatures used to this day (e.g., TLS 1.3) in essentially the original form it was invented. Goldwasser &amp; Micali (1982):â€œProbabilistic Encryptionâ€ Turning Award 2012. Defined what is now the gold-standard of security of public-key encryption(two equivalent definitions: indistinguishability and semantic security) GM-encryption: based on the difficulty of the quadratic residuosity problem, the first homomorphic encryption. Public-Key EncryptionItâ€™s also called Asymmetric Encryption. The goal is : Anyone can encrypt to Bob Bob, and only Bob can decrypt. Public-key Encryption SchemeThe public encryption scheme works as follows: Bob generate a pair of keys, a public key $pk$, and a private (or secret) key $sk$. Bob â€œpublishesâ€ $pk$ and keeps $sk$ to himself. Alice encrypts $m$ to Bob using $pk$. Bob decrypts using $sk$. Hence, there are a triple of PPT algorithms $(Gen, Enc, Dec)$ s.t. $Gen(1^n)\\rightarrow (pk,sk)$: PPT Key generation algorithm generates a public-private key pair. $Enc(pk,m)\\rightarrow c$: Encryption algorithm uses the public key to encrypt message $m$. $Dec(sk,c)\\rightarrow m$: Decryption algorithm uses the private key to decrypt ciphertext $c$. Correctness:For all $pk,sk,m$: $Dec(sk,Enc(pk,m))=m$. Define the AdversaryBut how to define security ? What dose Eve know ? Eve knows Bobâ€™s public key $pk$ Eve sees polynomially many ciphertexts $c_1,c_2,\\dots$ of messages $m_1,m_2,\\dots$ So the challenge is that Eve should not get any partial information about the set of messages. IND-Security (IND-CPA)We define the Indistinguishability Security, or Indistinguishability-CPA. CPA is the abbreviation of Chosen Plaintext Attack.That is, the adversary has the power of obtaining the encryption of arbitrary message of his choice. We give the definition by a game, which is actually the same as the Turning Test. Many Message SecurityDefine the Game: The Challenger generates a pair of key $(pk, sk)$ and publishes the $pk$. Eve sends two vectors of message, $\\vec{m_0}$ and $\\vec{m_1}$s.t. $|m_0^i|=|m_1^i|$ for all $i$.(A important restrict is $|m_0^i|=|m_1^i|$ for all $i$, or there is a length attack.) The challenger samples $b$ from ${0,1}$ and encrypts all the messages in vector $\\vec{m_b}$ using $pk$.And send the sequence of the ciphertext to Eve. Eve guesses which vector of message is encrypted and output $bâ€™$. Eve wins if $bâ€™=b$. IND-security Definition (Many-message) (unachievable): The encryption scheme is IND-secure if no PPT EVE can win in this game with probability better than $1/2 + \\text{negl}(n)$. Or written in more traditional (and cumbersome) notation as below.For all PPT pair of algorithms $(M,A)$, there is a negligible function $\\mu$ s.t.:[cumbersome:ç¬¨é‡çš„ï¼›ä¸æ–¹ä¾¿çš„]$$\\operatorname{Pr}\\left[\\begin{array}{c} (pk,sk)\\gets Gen(1^n); \\ (\\vec {m_o},\\vec{m_1},state)\\gets M(pk) s.t. |m_0^i|=|m_1^i|; \\ \\vec{c}\\gets Enc(pk,\\vec{m_b}) :A(state,\\vec{c})=b\\end{array}\\right] \\le \\frac{1}{2} +\\mu(n)$$ Yet the definition is unachievable. There is a simple way to win the game. You can construct two vector of message in which one is composed of the same message and the other is composed of the different message. Hence, it has to be randomized. In [Lecture 1], we gave two security definitions in Symmetric-key Encryption, Shannonâ€™s Perfect Secrecy and Perfect Indistinguishability. Similarly, in Asymmetric-key Encryption, there is an alternative definition, Semantic Security, corresponding to the Indistinguishability Security. The Semantic Security is the computational analog of Shannonâ€™s Shannonâ€™s Perfect Secrecy, and it turns to be equivalent to Indistinguishability Security. i.e. Semantic Security = IND-Security. But the proof is more complex. We will stick to IND-security as itâ€™s easy to work with. One Message SecurityItâ€™s cumbersome to define with many messages. Actually, the definition can be simplified to One Message Security. The only difference is that Eve can only see one ciphertext rather than a sequence of ciphertexts in the game. Define the Game: The Challenger generates a pair of key $(pk, sk)$ and publishes the $pk$. Eve send two single messages, $m_0$ and $m_1$s.t. $|m_0|=|m_1|$. The challenger sample $b$ from ${0,1}$ and encrypt the message $m_b$ using $pk$.And send the ciphertext to Eve. Eve guesses which message is encrypted and output $bâ€™$. Eve wins if $bâ€™=b$. One-message IND-security Definition (unachievable): The encryption scheme is single-message-IND-secure if no PPT EVE can win in this game with probability better than $1/2 + \\text{negl}(n)$. Similarly, itâ€™s unachievable. In public-key encryption, Eve knows the $pk$ as well. So she has the power of generating any ciphertext for arbitrary message of her choice, which is the meaning of CPA. When she gets the ciphertext from the Challenger, she can easily distinguish it. Because she can generate the ciphertexts of $m_0$ and $m_1$ using $pk$. Hence, only in public-key encryption, the many message security implies one message security. It dose not work with secret-key encryption. Theorem: A public-key encryption scheme is IND-secure iff it is single-message IND-secure. The proof is the simple use of Hybrid Argument. Weâ€™ll show four constructions in following blogs. Trapdoor Permutations (RSA) Quadratic Residuosity/Goldwasser-Micali Diffie-Hellman/El Gamal Learning with Errors/Regev In this blog, we introduce the Trapdoor Permutations. Trapdoor FunctionsWe know one-way function (family) is easy to compute but hard to invert. But Trapdoor One-way Function (family) is easy to invert given a trapdoor. Besides, itâ€™s Trapdoor One-way Permutation when domain = range. DefinitionDefinition: A function (family) $\\mathcal{F=\\{F_n\\}}_{n\\in\\mathbb{N}}$ where each $\\mathcal{F_n}$ is itself a collection of functions $\\mathcal{F_n}=\\{F_i:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{m(n)}\\}_{i\\in I_n}$ is a trapdoor one-way function family if Easy to sample function index with a trapdoor.There is a PPT algorithm $Gen(1^n)$ that outputs a function index $i\\in I_n$ together with a trapdoor $t_i$. Easy to compute $F_i(x)$ given $i$ and $x$. Easy to compute an inverse of $F_i(x)$ given $t_i$. It is one-way.That is, for every p.p.t. $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}\\left[\\begin{array}{c}(i, t) \\leftarrow \\operatorname{Gen}\\left(1^{n}\\right) ; x \\leftarrow\\{0,1\\}^{n} ; y=F_{i}(x) ; \\\\ A\\left(1^{n}, i, y\\right)=x^{\\prime}: y=F_{i}\\left(x^{\\prime}\\right)\\end{array}\\right] \\leq \\mu(n) $$ Public-key Encryption ConstructionWe can construct IND-Secure Public-key Encryption from Trapdoor Permutations. There are three p.p.t. algorithms. Not IND-Secure (without randomization): $Gen(1^n)$: Sample function index $i$ with a trapdoor $t_i$.The public key is $i$ and the private key is $t_i$. $Enc(pk=i,m)$: Output $c=F_i(m)$ as the ciphertext. $Dec(sk=t_i,c)$: Output $F_i^{-1}(c)$ computed using the private key $t_i$. However, it is NOT IND-secure since it could reveal partial information about $m$. IND-Security is unachievable without randomization as we mentioned before in the IND-security definition. In last lecture [(Lecture 7)], we introduced GL Theorem that every one-way function has a hardcore bit . Besides, we showed one-way permutation implies PRG. (In fact, one-way function implies PRG as well.) Hence, we can construct a PRG from the trapdoor permutation, which is pseudorandom. IND-Secure (with PRG from trapdoor permutations): $Gen(1^n)$: Sample function index $i$ with a trapdoor $t_i$.The public key is $i$ and the private key is $t_i$. $Enc(pk=i,m)$ where $m$ is a bit. Pick a random $r$. Output $c=(F_i(r),HCB(r)\\oplus m)$ as the ciphertext. $Dec(sk=t_i,c)$: Recover $r$ using the private key $t_i$. Decrypt $m$ using $HCB(r)$. Notation: If the message is $k$-bit, it has to run the encryption $k$ times, each of which is encrypting one bit. This public-key encryption is IND-secure. It looks familiar with the stateless secret-key encryption with PRF. â€œStateless Secret-key Encryption with PRF:â€ in Lecture 4 $Gen(1^n)$: Generate a random $n$-bit key $k$ that defines $f_k:\\{0,1\\}^l\\rightarrow \\{0,1\\}^m$. The domain size of $f_k$, $2^l$, had better be super-polynomially large in $n$. $Enc(k,m)$: Pick a random $x$ and let the ciphertext $c$ be the pair $(x,y=f_k(x)\\oplus m)$.Itâ€™s a polynomial time to evaluate $f_k(x)$ since $f_k$ are random accessible. $Dec(k,c=(x,y))$: Output $f_k(x)\\oplus y$. The proof is by hybrid argument. (It is also familiar with the proof of secret-key encryption using PRF in Lecture 4.) Proof: The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. From the (One-message) IND-Security definition,we want to prove the following two ciphertexts are indistinguishable. ciphertext of $m_0$: $c=(F_i(r),HCB(r)\\oplus m_0)$ ciphertext of $m_1$: $c=(F_i(r),HCB(r)\\oplus m_1)$ We define a sequence of hybrid distributions by changing the ciphertext a little bit.Consider the ciphertext as the distribution.Direction: Hybrid 0 â†’ Hybrid 3 â† Hybrid6 Hybrid 0: $D$ gets the ciphertext of $m_0$$c=(F_i(r),HCB(r)\\oplus m_0)$ Hybrid 1: replace with the hardcore bit$c=(F_i(r),HCB(r))$ Hybrid 2: replace with a random bit $r_b$$c=(F_i(r),r_b)$ Hybrid 3: replace with a random $r_x$ s.t. $|r_x|=|F_i(r)|$$c=(r_x,r_b)$ Hybrid 4: replace with a random bit $r_b$ (like Hybrid 2)$c=(F_i(r),r_b)$ Hybrid 5: replace with the hardcore bit (like Hybrid 1)$c=(F_i(r),HCB(r))$ Hybrid 6: $D$ gets the ciphertext of $m_1$$c=(F_i(r),HCB(r)\\oplus m_1)$ The thing we want to prove is that Hybrid 0 and Hybrid 6 are indistinguishable. Prove Hybrid 0 = Hybrid 1 (and Hybrid 6 = Hybrid 5) by birthday paradox.The probability of $D$ distinguishing from Hybrid 0 and Hybrid 1 is up to the collision probability of $r$. Prove Hybrid 1 = Hybrid 2 (and Hybrid 5 = Hybrid 4) by HCB security.The probability of $D$ distinguishing from Hybrid 1 and Hybrid 2 is determined by the advantage of computing the $HCB(r)$ from $F_i(r)$, which is negligible. Prove Hybrid 2 = Hybrid 3 ( and Hybrid 4 = Hybrid 3) by birthday paradox.The probability of $D$ distinguishing from Hybrid 0 and Hybrid 1 is up to the collision probability of $r$. QED. Trapdoor Permutation CandidatesTrapdoor Permutations are exceedingly rare. There are two candidates. (both need factoring to be hard) The RSA (Rivest-Shamir-Adleman) Function. The Rabin/Blum-Williams Function This blog only show the RSA Trapdoor Permutation. RSA Trapdoor PermutationLetâ€™s review some number theory from Lecture 6. Let $N=pq$ be a product of two large primes. Facts: $\\mathbb{Z}_N^* =\\{a\\in \\mathbb{Z}_N^*:\\operatorname{gcd}(a,N)=1\\}$ is a group. group operation is multiplication mod $N$. inverses exist and are easy to compute. the order of the group is $\\phi(N)=(p-1)(q-1)$ Let $e$ be an integer with $\\operatorname{gcd}(e,\\phi(N))=1$. Then, the map $F_{N,e}(x)=x^e\\mod N$ is a trapdoor permutation. The key fact is given $d$ such that $ed=1 \\mod \\phi(N)$, then it is easy to compute $x$ given $x^e$. This gives us the RSA trapdoor permutation collection $\\{F_{N,e}:\\operatorname{gcd}(e,N)=1\\}$. Function index is $(N,e)$ Trapdoor for inversion is $d=e^{-1} \\mod \\phi(N)$ The hardness of inversion without trapdoor = RSA assumption: Given $N,e$ (as above) and $x^e\\mod N$, hard to compute $x$.","link":"/2022/07/22/mit6875-lec8/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 6 - Number Theory","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ The motif of this blog is Number Theory, including the second half of Lecture 5 and the Lecture 6. Itâ€™s an excellent opportunity to learn Number Theory in manner of English. An important point in this blog is that we focus more on the statements, which is useful in later lectures, rather than the proof. About 70% of the content in this blog is originally and literally from the lecture notes. I just organize and refine it according to the logic of the professorâ€™s narration since the lecture note is awesome. The rest is my own understanding and derivation of some theorems. And I will be learning Number Theory and completing the omitted proof. So this blog will be updated continuously. Topics covered: Groups, Order of a group and the Order of an element, Cyclic Groups. The Multiplicative Group $\\mathbb{Z}_N^*$ and $\\mathbb{Z}_P^*$ for a prime $P$. Generators of $\\mathbb{Z}_P^*$. Primes, Primality Testing. The Discrete Logarithm (DLOG) problem and a candidate OWF. Diffie-Hellman assumptions: DDH and CDH. GroupsAn Abelian group $\\mathbb{G}=(S,\\star)$ is a set $S$ together with a operation $\\star :S\\times S\\rightarrow S$ which satisfies Identity: There is an element $\\mathcal{I}\\in S$ such that for $a\\in S$, $a\\star \\mathcal{I}=\\mathcal{I}\\star a=a$. Inverse: For every $a\\in S$, there is an element $b\\in S$ such that $a\\star b =b\\star a=\\mathcal{I}$. Associativity: For every $a,b,c\\in S$, $a\\star (b\\star c)=(a\\star b)\\star c$. Commutativity: For every $a,b\\in S$, $a\\star b=b\\star a$. Order of a group and the order of an element The order of a group is the number of elements in it, namely $|S|$. The order of an element $g\\in S$ is the minimal number of times one has to perform the group operation on $g$ to get to the identity element $\\mathcal{I}$.That is, $\\mathrm{ord}(g)=\\min_{i>0}\\{g^i=\\mathcal{I}\\}$. Theorem 1 (Lagrangeâ€™s Theorem): The order of any element divides the order of the group. Generator of a GroupA generator of a group $\\mathbb{G}$ is an element of order $|\\mathbb{G}|$. In other words, $$ \\mathbb{G}=\\{g,g^2,\\dots,g^{|\\mathbb{G}|}=\\mathcal{I}\\} $$ Cyclic groupA group $\\mathbb{G}$ is called cyclic if it has a generator. Theorem 2: Every group whose order is a prime number is cyclic.Moreover, every element other than the identity is a generator. Proof[2] Discrete LogarithmsLet $\\mathbb{G}$ be a cyclic group. We know that $\\mathbb{G}$ has a generator $g$, and that every $h\\in \\mathbb{G}$ can be written as $h=g^x$ for a unique $x\\in\\{1,2,\\dots,|\\mathbb{G}|\\}$. We write $$x=\\operatorname{dlog}_g(h)$$ to denote the fact that $x$ is the discrete logarithm of $h$ to the base $g$. We will look for groups where computing the group operation is easy (namely, polynomial time) but computing discrete logarithms is hard (namely, exponential or sub-exponential time). Our source for such groups will come from number theory. Discrete logarithms in $\\mathbb{Z}_N$ are, for better or worse, easy. Baby (Computational) Number TheoryThe complexity of basic operations with numbers. $n$ denotes the input length for each of these operations. Greatest Common Divisors: The greatest common divisor (gcd) of positive integers $a$ and $b$ is the largest positive integer $d$ that divides both $a$ and $b$. $a$ and $b$ are relatively prime if their gcd is 1. The Multiplicative Group $\\mathbb{Z}_N^*$The multiplicative group of numbers mod $N$, denoted $\\mathbb{Z}_N^*$, consists of the set $$ S=\\{1\\le a< N:\\operatorname{gcd}(a,N)=1\\} $$ with multiplication mod $N$ being the operation. Some further facts about $\\mathbb{Z}_N^*$ The order of $\\mathbb{Z}_N^*$, the number of positive integers smaller than $N$ that are relatively prime to it, is called the Euler totient function of $N$ denoted $\\varphi(N)$. $\\varphi(p)=p-1$ if $p$ is prime. $\\varphi(p^k)=p^k-p^{k-1}$ if $p$ is prime. $\\varphi(pq)=\\varphi(p)\\varphi(q)$ if $\\operatorname{gcd}(p,q)=1$. If $N=\\prod _i p_i^{\\alpha_i}$ is the prime factorization of $N$, then $\\varphi(N)=\\prod_i p_i^{\\alpha_i-1}(pi-1)$. The Multiplicative Group $\\mathbb{Z}_p^*$ $\\mathbb{Z}_p^*$ is Cyclic. The following theorem is a very important property of $\\mathbb{Z}_p^*$ when $P$ is prime. Theorem 4 : If $P$ is prime, then $\\mathbb{Z}_p^*$ is a cyclic group. Proof[2] Note: It is very tempting to prove this theorem by appealing the Theorem 2 which says that every group with prime order is cyclic. Be careful, and note that the order of $\\mathbb{Z}_p^*$ is $P-1$, which is decidedly not prime. There are several followup questions. How many generators are there for $\\mathbb{Z}_p^*$ ? How to tell (efficiency) if a given element $g$ is a generator ? How to sample a random generator for $\\mathbb{Z}_p^*$ ? $\\mathbb{Z}_p^{*}$ and $\\mathbb{Z}_{P-1}$ Before proceeding further, let us note the following structural fact about $\\mathbb{Z}_P^*$. There are two groups are isomorphic with an isomorphism $\\phi$ that maps $x\\in\\mathbb{Z}_{P-1}$ to $g^x\\in \\mathbb{Z}_P^*$. [isomorphic: åŒæž„çš„] In particular, consider $\\phi(x)=g^x \\pmod P$. We have $\\phi(x+y)=\\phi(x)\\cdot \\phi(y)$. The isomorphism is efficiently computable in the forward direction (exponentiation, using the repeated squaring algorithm) but not known to be efficiently computable in the reverse direction. For example, consider $\\mathbb{Z}_7^*$ and $\\mathbb{Z}_6$: $\\mathbb{Z}_7^* = \\{1,2,3,4,5,6\\}$ and there is a generator $g=5$. You can get $\\{g,g^2,g^3,g^4,g^5,g^6\\} =\\{5, 4,6,2,3,1\\}$.When you perform the multiplication on $g$, you will wrap around the group.Thatâ€™s an intuitive reason why discrete logarithm is hard in $\\mathbb{Z}_p^*$. $\\mathbb{Z}_6=\\{1,2,3,4,5,6\\}$ and the generator $g=1$. The group operation in $\\mathbb{Z}_6$ is addition.You can get $\\{g,g^2,g^3,g^4,g^5,g^6\\}=\\{1,2,3,4,5,6\\}$.When you perform the addition on $g$, you just walk along the group. We can know that there is a one-to-one mapping $\\phi$ from $\\mathbb{Z}_6$ to $\\mathbb{Z}_7^*$, that is $\\mathbb{Z}_6\\cong \\mathbb{Z}_7^*$ . Here is another quick application of this isomorphism: Lemma : Let $P$ be an odd prime. If $g$ is a generator of $\\mathbb{Z}_P^*$, then so is $g^x$ as long as $x$ and $P-1$ are relatively prime. Proof: The following proof is my own deduction since the proof is omitted in the lecture. Corrections and advice are welcome. There is a generator $gâ€™$ in $\\mathbb{Z}_{P-1}$corresponding to $g$ in $\\mathbb{Z}_P^*$ from the isomorphism $\\mathbb{Z}_P^*\\cong \\mathbb{Z}_{P-1}$. $x$ and $P-1$ are relatively prime, and there are two statements. In group $\\mathbb{Z}_P^*$, if $g$ is a generator, then $g^x$ is a generator. In group $\\mathbb{Z}_{P-1}$, if $gâ€™$ is a generator, then $xgâ€™$ is a generator. From the isomorphism, we can turn the question in $\\mathbb{Z}_P^*$ to a relative question in $\\mathbb{Z}_{P-1}$. Itâ€™s easy to prove that if $x$ is a generator in $\\mathbb{Z}_{P-1}$ (which can iterate all elements), then $ax$ is also a generator in $\\mathbb{Z}_{P-1}$ (which can also iterate all elements) where $\\operatorname{gcd}(a, P-1)=1$. The main idea is to suppose for a contradiction that there are two different $x_1$ and $x_2$,i.e. $x_1\\ne x_2 \\pmod{P-1}$, satisfying $ax_1=ax_2 \\pmod{P-1}$. Then we know $P-1\\mid a(x_1-x_2)$. From $\\operatorname{gcd}(a, P-1)=1$, we know $P-1\\mid x_1-x_2$. So $x_1= x_2 \\pmod{P-1}$. (contradiction) QED. As a corollary, we immediately derive the fact that $\\phi(P-1)$ elements of $\\mathbb{Z}_p^*$ are generators. $\\mathbb{Z}_p^*$ has lots of generatorsFor the first question, $\\mathbb{Z}_p^*$ has lots of generators. Theorem 5: The number of generators in $\\mathbb{Z}_p^*$ is $\\varphi(P-1)$. But how large is $\\varphi(P-1)$ asymptotically? This is answered by the following classical theorem. Theorem 6: For every integer $N$, $\\varphi(N)=\\Omega(N/\\log \\log N)$. In other words, if you pick a random element of $\\mathbb{Z}_p^*$, you will see a generator with probability $$\\varphi(P-1)/(P-1)=\\Omega(1/\\log\\log P)$$ which is polynomial in $1/\\log P$. So, reasonably often.[asymptotical: æ¸è¿›çš„] Difference between Big $\\mathcal{O}$ and Big $\\Omega$:Big $\\mathcal{O}$ and Big $\\Omega$ function are used in computer science to describe the performance or complexity of an algorithm. Big $\\mathcal{O}$ is used to describe the worst case running time for an algorithm. But, Big $\\Omega$ notation, on the other hand, is used to describe the best case running time for a given algorithm. If you want to get a generator in $\\mathbb{Z}_p^*$, you will hit a generator very quickly just keeping picking random in $\\mathbb{Z}_p^*$. Then it comes to the second question. How to tell a generator of $\\mathbb{Z}_p^*$ ?The answer is that you can tell in poly. time whether $g\\in \\mathbb{Z}_p^*$ is a generator given the factorization of $P-1$. Given the factorization of P-1â€‹At first, we know that a generator is an element of order $P-1$, so itâ€™s easy to check whether $g^{P-1}=1 \\pmod P$. Then we need to check if there is some smaller power of $g$ that equals $1$ since the order is the minimal power. However, there are a large number of divisors of $P-1$, roughly $P^{1/\\log \\log P}$ which is not polynomial (in $\\log P$). It turns out, you do not need to check all divisors, but rather only the terminal divisors (or, the maximal factors less than $P-1$). If $P-1=\\prod_i q_i^{\\alpha_i}$, the terminal divisors are $(P-1)/q_i$ for each $i$. For example: If $P-1=p_1^2p_2^3p_3^2$. Suppose $g=p_1^2p_2^3$, then $g^{p_1^2p_2^3}=1$.You can get some powers of $g^{p_1^2p_2^3}$ that equals $1$.And the power to $g^{p_1^2p_2^3}$ is the multiplication to the exponent $p_1^2p_2^3$. You can get $g^{p_1^2p_2^3p_3}=1$ which is smaller power than $P-1$. You can also get $g^{p_1^2p_2^3p_3^2}=1$ which is useless since $P-1=p_1^2p_2^3p_3^2$. So you only hit one terminal divisor, that is $(P-1)/p_3$. Suppose $g=p_1p_2^2p_3$, then $g^{p_1p_2^2p_3}=1$.Similarly, you can get some powers of $g^{p_1p_2^2p_3}$ that equals $1$. You can get $g^{p_1p_2^3p_3^2}=1$ by power $p_2p_3$.You hit one terminal divisor $(P-1)/p_1$. You can get $g^{p_1^2p_2^2p_3^2}=1$ by power $p_1p_3$.You hit one terminal divisor $(P-1)/p_2$. You can get $g^{p_1^2p_2^3p_3}=1$ by power $p_1p_2$.You hit one terminal divisor $(P-1)/p_3$. So you can hit 3 terminal divisors. The conclusion is that if there is a smaller power of $g$ that equals 1, then you can definitely hit some powers of terminal divisor that equals 1, which are maximal factors of $P-1$. So you can only check all the terminal divisiors. Algorithm: The following algorithm works on $g$ and the prime factorization of $P-1$. For each $i$, check if $g^{(P-1)/q_i}\\overset{?}= 1\\pmod P$. If yes for any $i$, say â€œnot a generatorâ€. Otherwise, say â€œgeneratorâ€. Given only gâ€‹ and â€‹Pâ€‹Thatâ€™s nice. But can one tell if $g$ is a generator given only $g$ and $P$ ?(as opposed to the prime factorization of $P-1$ which is in general hard to compute) We donâ€™t know, so there are some ways around it. Solution 1: Pick a random $P$ together with its prime factorization of $P-1$. This, it turns out, can be done due to a clever algorithm of Kalai [6]. Solution 2: Pick $P=2Q+1$ where $Q$ is prime. Such primes like $P$ are called safe primes, and $Q$ is called a Sophie-Germain prime after the famous mathematicians. While there are infinitely many primes, it has only been conjectured that there are infinitely many Sophie-Germain primes. This remains unproven. Moreover, the Sophie-Germain primes are considered as the hardest class of primes for discrete logarithms. Solution 2 is what people typically use in practice. In practice, you just pick a random prime as $Q$ and check whether $P=2Q+1$ is a prime. Itâ€™s efficient as long as the Sophie-Germain primes are in dense distribution. The above solution 2 brings about new questions. How to sample a random prime and how to test primality ? PrimesThere are some questions about primes. How many primes of $n$-bit length ? How to test primality ? How to sample a $n$-bit random prime ? How many primesThe prime number theorem tells us that there are sufficiently many prime numbers. In particular, letting $\\pi(N)$ denote the number of prime numbers less than $N$, we know that $\\pi(N)=\\Omega(N/\\log N)$. Thus, if you pick a random number smaller than $N$, with probability $1/\\log N$ (which is $1$/polynomial in bit-length $n$ in question) you have a prime number at hand. Primality TestingHow to recognize that a given number is prime ? This has been the subject of extensive research in computational number theory with many polynomial-time algorithms, culminating with the deterministic polynomial-time primality testing algorithm of Agrawal, Kayal and Saxena[1] (a.k.a. AKS) in 2002. Sample a primeHow to sample a $n$-bit random prime ? The above two facts put together tell us how to generate a random $n$-bit prime number. Just pick a random number less than $2^n$. And test if it is prime. In expected $n$ iterations of this procedure, you will find a $n$-bit prime number, even a random prime number. One-way FunctionsOne-way function is the atom of cryptography. A one-function is a function $f$ that is easy to compute but hard to invert on average. How to define a one-way function ? Take 1 (Wrong): For every p.p.t. algorithm $A$ and every chosen $x$,there is a negligible function $\\mu$, s.t.$$\\operatorname{Pr}[A(f(x))=x]&lt;\\mu(n)$$It is hard for $A$ to guess the inverse of $f(x)$ for every chosen $x$. It seems right. How about the function $f(x)=0$ which maps any $x$ to $0$ ? The probability of guessing the inverse of $0$ is negligible if the domain size of $x$ is sufficiently large. Because it is essentially random. But $f(x)=0$ is not one-way function obviously. Take 2: For every p.p.t. algorithm $A$ and every chosen $x$, there is a negligible function $\\mu$,s.t.$$\\operatorname{Pr}[A(f(x))\\in \\tilde{f}(f(x))]&lt;\\mu(n)$$where $\\tilde{f}$ is the inverse function. The difference of the new definition is that $A$ is trying to guess some possible inverses of $f(x)$ rather than the exactly chosen $x$. In this definition, $f(x)=0$ is not one-way function. Candidate: DLOGLet us present an informal one-way function candidate. $$f(P,g,x)=(P,g,g^x \\pmod P)$$ Computing this function can be done in time polynomial in the input length. However, inverting is the discrete logarithm problem which is conjectured to be hard. Defined formally below. Discrete Log Assumption (DLOG): For a random $n$-bit prime $P$ and random generator $g$ of $\\mathbb{Z}_P^*$, and a random $x\\in \\mathbb{Z}_{P-1}$, there is no polynomial (in $n$) time algorithm that computes $x$ given $P,g,g^x \\pmod P$. Shorthand: Easy: $P,g,x\\rightarrow g^x$ Hard: $P,g,g^x\\rightarrow x$ In fact, this is not only a one-way function, but can also be made into a family of one-way permutations. More on that later, we will see that one-way permutations can be used to build pseudorandom generators; and as we saw already, pseudorandom generators can be sued to build pseudorandom functions and stateless secret-key encryption and authentication. We can do all the crypto we saw so far based on the hardness of the discrete logarithm problem. Route: OWFâ†’PRGâ†’PRF. However, going via this route may not be the most efficient. So, we will look at related problems and try to build more efficient PRGs and PRFs. The Diffie-Hellman AssumptionsThere is another route to build PRGs and PRFs efficiently, that is Diffie-Hellman Assumptions. Route: DHâ†’PRG. Route: DHâ†’PRF. Given $g^x$ and $g^y\\mod P$, you can easily compute $g^{x+y}=g^x\\cdot g^y \\pmod P$. But it is hard to compute $g^{xy}\\pmod P$. If you can compute discrete logarithms, then you can compute $x$ from $g^x$, and raise $g^y$ to $x$ to get $(g^y)^x=g^{xy} \\pmod P$. But discrete log is hard, so this isnâ€™t an efficient way to solve the problem. CDH AssumptionThe problem, called the computational Diffie-Hellman (CDH) problem, appears to be computationally hard, in fact as hard as computing discrete logarithms! Computational Diffie-Hellman Assumption (CDH): For a random $n$-bit prime $P$ and random generator $g$ of $\\mathbb{Z}_P^*$, and random $x,y\\in \\mathbb{Z}_{P-1}$, there is no polynomial (in $n$) time algorithm that computes $g^{xy}\\pmod P$ given $P,g,g^x\\pmod P,g^y\\pmod P$. Shorthand: Easy: $P,g,g^x,g^y\\rightarrow g^{x+y}$ Hard: $P,g,g^x,g^y\\rightarrow g^{xy}$ Moreover, it appears hard to even tell if you are given the right answer or not! DDH AssumptionBut this requires some care to formalize. At first, one may think that given $P,g,g^x,g^y$, it is hard to distinguish between the right answer $g^{xy} \\pmod P$ versus a random number $u \\mod P$. Let us call the assumption that this decisional problem is hard the decisional Diffie-Hellman (DDH) assumption. Decisional Diffie-Hellman Assumption (first take): For a random $n$-bit prime $P$ and random generator $g$ of $\\mathbb{Z}_P^*$, and random $x,y\\in \\mathbb{Z}_{P-1}$ and a random number $u\\in \\mathbb{Z}_P^*$, there is no polynomial (in $n$) time algorithm that distinguishes between $(P,g,g^x\\pmod P, g^y \\pmod P, g^{xy} \\pmod P)$ and $(P,g,g^x\\pmod P,g^y \\pmod P, u\\pmod P)$. However, the assumption turns out to be false. DDH is False in $\\mathbb{Z}_P^*$However, the DDH assumption is false in $\\mathbb{Z}_P^*$. It seems awfully strong on first look. It says not only it is hard to compute $g^{xy}$ from $g^x$ and $g^y$, but also that not even a single bit of $g^{xy}$ can be computed (with any polynomial advantage beyond trivial guessing). However, there are some information about $g^{xy}$ indeed dose leak from $g^x$ and $g^y$. Before that, let us take a quick detour in Quadratic Residues. $$h=g^2 \\pmod P$$ Some facts about Quadratic Residues: $h$ is a QR if $h=g^2\\pmod P$. 1/2 of $\\mathbb{Z}_P^*$ are QR. We can tell efficiently if $h$ is a QR by evaluating $h^{(P-1)/2}\\overset{?}=1 \\pmod P$. $h=g^x$ is a QR if and only if $x$ is even. Use the facts of quadratic residue in $\\mathbb{Z}_P^*$, we know the following statements are equivalent. $g^{xy}$ is a QR. iff $xy$ is even. iff $x$ is even or $y$ is even. iff $g^x$ is a QR or $g^y$ is a QR. Now, we can distinguish between $(P,g,g^x,g^y,g^{xy})$ and $(P,g,g^x,g^y,u)$. $g^{xy}$ is a quadratic residue with probability $3/4$ while $u$ is a quadratic residue with probability $1/2$. So the advantage for distinguishing is $1/4$. Thus, we need to refine our assumption. Looking at the core reason behind the above attack, we see that there is a 1/2 chance that $g^x$ falls into a subgroup (the subgroup of quadratic residues, to be precise) and once that happens, $g^{xy}$ is also in the subgroup no matter what $y$ is. These properties are furthermore detectable in polynomial-time which led us to attack. A solution is to work with subgroup of $\\mathbb{Z}_P^*$ of prime order. In particular, we will take $P=2Q+1$ to be a safe prime and work with $\\mathcal{QR}_P$, the subgroup of quadratic residues in $\\mathbb{Z}_P^*$. $\\mathcal{QR}_P$ has order $(P-1)/2=Q$ which is indeed prime! By virtue of this, every non-identity element of $\\mathcal{QR}_P$ is its generator w.r.t. theorem 2. With this change, we can state the following DDH assumption which is widely believed to be true. Decisional Diffie-Hellman Assumption (final): Let $P=2Q+1$ be a random $n$-bit safe prime and let $\\mathcal{QR}_P$ denote the subgroup of quadratic residues in $\\mathbb{Z}_P^*$. For a random generator $g$ of $\\mathcal{QR}_P$, and random $x,y\\in \\mathbb{Z}_Q$ and a random number $u\\in \\mathcal{QR}_P$, there is no polynomial (in $n$) that distinguishes between $(P,g,g^x\\pmod P,g^y \\pmod P, g^{xy}\\pmod P)$ and $(P,g,g^x\\pmod P, g^y \\pmod P,u\\pmod P)$. Shorthand: Hard: distinguish between $(P,g,g^x,g^y,g^{xy})$ and $(P,g,g^x,g^y,u)$. We know DLOGâ†’CDHâ†’DDH but no implications are known in the reverse directions. References[1] M. Agrawal, N. Kayal, and N. Saxena. Primes is in p. Annals of mathematics, pages 781â€“793, 2004. [2] D. Angluin. Lecture notes on the complexity of some problems est number theory. 1982. [6] A. Kalai. Generating random factored numbers, easily. Journal of Cryptology, 16(4):287â€“289, 2003.","link":"/2022/07/13/mit6875-lec6/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 7","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ It is indeed possible for $F(x)$ to leak a lot of information about $x$ even if $F$ is one-way. The hardcore predicate $B(x)$ represent the specific piece of information about $x$ which is hard to compute given $F(x)$. Topics Covered: Definition of one-way functions (OWF) Definition of hardcore bit/predicate (HCB) One-way permutations â†’ PRG.(In fact, one-way functions â†’ PRG, but thatâ€™s a much harder theorem.) Goldreich-Levin Theorem: every OWF has a HCB.(Proof for an important special case.) One-way FunctionsInformally, one-way function is easy to compute and hard to invert. DefinitionIn last blog, we introduced briefly the definition of One-way functions. Take 1 (Not a useful definition): A function (family) $\\{F_n\\}_{n\\in \\mathbb{N}}$ where $F_n:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{m(n)}$ is one-way if for every p.p.t. adversary $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F_n(x):A(1^n,y)=x]\\le \\mu(n) $$ Consider $F_n(x)=0$ for all $x$. This is one-way according to the above definition. But itâ€™s impossible to find the inverse even if $A$ has unbounded time. The probability of guessing the inverse of $0$ is negligible since it is essentially random. But $f(x)=0$ is not one-way function obviously. Hence, itâ€™s not a useful or meaningful definition. The right definition should be that it is impossible to find an inverse in p.p.t. rather than the exactly chosen $x$. One-way Functions Definition: A function (family) $\\{F_n\\}_{n\\in \\mathbb{N}}$ where $F_n:\\{0,1\\}^n\\rightarrow \\{0,1\\}^{m(n)}$ is one-way if for every p.p.t. adversary $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F_n(x):A(1^n,y)=\\color{red}{x':y=F_n(x')}]\\le \\mu(n) $$ Iâ€™m not going to ask the adversary to come up with $x$ itself which may be impossible. Instead, Iâ€™m just going to ask the adversary to come up with an $xâ€™$ such that $y$ is equal to $F(xâ€™)$. Itâ€™s sort of the pre-image of $y$. Hence, the adversary can always find an inverse with unbounded time. But it should be hard with probabilistic polynomial time. Moreover, One-way Permutations are one-to-one one-way functions with $m(n)=n$. Hardcore BitsIf $F$ is a one-way function, itâ€™s hard to compute a pre-image of $F(x)$ for a randomly chosen $x$. How about computing partial information about an inverse ? We saw in last blog that we can tell efficiently if $h$ is quadratic residue. So for the discrete log function $x=\\operatorname{dlog}_g(h)$, computing the least significant bit(LSB) of $x$ is easy. But MSB turns out to be hard. Moreover, there are one-way functions for which it is easy to compute the first half of the bits of the inverse. One-way function family easy to compute the first half of the bits: There is an obvious fact that if $f(x)$ is one-way, then $fâ€™(r||x)=r||f(x)$ is one-way. ($|r|=|f(x)|$)Itâ€™s hard to compute the pre-image of $fâ€™(r||x)$ because if you can break $fâ€™$ then you can break $f$.But itâ€™s easy to compute the first half of the bits since they are written in the output. Nevertheless, there has to be a hardcore set of hard to invert inputs. Concretely, dose there necessarily exist some bit of $x$ that is hard to compute ? Particularly, â€œhard to computeâ€ means â€œhard to guess with probability non-negligibly better than 1/2â€ since any bit can be guessed correctly with probability 1/2. So dose there necessarily exist some bit of $x$ that is hard to guess with probability non-negligibly better than 1/2 ? Hardcore Bit DefHardcore Bit Definition (Take 1): For any function (family) $F:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ , a bit $i=i(n)$ is hardcore if for every p.p.t. adversary $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x):A(y)=x_i]\\le 1/2 +\\mu(n) $$ The definition says that it is hard to guess the $i$-th bit of the inverse given the $y$. I mentioned above that there are one-way functions for which it is easy to compute the first half of the bits of the inverse. Moreover, there are functions that are one-way, yet every bit is somewhat easy to predict with probability $\\frac{1}{2}+1/n$. This is actually an (hard) exercise in the lecture. Sadly, I havenâ€™t figured out the construction that every bit is easy to compute w.p. $\\frac{1}{2}+1/n$.Hope to change the ideas with you. Although the entire inverse of $f(x)$ is hard to compute, it is indeed possible for $f(x)$ to leak a lot of information about $x$ even if $f$ is one-way. Hardcore Predicate DefSo, we generalize the notion of a hardcore â€œbitâ€. We define hardcore predicate to identify a specific piece of information about $x$ that is â€œhiddenâ€ by $f(x)$. Informally, a hardcore predicate of a one-way function $f(x)$ is hard to compute given $f(x)$. Hardcore Predicate Definition: For any function (family) $F:\\{0,1\\}^n\\rightarrow \\{0,1\\}^m$ , a function $B:\\{0,1\\}^n\\rightarrow \\{0,1\\}$ is a hardcore predicate if for every p.p.t. adversary $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x):A(y)=B(x)]\\le 1/2 +\\mu(n) $$ The definition says that a hardcore predicate $B(x)$ of a one-way function $f(x)$ is hard to compute with probability non-negligibly better than $1/2$ given $f(x)$ since the predicate $B(x)$ is a boolean function that can be computed with probability $1/2$. Besides, this is perfectly consistent with the fact that the entire inverse is actually hard to compute. Otherwise, you can compute $B(x)$. Henceforth, for us, a hardcore bit will mean a hardcore predicate. We can represent the definition by the following picture. Itâ€™s easy to compute the one-way function $F(x)$ given $x$. Itâ€™s easy to compute the hardcore predicate $B(x)$ of $F(x)$ given $x$. But itâ€™s hard to compute $B(x)$ given $F(x)$. We know that it is indeed possible for $F(x)$ to leak a lot of information about $x$ even if $F$ is one-way. Hence, the hardcore predicate $B(x)$ represent the specific piece of information about $x$ which is hard to compute given $F(x)$. One-way Permutations â†’ PRGIn this section, we show that One-way Permutations imply PRG. The construction is as follows: PRG Construction from OWP: Let $F$ be a one-way permutation, and $B$ an associated hardcore predicate for $F$. Then, define $G(x)=F(x)||B(x)$. Note: $G$ stretches by one bit. We can turn this into a $Gâ€™$ that stretches to any poly. number of bits from PRG Length Extension. Theorem: $G$ is a PRG assuming $F$ is a one-way permutation. Proof (using NBU): The thing we want to prove is that if $F$ is a one-way permutation and $B$ is the hardcore predicate for $F$, then $G(x)=F(x)||B(x)$ is a PRG. We prove it using next-bit unpredictability. Assume for contradiction that $G$ is not a PRG. Therefore, there is a next-bit predictor $D$, and index $i$, and a polynomial function $p$ such that $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):D(y_1\\dots y_{i-1})=y_i]\\ge \\frac{1}{2}+1/p(n)$ If we want to get the contradiction to $B(x)$, the index $i$ has to be $n+1$.(Because the $G(x)=F(x)||B(x)$ where $F(x)$ is $n$-bit and $B(x)$ is $1$-bit.) $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):D(y_1\\dots y_{n})=y_{n+1}]\\ge \\frac{1}{2}+1/p(n)$ Then we can construct a hardcore bit (predicate) predictor from $D$.In fact, $D$ is a hardcore predictor. $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):D(F(x))=B(x)]\\ge \\frac{1}{2}+1/p(n)$ QED. Proof (using Indistinguishability): We can also prove it using indistinguishability. Assume for contradiction that $G$ is not a PRG.Therefore, there is a p.p.t. distinguisher $D$, and a polynomial function $p$ such that $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=G(x):D(y)=1] &- \\\\ \\operatorname{Pr}[y\\leftarrow \\{0,1\\}^{n+1}:D(y)] &\\ge 1/p(n)\\end{aligned} $$ We construct a hardcore predictor $A$ and show: $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n:A(F(x))=B(x)]\\ge \\frac{1}{2}+1/p'(n)\\end{aligned} $$ What information can we learn from the distinguisher $D$ ? Rewrite $y$ in the first term by definition. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=\\color{blue}{F(x)||B(x)}:D(y)=1] &- \\\\\\operatorname{Pr}[y\\leftarrow \\{0,1\\}^{n+1}:D(y)=1]&\\ge 1/p(n)\\end{aligned} $$ Rewrite the second term by a syntactic change. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x)||B(x):D(y)=1]&-\\\\ \\operatorname{Pr}[{\\color{blue}{y_0\\leftarrow \\{0,1\\}^{n},y_1\\leftarrow \\{0,1\\}},y=y_0||y_1}:D(y)=1] &\\ge 1/p(n)\\end{aligned} $$ Rewire the second term since $F$ is one-way permutation. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x)||B(x):D(y)=1]&-\\\\ \\operatorname{Pr}[{\\color{blue}{x\\leftarrow \\{0,1\\}^{n},y_1\\leftarrow \\{0,1\\},y=F(x)||y_1}}:D(y)=1] &\\ge 1/p(n)\\end{aligned} $$ â€‹ Rewrite the second term since $\\operatorname{Pr}[y_1=0]=\\operatorname{Pr}[y_1=0]=1/2$. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x)||B(x):D(y)=1] &-\\\\ \\frac{\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||0}}:D(y)=1]+\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||1}}:D(y)=1]}{2} &\\ge 1/p(n)\\end{aligned} $$ Rewrite the second term using $B(x)$. $$ \\begin{aligned}\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;y=F(x)||B(x):D(y)=1] &-\\\\ \\frac{\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||B(x)}}:D(y)=1]+\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||\\overline{B(x)}}}:D(y)=1]}{2} &\\ge 1/p(n)\\end{aligned} $$ Putting thins together. $$ \\begin{aligned}\\frac{1}{2}(\\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||B(x)}}:D(y)=1] &- \\\\ \\operatorname{Pr}[{x\\leftarrow \\{0,1\\}^{n},\\color{blue}{y=F(x)||\\overline{B(x)}}}:D(y)=1]) &\\ge 1/p(n)\\end{aligned} $$ Although the mathematical transformations seem complex, the main idea is to turn what we know to what we want. We want to know some information about $B(x)$. The takeaway is that $D$ says 1 more often when fed with the â€œright bitâ€ than the â€œwrong bitâ€. Itâ€™s a familiar statement. In [Lecture 3] , we construct a predictor from a distinguisher to prove the next-bit unpredictability â†’ the indistinguishability. In that proof, we got the takeaway from hybrid argument, that the distinguisher $D$ says 1 more often when fed with the â€œright bitâ€ than the â€œwrong bitâ€. Construct a hardcore bit predictor $A$ from the distinguisher $D$ using the takeaway. The task of $A$ is get as input $z=F(x)$ and try to guess the hardcore predicate. The predictor works as follows: $$ \\begin{aligned}A(F(x))=\\begin{cases} b,& \\text{if }D(F(x)||b)=1\\\\ \\overline{b},& \\text{otherwise}\\end{cases},b\\leftarrow\\{0,1\\} \\end{aligned} $$ Pick a random bit $b$. Feed $D$ with input $z||b$ If $D$ says â€œ1â€, output $b$ as the prediction for the hardcore bit and if $D$ says â€œ0â€, output $\\overline{b}$. Analysis of the predictor $A$. $\\operatorname{Pr}[x\\leftarrow {0,1}^n:A(F(x))=B(x)]$ $A$ output $b=B(x)$ or $\\overline{b}=B(x)$. $\\begin{aligned} =\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||b)=1 \\color{blue}{\\wedge b=B(x)}] + \\\\ \\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||b)=0 \\color{blue}{\\wedge b\\ne B(x)}]& \\\\\\end{aligned}$ $\\begin{aligned}=\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||b)=1 \\mid b=B(x)]\\operatorname{Pr}[b=B(x)]& + \\\\ \\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||b)=0 \\mid b\\ne B(x)]\\operatorname{Pr}[b\\ne B(x)]& \\end{aligned}$ Since $b$ is random.$\\begin{aligned}=\\color{blue}{\\frac{1}{2}}(\\operatorname{Pr}[x\\leftarrow{0,1}^n:D(F(x)||b)=1 \\mid b=B(x)]&amp; + \\ \\operatorname{Pr}[x\\leftarrow{0,1}^n:D(F(x)||b)=0 \\mid b\\ne B(x)]&amp;) \\end{aligned}$ Put the prior condition in it.$\\begin{aligned}=\\frac{1}{2}(\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||\\color{blue}{B(x)})=1]& + \\\\ \\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||\\color{blue}{\\overline{B(x)}})=0 ) \\end{aligned}$ $\\begin{aligned}=\\frac{1}{2}(\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||\\color{blue}{B(x)})=1]& + \\\\ 1-\\operatorname{Pr}[x\\leftarrow\\{0,1\\}^n:D(F(x)||\\color{blue}{\\overline{B(x)}})=1 ) \\end{aligned}$ From the takeaway, we can get$=\\frac{1}{2}(1+(*))\\ge \\frac{1}{2} + 1/p(n)$ So the $A$ can predict the hardcore bit (predicate) with probability non-negligible better than $1/2$. Goldreich-Levin Theorem: every OWF has a HCB.A Universal Hardcore Predicate for all OWFWe have defined the hardcore predicate $B(x)$ for the particular one-way function $F(x)$. Letâ€™s shoot for a universal hardcore predicate for every one-way function, i.e., a single predicate $B$ where it is hard to guess $B(x)$ given $F(x)$. Is this possible ? Turns out the answer is â€œnoâ€. Pick a favorite amazing $B$. We can construct a one-way function $F$ which $B$ is not hardcore. There is a contradiction example. Claim 1: If $f(x)$ is a one-way function, then $fâ€™(x)=f(x)||B(x)$ is one-way as well.Claim 2:But $B(x)$ is NOT a hardcore predicate for $fâ€™(x)$. For claim 1, if you can compute the inverse of $fâ€™(x)$, then you can compute the inverse of $f(x)$.For claim 2, $B(x)$ is actually written in $fâ€™(x)$.So $fâ€™(x)$ is one-way and dose not leak much information. Goldreich-Levin(GL) TheoremBut Goldreich-Levin(GL) Theorem changes the statement to a random hardcore and tells us every one-way function has a hardcore predicate/bit. Goldreich-Levin(GL) Theorem: Let $\\{B_r:\\{0,1\\}^n\\rightarrow \\{0,1\\}\\}$ where $$ B_r(x)=\\langle r, x \\rangle =\\sum_{i=1}^n r_ix_i \\mod 2 $$ be a collection of predicates (one for each $r$). Then a random $B_r$ is hardcore for every one-way function $F$. That is, for every one-way function $F$, every p.p.t. $A$, there is a negligible function $\\mu$ s.t. $$ \\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;r\\leftarrow \\{0,1\\}^n:A(F(x),r)=B_r(x)]\\le\\frac{1}{2} +\\mu(n) $$ It defines a collection of predicates ${B_r:{0,1}^n\\rightarrow {0,1}}$ indexed by $r$, a $n$-bit vector. So the evaluation of $B_r(x)$ is essentially an inner product of $r$ and $x$ (mod 2). The definition says that a random $B_r$ is hardcore for every one-way funciton $F$. So it picks a random $x$ and a random $r$. For every p.p.t. adversary $A$, it is hard to compute $B_r(x)$ given $F(x)$ and $r$. Alternative Interpretations to GL TheoremThere are some alternative interpretations to GL Theorem. Alternative Interpretation 1: For every one-way function $F$, there is a related one-way function $Fâ€™(x,r)=(F(x),r)$ which has a deterministic hardcore predicate. For every one-way function $F$, you can change it to a related one-way function $Fâ€™(x,r)=(F(x),r)$. Although $Fâ€™$ leak the second half of bits, $Fâ€™$ is one-way. Then $Fâ€™$has a deterministic hardcore predicate $B(x,r)=\\langle x,r \\rangle\\pmod 2$. $B(x,r)$ is a fixed function, which performs an inner-product mod 2 between the first half and the second half of the inputs. Hence, in this interpretation, GL Theorem says that $B$ is not a hardcore bit for every OWF, but itâ€™s a hardcore bit for the related function, which we created it from the OWF. Moreover, if $F(x)$ is one-way permutation, then $Fâ€™(x,r)=(F(x),r)$ is one-way permutation. And we can get a PRG from $Fâ€™(x,r)$. Then $G(x,r)=F(x)||r||\\langle x,r\\rangle$ where the last bit is the hardcore bit. Alternative Interpretation 2: For every one-way function $F$, there exists (non-uniformly) a (possibly different) hardcore predicate $\\langle r_F,x\\rangle$. To be honest, I do not comprehend the advanced statement. The professor left a cool open problem: remove the non-uniformity. Proof of GL TheoremAssume for contradiction there is a predictor $P$ and a polynomial $p$ s.t. $\\operatorname{Pr}[x\\leftarrow {0,1}^n;r\\leftarrow {0,1}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge\\frac{1}{2} +p(n)$ We need to show an inverter $A$ for $F$ and a polynomial $pâ€™$ such that $\\operatorname{Pr}[x\\leftarrow {0,1}^n:A(F(x))=xâ€™:F(xâ€™)=F(x)]\\ge 1/pâ€™(n)$ But it is too hard to prove this contradiction. Letâ€™s make our lives easier. A Perfect PredicatorFor simplicity, we assume a perfect predicator. Assume a perfect predictor: Assume a perfect predictor $P$, which can completely predicte the HCB correctly, s.t. $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]=1$ Now we can construct an inverter $A$ for $F$. The inverter $A$ works as follows: On input $y=F(x)$, $A$ runs the predictor $P$ $n$ times on input $(y, e_1),(y,e_2),\\dots,$and $(y,e_n)$ where $e_1=100..0,e_2=010..0,\\dots$ are the unit vectors. Since $A$ is perfect, it returns $\\langle e_i,x\\rangle=x_i$, the $i$-th bit of $x$ on the $i$-th invocation. A Pretty Good PredictorThen we assume less. Assume a pretty good predictor: Assume a pretty good predictor $P$, s.t. $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4} + 1/p(n)$ What can we learn from the predictor ? Claim: For at least a $1/2p(n)$ fraction of the $x$,$\\operatorname{Pr}[r\\leftarrow {0,1}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4} + 1/2p(n)$ How to derive the claim by averaging argument ? When I wrote this blog, I contemplated it for a long long time.I figured it out with the help of Wengjie Li. Thanks!I read the details about the proof of GL Theorem in some textbooks.Some proved the lower bound of fraction $1/2p(n)$, but how about the lower bound of probability, $\\frac{3}{4}+1/2p(n)$?In fact, they are related. Before that, letâ€™s introduce averaging argument. In computational complexity theory and cryptography, averaging argument is a standard argument for proving theorems. It usually allows us to convert probabilistic polynomial-time algorithms into non-uniform polynomial-size circuits.â€”â€”Wiki Averaging Argument: Let $f$ be some function. The averaging argument is the following claim: If we have a circuit $C$ such that $C(x,y)=f(x)$ with probability at least $\\rho$ where $x$ is chosen at random and $y$ is chosen independently from some distribution $Y$ over $\\{0,1\\}^m$ (which might not even be effciently sampleable), then there exists a single string $y_0\\in \\{0,1\\}^m$ such that $\\operatorname{Pr}_x[C(x,y_0)=f(x)]\\ge \\rho$. Indeed, for every $y$ define $p_y$ to be $\\operatorname{Pr}_x[C(x,y)=f(x)]$, then $$ \\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\\mathbb{E}_y[p_y] $$ and then this reduces to the claim that for every random variable $Z$, if $\\mathbb{E}[Z]\\ge \\rho$ then $\\operatorname{Pr}[Z\\ge \\rho]&gt;0$ (this holds since $\\mathbb{E}[Z]$ is the weighted average of Z and clearly if the average of some values is at least $\\rho$ then one of the values must be at least $\\rho$.) The claim tells us if $\\operatorname{Pr}_{x,y}[C(x,y)=f(x)]\\ge \\rho$, then there exist some $y_0$ such that $\\operatorname{Pr}_x[C(x,y_0)=f(x)]\\ge \\rho$ as well. So we can single out these $y_0$ using averaging argument. Actually, I think the equation, $\\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\\mathbb{E}_y[p_y]$, is the manifestation of averaging. The following analysis is mostly my own deduction and comprehension since the proof is omitted in the lecture. Corrections and advice are welcome. Some analysis to $\\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\\mathbb{E}_y[p_y]$: By definition of $p_y$ $y$ is a variable and $p_y$ is a variable. By definition, $p_y$ is up to $y$. So the distribution of $p_y$ is equal to the distribution of $y$. Expand the left term.$\\operatorname{Pr}_{x,y}[C(x,y)=f(x)]=\\sum _i \\operatorname{Pr}_x[C(x,y_i)=f(x)]\\operatorname{Pr}[y=y_i]$ The right term is the expected value of variable $p_y$. $\\mathbb{E}_y[p_y] = \\sum_i p_{y_i} \\cdot \\operatorname{Pr}[p_y=p_{y_i}]$ Because the distribution of $p_y$ is equal to the distribution of $y$.$\\operatorname{Pr}[p_y=p_{y_0}]=\\operatorname{Pr}[y={y_0}]$ So, $\\mathbb{E}_y[p_y] = \\sum_i p_{y_i} \\cdot \\operatorname{Pr}[y=y_i]$ = the left term. Now back to the claim. The following is mostly my own deduction and comprehension since the proof is omitted in the lecture. Corrections and advice are welcome. Proof of the Claim : What can we learn from the claim? What do we know from the predictor ? We know $\\operatorname{Pr}_{x,r}[P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4} + 1/p(n)$. For every $x$, define $p_x=\\operatorname{Pr}_r[P(F(x),r)=\\langle r,x\\rangle]$. By averaging argument, We know there exists some $x$ such that $p_x\\ge \\frac{3}{4} + 1/p(n)$. We know $\\operatorname{Pr}_{x,r}[P(F(x),r)=\\langle r,x\\rangle]=\\mathbb{E}[p_x]$ by averaging argument. So we know the lower bound of $\\mathbb{E}[p_x]$ (expected value of $p_x$) is $\\frac{3}{4} + 1/p(n)$. What do we really want ? We want to single out these $x$ such that $p_x$ is non-negligibly better than $\\frac{3}{4}$ in (probabilistic) polynomial time. So we want a set of $x$, which the fraction is polynomial , for every $x$ in the set, the $p_x$ is non-negligibly better than $\\frac{3}{4}$. We show the fraction and the (lower bound of ) probability are related. Notation $\\epsilon$ define the lower bound of $\\mathbb{E}[p_x]$, i.e. $\\epsilon =\\frac{3}{4} + 1/p(n)$. Define a good set of $x$ as $S$ and $s$ denotes the poly. fraction of the $x$. ($|S|=s\\cdot 2^n$). $S=\\{x\\mid p_x \\ge \\varepsilon' \\}$ where $\\varepsilonâ€™$ denotes the lower bound of $p_x$ for every $x\\in S$, non-negligibly better than $\\frac{3}{4}$. Rewrite $\\mathbb{E}[p_x]$ using $s$ and $\\varepsilon$. $p_x= \\begin{cases} \\varepsilon' \\le p_x\\le 1,& x\\in S \\\\ 0\\le p_x \\le \\varepsilon',& x\\notin S \\end{cases}$ Similarly, $p_x$ is up to $x$.Hence, the distribution of $p_x$ is equal to the distribution of $x$, i.e.$\\operatorname{Pr}[p_x]=\\operatorname{Pr}[x]$ $\\mathbb{E}[p_x] = \\sum p_x \\cdot \\operatorname{Pr}[p_x]=\\sum p_x \\cdot \\operatorname{Pr}[x]$ $=\\sum p_x \\cdot \\operatorname{Pr}[x\\in S] + \\sum p_x \\cdot \\operatorname{Pr}[x\\notin S]$ $=\\sum p_x \\cdot s + \\sum p_x \\cdot (1-s)$ We do not know the actual $p_x$ but we know the boundary of $p_x$. Calculate the boundary of $\\mathbb{E}[p_x]$ using $s$ and $\\varepsilon$. $\\mathbb{E}[p_x] =\\sum p_x \\cdot s + \\sum p_x \\cdot (1-s)$ The lower bound is $\\varepsilonâ€™ \\cdot s + 0\\cdot (1-s)=\\varepsilonâ€™ \\cdot s$ (useless) The upper bound is $1\\cdot s + \\varepsilonâ€™ \\cdot (1-s)=s+\\varepsilonâ€™ (1-s)$ (useful) This upper bound should be greater than the known lower bound $\\epsilon$. We get the relation of $s$ and $\\varepsilonâ€™$. $\\epsilon \\le s+\\varepsilonâ€™ (1-s)$ $\\frac{\\epsilon-s}{1-s}\\le \\varepsilonâ€™$ (since $s&lt;1$) Take $\\epsilon$ with $\\frac{3}{4} + 1/p(n)$. We want the fraction $s$ to be polynomial. Suppose the fraction $s=1/2p(n)$.We can get $\\varepsilonâ€™ \\ge \\epsilon -s = \\frac{3}{4} +1/2p(n)$. (since $1-s$ is very small). We can also suppose the fraction $s=1/3p(n)$ and so on. The point I want to elucidate here is the fraction $s$ and the lower bound of $p_x$ for every $x\\in S$ can both be polynomial if the advantage of predicting is non-negligible. Hence, the takeaway is if the probability of predicting is non-negligibly better than $3/4$, then we can single out these â€œgood $x$ â€in (probabilistic) polynomial time. Similarly, if the probability of predicting is non-negligibly better than $1/2$ (the predictor in general case), we can also single out these â€œgood $x$ â€ in (probabilistic) polynomial time. We get the takeaway that if the probability of predicting is non-negligibly better than $3/4$, then we can single out these â€œgood $x$ â€ in (probabilistic) polynomial time. Invert the i-th bit: Now we can compute the $i$-th bit of $x$ with the probability better than $1/2$.The key idea is linearity. Pick a random $r$ Ask $P$ to tell us $\\langle r,x\\rangle$ and $\\langle r+e_i,x \\rangle$. Subtract the two answers to get $\\langle e_i,x\\rangle =x_i$ Proof of invert $i$-th bit: $\\operatorname{Pr}[\\text{we compute }x_i \\text{ correctly}]$ $\\ge \\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ and }\\langle r+e_i,x \\rangle \\text{ correctly}]$ $=1-\\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ or }\\langle r+e_i,x \\rangle \\text{ wrong}]$ $\\ge1-\\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ wrong}]\\text{ + } \\operatorname{Pr}[P \\text{ predicts }\\langle r+e_i,x \\rangle \\text{ wrong}]$(by union bound) $\\ge 1 - 2\\cdot (\\frac{1}{4} -\\frac{1}{2p(n)})=\\frac{1}{2}+1/p(n)$ I think it takes the fraction as $1/2p(n)$ just to make the advantage equal to $1/p(n)$. Itâ€™s beautiful. Invert the entire x: Construct the Inverter $A$ Repeat for each bit $i\\in{1,2,\\dots, n}$: Repeat $p(n) \\cdot p(n)$ times:(one $p(n)$ is for singling out, and another is for computing correctly) Pick a random $r$ Ask $P$ to tell us $\\langle r,x\\rangle$ and $\\langle r+e_i,x \\rangle$. Subtract the two answers to get $\\langle e_i,x\\rangle =x_i$ Compute the majority of all such guesses and set the bit as $x_i$. Output the concatenation of all $x_i$ as $x$. Analysis of $A$ is ommited.Hint: Chernoff + Union Bound. A General PredictorLetâ€™s proceed to a general predictor, which is in the foremost contradiction. Assume a general good predictor: Assume there is a predictor $P$ and a polynomial $p$ s.t. $\\operatorname{Pr}[x\\leftarrow \\{0,1\\}^n;r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge\\frac{1}{2} +p(n)$ Likewise, there is a similar claim. Claim: For at least a $1/2p(n)$ fraction of the $x$, $\\operatorname{Pr}[r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{1}{2} + 1/2p(n)$ The takeaway is that probability of predicting is non-negligibly better than $1/2$ , we can single out these â€œgood $x$ â€ in (probabilistic) polynomial time. However, we cannot invert the $i$-th bit of $x$ with the probability better than $1/2$ using the above method. Analysis of Inverting $x_i$: (error doubling) (For at least $1/2p(n)$ fraction of the $x$) If the probability of predicting $\\langle r,x\\rangle$ correctly is non-negligibly better than $3/4 + 1/2p(n)$, i.e. $\\operatorname{Pr}[r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4} + 1/2p(n)$ $\\operatorname{Pr}[\\text{we compute }x_i \\text{ correctly}]$ $\\ge \\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ and }\\langle r+e_i,x \\rangle \\text{ correctly}]$ $=1-\\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ or }\\langle r+e_i,x \\rangle \\text{ wrong}]$ $\\ge1-\\operatorname{Pr}[P \\text{ predicts }\\langle r,x \\rangle \\text{ wrong}]\\text{ + } \\operatorname{Pr}[P \\text{ predicts }\\langle r+e_i,x \\rangle \\text{ wrong}]$ $\\ge 1 - \\color {blue}{2\\cdot(\\frac{1}{4} -\\frac{1}{2p(n)})}=\\frac{1}{2}+1/p(n)$ So it can compute $x_i$ with advantage $1/p(n)$. If the probability of predicting $\\langle r,x\\rangle$ correctly is exactly $3/4$, i.e. $\\operatorname{Pr}[r\\leftarrow \\{0,1\\}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{3}{4}$ $\\operatorname{Pr}[\\text{we compute }x_i \\text{ correctly}]$ $\\ge 1 - \\color{blue} {2\\cdot(\\frac{1}{4})}=1/2$ It is unlikely to compute $x_i$ since itâ€™s the same with random. If the probability of predicting $\\langle r,x\\rangle$ correctly is non-negligibly better than $1/2 + 1/2p(n)$, i.e.$\\operatorname{Pr}[r\\leftarrow {0,1}^n:P(F(x),r)=\\langle r,x\\rangle]\\ge \\frac{1}{2} + 1/2p(n)$ $\\operatorname{Pr}[\\text{we compute }x_i \\text{ correctly}]$ $\\ge 1 - \\color{blue} {2\\cdot(\\frac{1}{2} -\\frac{1}{2p(n)})}=1/p(n)$ It cannot compute $x_i$. The problem above is that it doubles the original error probability of predicting, i.e. $1-2\\cdot(*)$. When the probability of error is significantly smaller than $1/4$, the â€œerror-doublingâ€ phenomenon raises no problem. However, in general case (and even in special case where the error probability is exactly $1/4$), the procedure is unlikely to compute $x_i$. Hence, what is required is an alternative way of using the predictor $P$, which dose not double the original error probability of $P$. The complete proof is referred in Goldreich Book Part 1, Section 2.5.2. The key idea is pairwise independence. I read the reference. To be honest, I do not comprehend it thoroughly. Happy to exchange the ideas. The Coding-Theoretic View of GL To be honest, I just write it down so I could figure it out in the near future. Happy to exchange the ideas. $x\\rightarrow (\\langle x,r\\rangle )_{r\\in{0,1}^n}$ can be viewed as a highly redundant, exponentially long encoding of $x$ = the Hadamard code. $P(F(x),r)$ can be thought of as providing access to a noisy codeword. What we proved = unique decoding algorithm for Hadamard code with error rate $\\frac{1}{4}-1/p(n)$. The real proof = list-decoding algorithm for Hadamard code with error rate rate $\\frac{1}{2}-1/p(n)$.","link":"/2022/07/19/mit6875-lec7/"},{"title":"ã€ŒCryptography-MIT6875ã€: Lecture 9","text":"In this series, I will learn MIT 6.875, Foundations of Cryptography, lectured by Vinod Vaikuntanathan. Any corrections and advice are welcome. ^ - ^ Topics Covered: Quadratic Residue and Quadratic Residuosity Assumption Goldwasser-Micali Encryption and Homomorphism Diffie-Hellman Key Exchange El Gamal Encryption In last blog is showed one of the constructions of public-key encryption using Trapdoor Permutations, RSA encryption. The gist in this blog is the remaining three constructions of public-key encryption. Constructions of Public-Key Encryption: Trapdoor Permutations (RSA) Quadratic Residuosity/Goldwasser-Micali Diffie-Hellman/El Gamal Learning with Errors/Regev Quadratic ResidueQR mod PLet $P$ be prime. We saw (in Lecture 6) that exactly half of $\\mathbb{Z}_P^*$ are squares. Define Legendre symbol $\\left(\\frac{x}{P}\\right)=1$ is $x$ is square, $-1$ if $x$ is not a square, and $0$ if $x=0\\mod P$. We can tell efficiently whether $x$ is a quadratic residue mod P using Legendre symbol. $$ \\left(\\frac{x}{P}\\right)=x^{(P-1) / 2}=\\begin{cases} 1 &,\\text{ if }x \\text{ is square} \\\\ -1 &,\\text{ if }x \\text{ is non-square} \\\\ 0 &,\\text{ if }x \\text{ is 0}\\end{cases} $$ Then we split $\\mathbb{Z}_P^*$ in half, one is square and the other is non-square. It is easy to compute square roots mod $P$. Besides, itâ€™s an explicit formula for the case where $P=3\\pmod 4$. Claim: The square roots of $x \\mod P$ are $\\pm x^{(P+1) / 4}$. Proof: $\\left(\\pm x^{(P+1)/4}\\right)^2=x^{(P+1)/2}=x\\cdot x^{(P-1)/2}=x\\mod P$. QR mod NNow, let $N=PQ$ be a product of two primes and look at $\\mathbb{Z}_N^*$. Define Jacobi symbol $\\left(\\frac{x}{N}\\right)=\\left(\\frac{x}{P}\\right) \\left(\\frac{x}{Q}\\right)$ to be $+1$ if $x$ is a square mod both $P$ and $Q$ or a non-square mod both $P$ and $Q$. Jacobi Symbol â€” â€” from Wiki The Jacobi symbol is a generalization of the Legendre symbol.For any integer $a$ and any positive odd integer $n$, the Jacobi symbol $(\\frac{a}{n})$ is defined as the product of the Legendre symbols corresponding to the prime factors of $n$: $$ \\left(\\frac{a}{n}\\right)=\\left(\\frac{a}{p_1}\\right)^{\\alpha_1} \\left(\\frac{a}{p_2}\\right)^{\\alpha_2}\\dots \\left(\\frac{a}{p_k}\\right)^{\\alpha_k} $$ where $n=p_1^{\\alpha_1}p_2^{\\alpha_2}\\dots p_k^{\\alpha_k}$ is the prime factorization of $n$.Some properties: Fix the bottom argument: $\\left(\\frac{ab}{n}\\right)=\\left(\\frac{a}{n}\\right)\\left(\\frac{b}{n}\\right)$ Fix the the top argument: $\\left(\\frac{a}{mn}\\right)=\\left(\\frac{a}{m}\\right)\\left(\\frac{a}{n}\\right)$ So we can also split $\\mathbb{Z}_N^*$ in half, and the Jacobi symbol for one half is $+1$ and the other is $-1$. A surprising fact is Jacobi symbol $\\left(\\frac{x}{N}\\right)=\\left(\\frac{x}{P}\\right) \\left(\\frac{x}{Q}\\right)$ is computable in poly. time without knowing $P$ and $Q$. The key is Law of Quadratic Reciprocity. Law of Quadratic Reciprocity: For all $x, N$: $\\left(\\frac{x}{N}\\right)=\\left(\\frac{N}{x}\\right) \\cdot (-1)^{(N-1)\\cdot (x-1)/4}$ $\\left(\\frac{x}{N}\\right)=\\left(\\frac{x\\mod N}{N}\\right)$ $\\left(\\frac{2}{N}\\right)=\\left(-1\\right)^{\\frac{\\left(n-1\\right)^2}{8}}$ $\\left(\\frac{-1}{N}\\right)=(-1)^{\\frac{(n-1)}{2}}$ The one thing to notice is that the Legendre symbol$\\left(\\frac{x}{P}\\right)$ tells you exactly whether $x$ is the square mod $P$ or not. But the Jacobi symbol $\\left(\\frac{x}{N}\\right)=1$ only gives you partial information that is square mod both $P$ and $Q$ or non-square mod both $P$ and $Q$. But $x$ is square mod $N$ iff $x$ is square mod $P$ and it is a square mod $Q$. Claim: $x$ is square mod $N$ iff $x$ is square mod $P$ and it is a square mod $Q$. Hence, there are two cases for $Jac_{+1}$. $QR_N$: the set of squares mod $N$. $QNR_N$: the set of non-squares mod $N$. (but with Jacobi symbol $+1$) The Jacobi symbol can be $+1$ even though $x$ is not square mod $N$, so itâ€™s pseudo-square. The conjecture is that distinguishing between $QR_N$ and $QNR_N$ is a hard problem. We cannot distinguish from squares and pseudo-squares. Can we use this hardness? Finding Square Roots Mod NThe fact is finding square roots mod $N$ is as hard as factoring $N$. Suppose we know P and Q.Suppose we know $P$ and $Q$, and we want to find the square root of $x \\mod N$. Before that, letâ€™s introduce the Chinese Reminder Theorem(CRT). Itâ€™s a nice morphism.Informally, we can get $\\mathbb{Z}_N^*\\cong \\mathbb{Z}_P^* \\cdot \\mathbb{Z}_Q^*$. In forward direction, we can map $\\mathbb{Z}_N^*$ to $(\\mathbb{Z}_P^* ,\\mathbb{Z}_Q^*)$ uniquely. That is: $x \\rightarrow(x\\mod P, x\\mod Q)$. In back direction, we can map $(\\mathbb{Z}_P^* ,\\mathbb{Z}_Q^*)$ to $\\mathbb{Z}_N^*$ uniquely. That is: $(x_1, x_2)\\rightarrow c_Px_1+c_Qx_2\\mod N$.where $c_P$ and $c_Q$ are CRT coefficients. We can find the square root of $x\\mod N$ by following algorithm. Algorithm: Find the square roots of $y\\mod P$ and $y\\mod Q$. $x=y_P^2\\mod P$ $x=y_Q^2\\mod Q$ Let $y=c_Py_P+c_Qy_Q$ where the CRT coefficients. $c_P=1\\mod P \\text{ and }0 \\mod Q$ $c_Q=0\\mod P \\text{ and }1 \\mod Q$ Then $y$ is a square root of $x\\mod N$. Proof: The proof is easy using CRT. $y^2 = (c_Py_P + c_Qy_Q)^2\\mod N$ We can map $y^2$ to pair $(y^2\\mod P, y^2\\mod Q)$ Then we get pair $(y_P^2\\mod P,y_Q^2 \\mod Q)$ That is $x$. Moreover, we can get $x=y^2\\mod N \\longleftrightarrow \\begin{array}{c}x=y^2\\mod P \\\\x=y^2 \\mod Q\\end{array}$. Therefore, the takeaway is if $x$ is a square, it has $4$ distinct square roots $\\mod N$. Because it respectively has $2$ distinct square roots $\\mod P$ and $\\mod Q$. Suppose we know square root.Suppose we have a box that computes square roots $\\mod N$. The thing to notice is that the box only returns one square $y$ as it has $4$ distinct square roots. So if we feed the box $x=z^2\\mod N$ for a random $z$ of our choice, the box can return other square root. We can use the box to factor $N$. Feed the box $x=z^2 \\mod N$ for a random $z$. Claim: With probability $1/2$, $\\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$. Proof: Notation: $z$ denotes the square root of $x$, which is known. $x$ denotes the input of box s.t. $x=z^2 \\mod N$. $y$ denotes the output of box s.t. $x=y^2\\mod N$, which could be different with $z$ . We know $y^2=z^2\\mod N$. $N\\mid y^2-z^2$ $N\\mid (y-z)(y+z)$ There are three cases. If $N\\mid (y-z)$, we can get $y=z\\mod N$. (Useless) If $N\\mid (y+z)$, we can get $y=-z\\mod N$. (Useless) If $N\\nmid (y-z)$ and $N\\nmid (y+z)$, what can we get ? $(y-z)$ and $(y+z)$ respectively have a factor of $N$ since $N=PQ$. So $\\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$. Hence, $\\operatorname{gcd}(z+y,N)$ is a non-trivial factor of $N$ if $y\\ne z$ and $y\\ne-z$.The probability is $1/2$. QED. Itâ€™s a very clever trick to factor. I know a solution to a problem that turns it into an Oracle. The Oracle could actually help me in solving another problem. Itâ€™s an example of the reduction. Quadratic Residuosity Assumption(QRA)Finding square roots is as hard as factoring $N$. Moreover, recognizing squares $\\mod N$ also seems hard as we mentioned above. Quadratic Residuosity Assumption (QRA): Let $N=PQ$ be a product of two large primes. No PPT algorithm can distinguish between a random element of $QR_N$ from a random element of $QNR_N$ given only $N$. Goldwasser-Micali (GM) EncryptionGoldwasser-Micali (GM) Encryption is under the Quadratic Residuosity Assumption. GM Scheme $Gen(1^n)$: Generate random $n$-bit prime $p$ and $q$ and let $N=pq$. Let $y\\in QNR_N$ be some quadratic non-residue with Jacobi symbol $+1$. How to sample a quadratic non-residue with Jacobi symbol $+1$ ? Sample a random number $y$. Check $\\left(\\frac{y}{P}\\right)\\overset?{=} -1$ and $\\left(\\frac{y}{Q}\\right)\\overset?{=} -1$ Then we can get a quadratic non-residue with $\\left(\\frac{y}{N}\\right)=1$. Let $pk=(N,y)$. Let $sk=(p,q)$. $Enc(pk,b)$ where $b$ is a bit: Generate random $r\\in\\mathbb{Z}_N^*$.(randomness) Output $\\begin{cases}r^2\\mod N &, \\text{if }b =0 \\\\r^2y\\mod N &,\\text{if }b=1 \\end{cases}$. $Dec(sk,c)$: Check if $c\\in\\mathbb{Z}_N^*$ is a quadratic residue using $p$ and $q$. If yes, output $0$ else $1$. If $b=0$, the encryption is $r^2$, which is quadratic residue with Jacobi symbol $+1$. $\\left(\\frac{r^2}{N}\\right)=\\left(\\frac{r}{N}\\right)^2=1$. ( $r\\in \\mathbb{Z}_N^*$ so $r\\ne0$) If $b=1$, the encryption is $r^2y$, which is quadratic non-residue with Jacobi symbol $+1$. $\\left(\\frac{r^2y}{N}\\right)=\\left(\\frac{r^2}{N}\\right)\\left(\\frac{y}{N}\\right)=1$. Although you know the Jacobi symbol is $+1$, you have no idea that the ciphertext is square or pseudo square. Hence, IND-security follows directly from the quadratic residuosity assumption. GM is a Homomorphic EncryptionGiven a GM-ciphertext of $b$ and a GM-ciphertext of $bâ€™$, I can compute a GM-ciphertext of $b+bâ€™\\mod 2$ without knowing anything about $b$ or $bâ€™$. $Enc(pk,b)$ where $b$ is a bit:Generate random $r\\in\\mathbb{Z}_N^*$ and output $r^2y^b\\mod N$. Claim: $Enc(pk,b)\\cdot Enc(pk,bâ€™)$ is an encryption of $b\\oplus bâ€™=b+bâ€™\\mod2$. Proof: Consider $c_1=r_1^2y^b$ and $c_2=r_2^2y^{bâ€™}$. $c_1\\cdot c_2 = r_1^2r_2^2 y^{b+bâ€™}$ . $c_1\\cdot c_2$ is QR if $b+bâ€™=0\\mod 2$. The takeaway here is $QR\\cdot QR=QR$ and $QNR\\cdot QNR=QR$. Diffie-Hellman Key ExchangeThe main idea is the commutativity in the exponent, $(g^x)^y=(g^y)^x$, where $g$ is an element of some group. So you can compute $g^{xy}$ given either $g^x$ and $y$, or $g^y$ and $x$. We elaborated the Diffie-Hellman Assumption in Lecture 6. Diffie-Hellman Assumption(DHA): Hard to compute $g^{xy}$ given only $g,g^x$and $g^y$. Diffie-Hellman Key Exchange: Let $p=2q+1$ be a safe prime, and $g$ be the generator of $QR_p$. Alice picks a random number $x\\in \\mathbb{Z}_q$ and sends $g^x\\mod p$ to Bob. Bob picks a random number $y\\in\\mathbb{Z}_q$ and sends $g^y\\mod p$ to Alice. Alice and Bob have the shared key $k=g^{xy}\\mod p$ Alice computes it by $g^y$ and $x$. Bob computes it by $g^x$ and $y$. El Gamal Encryption $Gen(1^n)$: Generate an $n$-bit safe prime $p=2q+1$ and a generator $g$ of $\\mathbb{Z}_p^*$. Let $h=g^2\\mod p$ be a generator of $QR_p$.Choose a random number $x\\in \\mathbb{Z}_q$. Let $pk=(p,h,h^x)$. Let $sk=x$.(Finding $sk$ from $pk$ is the discrete logarithm problem.) $Enc(pk,m)$ where $m\\in QR_p$. Generate random $y\\in\\mathbb{Z}_q$. (randomness) Output $(h^y,h^{xy}\\cdot m)$. $Dec(sk=x,c)$ Compute $h^{xy}$ using $h^y$ and $x$. Divide the second component to retrieve $m$. Decisional Diffie-Hellman Assumption (DDHA):Hard to distinguish between $g^{xy}$ and a uniformly random group element, given $g,g^x$ and $g^y$.That is the following two distributions are computationally indistinguishable:$(g,g^x,g^y,g^{xy})\\approx (g,g^x,g^y,u)$ From DDH assumption, we know Itâ€™s hard to distinguish between $(h,h^x,h^y,h^{xy})$ and $(h,h^x,h^y,u)$. Moreover, itâ€™s hard to distinguish between $(h,h^x,h^y,h^{xy})$ and $(h,h^x,h^y,h^{xy}\\cdot m)$ since $m$ is random, so is $m\\cdot h^{xy}$. So itâ€™s hard to distinguish between $(h,h^x,h^y,h^{xy}\\cdot m)$ and $(h,h^x,h^y,u)$. Hence, DH/El Gamal is IND-secure under the DDH assumption. The source of hardness is different in RSA and GM. There is no factoring and only DH problem. Which Group to UseQuadratic Residue Group: We used the $QR_P$ group for a safe prime $P=2Q+1$ where $Q$ is prime so far. The order of the group is $Q$. But it is not used in practice. Because discrete log can be broken in sub-exponential time $2^{\\sqrt{\\log P\\log\\log P}}$. Itâ€™s better than $poly(P)$ but worse than $poly(\\log P)$. And the $poly(\\log P)$ is what weâ€™re referring to $poly(n)$. Elliptic Curve Groups: In practice, we use Elliptic Curve Groups in DH/El Gamal. It is the set of solutions $(x,y)$ to the equation $y^2=x^3+ax+b\\pmod P$ together with a very cool group addition law. The best known discrete log algorithm is $\\mathcal{O}(\\sqrt{P})$ time! That says that we can use much smaller keys. We can use 160-bit $P$ to suffice â€œ80-bit securityâ€. SummaryWe have elaborated three constructions of public-key encryption. Constructions of Public-Key Encryption: Trapdoor Permutations (RSA) Quadratic Residuosity/Goldwasser-Micali Diffie-Hellman/El Gamal Learning with Errors/Regev If factoring is easy, RSA is broken (and thatâ€™s the only known way to break RSA). If factoring is easy, Goldwasser-Micali is broken conjecturing that finding square roots or recognizing the squares are both as hard as factoring. Moreover, the discrete problem is similarly broken to factoring in quantum computer. Hence, the preceding three constructions are dead if the quantum computer comes out. Yet learning with errors is post-quantum secure as far as we know. We will see more when we do homomorphic encryption. Practical ConsiderationsThere are some practical considerations. How do I know the public key? Public-key Infrastructure: a directory of identities together with their public keys. But it needs to be â€œauthenticatedâ€.Otherwise Eve could replace Bobâ€™s $pk$ with her own. Public-key encryption is orders of magnitude slower than secret-key encryption. We mostly showed how to encrypt bit-by-bit! Super-duper inefficient. Exponentiation takes $O(n^2)$ time as opposed to typically linear time for secret key encryption (AES). The $n$ itself is large for PKE (RSA: $n\\ge 2048$) compared to SKE (AES: $n=128$).(For Elliptic Curve El-Gamal, itâ€™s $320$ bits. We can solve problem 1 and minimize problems 2&amp;3 using hybrid encryption. Hybrid Encryption To encrypt a long message $m$ (think 1GB) Pick a random key $K$ (think 128 bits) for a secret-key encryption. Encrypt $K$ with the $PKE$: $PKE.Enc(pk,K)$. Encrypt $m$ with the $SKE$: $SKE.Enc(K,m)$. To decrypt: Recover $K$ using $sk$. Then using $K$, recover $m$.","link":"/2022/07/26/mit6875-lec9/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€:HW1-Predict PM2.5","text":"åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œç”¨æ‰‹åˆ»Adagradå®Œæˆäº†ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€çš„HW1-é¢„æµ‹PM2.5çš„ä½œä¸šã€‚å…¶ä¸­åŒ…æ‹¬å¯¹æ•°æ®çš„å¤„ç†ï¼Œè®­ç»ƒæ¨¡åž‹ï¼Œé¢„æµ‹ï¼Œå¹¶ä½¿ç”¨sklearn toolkitçš„ç»“æžœè¿›è¡Œæ¯”è¾ƒã€‚æœ‰å…³HW1çš„ç›¸å…³æ•°æ®ã€æºä»£ç ã€é¢„æµ‹ç»“æžœç­‰ï¼Œæ¬¢è¿Žå…‰ä¸´å°é€æ˜Žçš„GitHub Task Descriptionkaggle link ä»Žä¸­å¤®æ°”è±¡å±€ç½‘ç«™ä¸‹è½½çš„çœŸå®žè§‚æµ‹èµ„æ–™ï¼Œå¿…é¡»åˆ©ç”¨linear regressionæˆ–å…¶ä»–æ–¹æ³•é¢„æµ‹PM2.5çš„å€¼ã€‚ è§‚æµ‹è®°å½•è¢«åˆ†ä¸ºtrain set å’Œ test set, å‰è€…æ˜¯æ¯ä¸ªæœˆå‰20å¤©æ‰€æœ‰èµ„æ–™ï¼›åŽè€…æ˜¯ä»Žå‰©ä¸‹çš„èµ„æ–™ä¸­éšæœºå–æ ·å‡ºæ¥çš„ã€‚ train.csv: æ¯ä¸ªæœˆå‰20å¤©çš„å®Œæ•´èµ„æ–™ã€‚ test.csv: ä»Žå‰©ä¸‹çš„10å¤©èµ„æ–™ä¸­å–å‡º240ç¬”èµ„æ–™ï¼Œæ¯ä¸€ç¬”èµ„æ–™éƒ½æœ‰è¿žç»­9å°æ—¶çš„è§‚æµ‹æ•°æ®ï¼Œå¿…é¡»ä»¥æ­¤è§‚æµ‹å‡ºç¬¬åå°æ—¶çš„PM2.5. Process Datatrain dataå¦‚ä¸‹å›¾ï¼Œæ¯18è¡Œæ˜¯ä¸€å¤©24å°æ—¶çš„æ•°æ®ï¼Œæ¯ä¸ªæœˆå–äº†å‰20å¤©ï¼ˆæ—¶é—´ä¸Šæ˜¯è¿žç»­çš„å°æ—¶ï¼‰ã€‚ test data å¦‚ä¸‹å›¾ï¼Œæ¯18è¡Œæ˜¯ä¸€ç¬”è¿žç»­9å°æ—¶çš„æ•°æ®ï¼Œå…±240ç¬”æ•°æ®ã€‚ æœ€å¤§åŒ–training data size æ¯è¿žç»­10å°æ—¶çš„æ•°æ®éƒ½æ˜¯train setçš„dataã€‚ä¸ºäº†å¾—åˆ°æ›´å¤šçš„dataï¼Œåº”è¯¥æŠŠæ¯ä¸€å¤©è¿žèµ·æ¥ã€‚å³ä¸‹å›¾è¿™ç§æ•ˆæžœï¼š æ¯ä¸ªæœˆå°±æœ‰ï¼š $20*24-9=471$ ç¬”data 123456789# Dictionary: key:month value:month datamonth_data = {}# make data timeline continuousfor month in range(12): temp = np.empty(shape=(18, 20*24)) for day in range(20): temp[:, day*24: (day+1)*24] = data[(month*20+day)*18: (month*20+day+1)*18, :] month_data[month] = temp ç­›é€‰éœ€è¦çš„Features : è¿™é‡Œï¼Œæˆ‘å°±åªè€ƒè™‘å‰9å°æ—¶çš„PM2.5ï¼Œå½“ç„¶è¿˜å¯ä»¥è€ƒè™‘å’ŒPM2.5ç­‰ç›¸å…³çš„æ°®æ°§åŒ–ç‰©ç­‰featureã€‚ training data 1234567# x_data v1: only consider PM2.5x_data = np.empty(shape=(12*471, 9))y_data = np.empty(shape=(12*471, 1))for month in range(12): for i in range(471): x_data[month*471+i][:] = month_data[month][9][i: i+9] y_data[month*471+i] = month_data[month][9][i+9] testing data 12345# Testing data featurestest_x = np.empty(shape=(240, 9))for day in range(240): test_x[day, :] = test_data[18*day+9, :]test_x = np.concatenate((np.ones(shape=(240, 1)), test_x), axis=1) Normalization 123456789101112# feature scale: normalizationmean = np.mean(x_data, axis=0)std = np.std(x_data, axis=0)for i in range(x_data.shape[0]): for j in range(x_data.shape[1]): if std[j] != 0: x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]for i in range(test_data.shape[0]): for j in range(test_data.shape[1]): if std[j] != 0: test_data[i][j] = (test_data[i][j] - mean[j])/std[j] Trainingæ‰‹åˆ»Adagrad è¿›è¡Œtrainingã€‚ï¼ˆæŒ–å‘ï¼šRMSpropã€Adam[1] Linear Pseudo code 123456Declare weight vector, initial lr ,and # of iterationfor i_th iteration : yâ€™ = the product of train_x and weight vector Loss = yâ€™ - train_y gradient = 2*np.dot((train_x)â€™, Loss ) weight vector -= learning rate * gradient å…¶ä¸­çš„çŸ©é˜µæ“ä½œæ—¶ï¼Œæ³¨æ„æ±‚gradientæ—¶çŸ©é˜µçš„ç»´åº¦ã€‚å¯å‚è€ƒä¸‹å›¾ã€‚ Adagrad Pseudo code 123456789Declare weight vector, initial lr ,and # of iterationDeclare prev_gra storing gradients in every previous iterations for i_th iteration : yâ€™ = the inner product of train_x and weight vector Loss = yâ€™ - train_y gradient = 2*np.dot((train_x)â€™, Loss ) prev_gra += gra**2 ada = np.sqrt(prev_gra) weight vector -= learning rate * gradient / ada æ³¨ï¼šä»£ç å®žçŽ°æ—¶ï¼Œå°†biaså­˜åœ¨w[0]å¤„ï¼Œx_dataçš„ç¬¬0åˆ—å…¨1ã€‚å› ä¸ºwå’Œbå¯ä»¥ä¸€åŒæ›´æ–°ã€‚ï¼ˆå½“ç„¶ï¼Œä¹Ÿå¯ä»¥åˆ†å¼€æ›´æ–°ï¼‰ Adagrad training 123456789101112131415161718192021# train-adagradbatch = x_data.shape[0] # full batchepoch = 400# some parameters for trainingdim = x_data.shape[1]+1w = np.zeros(shape=(dim, 1)) # concatenate bias = w[0]lr = np.full((dim, 1), 0.8) # learning rategrad = np.empty(shape=(dim, 1)) # gradient of loss to every paragradsum = np.zeros(shape=(dim, 1)) # sum of gradient**2x_data = np.concatenate((np.ones(shape=(x_data.shape[0], 1)), x_data), axis=1)loss_his = np.empty(shape=(epoch, 1))for T in range(epoch): L = y_data - np.dot(x_data, w) loss_his[T] = np.sum(L**2) / x_data.shape[0] grad = (-2)*np.dot(np.transpose(x_data), L) gradsum = gradsum + grad**2 w = w - lr*grad/(gradsum**0.5) Testing1answer = np.dot(test_x, w) Draw and Analysisåœ¨æ¯æ¬¡è¿­ä»£æ›´æ–°æ—¶ï¼Œæˆ‘å°†Lossçš„å€¼å­˜äº†ä¸‹æ¥ï¼Œä»¥ä¾¿å¯è§†åŒ–Lossçš„å˜åŒ–å’Œæ›´æ–°é€Ÿåº¦ã€‚ Lossçš„å˜åŒ–å¦‚ä¸‹å›¾ï¼š(çº¢è‰²çš„æ˜¯sklearn toolkitçš„lossç»“æžœ) æ­¤å¤–ï¼Œåœ¨æºä»£ç ä¸­ï¼Œä½¿ç”¨sklearn toolkitæ¥æ¯”è¾ƒç»“æžœã€‚ ç»“æžœå¦‚ä¸‹ï¼š 123456789101112131415161718192021222324252627v1: only consider PM2.5Using sklearnLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)bias= [21.37402689]w= [[ 0.00000000e+00] [-5.54801503e-01] [-4.32873874e-01] [ 3.63669814e+00] [-3.99037687e+00] [-9.07364636e-01] [ 8.83495803e+00] [-9.51785135e+00] [ 1.32734655e-02] [ 1.81886444e+01]]In our modelbias= [19.59387132]w= [[-0.14448468] [ 0.39205748] [ 0.26897134] [-1.02415371] [ 1.21151411] [ 2.21925424] [-5.48242478] [ 4.01080346] [13.56369122]] å‘çŽ°å‚æ•°æœ‰ä¸€å®šå·®å¼‚ï¼ŒäºŽæ˜¯æˆ‘åœ¨testingæ—¶ï¼Œä¹ŸæŠŠsklearnçš„ç»“æžœè¿›è¡Œé¢„æµ‹æ¯”è¾ƒã€‚ ä¸€éƒ¨åˆ†ç»“æžœå¦‚ä¸‹ï¼š 1234567891011121314151617['id', 'value', 'sk_value']['id_0', 3.551092352912313, 5.37766865368331]['id_1', 13.916795471648756, 16.559245678900034]['id_2', 24.811333478647043, 23.5085950470451]['id_3', 5.101440436158914, 6.478306159981166]['id_4', 26.7374726797937, 27.207516152986663]['id_5', 19.43735346531517, 21.916809502961648]['id_6', 22.20460696285646, 24.751295357256392]['id_7', 29.660872382552682, 30.24344042612033]['id_8', 17.5964527734513, 16.64242443764712]['id_9', 56.58017426943178, 59.760988216575115]['id_10', 13.767504260132299, 10.808372404511037]['id_11', 11.743000466164233, 11.526958393801682]['id_12', 59.509878887026105, 64.201008247897]['id_13', 53.19824337746267, 54.3856368053018]['id_14', 21.97191108867921, 24.530720709840974]['id_15', 10.833283625735444, 14.350345549104446] Codeæœ‰å…³HW1çš„ç›¸å…³æ•°æ®ã€æºä»£ç ã€é¢„æµ‹ç»“æžœç­‰ï¼Œæ¬¢è¿Žå…‰ä¸´å°é€æ˜Žçš„GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137########################## Date: 2020-4-4# Author: FredLau# HW1: predict the PM2.5##########################import sysimport numpy as npimport pandas as pdimport csvfrom sklearn import linear_modelimport matplotlib.pyplot as plt###################### process data# process train dataraw_data = np.genfromtxt('data/train.csv', delimiter=',')data = raw_data[1:, 3:]data[np.isnan(data)] = 0 # process nan# Dictionary: key:month value:month datamonth_data = {}# make data timeline continuousfor month in range(12): temp = np.empty(shape=(18, 20*24)) for day in range(20): temp[:, day*24: (day+1)*24] = data[(month*20+day)*18: (month*20+day+1)*18, :] month_data[month] = temp# x_data v1: only consider PM2.5x_data = np.empty(shape=(12*471, 9))y_data = np.empty(shape=(12*471, 1))for month in range(12): for i in range(471): x_data[month*471+i][:] = month_data[month][9][i: i+9] y_data[month*471+i] = month_data[month][9][i+9]# process test datatest_raw_data = np.genfromtxt('data/test.csv', delimiter=',')test_data = test_raw_data[:, 2:]test_data[np.isnan(test_data)] = 0# feature scale: normalizationmean = np.mean(x_data, axis=0)std = np.std(x_data, axis=0)for i in range(x_data.shape[0]): for j in range(x_data.shape[1]): if std[j] != 0: x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]for i in range(test_data.shape[0]): for j in range(test_data.shape[1]): if std[j] != 0: test_data[i][j] = (test_data[i][j] - mean[j])/std[j]# Testing data featurestest_x = np.empty(shape=(240, 9))for day in range(240): test_x[day, :] = test_data[18*day+9, :]test_x = np.concatenate((np.ones(shape=(240, 1)), test_x), axis=1)################################# train-adagradbatch = x_data.shape[0] # full batchepoch = 400# some parameters for trainingdim = x_data.shape[1]+1w = np.zeros(shape=(dim, 1)) # concatenate bias = w[0]lr = np.full((dim, 1), 0.8) # learning rategrad = np.empty(shape=(dim, 1)) # gradient of loss to every paragradsum = np.zeros(shape=(dim, 1)) # sum of gradient**2x_data = np.concatenate((np.ones(shape=(x_data.shape[0], 1)), x_data), axis=1)loss_his = np.empty(shape=(epoch, 1))for T in range(epoch): L = y_data - np.dot(x_data, w) loss_his[T] = np.sum(L**2) / x_data.shape[0] grad = (-2)*np.dot(np.transpose(x_data), L) gradsum = gradsum + grad**2 w = w - lr*grad/(gradsum**0.5)f = open('output/v1.csv', 'w')sys.stdout = fprint('v1: only consider PM2.5\\n')################################ train by sklearn linear modelprint('Using sklearn')reg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print('bias=', reg.intercept_)print('w=', reg.coef_.transpose())print('\\n')# In our modelprint('In our model')print('bias=', w[0])print('w=', w[1:])############################ draw change of lossplt.xlim(0, epoch)plt.ylim(0, 10)plt.xlabel('$iteration$', fontsize=16)plt.ylabel('$Loss$', fontsize=16)iteration = np.arange(0, epoch)plt.plot(iteration, loss_his/100, '-', ms=3, lw=2, color='black')sk_w = reg.coef_.transpose()sk_w[0] = reg.intercept_sk_loss = np.sum((y_data - np.dot(x_data, sk_w))**2) / x_data.shape[0]plt.hlines(sk_loss/100, 0, epoch, colors='red', linestyles='solid')plt.legend(['adagrad', 'sklearn'])plt.show()# plt.savefig('output/v1.png')f.close()############### test (sklearn vs our adagradf = open('output/v1test.csv', 'w')sys.stdout = ftitle = ['id', 'value', 'sk_value']answer = np.dot(test_x, w)sk_answer = np.dot(test_x, sk_w)print(title)for i in range(test_x.shape[0]): content = ['id_'+str(i), answer[i][0], sk_answer[i][0]] print(content)f.close() Reference å¾…å®Œæˆ","link":"/2020/04/05/ml-lee-hw1/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šHW2-Binary Income Predicting","text":"è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæ‰‹åˆ»å®žçŽ°äº†ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€çš„HW2-Binary Income Predictionçš„ä½œä¸šã€‚åˆ†åˆ«ç”¨Logistic Regressionå’ŒGenerative Modelå®žçŽ°ã€‚åŒ…æ‹¬å¯¹æ•°æ®é›†çš„å¤„ç†ï¼Œè®­ç»ƒæ¨¡åž‹ï¼Œå¯è§†åŒ–ï¼Œé¢„æµ‹ç­‰ã€‚æœ‰å…³HW2çš„ç›¸å…³æ•°æ®ã€æºä»£ç ã€é¢„æµ‹ç»“æžœç­‰ï¼Œæ¬¢è¿Žå…‰ä¸´å°é€æ˜Žçš„GitHub Task introduction and Dataset Kaggle competition: link Task: Binary Classification Predict whether the income of an individual exceeds $50000 or not ? *Dataset: * Census-Income (KDD) Dataset (Remove unnecessary attributes and balance the ratio between positively and negatively labeled data) Feature Format train.csv, test_no_label.csvã€éƒ½æ˜¯æ²¡æœ‰å¤„ç†è¿‡çš„æ•°æ®ï¼Œå¯ä½œä¸ºæ•°æ®å‚è€ƒå’Œä¼˜åŒ–å‚è€ƒã€‘ text-based raw data unnecessary attributes removed, positive/negative ratio balanced. X_train, Y_train, X_testã€å·²ç»å¤„ç†è¿‡çš„æ•°æ®ï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨ã€‘ discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial stateâ€¦) continuous features in train.csv =&gt; remain the same in X_train (age, capital lossesâ€¦). X_train, X_test : each row contains one 510-dim feature represents a sample. Y_train: label = 0 means â€œ&lt;= 50Kâ€ ã€ label = 1 means â€œ &gt;50K â€ æ³¨ï¼šæ•°æ®é›†è¶…å¤§ï¼Œç”¨notepadæŸ¥çœ‹æ¯”è¾ƒèˆ’æœï¼›è°ƒè¯•æ—¶ï¼Œä¹Ÿå¯ä»¥å…ˆè°ƒè¯•å°ä¸€ç‚¹çš„æ•°æ®é›†ã€‚ Logistic RegressionLogistic Regression åŽŸç†éƒ¨åˆ†è§è¿™ç¯‡åšå®¢ã€‚ Prepare dataæœ¬æ–‡ç›´æŽ¥ä½¿ç”¨X_train Y_train X_test å·²ç»å¤„ç†å¥½çš„æ•°æ®é›†ã€‚ 12345678910111213141516# prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_Train'X_test_fpath = './data/X_test'output_fpath = './logistic_output/output_logistic.csv'fpath = './logistic_output/logistic'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:] ç»Ÿè®¡ä¸€ä¸‹æ•°æ®é›†ï¼š 123456789101112train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In logistic model:\\n') f.write('Size of Training set: {}\\n'.format(train_size)) f.write('Size of development set: {}\\n'.format(dev_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n'.format(data_dim)) ç»“æžœå¦‚ä¸‹ï¼š 12345In logistic model:Size of Training set: 48830Size of development set: 5426Size of test set: 27622Dimension of data: 510 normalizenormalize data. å¯¹äºŽtrain dataï¼Œè®¡ç®—å‡ºæ¯ä¸ªfeatureçš„meanå’Œstdï¼Œä¿å­˜ä¸‹æ¥ç”¨æ¥normalize test dataã€‚ ä»£ç å¦‚ä¸‹ï¼š 12345678910111213141516def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X. # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std # Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std) Development set splitåœ¨logistic regressionä¸­ä½¿ç”¨çš„gradientï¼Œæ²¡æœ‰closed-formè§£ï¼Œæ‰€ä»¥åœ¨train setä¸­åˆ’å‡ºä¸€éƒ¨åˆ†ä½œä¸ºdevelopment set ä¼˜åŒ–å‚æ•°ã€‚ 12345678def _train_dev_split(X, Y, dev_ratio=0.25): # This function splits data into training set and development set. train_size = int(X.shape[0] * (1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]# Split data into train data and development datadev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio) Useful function_shuffle(X, Y)æœ¬æ–‡ä½¿ç”¨mini-batch gradientã€‚ æ‰€ä»¥åœ¨æ¯æ¬¡epochæ—¶ï¼Œä»¥ç›¸åŒé¡ºåºåŒæ—¶æ‰“ä¹±X_train,Y_trainæ•°ç»„ï¼Œå†mini-batchã€‚ 12345678np.random.seed(0)def _shuffle(X, Y): # This function shuffles two two list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize] _sigmod(z)è®¡ç®— $\\frac{1}{1+e^{-z}}$ ï¼Œæ³¨æ„ï¼šé˜²æ­¢æº¢å‡ºï¼Œç»™å‡½æ•°è¿”å›žå€¼è§„å®šä¸Šç•Œå’Œä¸‹ç•Œã€‚ 12345def _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8)) _f(X, w, b)æ˜¯sigmodå‡½æ•°çš„è¾“å…¥ï¼Œlinear partã€‚ è¾“å…¥ï¼š Xï¼šshape = [size, data_dimension] wï¼šweight vector, shape = [data_dimension, 1] b: bias, scalar è¾“å‡ºï¼š å±žäºŽClass 1çš„æ¦‚çŽ‡ï¼ˆLabel=0ï¼Œå³æ”¶å…¥å°äºŽ$50kçš„æ¦‚çŽ‡ï¼‰ 12345678910def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b) _predict(X, w, b)é¢„æµ‹Label=0ï¼Ÿï¼ˆ0æˆ–è€…1ï¼Œä¸æ˜¯æ¦‚çŽ‡ï¼‰ 1234def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int) _accuracy(Y_pred, Y_label)è®¡ç®—é¢„æµ‹å‡ºçš„ç»“æžœï¼ˆ0æˆ–è€…1ï¼‰å’ŒçœŸå®žç»“æžœçš„æ­£ç¡®çŽ‡ã€‚ è¿™é‡Œä½¿ç”¨ $1-\\overline{error}$ æ¥è¡¨ç¤ºæ­£ç¡®çŽ‡ã€‚ 123456def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc _cross_entropy_loss(y_pred, Y_label)è®¡ç®—é¢„æµ‹å‡ºçš„æ¦‚çŽ‡ï¼ˆæ˜¯sigmodçš„å‡½æ•°è¾“å‡ºï¼‰å’ŒçœŸå®žç»“æžœçš„äº¤å‰ç†µã€‚ è®¡ç®—å…¬å¼ä¸ºï¼š $\\sum_n {C(y_{pred},Y_{label})}=-\\sum[Y_{label}\\ln{y_{pred}}+(1-Y_{label})\\ln(1-{y_{pred}})]$ 123456789def _cross_entropy_loss(y_pred, Y_label): # This function calculates the cross entropy of Y_pred and Y_label # # Argument: # y_pred: predictions, float vector # Y_label: truth labels, bool vector cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((1 - Y_label).T, np.log(1 - y_pred)) return cross_entropy[0][0] _gradient(X, Y_label, w, b)å’ŒRegressionçš„æœ€å°äºŒä¹˜ä¸€æ ·ã€‚ï¼ˆä¸¥è°¨çš„è¯´ï¼Œæœ€å¤šä¸€ä¸ªç³»æ•°ä¸åŒï¼‰ 12345678def _gradient(X, Y_label, w, b): # This function calculates the gradient of cross entropy # X, Y_label, shape = [batch_size, ] y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = - np.dot(X.T, pred_error) b_grad = - np.sum(pred_error) return w_grad, float(b_grad) Training (Adagrad)åˆå§‹åŒ–ä¸€äº›å‚æ•°ã€‚ è¿™é‡Œç‰¹åˆ«æ³¨æ„ : ç”±äºŽadagradçš„å‚æ•°æ›´æ–°æ˜¯ $w \\longleftarrow w-\\eta \\frac{gradient}{ \\sqrt{gradsum}}$ . é˜²æ­¢é™¤0ï¼Œåˆå§‹åŒ–gradsumçš„å€¼ä¸ºä¸€ä¸ªè¾ƒå°å€¼ã€‚ 12345678910111213# training by logistic model# Initial weights and biasw = np.zeros((data_dim, 1))b = np.float(0.)w_grad_sum = np.full((data_dim, 1), 1e-8) # avoid divided by zerosb_grad_sum = np.float(1e-8)# Some parameters for trainingepoch = 20batch_size = 2**3learning_rate = 0.2 AdagradAagradå…·ä½“åŽŸç†è§è¿™ç¯‡åšå®¢çš„1.2èŠ‚ã€‚ è¿­ä»£æ›´æ–°æ—¶ï¼Œæ¯æ¬¡epochè®¡ç®—ä¸€æ¬¡losså’Œaccuracyï¼Œä»¥ä¾¿å¯è§†åŒ–æ›´æ–°è¿‡ç¨‹ï¼Œè°ƒæ•´å‚æ•°ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738# Keep the loss and accuracy history at every epoch for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Iterative trainingfor it in range(epoch): # Random shuffle at every epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for id in range(int(np.floor(train_size / batch_size))): X = X_train[id*batch_size: (id+1)*batch_size] Y = Y_train[id*batch_size: (id+1)*batch_size] # calculate gradient w_grad, b_grad = _gradient(X, Y, w, b) # adagrad gradient update w_grad_sum = w_grad_sum + w_grad**2 b_grad_sum = b_grad_sum + b_grad**2 w_ada = np.sqrt(w_grad_sum) b_ada = np.sqrt(b_grad_sum) w = w - learning_rate * w_grad / np.sqrt(w_grad_sum) b = b - learning_rate * b_grad / np.sqrt(b_grad_sum) # compute loss and accuracy of training set and development set at every epoch y_train_pred = _f(X_train, w, b) Y_train_pred = np.around(y_train_pred) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size) train_acc.append(_accuracy(Y_train_pred, Y_train)) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.around(y_dev_pred) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size) dev_acc.append(_accuracy(y_dev_pred, Y_dev)) Loss &amp; accuracyè¾“å‡ºæœ€åŽä¸€æ¬¡è¿­ä»£çš„losså’Œaccuracyã€‚ ç»“æžœå¦‚ä¸‹ï¼š 1234Training loss: 0.2933570286596322Training accuracy: 0.8839238173254147Development loss: 0.31029505347634456Development accuracy: 0.8336166253549906 ç”»å‡ºloss å’Œ accuracyçš„æ›´æ–°è¿‡ç¨‹ï¼š lossï¼š accuracyï¼š ç”±äºŽFeatureæ•°é‡è¾ƒå¤§ï¼Œå°†æƒé‡å½±å“æœ€å¤§çš„featureè¾“å‡ºçœ‹çœ‹ï¼š 12345678910Other Rel &lt;18 spouse of subfamily RP: [7.11323764] Grandchild &lt;18 ever marr not in subfamily: [6.8321061] Child &lt;18 ever marr RP of subfamily: [6.77322397] Other Rel &lt;18 ever marr RP of subfamily: [6.76688406] Other Rel &lt;18 never married RP of subfamily: [6.37488958] Child &lt;18 spouse of subfamily RP: [5.97717831] United-States: [5.53932651] Grandchild 18+ spouse of subfamily RP: [5.42948497] United-States: [5.41543809] Mexico: [4.79920763] Codeå®Œæ•´æ•°æ®é›†ã€ä»£ç ç­‰ï¼Œæ¬¢è¿Žå…‰ä¸´å°é€æ˜ŽGitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222################## Data:2020-04-05# Author: Fred Lau# ML-Lee: HW2 : Binary Classification###########################################################import numpy as npimport csvimport sysimport matplotlib.pyplot as plt########################################################### prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_Train'X_test_fpath = './data/X_test'output_fpath = './logistic_output/output_logistic.csv'fpath = './logistic_output/logistic'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X. # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): # This function splits data into training set and development set. train_size = int(X.shape[0] * (1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)# Split data into train data and development datadev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In logistic model:\\n') f.write('Size of Training set: {}\\n'.format(train_size)) f.write('Size of development set: {}\\n'.format(dev_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n'.format(data_dim))np.random.seed(0)################################################################ useful functiondef _shuffle(X, Y): # This function shuffles two two list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize]def _sigmod(z): # Sigmod function can be used to calculate probability # To avoid overflow return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic function, parameterized by w and b # # Arguments: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probability of each row of X being positively labeled, shape = [batch_size, 1] return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This fucntion returns a truth value prediction for each row of X by logistic regression return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return accdef _cross_entropy_loss(y_pred, Y_label): # This function calculates the cross entropy of Y_pred and Y_label # # Argument: # y_pred: predictions, float vector # Y_label: truth labels, bool vector cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((1 - Y_label).T, np.log(1 - y_pred)) return cross_entropy[0][0]def _gradient(X, Y_label, w, b): # This function calculates the gradient of cross entropy # X, Y_label, shape = [batch_size, ] y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = - np.dot(X.T, pred_error) b_grad = - np.sum(pred_error) return w_grad, float(b_grad)######################################## training by logistic model# Initial weights and biasw = np.zeros((data_dim, 1))b = np.float(0.)w_grad_sum = np.full((data_dim, 1), 1e-8) # avoid divided by zerosb_grad_sum = np.float(1e-8)# Some parameters for trainingepoch = 20batch_size = 2**3learning_rate = 0.2# Keep the loss and accuracy history at every epoch for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Iterative trainingfor it in range(epoch): # Random shuffle at every epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for id in range(int(np.floor(train_size / batch_size))): X = X_train[id*batch_size: (id+1)*batch_size] Y = Y_train[id*batch_size: (id+1)*batch_size] # calculate gradient w_grad, b_grad = _gradient(X, Y, w, b) # adagrad gradient update w_grad_sum = w_grad_sum + w_grad**2 b_grad_sum = b_grad_sum + b_grad**2 w_ada = np.sqrt(w_grad_sum) b_ada = np.sqrt(b_grad_sum) w = w - learning_rate * w_grad / np.sqrt(w_grad_sum) b = b - learning_rate * b_grad / np.sqrt(b_grad_sum) # compute loss and accuracy of training set and development set at every epoch y_train_pred = _f(X_train, w, b) Y_train_pred = np.around(y_train_pred) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size) train_acc.append(_accuracy(Y_train_pred, Y_train)) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.around(y_dev_pred) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size) dev_acc.append(_accuracy(y_dev_pred, Y_dev))with open(fpath, 'a') as f: f.write('Training loss: {}\\n'.format(train_loss[-1])) f.write('Training accuracy: {}\\n'.format(train_acc[-1])) f.write('Development loss: {}\\n'.format(dev_loss[-1])) f.write('Development accuracy: {}\\n'.format(dev_acc[-1]))#################### Plotting Loss and accuracy curve# Loss curveplt.plot(train_loss, label='train')plt.plot(dev_loss, label='dev')plt.title('Loss')plt.legend()plt.savefig('./logistic_output/loss.png')plt.show()plt.plot(train_acc, label='train')plt.plot(dev_acc, label='dev')plt.title('Accuracy')plt.legend()plt.savefig('./logistic_output/acc.png')plt.show()################################## Predictpredictions = _predict(X_test, w, b)with open(output_fpath, 'w') as f: f.write('id, label\\n') for id, label in enumerate(predictions): f.write('{}, {}\\n'.format(id, label[0]))################################ Output the weights and biasind = (np.argsort(np.abs(w), axis=0)[::-1]).reshape(1, -1)with open(X_test_fpath) as f: content = f.readline().strip('\\n').split(',')content = content[1:]with open(fpath, 'a') as f: for i in ind[0, 0: 10]: f.write('{}: {}\\n'.format(content[i], w[i])) Generative ModelGenerative Model åŽŸç†éƒ¨åˆ†è§ è¿™ç¯‡åšå®¢ Prepare dataè¿™éƒ¨åˆ†å’ŒLogistic regressionä¸€æ ·ã€‚ åªæ˜¯ï¼Œå› ä¸ºgenerative modelæœ‰closed-form solutionï¼Œä¸éœ€è¦åˆ’åˆ†development setã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041# Prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './generative_output/output_{}.csv'fpath = './generative_output/generative'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)train_size = X_train.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In generative model:\\n') f.write('Size of training data: {}\\n'.format(train_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n\\n'.format(data_dim)) Useful functions1234567891011121314151617181920212223242526# Useful functionsdef _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8))def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function computes prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc Trainingå…¬å¼å†æŽ¨å¯¼è®¡ç®—å…¬å¼ï¼š $$ \\begin{equation}\\begin{aligned}P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\&=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z)\\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\end{aligned}\\end{equation} $$ è®¡ç®—zçš„è¿‡ç¨‹ï¼š é¦–å…ˆè®¡ç®—Prior Probabilityã€‚ å‡è®¾æ¨¡åž‹æ˜¯Gaussiançš„ï¼Œç®—å‡º $\\mu_1,\\mu_2 ,\\Sigma$ çš„closed-form solution ã€‚ æ ¹æ® $\\mu_1,\\mu_2,\\Sigma$ è®¡ç®—å‡º $w,b$ ã€‚ è®¡ç®—Prior Probabilityã€‚ ç¨‹åºä¸­ç”¨list comprehensionå¤„ç†è¾ƒç®€å•ã€‚ 1234# compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1]) è®¡ç®— $\\mu_1,\\mu_2 ,\\Sigma$ ï¼ˆGaussianï¼‰ $\\mu_0=\\frac{1}{C0} \\sum_{n=1}^{C0} x^{n} $ (Label=0) $\\mu_1=\\frac{1}{C1} \\sum_{n=1}^{C1} x^{n} $ (Label=0) $\\Sigma_0=\\frac{1}{C0} \\sum_{n=1}^{C0}\\left(x^{n}-\\mu^{}\\right)^{T}\\left(x^{n}-\\mu^{}\\right)$ (æ³¨æ„ ï¼šè¿™é‡Œçš„ $x^n,\\mu$ éƒ½æ˜¯è¡Œå‘é‡ï¼Œæ³¨æ„è½¬ç½®çš„ä½ç½®ï¼‰ $\\Sigma_1=\\frac{1}{C1} \\sum_{n=1}^{C1}\\left(x^{n}-\\mu^{}\\right)^{T}\\left(x^{n}-\\mu^{}\\right)$ $\\Sigma=(C0 \\times\\Sigma_0+C1\\times\\Sigma_1)/(C0+C1)$ (shared covariance) 12345678910111213141516mean_0 = np.mean(X_train_0, axis=0)mean_1 = np.mean(X_train_1, axis=0)# compute in-class covariancecov_0 = np.zeros(shape=(data_dim, data_dim))cov_1 = np.zeros(shape=(data_dim, data_dim))for x in X_train_0: # (D,1)@(1,D) np.matmul(np.transpose([x]), x) cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# shared covariancecov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train.shape[0]) è®¡ç®— $w,b$ åœ¨ è¿™ç¯‡åšå®¢ä¸­çš„ç¬¬2å°èŠ‚ä¸­çš„å…¬å¼æŽ¨å¯¼ä¸­ï¼Œ $x^n,\\mu$ éƒ½æ˜¯åˆ—å‘é‡ï¼Œå…¬å¼å¦‚ä¸‹ï¼š $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ ä½†æ˜¯ ï¼Œä¸€èˆ¬æˆ‘ä»¬åœ¨å¤„ç†çš„æ•°æ®é›†ï¼Œ$x^n,\\mu$ éƒ½æ˜¯è¡Œå‘é‡ã€‚æŽ¨å¯¼è¿‡ç¨‹ç›¸åŒï¼Œå…¬å¼å¦‚ä¸‹ï¼š ï¼ˆä¸»è¦æ³¨æ„è½¬ç½®å’ŒçŸ©é˜µä¹˜ç§¯é¡ºåºï¼‰ $$ z=x\\cdot \\Sigma^{-1}\\left(\\mu^{1}-\\mu^{2}\\right)^{T} -\\frac{1}{2} \\mu^{1}\\Sigma^{-1}\\left(\\mu^{1}\\right)^{T}+\\frac{1}{2}\\mu^{2}\\Sigma^{-1} \\left(\\mu^{2}\\right)^{T} +\\ln \\frac{N_{1}}{N_{2}} $$ $w=\\Sigma^{-1}\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\qquad b=-\\frac{1}{2} \\mu^{1}\\Sigma^{-1}\\left(\\mu^{1}\\right)^{T}+\\frac{1}{2}\\mu^{2}\\Sigma^{-1} \\left(\\mu^{2}\\right)^{T} +\\ln \\frac{N_{1}}{N_{2}}$ ä½†æ˜¯ï¼Œåæ–¹å·®çŸ©é˜µçš„é€†æ€Žä¹ˆæ±‚å‘¢ï¼Ÿ numpyä¸­æœ‰ç›´æŽ¥æ±‚é€†çŸ©é˜µçš„æ–¹æ³•(np.linalg.inv)ï¼Œä½†å½“è¯¥çŸ©é˜µæ˜¯nearly singularï¼Œæ˜¯å¥‡å¼‚çŸ©é˜µæ—¶ï¼Œå°±ä¼šæŠ¥é”™ã€‚ è€Œæˆ‘ä»¬çš„åæ–¹å·®çŸ©é˜µï¼ˆ510*510ï¼‰å¾ˆå¤§ï¼Œå¾ˆéš¾ä¿è¯ä»–ä¸æ˜¯å¥‡å¼‚çŸ©é˜µã€‚ äºŽæ˜¯ï¼Œæœ‰ä¸€ä¸ª ç‰›é€¼ å¼ºå¤§çš„æ•°å­¦æ–¹æ³•ï¼Œå«SVD(singular value decomposition, å¥‡å¼‚å€¼åˆ†è§£) ã€‚ åŽŸç†æ­¥éª¤æˆ‘â€¦â€¦è¿˜æ²¡æœ‰å®Œå…¨æžæ¸…æ¥šQAQï¼ˆå…ˆæŒ–ä¸ªå‘ï¼‰[1] åˆ©ç”¨SVDï¼Œå¯ä»¥å°†ä»»ä½•ä¸€ä¸ªçŸ©é˜µï¼ˆå³ä½¿æ˜¯å¥‡å¼‚çŸ©é˜µï¼‰ï¼Œåˆ†ç•Œæˆ $A=u s v^T$ çš„å½¢å¼ï¼šå…¶ä¸­u,véƒ½æ˜¯æ ‡å‡†æ­£äº¤çŸ©é˜µï¼Œsæ˜¯å¯¹è§’çŸ©é˜µã€‚ï¼ˆnumpy.linalg.svdæ–¹æ³•å®žçŽ°äº†SVDï¼‰ å¯ä»¥åˆ©ç”¨SVDæ±‚çŸ©é˜µçš„ä¼ªé€† $A=u s v^T$ u,væ˜¯æ ‡å‡†æ­£äº¤çŸ©é˜µï¼Œå…¶é€†çŸ©é˜µç­‰äºŽå…¶è½¬ç½®çŸ©é˜µ sæ˜¯å¯¹è§’çŸ©é˜µï¼Œå…¶â€é€†çŸ©é˜µâ€œï¼ˆæ³¨æ„sçŸ©é˜µçš„å¯¹è§’ä¹Ÿå¯èƒ½æœ‰0å…ƒç´ ï¼‰ å°†éž0å…ƒç´ å–å€’æ•°å³å¯ã€‚ $A^{-1}=v s^{-1} u$ è®¡ç®— $w,b$ çš„ä»£ç å¦‚ä¸‹ï¼š 12345678910111213141516# compute weights and bias# Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.# cov = u@s@vh# cov_inv = dot(vh.T * 1 / s, u.T)u, s, vh = np.linalg.svd(cov, full_matrices=False)s_inv = s # s_inv avoid &lt;1e-8for i in range(s.shape[0]): if s[i] &lt; (1e-8): break s_inv[i] = 1./s[i]cov_inv = np.matmul(vh.T * s_inv, u.T)w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))b = (-0.5) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (0.5) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) Accuracy accuracyç»“æžœï¼š 1Training accuracy: 0.8756450899439694 ä¹Ÿå°†æƒé‡è¾ƒå¤§çš„featureè¾“å‡ºçœ‹çœ‹ï¼š 12345678910age: [-0.51867291] Masters degree(MA MS MEng MEd MSW MBA): [-0.49912643] Spouse of householder: [0.49786805]weeks worked in year: [-0.44710924] Spouse of householder: [-0.43305697]capital gains: [-0.42608727]dividends from stocks: [-0.41994666] Doctorate degree(PhD EdD): [-0.39310961]num persons worked for employer: [-0.37345994] Prof school degree (MD DDS DVM LLB JD): [-0.35594107] Codeå…·ä½“æ•°æ®é›†å’Œä»£ç ï¼Œæ¬¢è¿Žå…‰ä¸´å°é€æ˜ŽGitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import numpy as npnp.random.seed(0)############################################### Prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './generative_output/output_{}.csv'fpath = './generative_output/generative'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)train_size = X_train.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In generative model:\\n') f.write('Size of training data: {}\\n'.format(train_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n\\n'.format(data_dim))######################### Useful functionsdef _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8))def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function computes prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc######################## Generative Model: closed-form solution, can be computed directly# compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis=0)mean_1 = np.mean(X_train_1, axis=0)# compute in-class covariancecov_0 = np.zeros(shape=(data_dim, data_dim))cov_1 = np.zeros(shape=(data_dim, data_dim))for x in X_train_0: # (D,1)@(1,D) np.matmul(np.transpose([x]), x) cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# shared covariancecov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train.shape[0])# compute weights and bias# Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.# cov = u@s@vh# cov_inv = dot(vh.T * 1 / s, u.T)u, s, vh = np.linalg.svd(cov, full_matrices=False)s_inv = s # s_inv avoid &lt;1e-8for i in range(s.shape[0]): if s[i] &lt; (1e-8): break s_inv[i] = 1./s[i]cov_inv = np.matmul(vh.T * s_inv, u.T)w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))b = (-0.5) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (0.5) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0])# compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)with open(fpath, 'a') as f: f.write('\\nTraining accuracy: {}\\n'.format(_accuracy(Y_train_pred, Y_train)))# Predictpredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id, label\\n') for i, label in enumerate(predictions): f.write('{}, {}\\n'.format(i, label))# Output the most significant weightwith open(X_test_fpath) as f: content = f.readline().strip('\\n').split(',')content = content[1:]ind = np.argsort(np.abs(np.concatenate(w)))[::-1]with open(fpath, 'a')as f: for i in ind[0:10]: f.write('{}: {}\\n'.format(content[i], w[i])) Reference SVDåŽŸç†ï¼Œå¾…è¡¥å……","link":"/2020/04/14/ml-lee-hw2/"},{"title":"ã€ŒPyTorchã€ï¼š3-Data And Data Processing","text":"PyTorchæ¡†æž¶å­¦ä¹ ã€‚ è¿™ç¯‡æ–‡ç« ä»‹ç»äº†æ·±åº¦å­¦ä¹ ä¸­çš„æ•°æ®å’Œæ•°æ®å¤„ç†çš„å¸¸ç”¨ç±»DataSetå’ŒDataLoaderã€‚ colabç¬”è®°ï¼šData And Data Processing Data-Fashion MNISTWhy Study A Dataset?Data is the primary ingredient of deep learning. ã€Dataæ˜¯deep learningçš„åŽŸææ–™ã€‘ Data focused considerations: Who created the dataset?ã€è°æ”¶é›†çš„æ•°æ®é›†ã€‘ How was the dataset created?ã€æ•°æ®é›†æ˜¯å¦‚ä½•æ”¶é›†çš„ã€‘ What transformations were used?ã€æ•°æ®è¿ç”¨äº†å“ªäº›å˜æ¢ã€‘ What intent does the dataset have?ã€æ•°æ®é›†çš„æ„å›¾æ˜¯ä»€ä¹ˆã€‘ Possible unintentional consequences?ã€è¿˜å¯èƒ½æœ‰ä»€ä¹ˆå…¶ä»–ç»“æžœå—ã€‘ Is the dataset biased?ã€æ•°æ®é›†æ˜¯å¦æ˜¯biasedã€‘ Are there ethical issues with the dataset?ã€æ•°æ®é›†ä¼šå¼•èµ·é“å¾·é—®é¢˜å—ã€‘ What Is The MNIST Dataset?The MNIST dataset, Modified National Institute of Standards and Technology database, is a famous dataset of handwritten digits that is commonly used for training image processing systems for machine learning. NIST stands for National Institute of Standards and Technology. ã€MNIST, å…¨ç§°Modified National Institute of Standards and Technologyã€‚è‘—åçš„æ‰‹å†™æ•°æ®é›†ï¼Œç”¨äºŽè®­ç»ƒå›¾åƒå¤„ç†ç³»ç»Ÿã€‚ã€‘ MNIST is famous because of how often the dataset is used. Itâ€™s common for two reasons: Beginners use it because itâ€™s easy Researchers use it to benchmark (compare) different models. ã€MNISTç®€å•ï¼›å…¶æ¬¡ç ”ç©¶è€…å¸¸å¸¸ç”¨MNISTä½œä¸ºå…¶ä»–æ¨¡åž‹çš„åŸºå‡†ã€‘ The dataset consists of 70,000 images of hand written digits with the following split: 60,000 training images 10,000 testing images ã€MNISTçš„ç»„æˆï¼Œ60000ä¸ªtraining å›¾åƒï¼Œ10000ä¸ªtestingå›¾åƒã€‘ MNIST has been so widely used, and image recognition tech has improved so much that the dataset is considered to be too easy. This is why the Fashion-MNIST dataset was created. ã€å› ä¸ºMNISTæ•°æ®é›†å¤ªè¿‡ç®€å•äº†ï¼Œå› æ­¤Fashion-MNISTæ•°æ®é›†å‡ºçŽ°äº†ã€‘ What Is Fashion-MNIST?Fashion-MNIST as the name suggests is a dataset of fashion items. Specifically, the dataset has the following ten classes of fashion items: ã€Fashion-MNISTæ•°æ®é›†ç”±è®¸å¤šfashionçš„ç‰©ä»¶ç»„æˆï¼Œç‰©ä»¶ç±»åˆ«å¦‚ä¸‹ã€‚ã€‘ Index Label 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot Fashion-MNIST is based on the assortment on Zalandoâ€™s website. Zalando is a German based multi-national fashion commerce company that was founded in 2008. ã€Fashion-MNISTæ•°æ®é›†ä¸­çš„æ•°æ®éƒ½æ˜¯æ¥ç€Zalandoç½‘å€ä¸Šå”®å–çš„æ ·å›¾ï¼ŒZalandoåˆ›ç«‹äºŽ2008å¹´ï¼Œæ˜¯ä¸€å®¶å¾·å›½è·¨å›½æ—¶å°šå…¬å¸ã€‘ Weâ€™ll see the specific ways that Fashion-MNIST mirrors the original dataset in the paper, but one thing we have already seen is the number of classes. MNIST â€“ has 10 classes (one for each digit 0-9) Fashion-MNIST â€“ has 10 classes (this is intentional) ã€Fashion-MNISTå’ŒMNISTæ˜¯é•œåƒå¯¹åº”çš„ï¼Œæ¯”å¦‚ä»–ä»¬éƒ½æœ‰10ä¸ªç±»åˆ«ã€‘ How Fashion-MNIST Was BuiltUnlike the MNIST dataset, the fashion set wasnâ€™t hand-drawn, but the images in the dataset are actual images from Zalandoâ€™s website. ã€Fashion-MNNSTä¸­æ‰€æœ‰çš„å›¾åƒéƒ½æ¥è‡ªZalandoçš„å®˜ç½‘çš„å›¾ç‰‡ï¼Œå†é€šè¿‡å¤šç§å˜æ¢ï¼Œå˜æˆ28*28çš„å›¾åƒã€‘ Extract, Transform, Load(ETL)There are four general steps that weâ€™ll be following as we move through this project: ã€ä¸€èˆ¬åˆ†ä¸º4æ­¥ï¼šå‡†å¤‡æ•°æ®ï¼›æž„å»ºæ¨¡åž‹ï¼›è®­ç»ƒæ¨¡åž‹ï¼›åˆ†æžç»“æžœã€‘ Prepare the data Build the model Train the model Analyze the modelâ€™s results The ETL ProcessIn this post, weâ€™ll kick things off by preparing the data. To prepare our data, weâ€™ll be following what is loosely known as an ETL process. ã€å‡†å¤‡æ•°æ®çš„è¿‡ç¨‹ä¸€èˆ¬åˆå«ETLè¿‡ç¨‹ï¼šæå–ã€è½¬åŒ–ã€è£…è½½ã€‘ Extract data from a data source.ã€ä»Žæ•°æ®æºæå–æ•°æ®ã€‘ Transform data into a desirable format.ã€è½¬åŒ–ä¸ºä¾¿äºŽå¤„ç†çš„æ ¼å¼ã€‘ Load data into a suitable structure.ã€è£…è½½æ•°æ®ï¼Œä¾¿äºŽè¯»å–ã€‘ PyTorchåŒ…çš„ä¸»è¦ç»„æˆï¼š Package Description torch PyTorchçš„é¡¶å±‚åŒ…å’Œtensoråº“ torch.nn åŒ…å«æž„å»ºNNçš„æ¨¡åž‹å’Œæ‰©å±•ç±» torch.autograd PyTorchä¸­æ”¯æŒçš„Tensoræ“ä½œ torch.nn.functional åŒ…å«æž„å»ºNNçš„å‡½æ•°æŽ¥å£ï¼Œåƒloss function, activation fucntion, convolution operation torch.optim åŒ…å«æ ‡å‡†çš„ä¼˜åŒ–ï¼ŒåƒSGD, Adam torch.utils åŒ…å«å®žç”¨ç±»ï¼Œåƒæ•°æ®é›†ï¼Œæ•°æ®è£…è½½å™¨ï¼Œæ–¹ä¾¿æ•°æ®é¢„å¤„ç† torchvision æä¾›è‘—åçš„æ•°æ®é›†ï¼Œæ¨¡åž‹æž¶æž„å’Œè®¡ç®—æœºè§†è§‰å›¾åƒè½¬æ¢ torchvision.transforms: An interface that contains common transforms for image processing. ã€ä¸€ä¸ªåŒ…å«å›¾åƒè½¬æ¢ï¼ˆç”¨äºŽå›¾åƒå¤„ç†ï¼‰çš„æŽ¥å£ã€‚ã€‘ å¸¸ç”¨åŒ…ï¼š pandas:https://www.pypandas.cn/ Pandasæ˜¯ä¸€ä¸ªå¼ºå¤§çš„åˆ†æžç»“æž„åŒ–æ•°æ®çš„å·¥å…·é›†ï¼›å®ƒçš„ä½¿ç”¨åŸºç¡€æ˜¯Numpyï¼ˆæä¾›é«˜æ€§èƒ½çš„çŸ©é˜µè¿ç®—ï¼‰ï¼›ç”¨äºŽæ•°æ®æŒ–æŽ˜å’Œæ•°æ®åˆ†æžï¼ŒåŒæ—¶ä¹Ÿæä¾›æ•°æ®æ¸…æ´—åŠŸèƒ½ã€‚ NumPy: https://www.numpy.org.cn/user/setting-up.html NumPyæ˜¯Pythonä¸­ç§‘å­¦è®¡ç®—çš„åŸºç¡€åŒ…ã€‚å®ƒæ˜¯ä¸€ä¸ªPythonåº“ï¼Œæä¾›å¤šç»´æ•°ç»„å¯¹è±¡ï¼Œå„ç§æ´¾ç”Ÿå¯¹è±¡ï¼ˆå¦‚æŽ©ç æ•°ç»„å’ŒçŸ©é˜µï¼‰ï¼Œä»¥åŠç”¨äºŽæ•°ç»„å¿«é€Ÿæ“ä½œçš„å„ç§APIï¼Œæœ‰åŒ…æ‹¬æ•°å­¦ã€é€»è¾‘ã€å½¢çŠ¶æ“ä½œã€æŽ’åºã€é€‰æ‹©ã€è¾“å…¥è¾“å‡ºã€ç¦»æ•£å‚…ç«‹å¶å˜æ¢ã€åŸºæœ¬çº¿æ€§ä»£æ•°ï¼ŒåŸºæœ¬ç»Ÿè®¡è¿ç®—å’Œéšæœºæ¨¡æ‹Ÿç­‰ç­‰ã€‚ Matplotlibï¼šhttps://www.matplotlib.org.cn/ Matplotlib æ˜¯ä¸€ä¸ª Python çš„ 2Dç»˜å›¾åº“ï¼Œå®ƒä»¥å„ç§ç¡¬æ‹·è´æ ¼å¼å’Œè·¨å¹³å°çš„äº¤äº’å¼çŽ¯å¢ƒç”Ÿæˆå‡ºç‰ˆè´¨é‡çº§åˆ«çš„å›¾å½¢ã€‚Matplotlibå¯ç”¨äºŽPythonè„šæœ¬ï¼ŒPythonå’ŒIPython Shellã€Jupyterç¬”è®°æœ¬ï¼ŒWebåº”ç”¨ç¨‹åºæœåŠ¡å™¨å’Œå››ä¸ªå›¾å½¢ç”¨æˆ·ç•Œé¢å·¥å…·åŒ…ã€‚ ä¸ºäº†ç®€å•ç»˜å›¾ï¼Œè¯¥ pyplot æ¨¡å—æä¾›äº†ç±»ä¼¼äºŽMATLABçš„ç•Œé¢ï¼Œå°¤å…¶æ˜¯ä¸ŽIPythonç»“åˆä½¿ç”¨æ—¶ã€‚ å¯¹äºŽé«˜çº§ç”¨æˆ·ï¼Œæ‚¨å¯ä»¥é€šè¿‡é¢å‘å¯¹è±¡çš„ç•Œé¢æˆ–MATLABç”¨æˆ·ç†Ÿæ‚‰çš„ä¸€ç»„åŠŸèƒ½æ¥å®Œå…¨æŽ§åˆ¶çº¿åž‹ï¼Œå­—ä½“å±žæ€§ï¼Œè½´å±žæ€§ç­‰ã€‚ pdb æ˜¯Pythonçš„è°ƒè¯•å™¨ã€‚ Preparing Our Data Extract â€“ Get the Fashion-MNIST image data from the source.ã€èŽ·å¾—Fashion-MNISTæ•°æ®é›†ã€‘ Transform â€“ Put our data into tensor form.ã€è½¬æ¢ï¼šå°†æˆ‘ä»¬çš„æ•°æ®è½¬æ¢ä¸ºtensorã€‘ Load â€“ Put our data into an object to make it easily accessible.ã€è£…è½½ï¼šèšåˆæ•°æ®ä¸ºä¸€ä¸ªå¯¹è±¡ï¼Œæ–¹ä¾¿èŽ·å–ã€‘ For these purposes, PyTorch provides us with two classes: ã€PyTorchä¸ºå¤„ç†æ•°æ®æ‰€æä¾›çš„ä¸¤ä¸ªç±»ã€‘ Class Description torch.utils.data.Dataset æ•°æ®é›†çš„æŠ½è±¡ç±» torch.utils.data.DataLoader æ‰“åŒ…æ•°æ®é›†ï¼Œæä¾›è®¿é—®åº•å±‚æ•°æ®çš„æŽ¥å£ã€‚ To create a custom dataset using PyTorch, we extend the Dataset class by creating a subclass that implements these required methods. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object. ã€åˆ›å»ºæ•°æ®é›†å¿…é¡»ç»§æ‰¿Datasetç±»ï¼Œç»§æ‰¿çš„å­ç±»ä½œä¸ºå‚æ•°ä¼ é€’ç»™DataLoaderå¯¹è±¡ã€‘ All subclasses of the Dataset class must override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive. ã€Datesetçš„å­ç±»å¿…é¡»é‡å†™__len__ æ–¹æ³•ï¼ˆè¡¨ç¤ºæ•°æ®é›†çš„å¤§å°ï¼‰ï¼Œé‡å†™__getitem__ ï¼ˆæŒ‰ç´¢å¼•èŽ·å¾—ç‰¹å®šæ•°æ®ï¼‰ã€‘ PyTorch Torchvision PackageThe torchvision package, gives us access to the following resources: ã€torchvisionä¸»è¦æä¾›ä¸€äº›å…¸åž‹æ•°æ®é›†ã€æ¨¡åž‹ã€è½¬æ¢ã€å·¥å…·ã€‘ Datasets (like MNIST and Fashion-MNIST) Models (like VGG16) Transforms Utils The PyTorch FashionMNIST dataset simply extends the MNIST dataset and overrides the urls. ã€FashionMNISTæ•°æ®é›†ç»§æ‰¿äº†MNISTæ•°æ®é›†ï¼Œåªé‡å†™äº†æ•°æ®é›†çš„urlã€‘ 12345678910111213141516171819202122class FashionMNIST(MNIST): &quot;&quot;&quot;`Fashion-MNIST &lt;https://github.com/zalandoresearch/fashion-mnist&gt;`_ Dataset. Args: root (string): Root directory of dataset where ``processed/training.pt`` and ``processed/test.pt`` exist. train (bool, optional): If True, creates dataset from ``training.pt``, otherwise from ``test.pt``. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. &quot;&quot;&quot; urls = [ 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz', 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz', 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz', 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz', ] Letâ€™s see now how we can take advantage of torchvision. PyTorch Dataset ClassTo get an instance of the FashionMNIST dataset using torchvision, we just create one like so: 12345678train_set = torchvision.datasets.FashionMNIST( root='./data' ,train=True ,download=True ,transform=transforms.Compose([ transforms.ToTensor() ])) We specify the following arguments: Parameter Description root The location on disk where the data is located.ã€æ•°æ®é›†çš„ä½ç½®ã€‘ train If the dataset is the training set.ã€æ˜¯å¦æ˜¯è®­ç»ƒæ•°æ®é›†ã€‘ download If the data should be downloaded.ã€å¦‚æžœæ•°æ®é›†ä¸å­˜åœ¨ï¼Œæ˜¯å¦ä¸‹è½½ã€‘ transform A composition of transformations that should be performed on the dataset elements.ã€å˜æ¢çš„ç»„åˆã€‘ PyTorch DataLoader ClassTo create a DataLoader wrapper for our training set, we do it like this: 1234train_loader = torch.utils.data.DataLoader(train_set ,batch_size=1000 ,shuffle=True) ã€DataLoaderï¼Œèšåˆæ•°æ®é›†å’Œå–æ ·å™¨ï¼Œæä¾›æ•°æ®é›†çš„è¿­ä»£å™¨ã€‘ We just pass train_set as an argument. Now, we can leverage the loader for tasks that would otherwise be pretty complicated to implement by hand: ã€DataLoaderèƒ½è®©ä¸€äº›æ‰‹åŠ¨å®žçŽ°å¤æ‚çš„ç®€å•åŒ–ã€‘ batch_size (1000 in our case) shuffle (True in our case) num_workers (Default is 0 which means the main process will be used) PyTorch Datasets And DataLoadersWorking With The Training SetIn this post, we are going to see how we can work with the dataset and the data loader objects that we created in the previous post. PyTorch Dataset:Suppose we want to see the labels for each image. This can be done like so: 123# Starting with torchvision 0.2.2&gt; train_set.targetstensor([9, 0, 0, ..., 3, 0, 5]) If we want to see how many of each label exists in the dataset, we can use the PyTorch bincount() function like so: Class Imbalance: Balanced And Unbalanced DatasetsThis shows us that the Fashion-MNIST dataset is uniform with respect to the number of samples in each class. This means we have 6000 samples for each class. As a result, this dataset is said to be balanced. If the classes had a varying number of samples, we would call the set an unbalanced dataset. ã€Fashion-MNISTæ•°æ®é›†æ˜¯å‡åŒ€åˆ†å¸ƒï¼Œå³æ¯ä¸ªclassçš„samplesæ•°é‡ç›¸åŒã€‚å‡åŒ€åˆ†å¸ƒçš„æ•°æ®é›†è¢«ç§°ä¸ºæ˜¯balancedã€‚ã€‘ To read more about the ways to mitigate unbalanced datasets in deep learning, see this paper: A systematic study of the class imbalance problem in convolutional neural networks. Accessing Data In The Training Setã€èŽ·å¾—æ•°æ®é›†ä¸­çš„æ•°æ®ï¼šå…ˆå°†train_set ä¼ é€’ç»™Pythonå‡½æ•°iter() ç”Ÿæˆè¿­ä»£å™¨ï¼Œå†å°†è¿­ä»£å™¨ä¼ é€’ç»™å†…ç½®å‡½æ•°next ç”¨æ¥è¿­ä»£ã€‘ iter(object) ï¼š objectï¼šæ”¯æŒè¿­ä»£çš„é›†åˆå¯¹è±¡ è¿”å›žå€¼ï¼šè¿­ä»£å™¨å¯¹è±¡ next(iterable[, default]) : å¸¸å’Œiter() ä¸€åŒä½¿ç”¨ è¿”å›žè¿­ä»£å™¨ä¸­çš„ä¸‹ä¸€ä¸ªé¡¹ç›® To access an individual element from the training set, we first pass the train_set object to Pythonâ€™s iter() built-in function, which returns an object representing a stream of data. With the stream of data, we can use Python built-in next() function to get the next data element in the stream of data. After passing the sample to the len() function, we can see that the sample contains two items, and this is because the dataset contains image-label pairs. Each sample we retrieve from the training set contains the image data as a tensor and the corresponding label as a tensor. ã€train_setä¸­çš„sampleæ˜¯ä¸€ä¸ªimage-labelå¯¹ï¼Œå› æ­¤sampleçš„lenä¸º2ã€‘ 12345image,label = sampleprint(type(image))print(type(label))&lt;class 'torch.Tensor'&gt;&lt;class 'int'&gt; Working With Batches Of DataWeâ€™ll start by creating a new data loader with a smaller batch size of 10 so itâ€™s easy to demonstrate whatâ€™s going on: 123456display_loader = torch.utils.data.DataLoader( train_set, batch_size=10)batch = next(iter(display_loader))len(batch)2 We get a batch from the loader in the same way that we saw with the training set. We use the iter() and next() functions. ã€ç›¸è¾ƒäºŽä½¿ç”¨training setï¼Œä»ŽDataLoaderä¸­å¯ä»¥å–å‡ºa batch of dataã€‘ Checking the length of the returned batch, we get 2 just like we did with the training set. Letâ€™s unpack the batch and take a look at the two tensors and their shapes: 12345images, labels = batchprint('types:', type(images), type(labels))print('shapes:', images.shape, labels.shape)types: &lt;class 'torch.Tensor'&gt; &lt;class 'torch.Tensor'&gt;shapes: torch.Size([10, 1, 28, 28]) torch.Size([10]) The size of each dimension in the tensor that contains the image data is defined by each of the following values: (batch size, number of color channels, image height, image width) 1234567grid = torchvision.utils.make_grid(images, nrow=10)plt.figure(figsize=(15, 15))plt.imshow(np.transpose(grid, (1,2,0)))//grid = torchvision.utils.make_grid(images, nrow=10)plt.figure(figsize=(15,15))plt.imshow(grid.permute(1,2,0)) plt.imshow(X) : X=(M,N,3) RGB2Då›¾ è€Œgridè¿”å›žçš„åº”è¯¥æ˜¯ï¼ˆC-H-Wï¼‰ï¼Œæ‰€ä»¥è¦ç½®æ¢ä¸€ä¸‹axesã€‚ç½®æ¢ä½¿ç”¨np.transpose(grid, (1,2,0))æˆ–grid.permute(1,2,0) Plot Images Using PyTorch DataLoaderHere is another was to plot the images using the PyTorch DataLoader.","link":"/2021/02/26/pytorch-data/"},{"title":"ã€ŒPyTorchã€ï¼š1-PyTorch Explained","text":"PyTorchæ¡†æž¶å­¦ä¹ ã€‚ æœ¬ç¯‡æ–‡ç« ä¸»è¦ä»‹ç»PyTorchçš„ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€‚ PyTorch Explained: Python DNN APIã€PyTorchï¼Œæ˜¯ä½¿ç”¨Pythonæ¥æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªAPIã€‚ã€‘ PyTorch torch.Tensor objects that are created from NumPy ndarray objects, share memory. This makes the transition between PyTorch and NumPy very cheap from a performance perspective. With PyTorch tensors, GPU support is built-in. Itâ€™s very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system. ã€PyTorchåŽŸç”Ÿæ”¯æŒGPUã€‘ A Brief HistoryThe initial release of PyTorch was in October of 2016, and before PyTorch was created, there was and still is, another framework called Torch. Torch is a machine learning framework thatâ€™s been around for quite a while and is based on the Lua programming language. The connection between PyTorch and this Lua version, called Torch, exists because many of the developers who maintain the Lua version are the individuals who created PyTorch. PyTorch is that it was created and is maintained by Facebook. This is because Soumith Chintala worked at Facebook AI Research when PyTorch was created (still does at the time of this writing). However, there are many other companies with a vested interest in PyTorch. Deep Learning With PyTorchPyTorchçš„åŒ…å’Œä¸»è¦ç»„æˆï¼š Package Description torch PyTorchçš„é¡¶å±‚åŒ…å’Œtensoråº“ torch.nn åŒ…å«æž„å»ºNNçš„æ¨¡åž‹å’Œæ‰©å±•ç±» torch.autograd PyTorchä¸­æ”¯æŒçš„Tensoræ“ä½œ torch.nn.functional åŒ…å«æž„å»ºNNçš„å‡½æ•°æŽ¥å£ï¼Œåƒloss function, activation fucntion, convolution operation torch.optim åŒ…å«æ ‡å‡†çš„ä¼˜åŒ–ï¼ŒåƒSGD, Adam torch.utils åŒ…å«å®žç”¨ç±»ï¼Œåƒæ•°æ®é›†ï¼Œæ•°æ®è£…è½½å™¨ï¼Œæ–¹ä¾¿æ•°æ®é¢„å¤„ç† torchvision æä¾›è‘—åçš„æ•°æ®é›†ï¼Œæ¨¡åž‹æž¶æž„å’Œè®¡ç®—æœºè§†è§‰å›¾åƒè½¬æ¢ Why use PyTorch for Deep Learning ? PyTorch is thin and stays out of the way! PyTorch is as close as it gets to the real thing! Investing In PyTorch As A Deep Learning Framework To optimize neural networks, we need to calculate derivatives, and to do this computationally, deep learning frameworks use what are called computational graphs. è®¡ç®—å›¾ã€‚ Computational graphs are used to graph the function operations that occur on tensors inside neural networks. è®¡ç®—å›¾ç”¨æ¥ç»˜åˆ¶NNä¸­å†…éƒ¨å¼ é‡å‘ç”Ÿçš„å‡½æ•°è¿ç®—ã€‚ These graphs are then used to compute the derivatives needed to optimize the neural network. PyTorch uses a computational graph that is called a dynamic computational graph. This means that the graph is generated on the fly as the operations are created. è®¡ç®—å›¾ç”¨æ¥è®¡ç®—åå¾®åˆ†ä¼˜åŒ–NNã€‚PyTorchä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾ï¼Œå›¾æ˜¯å³æ—¶åˆ›å»ºçš„ã€‚ CUDA Explained - Why Deep Learning Uses GPUs In this post, we are going to introduce CUDA at a high-level. The goal of this post is to help beginners understand what CUDA is and how it fits in with PyTorch, and more importantly, why we even use GPUs in neural network programming anyway. Graphics Processing Unit(GPU) To understand CUDA, we need to have a working knowledge of graphics processing units (GPUs). A GPU is a processor that is good at handling specialized computations. ã€GPUæ“…é•¿å¤„ç†æŸç§ç‰¹å®šçš„è®¡ç®—ã€‘ This is in contrast to a central processing unit (CPU), which is a processor that is good at handling general computations. ã€CPUæ“…é•¿å¤„ç†general computations.ã€‘ Parallel Computing Parallel computing is a type of computation where by a particular computation is broken into independent smaller computations that can be carried out simultaneously. The resulting computations are then recombined, or synchronized, to form the result of the original larger computation. Neural Networks Are Embarrassingly ParallelIn parallel computing, an embarrassingly parallel task is one where little or no effort is needed to separate the overall task into a set of smaller tasks to be computed in parallel. Tasks that embarrassingly parallel are ones where itâ€™s easy to see that the set of smaller tasks are independent with respect to each other. Nvidia Hardware(GPU) And Software(CUDA) Nvidia is a technology company that designs GPUs, and they have created CUDA as a software platform that pairs with their GPU hardware making it easier for developers to build software that accelerates computations using the parallel processing power of Nvidia GPUs. An Nvidia GPU is the hardware that enables parallel computations, while CUDA is a software layer that provides an API for developers. GPUæ˜¯èƒ½å¹³è¡Œè¿ç®—çš„ç¡¬ä»¶ï¼Œè€ŒCUDAæ˜¯ä¸ºå¼€å‘è€…æä¾›çš„ä¸Šå±‚APIã€‚ PyTorch Comes With CUDAOne of the benefits of using PyTorch, or any other neural network API is that parallelism comes baked into the API. This means that as neural network programmers, we can focus more on building neural networks and less on performance issues. å…³æ³¨æ€Žä¹ˆæž„å»ºNNï¼Œè€Œä¸æ˜¯ä¸€ä¸‹performance issues. Now, if we wanted to work on the PyTorch core development team or write PyTorch extensions, it would probably be useful to know how to use CUDA directly. After all, PyTorch is written in all of these: Python C++ CUDA","link":"/2020/10/19/pytorch-introduction/"},{"title":"ã€ŒPythonã€ï¼šModule &amp; Method","text":"é•¿æœŸè®°å½•å¸–ï¼šå…³äºŽé‡åˆ°è¿‡çš„é‚£äº›Python çš„Packets &amp; Module &amp; Method &amp; Attributeã€‚ä¸­è‹±è®°å½•ã€‚ Trickylist comprehension List comprehension provides a concise way to create lists. e.g. : squares = [x**2 for x in range(10)] A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it. e.g. : Double loop: [(x,y) for x in [1,2,3] for y in [3,1,4] if x != y] è¾“å‡º7ä¸ª e.g. : (Using zip() to loop together): [(x, y) for x,y in zip([1,2,3], [3,1,4]) if x!=y] è¾“å‡º2ä¸ª Python-Build functionprint print(*objects, sep=â€™ â€˜, end=â€™\\nâ€™, file=sys.stdout) len Return the length(the number of items) of an object. str.format() å­—ç¬¦ä¸²æ ¼å¼åŒ– eg: â€œ{} {}â€.format(â€œHelloâ€,â€World) â€˜Hello Worldâ€™ zip(*iterables) Make an iterator that aggregatesã€èšé›†ã€‘ elements from each of the iterales. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. ã€è¿”å›žä¸€ä¸ªå…ƒç»„çš„è¿­ä»£å™¨ã€‘ ä½¿ç”¨zipå¯ä»¥åŒæ—¶å¯¹å¤šä¸ªè¿­ä»£å™¨è¿›è¡Œè¿­ä»£ enumerate Enumerate is a built-in funciton of Python. It allows us to loop over something and have an automatic counter. e.g. for counter, value in enumerate(some_list): print(counter, value) e.g. : an optional argument: tell enumerate from where to start the index. for c, value in enumerate(my_list, 1): print(c, value) with open(path) as f: ç”±äºŽè¯»å†™æ–‡ä»¶éƒ½å¯èƒ½äº§ç”ŸIOErrorï¼Œä¸€æ—¦å‡ºé”™ï¼ŒåŽé¢çš„f.close()å°±ä¸ä¼šè°ƒç”¨ã€‚ ç”¨tryâ€¦â€¦finallyæ¥å®žçŽ°ï¼Œæ¯”è¾ƒéº»çƒ¦ã€‚ try: â€‹ f = open(path, â€˜râ€™) â€‹ print(f.read()) finally: â€‹ if f: â€‹ f.close() ç”¨with as ç®€åŒ– with open(path, â€˜râ€™) as f: â€‹ print(f.read()) numpynumpy.argsort numpy.argsort(a, axis=-1, kind=None, order=None) Returns the indices that would sort an array. Perform an indirect sort along the given axis using the algorithm specified by the kind keyword. It returns an array of indices of the same shape as a that index data along the given axis in sorted order. Parameters: a :array_like. axis : int or None, optional Axis along which to sort. The default is -1(the last axis). ã€é»˜è®¤æŒ‰ç…§æœ€åŽä¸€ä¸ªç»´åº¦ã€‘ 2-D: axis = 0æŒ‰åˆ—æŽ’åº 2-D: axis = 1 æŒ‰è¡ŒæŽ’åº kind :{â€˜quicksortâ€™, â€˜mergesortâ€™, â€˜heapsortâ€™, â€˜stableâ€™}, optional The default is â€˜quicksortâ€™ Return: index_array: ndarray, int.ã€è¿”å›žçš„æ˜¯é™åºæŽ’åˆ—çš„ç´¢å¼•æ•°ç»„ã€‘ e.g.: x = np.array([5, 1, 2]) np.argsort(x) # é™åº array([1,2,0]) np.argsort(-x) # å‡åº array([0,2,1]) Linear algebra(numpy.linalg)numpy.dot numpy.dot(a,b) Dot product of two arrays. If both a and b are 1-D arrays, it is inner porduct of vectors. If both a and b are 2-D arrays, it is matrix multiplication, but using matmul is preferred. Id either a or b is 0-D(scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a*b is preferred. â€¦â€¦ numpy.matmul Matrix product of two arrays. numpy.matmul(x1, x2) numpy.linalg.inv(a) Compute the inverse of a matrix. Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])/ numpy.linalg.inv(a) Parameters: a :(â€¦, M, M) array_like. Matrix to e inverted. Return: ainv. numpy.linalg.svd numpy.linalg.svd(a, full_matrices=True, compute_uv=True, hermitian=False) Singular Value Decomposition çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£ A = u @ s @ vh u, vhæ˜¯æ ‡å‡†æ­£äº¤çŸ©é˜µ, inv(u) = uh sæ˜¯å¯¹è§’çŸ©é˜µ Parameters: a :array_like, a real or complex array with a.ndim &gt;=2 full_matrices :bool, optional. True(default) If True, u and vh have the shapes(â€¦, M, M) and (â€¦, N, N), respectively. Otherwise, the shapes are(â€¦, M, K) and (â€¦, K, N), respectively, where K = min(M,N) compute_uv : bool, optional.True(default) Whether or not to compute u and vh in addition to s.ã€æ³¨ï¼Œvhå°±æ˜¯vçš„è½¬ç½®ã€‘ Returnï¼š u: array s: array vh:array numpy.zeros numpy.zeros(shape, dtype=float, order=â€™Câ€™) Return a new array of given shape and type, filled with zeros. parameters: shape: int or truple of ints. e.g.,(2,3) or 2 dtype: data-type, optional.(Defaults is numpy.float64) oder: optional Returns: out ndarray numpy.full numpy.full(shape, fill_value, dtype=None) Return a new array of given shape and type, filled with fill_value. parameters: shape :int or sequence of ints (2,3) or 2 fill_value :scalar dtype :data-type, optional numpy.arange numpy.arange([start, ]stop, [step, ]dtype=None) Return evenly spaced values within a given interval. parameters: start: number, optional. (Defaults is 0) stop :number. [start,stop) step :number, optional(Defaults is 1) dtype : Returns :ndarray differ with built-in range function numpy.arange returnan ndarray rathan than a list. numpy.arangeâ€™s step can be float. numpy.meshgrid numpy.meshgrid(x, y) ç”Ÿæˆç”¨xå‘é‡ä¸ºè¡Œï¼Œyå‘é‡ä¸ºåˆ—çš„çŸ©é˜µï¼ˆåæ ‡ç³»ï¼‰ è¿”å›ž XçŸ©é˜µå’ŒYçŸ©é˜µ XçŸ©é˜µï¼šç½‘æ ¼ä¸Šæ‰€æœ‰ç‚¹çš„xå€¼ YçŸ©é˜µï¼šç½‘æ ¼ä¸Šæ‰€æœ‰ç‚¹çš„yå€¼ e.g., X, Y = np.meshgrid(x, y) ã€X,Y éƒ½æ˜¯ç½‘æ ¼ç‚¹åæ ‡çŸ©é˜µã€‘ numpy.genfromtext numpy.genfromtxt (fname, delimiter=None) Load data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments characters are discarded. Parameters: fname :file, str, list of str, generator. dtype :dtype, optional. delimiter :str, int, or sequence, optional. (default = whitespace) The strin used to separate values. Pythonçš„åˆ—è¡¨è¯»å–å¤„ç†æ•°æ®å¾ˆæ…¢ï¼Œnumpy.genfromtextå°±å¾ˆæ£’ã€‚ numpy.isnan numpy.isnan(x) Test element-wise for NaN(Not a number) and return result as a boolean array. Parameters: x :array_like Returns: y:ndarray or bool. True where x is NaN, false otherwise. numpy.empty nmpy.empty(shape, dtype=float, order=â€™Câ€™) Return a new arry of given shape and type, without initializing entries. Parameters: shape :int or tuple of int dtype :data-type,optional Default is numpy.float64. Returns: out: ndarray numpy.reshape numpy.reshape(a, newshape, order=â€™Câ€™) Gives a new shape to an array without changing its data.ã€æ”¹å˜å¼ é‡çš„shapeï¼Œä¸æ”¹å˜å¼ é‡çš„æ•°æ®ã€‘ Parameters: a : array-like newshape : int or tuple of ints One shape dimension can be -1. The value is inferred from the length of the array and remaning dimensions. Returns: reshaped_array:ndarray numpy.mean numpy.mean(a, axis=None) Compute the arithmetic mean along the specifiied axis.(the average of the array elements) Parameters: a : array_like axis ï¼šNone or int or tuple of ints, optional Axis or axes along which the means are computed. axis=0 ï¼šæ²¿è¡Œçš„åž‚ç›´å¾€ä¸‹ï¼ˆåˆ—ï¼‰ axis=1 ï¼šæ²¿åˆ—çš„æ–¹å‘æ°´å¹³å‘å³ï¼ˆè¡Œï¼‰ numpy.std numpy.std(a, axis=None,) Compute the standard deviation along the specified axis.ã€æ ‡å‡†å·®ã€‘ Parameters: a :array_like axis :Axis or axes along which the means are computed. numpy.shape attribute Tuple of array dimensions. numpy.concatenate numpy.concatenate((a1, a2, â€¦), axis=0) Join a sequence of arrays along an existing axis. Parameters: a1, a2, â€¦ :sequence of array_like The arrays must have the same shape, excepting in the dimension corresponding to axis.ã€é™¤äº†axiaæ–¹å‘ï¼Œå…¶ä»–ç»´åº¦çš„shapeè¦ç›¸åŒã€‘ If axis is None, arrays are flattened before use.ã€å€¼ä¸ºNoneï¼Œå°±å…ˆå°†å‘é‡å˜æˆä¸€ç»´çš„ã€‘ Default=0 numpy.ndarray.astype method numpy.ndarray.astype(dtype) Copy of the array cast to a specified type.ã€å¼ºåˆ¶è½¬æ¢æ•°æ®ç±»åž‹ã€‘ Parameters: dtype : str or dtype numpy.ones numpy.ones(shape, dtype=None) Return a new array of given shape and type, filled with ones. Parameters: shape : int or sequence of ints. dtype : data-type, optional numpy.array numpy.array(object, dtype = none) Create an array Parameters: object :array_like An array, any object exposing the array interface, an object whose array method returns an array, or any(nested) sequence. numpy ndarray è¿ç®— [[1]]*3 = [[1],[1],[1]] A * B å…ƒç´ ç›¸ä¹˜ numpy.dot(A, B) çŸ©é˜µç›¸ä¹˜ numpy.power numpy.power(x1, x2) First array elements raised to powers from second array. Parameters: x1 :array_like . The bases. x2 :array_like The exponents. numpy.sum numpy.sum(a, axis=None, dtype=None) Sum of arrays elements over a given axis. Parameters: a :array_like Elements to sum. axis :None or int or tuple of ints, optional Axis or axes along which a sum is perfomed. The default, None, will sum all of the elementsof the input array. numpy.transpose numpy.transpose(a, axes=None) Permute the dimensions of the array.ã€tensorçš„ç»´åº¦æ¢ä½ã€‘ Parameters: a : array_like axes : list of ints, optinal Default, reverse the dimensions. Otherwise permute the axes according to the values given. Returns : ndarray å¼ é‡açš„shapeæ˜¯(10,2,15), numpy.transport(2,0,1)çš„shapeå°±æ˜¯(15,10,2) å¯¹äºŽä¸€ç»´ï¼šè¡Œå‘é‡å˜æˆåˆ—å‘é‡ å¯¹äºŽäºŒç»´ï¼šçŸ©é˜µçš„è½¬ç½® numpy.save numpy.save(file, arr) Save an array to a binary file in Numpy .npy format. Parameters: file :file, str, or pathlib arr :array_like Array data to be saved. numpy.clip Clip(limit) the values in an array numpy.clip(a, a_min, a_max) Parameters: a :array_like a_min : scalar or array_like a_max :scalar or array_like numpy.around Evenly round to the given number of decimals(åè¿›åˆ¶) numpy.around(a) Parametersï¼š a :array_like Notes: For values exactly halfway between rounded decimal values, Numpy rounds to the nearest even values. ã€è¿™ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ å°±æ˜¯è¯´å¯¹äºŽ0.5çš„è¿™ç§ï¼Œä¸ºäº†ç»Ÿè®¡ä¸Šå¹³è¡¡ï¼Œä¸ä¼šå…¨éƒ¨å‘ä¸Šå–æ•´æˆ–è€…å‘ä¸‹å–æ•´ï¼Œä¼šå‘æœ€è¿‘çš„å¶æ•°å–æ•´ï¼Œaroundï¼ˆ2.5ï¼‰=2ã€‘ numpy.log The natural logarithm log is the inverse of exponential functions, so that log(exp(x))=x. numpy.log(x) Parameters: x : array_like numpy.ndarray.T attribute, the transpose array. ndarray.T numpy.random.shuffle Modify a sequence in-space by shufflng its contents. This function only shuffles the array along the first axis of a multi-diensional array. The order of sub-arrays is changed but their contents remains the same. numpy.random.shuffle(x) Parameters: x : array_like e.g. shuffle two list, X and Y, together. ã€ä»¥ç›¸åŒçš„é¡ºåºæ‰“ä¹±ä¸¤ä¸ªarrayã€‘ np.random.seed(0) randomize = np.arrange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize] sklearnskelearn.linear_model.LinearRegression class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None) Ordinary least squares Linear Regress(æœ€å°äºŒä¹˜æ³•å›žå½’è¿ç®—) Parameters: fit_intercept :bool, optional, defalut Trueã€Trueï¼šéœ€è¦biasçš„æˆªè·é¡¹ã€‘ normalize :bool, optional, default False ã€Trueï¼šå¯¹æ ·æœ¬åšfeature scalingã€‘ Attributesï¼š coef_ :array of shape(n_features) ã€æƒé‡ã€‘ intercept_ :bias Methodsï¼š fit(self,X,y[,sample_weight]) :Fit linear model e.g. , LinearRegression().fit(x_data, y_data) matplotlib.pyplotmatplotlib.pyplot.contourf contour and contourf draw contour lines and filled contours, respectively.ã€ä¸€ä¸ªç”»ç­‰é«˜çº¿ï¼Œä¸€ä¸ªå¡«å……ç­‰é«˜çº¿/è½®å»“ã€‘ contour([X, Y, ] Z, [levels], **kwargs) Parameters X, Y: The coordinates of the values in Z. A and Y must both be 2-D with the sanme shape as Z(e.g. created via numpy.meshgrid), or they must both be 1-D such that len(X) == M is the number of columns in Z and len(Y) = N is the number of rows in Z. ã€Xï¼ŒYè¦ä¹ˆæ˜¯ç”±åƒnumpy.mershgrid(x, y) ç”Ÿæˆçš„ç½‘æ ¼ç‚¹åæ ‡çŸ©é˜µï¼Œè¦ä¹ˆXï¼ŒYæ˜¯ï¼ˆåŸºï¼‰å‘é‡ï¼ŒXå‘é‡æ˜¯xè½´çš„ï¼Œå¯¹åº”åˆ°ZçŸ©é˜µï¼Œæ˜¯ZçŸ©é˜µçš„åˆ—æ•°ï¼ŒYå‘é‡åŒç†ã€‘ Z ï¼šarray-like(N,M) levels : int or array-like, optional. Determines the number and positions of contour lines / religions.ã€åˆ’åˆ†å¤šå°‘å—ç­‰é«˜åŒºåŸŸã€‘ alpha :float, optional. Between 0(transparent) and 1(opaque).ã€é€æ˜Žåº¦ã€‘ cmap :str or Colormap, optional. e.g., pyplot.contourf(x, y, Z, 50, alpha=0.5, cmap=pyplot.get_cmap(â€˜jetâ€™))ã€â€˜jetâ€™æ˜¯å¸¸ç”¨çš„é‚£ç§çº¢æ©™é»„ç»¿é’è“ç´«ã€‘ matplotlib.pyplot.plot plot([x], y, [fmt], , data=None, *kwargs) The coordinates of the points or line nodes are given by x, y. Parameters: x, y :array-like or scalar. fmt :str, optional. A format string. e.g., â€˜.â€™, point marker. â€˜-â€˜, solid line style. â€˜â€“â€™,dashed line style. â€˜bâ€™, blue. ms/markersize : float lw/linewidth :float color : matplotlib.pyplot.xlim xlim(args, *kwargs) Get or set the x limits of the current axes. e.g. left, right = xlim() :get xlim(left, right) :set matplotlib.pyplot.show show(args, *kwargs) display a figure. matplotlib.pyplot.vlines Plot vertical lines. vlines(x, ymin, ymax, color=â€™kâ€™, linestyles=â€™solidâ€™) Parameters: x :scalar or 1D array_like ymin, ymax :scalar or 1D array_like matplotlib.pyplot.hlines Plot horizontal lines. vlines(y, xmin, xmax, color=â€™kâ€™, linestyles=â€™solidâ€™) Parameters: y :scalar or 1D array_like xmin, xmax :scalar or 1D array_like matplotlib.pyplot.savefig Save the current figure. savefig(fname) Parameters: fname :str ot Pathlike matplotlib.pyplot.legend Place a lengend on the axes. e.g. : legend() Labeling exisiting plot elements plt.plot(train_loss) plt.plot(dev_loss) plt.legend([â€˜trainâ€™, â€˜devâ€™]) e.g. : le syssys.argv[] python a.py data.csv sys.argv = [â€˜a.pyâ€™, â€˜data.csvâ€™] é‡å®šå‘åˆ°æ–‡ä»¶ f = open(â€˜out.csvâ€™, â€˜wâ€™) sys.stdout = f print(â€˜æ­¤æ—¶printæŽ‰ç”¨çš„å°±æ˜¯æ–‡ä»¶å¯¹è±¡çš„writeæ–¹æ³•â€™)","link":"/2020/03/06/python/"},{"title":"ã€ŒPyTorchã€ï¼š4-Neural Network Design","text":"PyTorchæ¡†æž¶å­¦ä¹ ã€‚ è¿™ç¯‡æ–‡ç« ä¸»è¦ä»‹ç»å¦‚ä½•ç”¨PyTorchè®¾è®¡å®žçŽ°ä¸€ä¸ªNNã€‚ colabç¬”è®°ï¼š Neural Network Design 1: The Layers Neural Network Design 2: Callable Neural Networks Neural Network Design 3: CNN Forward Method Neural Network Design 4: Pass A Batch of Images ä»¥CNNä¸ºä¾‹ï¼Œè®²è§£PyTorchä¸­çš„layerã€weightã€ Overviewï¼š Build PyTorch CNN - Object Oriented Neural Networks CNN Layers - Deep Neural Network Architecture CNN Weights - Learnable Parameters in Neural Networks Callable Neural Networks - Linear Layers in Depth CNN Forward Method - Deep Learning Implementation Forward Propagation Explained - Pass Image to PyTorch Neural Network Neural Network Batch Processing - Pass Image Batch to PyTorch CNN CNN Output Size Formula - Bonus Neural Network Debugging Session Building Neural Networks With PyTorchFrom a high-level perspective or birdâ€™s eye view of our deep learning project, we prepared our data, and now, we are ready to build our model. ã€ä»Žé«˜å±‚æ¬¡çœ‹ï¼Œè¿™ä¸€éƒ¨åˆ†ä¸»è¦è®²è§£å¦‚ä½•ç”¨PyTorchè®¾è®¡modelã€‘ Prepare the data Build the model Train the model Analyze the modelâ€™s results Weâ€™ll do a quick OOP review in this post to cover the details needed for working with PyTorch neural networks, but if you find that you need more, the Python docs have an overview tutorial here. ã€OOPçš„ç»†èŠ‚ã€‘ PyTorchâ€™s torch.nn PackageTo build neural networks in PyTorch, we use the torch.nn package, which is PyTorchâ€™s neural network (nn) library. We typically import the package like so: 1import torch.nn as nn PyTorchâ€™s neural network library contains all of the typical components needed to build neural networks. ã€nnåº“åŒ…å«æ‰€æœ‰æž„å»ºNNçš„å…¸åž‹ç»„ä»¶ã€‘ PyTorchâ€™s nn.Module ClassAs we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components: ã€NNä¸­çš„æ¯ä¸€layeréƒ½ç”±ä»£ç ï¼ˆinput tensoråˆ°output tensor çš„è½¬æ¢ï¼‰å’Œæƒé‡ï¼ˆweightsï¼‰ç»„æˆï¼Œå› æ­¤å¯ä»¥ç”¨OOPçš„æ€æƒ³æ¥æŠ½è±¡è¡¨ç¤ºã€‚ã€‘ A transformation (code) A collection of weights (data) In fact, this is the case with PyTorch. Within the nn package, there is a class called Module, and it is the base class for all of neural network modules which includes layers. ã€nnåº“çš„Moduleç±»æ˜¯æ‰€æœ‰NNæ¨¡åž‹ä¸­Layersçš„çˆ¶ç±»ï¼Œå³æ‰€æœ‰networkséƒ½è¦ç»§æ‰¿nn.Modulesç±»ã€‘ PyTorch nn.Modules Have A forward() MethodWhen we pass a tensor to our network as input, the tensor flows forward though each layer transformation until the tensor reaches the output layer. This process of a tensor flowing forward though the network is known as a forward pass. ã€forward passï¼štensorå‘å‰æµï¼Œç›´è‡³è¾“å‡ºå±‚ã€‘ Every PyTorch nn.Module has a forward() method, and so when we are building layers and networks, we must provide an implementation of the forward() method. The forward method is the actual transformation. ã€æ‰€æœ‰layers å’Œ networksåœ¨ç»§æ‰¿nn.Moduleæ—¶ï¼Œéƒ½è¦å®žçŽ°forward()æŽ¥å£ï¼Œè¿™ä¸ªforwardæ–¹æ³•å°±æ˜¯å®žé™…çš„è¾“å…¥åˆ°è¾“å‡ºçš„è½¬æ¢ã€‘ PyTorchâ€™s nn.functional PackageWhen we implement the forward() method of our nn.Module subclass, we will typically use functions from the nn.functional package. This package provides us with many neural network operations that we can use for building layers. In fact, many of the nn.Module layer classes use nn.functional functions to perform their operations. ã€nn.functionalåŒ…æœ‰å¾ˆå¤šå®žç”¨çš„å‡½æ•°æ“ä½œï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æž„å»ºlayersã€‚äº‹å®žä¸Šï¼Œnn.Moduleçš„è®¸å¤šå­ç±»å°±ä½¿ç”¨äº†nn.functionalçš„æ–¹æ³•æ¥å®Œæˆä»–ä»¬çš„æ“ä½œã€‚ã€‘ Building A Neural Network In PyTorchWe now have enough information to provide an outline for building neural networks in PyTorch. The steps are as follows: Short version: Extend the nn.Module base class.ã€ç»§æ‰¿nn.Moduleç±»ã€‘ Define layers as class attributes.ã€å®šä¹‰layersä½œä¸ºè¯¥ç±»çš„å±žæ€§ã€‘ Implement the forward() method.ã€å®žçŽ°forward()æŽ¥å£ã€‘ More detailed version: Create a neural network class that extends the nn.Module base class. In the class constructor, define the networkâ€™s layers as class attributes using pre-built layers from torch.nn. Use the networkâ€™s layer attributes as well as operations from the nn.functional API to define the networkâ€™s forward pass. Define The Networkâ€™s Layers As Class AttributesWeâ€™re building a CNN, so the two types of layers weâ€™ll use are linear layers and convolutional layers. 12345678910111213class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # implement the forward pass return t Inside of our Network class, we have five layers that are defined as attributes. We have two convolutional layers, self.conv1 and self.conv2, and three linear layers, self.fc1, self.fc2, self.out. ã€åœ¨Networkä¸­ï¼Œæœ‰5ä¸ªlayersä½œä¸ºè¯¥ç±»çš„attributesã€‘ We used the abbreviation fc in fc1 and fc2 because linear layers are also called fully connected layers. They also have a third name that we may hear sometimes called dense. So linear, dense, and fully connected are all ways to refer to the same type of layer. PyTorch uses the word linear, hence the nn.Linear class name. We used the name out for the last linear layer because the last layer in the network is the output layer. ã€fcæ˜¯fully connected layersçš„ç¼©å†™ï¼Œå…¨è¿žæŽ¥å±‚ï¼Œnn.Linearã€‘ ã€outæ˜¯è¾“å‡ºå±‚ã€‚ã€‘ Our CNN LayersEach of our layers extends PyTorchâ€™s neural network Module class. For each layer, there are two primary items encapsulated inside, a forward function definition and a weight tensor. ã€æ¯ä¸€layeréƒ½ä¼šç»§æ‰¿PyTorchçš„Moduleç±»ã€‚å¯¹äºŽæ¯ä¸€layerï¼Œéƒ½ä¼šå°è£…ä¸¤ä¸ªç»„ä»¶ï¼šforwardå‡½æ•°å’Œæƒé‡tensorã€‘ The weight tensor inside each layer contains the weight values that are updated as the network learns during the training process. ã€æ¯ä¸€å±‚çš„weitght tensoréƒ½åŒ…å«åœ¨NNè®­ç»ƒä¸­æ›´æ–°çš„æƒé‡å‚æ•°ã€‚ã€‘ PyTorchâ€™s neural network Module class keeps track of the weight tensors inside each layer. The code that does this tracking lives inside the nn.Module class, and since we are extending the neural network module class, we inherit this functionality automatically. ã€å…¶ä¸­ï¼Œæƒé‡tensorå°±æ˜¯åœ¨è®­ç»ƒNNä¸­ä¼šæ›´æ–°çš„å‚æ•°ï¼ŒModuleç±»ä¼šè‡ªåŠ¨è·Ÿè¸ªå…¶æ¯ä¸€å±‚çš„weight tensorã€‘ CNN Layer ParametersParameter Vs ArgumentWeâ€™ll parameters are used in function definitions as place-holders while arguments are the actual values that are passed to the function. The parameters can be thought of as local variables that live inside a function. ã€Parametersä½œä¸ºå ä½ç¬¦ç”¨äºŽå‡½æ•°å®šä¹‰ï¼Œè€ŒArgumentsæ˜¯ä¼ é€’ç»™å‡½æ•°çš„å®žé™…çš„å€¼ã€‚ã€‘ Two Types Of ParametersTo better understand the argument values for these parameters, letâ€™s consider two categories or types of parameters that we used when constructing our layers. ã€åœ¨æž„å»ºlayersæ—¶ï¼Œæœ‰ä¸¤ç§å‚æ•°ã€‘ Hyperparametersã€è¶…å‚æ•°ã€‘ Data dependent hyperparametersã€æ•°æ®ä¾èµ–è¶…å‚æ•°ã€‘ When we construct a layer, we pass values for each parameter to the layerâ€™s constructor. With our convolutional layers have three parameters and the linear layers have two parameters. ã€åœ¨æž„å»ºlayeræ—¶ï¼Œæˆ‘ä»¬å‘layer constructorä¼ é€’å‚æ•°ã€‚ã€‘ Convolutional layers in_channels out_channels kernel_size Linear layers in_features out_features HyperparametersIn general, hyperparameters are parameters whose values are chosen manually and arbitrarily. ã€hyperparametersæ˜¯æ‰‹åŠ¨ä¸»è§‚ç¡®å®šçš„å‚æ•°ã€‚ã€‘ As neural network programmers, we choose hyperparameter values mainly based on trial and error and increasingly by utilizing values that have proven to work well in the past. For building our CNN layers, these are the parameters we choose manually. ã€è¶…å‚æ•°å¾€å¾€æ˜¯åŸºäºŽç»éªŒtrialå’Œè¯¯å·®errorç¡®å®šçš„.ã€‘ ã€åœ¨CNNä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šè¿™äº›å‚æ•°ã€‚ã€‘ Parameter Description kernel_size Sets the filter size. The words kernel and filter are interchangeable. out_channels Sets the number of filters. One filter produces one output channel. out_features Sets the size of the output tensor. Data Dependent HyperparametersData dependent hyperparameters are parameters whose values are dependent on data. The first two data dependent hyperparameters that stick out are the in_channels of the first convolutional layer, and the out_features of the output layer. ã€ä¾èµ–äºŽæ•°æ®çš„è¶…å‚æ•°ã€‚æ¯”å¦‚åœ¨ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„in_channleså’Œè¾“å‡ºå±‚çš„out_featureså‚æ•°çš„ç¡®å®šéƒ½ä¾èµ–äºŽæ•°æ®ã€‚ã€‘ In general, the input to one layer is the output from the previous layer, and so all of the in_channels in the conv layers and in_features in the linear layers depend on the data coming from the previous layer. ã€ä¸€ä¸ªlayerçš„è¾“å…¥ä¾èµ–äºŽå‰ä¸€å±‚çš„è¾“å‡ºã€‚ã€‘ When we switch from a conv layer to a linear layer, we have to flatten our tensor. This is why we have 12*4*4. Summary Of Layer Parameters123456self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)self.fc2 = nn.Linear(in_features=120, out_features=60)self.out = nn.Linear(in_features=60, out_features=10) Layer Param name Param value The param value is conv1 in_channels 1 the number of color channels in the input image. conv1 kernel_size 5 a hyperparameter. conv1 out_channels 6 a hyperparameter. conv2 in_channels 6 the number of out_channels in previous layer. conv2 kernel_size 5 a hyperparameter. conv2 out_channels 12 a hyperparameter (higher than previous conv layer). fc1 in_features 12 * 4 * 4 the length of the flattened output from previous layer. fc1 out_features 120 a hyperparameter. fc2 in_features 120 the number of out_features of previous layer. fc2 out_features 60 a hyperparameter (lower than previous linear layer). out in_features 60 the number of out_channels in previous layer. out out_features 10 the number of prediction classes. CNN Weights - Learnable Parameters In Neural NetworksColab: Neural Network Design: The Layers Learnable ParametersLearnable parameters are parameters whose values are learned during the training process. ã€Learnable parametersæ˜¯åœ¨è®­ç»ƒä¸­å¯ä»¥å­¦ä¹ çš„å‚æ•°ã€‚ã€‘ With learnable parameters, we typically start out with a set of arbitrary values, and these values then get updated in an iterative fashion as the network learns. ã€ä»Žä¸€ä¸ªä¸»è§‚ç¡®å®šçš„å€¼å¼€å§‹ï¼Œåœ¨ç½‘ç»œå­¦ä¹ ä¸­è¿­ä»£æ›´æ–°ã€‚ã€‘ Where are the learnable parameters? Weâ€™ll the learnable parameters are the weights inside our network, and they live inside each layer. ã€Learnable parameterså­˜åœ¨åœ¨ç½‘ç»œä¸­çš„æ¯ä¸€å±‚ï¼Œæ˜¯åœ¨æˆ‘ä»¬ç½‘ç»œä¸­çš„æƒé‡å‚æ•°ã€‚ã€‘ Getting An Instance The Network Letâ€™s grab an instance of our network class and see this. 1network = Network() After the object is initialized, we can then access our object using the network variable. ã€èŽ·å¾—ä¸€ä¸ªç½‘ç»œçš„å®žä¾‹ï¼Œå³ä¼šè‡ªåŠ¨è¿è¡Œ__init__ å¯¹å…¶åˆå§‹åŒ–ã€‚ã€‘ How Overriding WorksAll Python classes automatically extend the object class. If we want to provide a custom string representation for our object, we can do it, but we need to introduce another object oriented concept called overriding. ã€æ‰€æœ‰Pythonçš„ç±»éƒ½ä¼šç»§æ‰¿oobject classï¼Œå¯ä»¥é‡å†™è¯¥ç±»çš„å­—ç¬¦è¡¨è¾¾ï¼ˆstring representationï¼‰ã€‚ã€‘ We can override Pythonâ€™s default string representation using the __repr__ function. This name is short for representation. ã€é‡å†™__repr__ å‡½æ•°ã€‘ 12345678910network = Network()print (network)Network( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=192, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=60, bias=True) (out): Linear(in_features=60, out_features=10, bias=True)) Whatâ€™s In The String Representation?Convolutional LayersFor the convolutional layers, the kernel_size argument is a Python tuple (5,5) even though we only passed the number 5 in the constructor. ã€kernel_sizeçš„å€¼ï¼Œç¡®å®šfilterçš„å¤§å°ã€‚å½“ä¼ é€’ä¸€ä¸ªå€¼æ—¶ï¼Œé»˜è®¤ä¸ºsquare filterã€‚ã€‘ The stride is an additional parameter that we could have set, but we left it out. When the stride is not specified in the layer constructor the layer automatically sets it. ã€kernelç§»åŠ¨çš„strideå¦‚æžœæ²¡æœ‰è®¾ç½®ä¼šè‡ªåŠ¨è®¾ç½®ã€‚ã€‘ Linear LayersFor the linear layers, we have an additional parameter called bias which has a default parameter value of true. It is possible to turn this off by setting it to false. ã€linear layersè¿˜æœ‰ä¸€ä¸ªè‡ªåŠ¨è®¾ç½®ä¸ºTrueçš„biaså‚æ•°ã€‚ã€‘ Accessing The Networkâ€™s LayersWell, now that weâ€™ve got an instance of our network and weâ€™ve reviewed our layers, letâ€™s see how we can access them in code. ã€å¦‚ä½•è®¿é—®NNä¸­çš„layersï¼šå½“ä¸€èˆ¬å±žæ€§è®¿é—®ï¼Œæ¯ä¸€å±‚éƒ½ä¼šè¿”å›žä¸€ä¸ªå­—ç¬¦è¡¨è¾¾ã€‘ 1234567891011121314&gt; network.conv1Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))&gt; network.conv2Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))&gt; network.fc1Linear(in_features=192, out_features=120, bias=True)&gt; network.fc2 Linear(in_features=120, out_features=60, bias=True)&gt; network.outLinear(in_features=60, out_features=10, bias=True) Accessing The Layer WeightsNow that we have access to each of our layers, we can access the weights inside each layer. ã€è®¿é—®æ¯ä¸€å±‚çš„æƒé‡å‚æ•°ã€‚ã€‘ Colab: Neural Network Design: The Layers 1&gt; network.conv1.weight PyTorch Parameter ClassPyTorch has a special class called Parameter. The Parameter class extends the tensor class, and so the weight tensor inside every layer is an instance of this Parameter class. ã€PyTorchè¿˜æœ‰ä¸€ä¸ªç‰¹æ®Šçš„ç±»ï¼šParameterç±»ã€‚è¿™ä¸ªç±»ç»§æ‰¿äº†tensorç±»ï¼Œæ‰€ä»¥æ¯ä¸€å±‚ä¸­çš„weight tensorå®žåˆ™éƒ½æ˜¯Parameterç±»çš„å®žä¾‹ã€‚ã€‘ Weight Tensor ShapeFor the convolutional layers, the weight values live inside the filters, and in code, the filters are actually the weight tensors themselves. ã€å¯¹å·ç§¯å±‚æ¥è¯´ï¼Œweightæ˜¯åœ¨filterä¸­çš„ï¼Œè€Œfilteråœ¨ä»£ç ä¸­çš„ä½“çŽ°å°±æ˜¯weight tensorã€‚ã€‘ The convolution operation inside a layer is an operation between the input channels to the layer and the filter inside the layer. This means that what we really have is an operation between two tensors. ã€å·ç§¯æ“ä½œå®žåˆ™å°±æ˜¯input tensor å’Œfilter çš„weight tensorä¹‹é—´çš„æ“ä½œã€‚ã€‘ For the first conv layer, we have 1 color channel that should be convolved by 6 filters of size 5x5 to produce 6 output channels. This is how we interpret the values inside our layer constructor. ã€å¯¹äºŽç¬¬ä¸€ä¸ªå·ç§¯å±‚æ¥è¯´ï¼Œæœ‰6ä¸ª 5 * 5çš„filerï¼Œä¼šç”Ÿæˆ6ä¸ªè¾“å‡ºchannelã€‚ã€‘ 12&gt; network.conv1Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) Inside our layer though, we donâ€™t explicitly have 6 weight tensors for each of the 6 filters. We actually represent all 6 filters using a single weight tensor whose shape reflects or accounts for the 6 filters. ã€ä½†æˆ‘ä»¬ä¸ä¼šä½¿ç”¨6ä¸ªweight tensoræ¥è¡¨ç¤ºè¯¥å·ç§¯å±‚çš„6ä¸ªfiltersï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„weight tensoræ¥è¡¨ç¤ºè¯¥å±‚çš„æ‰€æœ‰filersã€‚ã€‘ 12&gt; network.conv1.weight.shapetorch.Size([6, 1, 5, 5]) The second axis has a length of 1 which accounts for the single input channel, and the last two axes account for the height and width of the filter. ã€weight tensorçš„ç¬¬ä¸€ç»´åº¦è¡¨ç¤ºfiltersçš„æ•°é‡ï¼Œè¯¥tensoræŠŠæ‰€æœ‰filterséƒ½æ‰“åŒ…åœ¨ä¸€èµ·ã€‚ç¬¬äºŒç»´åº¦è®¤ä¸ºæ˜¯filterçš„depthï¼Œå’Œè¾“å…¥tensorçš„channelç›¸åŒï¼Œæœ€åŽä¸¤ç»´ä¸ºheight å’Œwidthã€‘ The two main takeaways about these convolutional layers is that our filters are represented using a single tensor and that each filter inside the tensor also has a depth that accounts for the input channels that are being convolved. ã€å·ç§¯å±‚çš„ä¸¤ä¸ªè¦ç‚¹ï¼ˆtakewaysï¼‰ã€‘ All filters are represented using a single tensor.ã€ç”¨ä¸€ä¸ªtensorè¡¨ç¤ºè¯¥å±‚çš„æ‰€æœ‰filtersã€‘ Filters have depth that accounts for the input channels.ã€å…¶ä¸­æ¯ä¸€ä¸ªfilteræœ‰depthï¼Œå€¼ç­‰äºŽè¾“å…¥çš„channelsã€‘ å·ç§¯å±‚Weight Tensorçš„shapeï¼š(Number of filters, Depth, Height, Width) Weight MatrixWith linear layers or fully connected layers, we have flattened rank-1 tensors as input and as output. The way we transform the in_features to the out_features in a linear layer is by using a rank-2 tensor that is commonly called a weight matrix. ã€å¯¹äºŽå…¨è¿žæŽ¥å±‚ï¼Œæˆ‘ä»¬éœ€è¦æ‹‰ç›´ï¼ˆflattenï¼‰è¾“å…¥/è¾“å‡ºä¸ºrank-1çš„tensorã€‘ ã€è¿™ç§åœ¨å…¨è¿žæŽ¥å±‚ä¸­in_featuresåˆ°out_featuresçš„è½¬æ¢ï¼Œä½¿ç”¨weight matrixæ¥å®žçŽ°ï¼Œæ‰€ä»¥è¯¥å±‚çš„å‚æ•°å°±æ˜¯ä¸€ä¸ªrank-2çš„tensorã€‘ Linear Function Represented Using A MatrixThe important thing about matrix multiplications like this is that they represent linear functions that we can use to build up our neural network. Specifically, the weight matrix is a linear function also called a linear map that maps a vector space of 4 dimensions to a vector space of 3 dimensions. ã€çŸ©é˜µä¹˜æ³•å®žåˆ™æ˜¯çº¿æ€§å‡½æ•°ã€‚å…·ä½“æ¥è¯´ï¼ŒçŸ©é˜µä¹˜æ³•ä¹Ÿç§°ä¸ºä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼Œå°†ä¸€ä¸ª4D vectoræ˜ å°„ä¸ºä¸€ä¸ª3D vectorã€‚ã€‘ Accessing The Networks Parametersã€è®¿é—®NNçš„æ‰€æœ‰å‚æ•°ã€‘ The first example is the most common way, and weâ€™ll use this to iterate over our weights when we update them during the training process. ã€éåŽ†network.parameters() ã€‘ 12345678910111213for param in network.parameters(): print(param.shape)torch.Size([6, 1, 5, 5])torch.Size([6])torch.Size([12, 6, 5, 5])torch.Size([12])torch.Size([120, 192])torch.Size([120])torch.Size([60, 120])torch.Size([60])torch.Size([10, 60])torch.Size([10]) The second way is just to show how we can see the name as well. This reveals something that we wonâ€™t cover in detail, the bias is also a learnable parameter. Each layer has a bias by default, so for each layer we have a weight tensor and a bias tensor. ã€æ¯ä¸€å±‚ä¸­çš„biasä¹Ÿæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚ã€‘ ã€éåŽ†network.named_parameters() ã€‘ 12345678910111213for name, param in network.named_parameters(): print(name, '\\t\\t', param.shape)conv1.weight torch.Size([6, 1, 5, 5])conv1.bias torch.Size([6])conv2.weight torch.Size([12, 6, 5, 5])conv2.bias torch.Size([12])fc1.weight torch.Size([120, 192])fc1.bias torch.Size([120])fc2.weight torch.Size([60, 120])fc2.bias torch.Size([60])out.weight torch.Size([10, 60])out.bias torch.Size([10]) Callable Neural Networks - Linear Layers In DepthColab: Neural Network Design 2: Callable Neural Networks In this one, weâ€™ll learn about how PyTorch neural network modules are callable, what this means, and how it informs us about how our network and layer forward methods are called. ã€åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬èƒ½çŸ¥é“åœ¨networkå’Œlayerä¸­forwardæ–¹æ³•æ˜¯å¦‚ä½•è°ƒç”¨çš„ï¼Ÿã€‘ How Linear Layers WorkTransform Using A Matrixã€ä½¿ç”¨çŸ©é˜µä¹˜æ³•æ¥è½¬æ¢ã€‘ 12345678910in_features = torch.tensor([1,2,3,4], dtype=torch.float32)weight_matrix = torch.tensor([ [1,2,3,4], [2,3,4,5], [3,4,5,6]], dtype=torch.float32)&gt; weight_matrix.matmul(in_features)tensor([30., 40., 50.]) Transform Using A PyTorch Linear Layerã€ä½¿ç”¨PyTorch Linear Layeræ¥è½¬æ¢ã€‚ã€‘ 1fc = nn.Linear(in_features=4, out_features=3, bias=False) ã€æ ¹æ®æºç ï¼Œåœ¨LinearLayerä¸­ä¼šæœ‰ä¸€ä¸ª3 * 4 çš„weight matrixã€‘ 123456789101112# torch/nn/modules/linear.py (version 1.0.1)def __init__(self, in_features, out_features, bias=True): super(Linear, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = Parameter(torch.Tensor(out_features, in_features)) if bias: self.bias = Parameter(torch.Tensor(out_features)) else: self.register_parameter('bias', None) self.reset_parameters() Letâ€™s see how we can call our layer now by passing the in_features tensor. ã€ç›´æŽ¥ä¼ tensoræ¥è°ƒç”¨è¯¥layerã€‘ 12&gt; fc(in_features)tensor([-0.8877, 1.4250, 0.8370], grad_fn=&lt;SqueezeBackward3&gt;) We can call the object instance like this because PyTorch neural network modules are callable Python objects. ã€å› ä¸ºPyTorchä¸­çš„moduleæ˜¯å¯ä»¥è°ƒç”¨çš„ç±»ï¼Œå³ç±»ä¸­æœ‰__call__ æ–¹æ³•ã€‚ Letâ€™s explicitly set the weight matrix of the linear layer to be the same as the one we used in our other example. ã€å¯ä»¥å•ç‹¬è®¾ç½®linear layerä¸­weight matrixçš„å€¼ã€‘ 1fc.weight = nn.Parameter(weight_matrix) Callable Layers And Neural Networksã€å¯è°ƒç”¨çš„Layerså’ŒNNã€‘ We pointed out before how it was kind of strange that we called the layer object instance as if it were a function. ã€ä¸ºä»€ä¹ˆå¯ä»¥å°†å®žä¾‹ä½œä¸ºå‡½æ•°è°ƒç”¨ï¼Ÿã€‘ 12&gt; fc(in_features)tensor([30.0261, 40.1404, 49.7643], grad_fn=&lt;AddBackward0&gt;) What makes this possible is that PyTorch module classes implement another special Python function called __call__(). If a class implements the __call__() method, the special call method will be invoked anytime the object instance is called. ã€å¦‚æžœè¯¥ç±»å®žçŽ°äº†__call()__ æ–¹æ³•ï¼Œé‚£ä¹ˆè¯¥ç±»çš„å®žä¾‹å°±å¯ä»¥ä½œä¸ºå‡½æ•°è°ƒç”¨ã€‘ This fact is an important PyTorch concept because of the way the __call__() method interacts with the forward() method for our layers and networks. ã€è€ŒPyTorchä¸­è¯¥ç±»çš„__call__ æ–¹æ³•æ˜¯å’Œforward() æ–¹æ³•äº¤äº’çš„ã€‘ 123456789101112131415161718192021222324252627def __call__(self, *input, **kwargs): for hook in self._forward_pre_hooks.values(): hook(self, input) if torch._C._get_tracing_state(): result = self._slow_forward(*input, **kwargs) else: result = self.forward(*input, **kwargs) for hook in self._forward_hooks.values(): hook_result = hook(self, input, result) if hook_result is not None: raise RuntimeError( &quot;forward hooks should never return any values, but '{}'&quot; &quot;didn't return None&quot;.format(hook)) if len(self._backward_hooks) &gt; 0: var = result while not isinstance(var, torch.Tensor): if isinstance(var, dict): var = next((v for v in var.values() if isinstance(v, torch.Tensor))) else: var = var[0] grad_fn = var.grad_fn if grad_fn is not None: for hook in self._backward_hooks.values(): wrapper = functools.partial(hook, self) functools.update_wrapper(wrapper, hook) grad_fn.register_hook(wrapper) return result The extra code that PyTorch runs inside the __call__() method is why we never invoke the forward() method directly. If we did, the additional PyTorch code would not be executed. As a result, any time we want to invoke our forward() method, we call the object instance. This applies to both layers, and networks because they are both PyTorch neural network modules. ã€å› ä¸ºæœ‰__call__() ï¼Œæ‰€ä»¥ä¸éœ€è¦ç›´æŽ¥è°ƒç”¨forward() æ–¹æ³•ã€‚æ‰€ä»¥ï¼Œå¦‚æžœä»»ä½•æ—¶å€™æˆ‘ä»¬æƒ³è¦è°ƒç”¨forward() æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬éƒ½è°ƒç”¨å¯¹è±¡å®žä¾‹ã€‚ã€‘ CNN Forward Method - PyTorch Deep Learning ImplementationColab: CNN Forward Method We created our network by extending the nn.Module PyTorch base class, and then, in the class constructor, we defined the networkâ€™s layers as class attributes. Now, we need to implement our networkâ€™s forward() method, and then, finally, weâ€™ll be ready to train our model. ã€å‰é¢é€šè¿‡ç»§æ‰¿nn.Module æ¥æž„å»ºmodelï¼Œåœ¨modelçš„æž„é€ å™¨ä¸­ï¼Œå®šä¹‰ç½‘ç»œçš„layerä½œä¸ºmodelçš„å±žæ€§ã€‚è€Œæž„å»ºmodelçš„æœ€åŽä¸€æ­¥æ˜¯å®žçŽ°modelä¸­çš„forward() æ–¹æ³•ã€‘ ã€æ­¥éª¤ã€‘ Prepare the data Build the model Create a neural network class that extends the nn.Module base class. In the class constructor, define the networkâ€™s layers as class attributes. Use the networkâ€™s layer attributes as well nn.functional API operations to define the networkâ€™s forward pass. ã€ç”¨ç½‘ç»œçš„layerå±žæ€§å’Œnn.functional åº“çš„æ¿€æ´»å‡½æ•°ç­‰æ¥å®šä¹‰ç½‘ç»œçš„å‰å‘ä¼ æ’­ã€‘ Train the model Analyze the modelâ€™s results Implementing The forward() MethodColab: Neural Network Design 3: CNN Forward Method ã€å®žçŽ°forward()æ–¹æ³•ã€‘ Input Layer #1The input layer of any neural network is determined by the input data. ã€input layer ä¾èµ–äºŽè¾“å…¥çš„æ•°æ®ã€‘ For this reason, we can think of the input layer as the identity transformation. Mathematically, this is the function, $f(x)=x$ . ã€å¯ä»¥æŠŠinput layerçœ‹ä½œidentification transformationã€‘ Hidden Convolutional Layers: Layers #2 And #3123456789# (2) hidden conv layert = self.conv1(t)t = F.relu(t)t = F.max_pool2d(t, kernel_size=2, stride=2)# (3) hidden conv layert = self.conv2(t)t = F.relu(t)t = F.max_pool2d(t, kernel_size=2, stride=2) Each of these layers is comprised of a collection of weights (data) and a collection operations (code). The weights are encapsulated inside the nn.Conv2d() class instance. The relu() and the max_pool2d() calls are just pure operations. ã€æ¯ä¸€å±‚éƒ½æ˜¯weightså’Œoperationsçš„ç»„åˆï¼Œweightså°è£…åœ¨nn.Conv2då®žä¾‹ä¸­ï¼Œè€Œrelu()å’Œmax_pool2d()éƒ½æ˜¯å•çº¯çš„operationsã€‘ For example, weâ€™ll say that the second layer in our network is a convolutional layer that contains a collection of weights, and preforms three operations, a convolution operation, the relu activation operation, and the max pooling operation. ã€åªæ˜¯å…¶ä¸­çš„ä¸€ç§è¡¨ç¤ºï¼šè®¤ä¸ºå·ç§¯å±‚æœ‰ä¸€ç»„weightsï¼ˆlayerä¸­åŒ…å«çš„weightsï¼‰ï¼Œä¸‰ç»„æ“ä½œï¼šå·ç§¯æ“ä½œã€reluæ“ä½œå’Œmax pooling æ“ä½œã€‚ã€‘ Mathematically, the entire network is just a composition of functions, and a composition of functions is a function itself. So a network is just a function. All the terms like layers, activation functions, and weights, are just used to help describe the different parts. ã€æ•´ä¸ªç½‘ç»œï¼Œå…¶å®žå°±æ˜¯functionsçš„ç»„åˆã€‚å› æ­¤ï¼Œnetworkæœ¬èº«å°±æ˜¯ä¸€ä¸ªfunctionã€‚layers, activation functions, weightsåªæ˜¯æ¥å¸®åŠ©æè¿°è¿™ä¸ªfunctionã€‘ Hidden Linear Layers: Layers #4 And #5Before we pass our input to the first hidden linear layer, we must reshape() or flatten our tensor. This will be the case any time we are passing output from a convolutional layer as input to a linear layer. ã€åœ¨å°†å·ç§¯å±‚çš„è¾“å‡ºä¼ é€’ç»™å…¨è¿žæŽ¥å±‚ä¹‹å‰ï¼Œéœ€è¦å°†ä»–flattenã€‘ 12345678# (4) hidden linear layert = t.reshape(-1, 12 * 4 * 4)t = self.fc1(t)t = F.relu(t)# (5) hidden linear layert = self.fc2(t)t = F.relu(t) Output Layer #6The sixth and last layer of our network is a linear layer we call the output layer. When we pass our tensor to the output layer, the result will be the prediction tensor. ã€ç¬¬å…­å±‚æ˜¯è¾“å‡ºå±‚ï¼Œè¯¥å±‚çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæœ‰10ä¸ªå…ƒç´ çš„tensorã€‘ Inside the network we usually use relu() as our non-linear activation function, but for the output layer, whenever we have a single category that we are trying to predict, we use softmax(). The softmax function returns a positive probability for each of the prediction classes, and the probabilities sum to 1. ã€å‰é¢çš„å±‚æˆ‘ä»¬éƒ½æ˜¯ç”¨relu()æ¥ä½œä¸ºéžçº¿æ€§å‡½æ•°ï¼Œä½†è¾“å‡ºå±‚éœ€è¦å¾—åˆ°æ¯ä¸€ç±»çš„é¢„æµ‹å€¼ï¼Œå› æ­¤ä½¿ç”¨softmax()ã€‘ ã€softmaxèƒ½è¿”å›žæ¯ä¸€ç±»çš„é¢„æµ‹æ¦‚çŽ‡ï¼Œæ‰€æœ‰ç±»çš„æ¦‚çŽ‡å’Œä¸º1ã€‘ 1234567891011121314151617181920212223242526272829def forward(self, t): # (1) input layer t = t # (2) hidden conv layer t = self.conv1(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2) # (3) hidden conv layer t = self.conv2(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2) # (4) hidden linear layer t = t.reshape(-1, 12 * 4 * 4) t = self.fc1(t) t = F.relu(t) # (5) hidden linear layer t = self.fc2(t) t = F.relu(t) # (6) output layer t = self.out(t) #t = F.softmax(t, dim=1) return t Forward Propagation ExplainedForward Propagation ExplainedForward propagation is the process of transforming an input tensor to an output tensor. ã€å‰é¦ˆä¼ æ’­æ˜¯å°†è¾“å…¥tensorè½¬æ¢ä¸ºè¾“å‡ºtensorçš„è¿‡ç¨‹ã€‚ã€‘ Predicting With The Network: Forward PassBefore we being, we are going to turn off PyTorchâ€™s gradient calculation feature. This will stop PyTorch from automatically building a computation graph as our tensor flows through the network. ã€åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…³é—­PyTorchçš„gradientè®¡ç®—ã€‚å½“tensoræµè¿‡ç½‘ç»œå›¾æ—¶ï¼Œè¿™ä¼šé˜»æ­¢PyTorchè‡ªåŠ¨æž„å»ºè®¡ç®—å›¾ã€‚ã€‘ The computation graph keeps track of the networkâ€™s mapping by tracking each computation that happens. The graph is used during the training process to calculate the derivative (gradient) of the loss function with respect to the networkâ€™s weights. ã€è®¡ç®—å›¾é€šè¿‡è·Ÿè¸ªè®¡ç®—æ¥è·Ÿè¸ªç½‘ç»œå›¾ï¼Œè¯¥è®¡ç®—å›¾åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºŽè®¡ç®—æŸå¤±å‡½æ•°å¯¹æƒé‡å‚æ•°çš„æ¢¯åº¦ã€‚ã€‘ Since we are not training the network yet, we arenâ€™t planning on updating the weights, and so we donâ€™t require gradient calculations. We will turn this back on when training begins. ã€å› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒç½‘ç»œï¼Œæ‰€ä»¥æˆ‘ä»¬å¹¶ä¸æ‰“ç®—æ›´æ–°å‚æ•°ï¼Œä¹Ÿå°±ä¸éœ€è¦æ¢¯åº¦è®¡ç®—ã€‚ã€‘ Passing A Single Image To The NetworkLetâ€™s continue by creating an instance of our Network class: ã€åˆ›å»ºNNå®žä¾‹ã€‘ 1&gt; network = Network() Next, weâ€™ll procure a single sample from our training set, unpack the image and the label, and verify the imageâ€™s shape: ã€ä»Žtraining setä¸­ç”Ÿæˆä¸€ä¸ªå•ç‹¬çš„ä¾‹å­ã€‚ã€‘ 1234&gt; sample = next(iter(train_set)) &gt; image, label = sample &gt; image.shape torch.Size([1, 28, 28]) Now, thereâ€™s a second step we must preform before simply passing this tensor to our network. When we pass a tensor to our network, the network is expecting a batch, so even if we want to pass a single image, we still need a batch. ã€ç¬¬äºŒæ­¥ï¼Œç½‘ç»œæœŸæœ›ä¼ é€’çš„tensoræ˜¯ä¸€æ‰¹ï¼Œå› æ­¤éœ€è¦å°†å•ç‹¬çš„ä¾‹å­ä¹Ÿæ‰“åŒ…ã€‚ã€‘ 12345678910111213&gt; pred = network(image.unsqueeze(0)) # image shape needs to be (batch_size Ã— in_channels Ã— H Ã— W)&gt; predtensor([[0.0991, 0.0916, 0.0907, 0.0949, 0.1013, 0.0922, 0.0990, 0.1130, 0.1107, 0.1074]])&gt; pred.shapetorch.Size([1, 10])&gt; label9&gt; pred.argmax(dim=1)tensor([7]) For each input in the batch, and for each prediction class, we have a prediction value. If we wanted these values to be probabilities, we could just the softmax() function from the nn.functional package. ã€ç”¨F.softmax()å°†é¢„æµ‹å€¼è½¬æ¢ä¸ºæ¦‚çŽ‡ã€‚ã€‘ 12345&gt; F.softmax(pred, dim=1)tensor([[0.1096, 0.1018, 0.0867, 0.0936, 0.1102, 0.0929, 0.1083, 0.0998, 0.0943, 0.1030]])&gt; F.softmax(pred, dim=1).sum()tensor(1.) Neural Network Batch Processing With PyTorch Prepare the data Build the model Understand how batches are passed to the network Train the model Analyze the modelâ€™s results Colab: Pass A Batch of Images Using Argmax: Prediction Vs LabelColab: Pass A Batch of Images CNN Output Size FormulaCNN Output Size FormulaCNN Output Size Formula (Square) Suppose we have an $n\\times n$ input.ã€è¾“å…¥å°ºå¯¸ã€‘ Suppose we have an $f\\times f$ filter.ã€filterå°ºå¯¸ã€‘ Suppose we have a padding of $p$ and a stride of $s$ .ã€paddingå’Œstrideã€‘ The output size $O$ is given by this formula: $O = \\frac{n-f+2p}{s}+1$ ã€è¾“å‡ºã€‘ CNN Output Size Formula (Non-Square) Suppose we have an $n_hÃ—n_w$ input. Suppose we have an $f_hÃ—f_w$ filter. Suppose we have a padding of $p$ and a stride of $s$. The height of the output size $O_h$ is given by this formula:$O_h = \\frac{n_h-f_h+2p}{s}+1$ The width of the output size $O_w$ is given by this formula: $O_w = \\frac{n_w-f_w+2p}{s}+1$","link":"/2021/02/27/pytorch-nn-design/"},{"title":"ã€ŒPyTorchã€ï¼š2-Tensors Explained And Operations","text":"PyTorchæ¡†æž¶å­¦ä¹ ã€‚ æœ¬ç¯‡æ–‡ç« ä¸»è¦ä»‹ç»PyTorchä¸­çš„TensoråŠå…¶åŸºæœ¬æ“ä½œï¼Œä¸»è¦åˆ†ä¸ºå››ä¸ªæ–¹é¢ï¼šReshape, Element-wise, Reductionå’ŒAccessã€‚ Tensorçš„å…·ä½“æ“ä½œä»‹ç»ï¼Œå»ºè®®é…åˆColabç¬”è®°ä½¿ç”¨ï¼š PyTorch Tensors Explained Tensor Operations: Reshape Tensor Operations: Element-wise Tensor Operation: Reduction and Access è‹±æ–‡çš„è¡¨è¾¾è§£é‡Šéƒ½æ˜¯æ¯”è¾ƒæ¸…æ™°ä¸”ç²¾ç¡®çš„ï¼Œæ‰€ä»¥ä»¥è‹±è¯­çš„å½¢å¼ä½œä¸ºä¸»è¦è®°å½•ï¼Œæ–‡ä¸­ä¼šå¤¹å¸¦ä¸€äº›ä¸­æ–‡æ€»ç»“è¯­å¥ï¼Œæ–¹ä¾¿é˜…è¯»ã€‚ Introducing TensorsTensor Explained - Data Structures of Deep LearningWhat Is A Tensor?A tensor is the primary data structure used by neural networks. ã€Tensoræ˜¯NNä¸­æœ€ä¸»è¦çš„æ•°æ®ç»“æž„ã€‘ Indexes Required To Access An ElementThe relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure. ã€ä»¥ä¸‹pairséƒ½æ˜¯éœ€è¦åŒç­‰æ•°é‡çš„indexesæ‰èƒ½ç¡®å®šç‰¹å®šçš„å…ƒç´ ã€‚ã€‘ ã€è€Œtensoræ˜¯generalizationsï¼Œæ˜¯ä¸€ç§ç»Ÿä¸€è€Œæ™®éçš„å®šä¹‰ã€‚ã€‘ Indexes required Computer science Mathematics 0 number scalar 1 array vector 2 2d-array matrix Tensors Are GeneralizationsWhen more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language. MathematicsIn mathematics, we stop using words like scalar, vector, and matrix, and we start using the word tensor or nd-tensor. The n tells us the number of indexes required to access a specific element within the structure. ã€æ•°å­¦ä¸­ï¼Œå½“æˆ‘ä»¬éœ€è¦ç”¨å¤§äºŽä¸¤ä¸ªçš„indexesæ‰èƒ½ç¡®å®šç‰¹ç‚¹å…ƒç´ æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨tensoræˆ–è€…nd-tensoræ¥è¡¨ç¤ºè¯¥æ•°æ®ç»“æž„ï¼Œè¯´æ˜Žéœ€è¦nä¸ªindexæ‰èƒ½ç¡®å®šè¯¥æ•°æ®ç»“æž„ä¸­çš„ç‰¹å®šå…ƒç´ ã€‚ã€‘ Computer ScienceIn computer science, we stop using words like, number, array, 2d-array, and start using the word multidimensional array or nd-array. The n tells us the number of indexes required to access a specific element within the structure. ã€è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨nd-arrayæ¥è¡¨ç¤ºï¼Œå› æ­¤ï¼Œnd-arrayå’Œtensorå®žåˆ™æ˜¯ä¸€ä¸ªä¸œè¥¿ã€‚ã€‘ Indexes required Computer science Mathematics n nd-array nd-tensor Tensors and nd-arrays are the same thing! One thing to note about the dimension of a tensor is that it differs from what we mean when we refer to the dimension of a vector in a vector space. The dimension of a tensor does not tell us how many components exist within the tensor. ã€éœ€è¦æ³¨æ„çš„åœ°æ–¹æ˜¯ï¼Œtensorä¸­çš„ç»´åº¦å’Œvectorå‘é‡ç©ºé—´ä¸­çš„ç»´åº¦ä¸æ˜¯åŒä¸€ä¸ªä¸œè¥¿ï¼Œvectorå‘é‡ç©ºé—´ä¸­çš„ç»´åº¦è¡¨ç¤ºè¯¥vectoræœ‰å¤šå°‘ä¸ªå…ƒç´ ç»„æˆçš„ï¼Œè€Œtensorä¸­çš„ç»´åº¦æ˜¯ä¸‹æ–‡ä¸­rankçš„å«ä¹‰ã€‚ã€‘ Rank, Axes, And Shape Explainedã€ä¸‹æ–‡ä¼šè¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ tensorçš„å‡ ä¸ªé‡è¦æ€§è´¨ï¼šRank, Axes, Shape.ã€‘ The concepts of rank, axes, and shape are the tensor attributes that will concern us most in deep learning. Rank Axes Shape Rank And IndexesWe are introducing the word rank here because it is commonly used in deep learning when referring to the number of dimensions present within a given tensor. The rank of a tensor tells us how many indexes are required to access (refer to) a specific data element contained within the tensor data structure. A tensorâ€™s rank tells us how many indexes are needed to refer to a specific element within the tensor. ã€è¿™é‡Œçš„rankå®žåˆ™å°±æ˜¯tensorçš„ç»´åº¦ã€‚ã€‘ ã€tensorçš„rankå€¼å‘Šè¯‰æˆ‘ä»¬éœ€è¦å¤šå°‘ä¸ªindexesæ‰èƒ½ç¡®å®šè¯¥tensorä¸­çš„ç‰¹å®šå…ƒç´ ã€‚ã€‘ Axes Of A TensorIf we have a tensor, and we want to refer to a specific dimension, we use the word axis in deep learning. An axis of a tensor is a specific dimension of a tensor. Elements are said to exist or run along an axis. This running is constrained by the length of each axis. Letâ€™s look at the length of an axis now. Length Of An AxisThe length of each axis tells us how many indexes are available along each axis. ã€å½“æˆ‘ä»¬å…³æ³¨tensorçš„æŸä¸€å…·ä½“ç»´åº¦æ—¶ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­æˆ‘ä»¬ä½¿ç”¨axisæ¥è¡¨è¾¾ã€‚ã€‘ ã€å…ƒç´ è¢«è®¤ä¸ºæ˜¯åœ¨æŸä¸€axieä¸Šå­˜åœ¨æˆ–å»¶ä¼¸çš„ï¼Œå…ƒç´ å»¶ä¼¸çš„é•¿åº¦å–å†³äºŽaxisçš„é•¿åº¦ã€‚ã€‘ ã€Axisçš„é•¿åº¦è¡¨ç¤ºåœ¨æ¯ä¸€ç»´åº¦ï¼ˆaxisï¼‰ä¸Šæœ‰å¤šå°‘ä¸ªç´¢å¼•ã€‘ Shape Of A TensorThe shape of a tensor is determined by the length of each axis, so if we know the shape of a given tensor, then we know the length of each axis, and this tells us how many indexes are available along each axis. The shape of a tensor gives us the length of each axis of the tensor. ã€tensorçš„shapeç”±æ¯ä¸€axisçš„é•¿åº¦å†³å®šï¼Œå³æ¯ä¸€axisçš„ç´¢å¼•æ•°ç›®ã€‘ Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called reshaping. Reshaping changes the shape but not the underlying data elements. ã€tensorçš„å¸¸è§æ“ä½œreshapeåªæ”¹å˜tensorçš„shapeï¼Œè€Œä¸æ”¹å˜åº•å±‚çš„æ•°æ®ã€‚ã€‘ CNN Tensors Shape ExplainedCNNçš„ç›¸å…³ä»‹ç»ï¼Œå¯è§ è¿™ç¯‡æ–‡ç«  What I want to do now is put the concepts of rank, axes, and shape to use with a practical example. To do this, weâ€™ll consider an image input as a tensor to a CNN. Remember that the shape of a tensor encodes all the relevant information about a tensorâ€™s axes, rank, and indexes, so weâ€™ll consider the shape in our example, and this will enable us to work out the other values. ã€tensorçš„shapeèƒ½ä½“çŽ°tensorçš„axesã€rankã€indexæ‰€æœ‰ä¿¡æ¯ã€‘ ã€ä»¥CNNä¸ºä¾‹æ¥è¯´æ˜Žrank, axes, shape.ã€‘ Shape Of A CNN InputThe shape of a CNN input typically has a length of four. This means that we have a rank-4 tensor with four axes. Each index in the tensorâ€™s shape represents a specific axis, and the value at each index gives us the length of the corresponding axis. ã€CNNçš„input æ˜¯ä¸€ä¸ªrank4-tensor.ã€‘ Each axis of a tensor usually represents some type of real world or logical feature of the input data. If we understand each of these features and their axis location within the tensor, then we can have a pretty good understanding of the tensor data structure overall. ã€tensorçš„æ¯ä¸ªaxiså¾€å¾€ä»£è¡¨ç€æŸä¸€ä¸ªé€»è¾‘featureï¼Œæ‰€ä»¥ç†è§£featureså’Œtensorä¸­axisçš„ä½ç½®çš„å…³ç³»èƒ½å¸®åŠ©æˆ‘ä»¬æ›´å¥½çš„ç†è§£tensorã€‚ã€‘ Image Height And WidthTo represent two dimensions, we need two axes. The image height and width are represented on the last two axes. ã€è¡¨ç¤ºå›¾åƒçš„heightå’Œwidthï¼Œéœ€è¦2ä¸ªaxesï¼Œä½¿ç”¨æœ€åŽä¸¤ä¸ªaxesè¡¨ç¤ºã€‚ã€‘ Image Color ChannelsThe next axis represents the color channels. Typical values here are 3 for RGB images or 1 if we are working with grayscale images. This color channel interpretation only applies to the input tensor. ã€ä¸‹ä¸€ä¸ªaxis(ä»Žå³è‡³å·¦)è¡¨ç¤ºå›¾åƒçš„color channelsï¼ˆé¢œè‰²é€šé“ï¼Œå¦‚ç°åº¦å›¾åƒå°±æœ‰1ä¸ªé¢œè‰²é€šé“ï¼ŒRGBå›¾åƒæœ‰ä¸‰ä¸ªï¼‰ã€‚ã€‘ ã€æ³¨æ„ï¼šcolor channelçš„è¯´æ³•åªé€‚ç”¨äºŽinput tensorã€‚ã€‘ Image BatchesThis brings us to the first axis of the four which represents the batch size. In neural networks, we usually work with batches of samples opposed to single samples, so the length of this axis tells us how many samples are in our batch. Suppose we have the following shape [3, 1, 28, 28] for a given tensor. Using the shape, we can determine that we have a batch of three images. ã€ç¬¬ä¸€ä¸ªaxisè¡¨ç¤ºbatchå±žæ€§ï¼Œè¡¨æ˜Žè¯¥batchçš„sizeã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ä¸€æ‰¹æ ·æœ¬ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ç‹¬çš„æ ·æœ¬ï¼Œæ‰€ä»¥è¿™ä¸€ç»´åº¦è¡¨æ˜Žäº†æˆ‘ä»¬çš„batchä¸­æœ‰å¤šå°‘æ ·æœ¬ã€‚ã€‘ tensorï¼š[Batch, Channels, Height, Width] Each image has a single color channel, and the image height and width are 28 x 28 respectively. Batch size Color channels Height Width NCHW vs NHWC vs CHWNItâ€™s common when reading API documentation and academic papers to see the B replaced by an N. The N standing for number of samples in a batch. ã€åœ¨APIæ–‡æ¡£æˆ–å­¦æœ¯è®ºæ–‡ä¸­ï¼ŒNç»å¸¸ä¼šä»£æ›¿ä»£æ›¿Bï¼Œè¡¨ç¤ºthe number of samples in a batchã€‚ã€‘ Furthermore, another difference we often encounter in the wild is a reordering of the dimensions. Common orderings are as follows: NCHW NHWC CHWN ã€é™¤æ­¤ä¹‹å¤–ï¼Œä¹Ÿä¼šç»å¸¸é‡åˆ°è¿™äº›axesçš„å…¶ä»–é¡ºåºã€‚ã€‘ As we have seen, PyTorch uses NCHW, and it is the case that TensorFlow and Keras use NHWC by default (it can be configured). Ultimately, the choice of which one to use depends mainly on performance. Some libraries and algorithms are more suited to one or the other of these orderings. ã€PyTorch é»˜è®¤ä½¿ç”¨NCHWï¼Œè€ŒTensorFlowå’ŒKerasä½¿ç”¨NHWCã€‘ Output Channels And Feature MapsLetâ€™s look at how the interpretation of the color channel axis changes after the tensor is transformed by a convolutional layer. Suppose we have three convolutional filters, and lets just see what happens to the channel axis. Since we have three convolutional filters, we will have three channel outputs from the convolutional layer. These channels are outputs from the convolutional layer, hence the name output channels opposed to color channels. ã€tensoré€å…¥convolutional layerï¼ˆå·ç§¯å±‚ï¼‰åŽï¼Œcolor channel è¿™ä¸€axisçš„é•¿åº¦å‘ç”Ÿå˜åŒ–ã€‚ ã€åœ¨Post not found: % CNN CNNçš„ä»‹ç»æ–‡ç« ä¸­è§£é‡Šåˆ°ï¼Œæœ‰å‡ ä¸ªconvolutional filtersï¼Œå·ç§¯å±‚è¾“å‡ºçš„tensorå°±æœ‰å‡ ä¸ªchannelï¼ˆchannelä»£æ›¿color channelçš„è¡¨è¾¾ï¼‰ã€‚ã€‘ Feature MapsWith the output channels, we no longer have color channels, but modified channels that we call feature maps. These so-called feature maps are the outputs of the convolutions that take place using the input color channels and the convolutional filters. Feature maps are the output channels created from the convolutions. ã€å·ç§¯å±‚è¾“å‡ºtensorçš„channelç»´åº¦ä»£æ›¿color channelsçš„å«æ³•ã€‚ã€‘ ã€å·ç§¯å±‚çš„è¾“å‡ºä¹Ÿå«å«feature mapsã€‘ PyTorch TensorsWhen programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form. ã€æ•°æ®é¢„å¤„ç†å¾€å¾€æ˜¯ç¼–å†™NNçš„ç¬¬ä¸€æ­¥ï¼Œå°†åŽŸå§‹æ•°æ®è½¬æ¢ä¸ºtensor formã€‚ã€‘ Tensorçš„åŸºæœ¬æ“ä½œè§Colabè¿è¡Œç¬”è®°é“¾æŽ¥ï¼šPyTorch Tensors Explained (ä¸ä¼šç”¨çš„ä¹Ÿå¯ä»¥ç›´æŽ¥çœ‹github ä¸Šçš„) PyTorch Tensors Attributes torch.dtypeï¼štensoråŒ…å«æ•°æ®ç±»åž‹ã€‚ å¸¸è§æ•°æ®ç±»åž‹ï¼š Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point torch.float16 torch.HalfTensor torch.cuda.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 torch.LongTensor torch.cuda.LongTensor torch.device: tensoræ•°æ®æ‰€åˆ†é…çš„è®¾å¤‡ï¼Œå¦‚CPUï¼Œcuda:0 torch.layout: tensoråœ¨å†…å­˜ä¸­çš„å­˜å‚¨æ–¹å¼ã€‚ As neural network programmers, we need to be aware of the following: Tensors contain data of a uniform type (dtype). Tensor computations between tensors depend on the dtype and the device. ã€TensorsåŒ…å«ç›¸åŒç±»åž‹çš„æ•°æ®ã€‘ ã€Tensorsä¹‹é—´çš„è®¡ç®—å–å†³äºŽä»–çš„ç±»åž‹å’Œä»–æ‰€åˆ†é…çš„è®¾å¤‡ã€‘ Creating TensorsThese are the primary ways of creating tensor objects (instances of the torch.Tensor class), with data (array-like) in PyTorch: Creating Tensors with data. ã€å››ç§ç”¨æ•°æ®åˆ›å»ºtensorçš„æ–¹å¼ã€‘ torch.Tensor(data) torch.tensor(data) torch.as_tensor(data) torch.from_numpy(data) torch.Tensor() Vs torch.tensor()The first option with the uppercase T is the constructor of the torch.Tensor class, and the second option is what we call a factory function that constructs torch.Tensor objects and returns them to the caller. However, the factory function torch.tensor() has better documentation and more configuration options, so it gets the winning spot at the moment. ã€torch.Tensor(data) æ˜¯ torch.Tensor classçš„Constructorï¼Œè€Œtorch.tensor(data) æ˜¯ç”Ÿæˆ/è¿”å›ž torch.Tensor classçš„å‡½æ•°ï¼ˆfactory functions)ã€‘ ã€å› ä¸ºtorch.tensor() æœ‰æ›´å¤šçš„é€‰é¡¹è®¾ç½®ï¼Œæ¯”å¦‚å¯ä»¥è®¾ç½®æ•°æ®ç±»åž‹ï¼Œæ‰€ä»¥ä¸€èˆ¬ç”¨torch.tensor() æ¥ç”Ÿæˆã€‚ã€‘ Default dtype Vs Inferred dtypeThe difference here arises in the fact that the torch.Tensor() constructor uses the default dtype when building the tensor. The other calls choose a dtype based on the incoming data. This is called type inference. The dtype is inferred based on the incoming data. ã€torch.Tensor() åœ¨ç”Ÿæˆtensoræ—¶ï¼Œä½¿ç”¨çš„æ˜¯é»˜è®¤dtype=torch.float32 ï¼Œè€Œå…¶ä»–ä¸‰ç§æ˜¯ä½¿ç”¨çš„å¼•ç”¨dtype ï¼Œå³ç”Ÿæˆtensorçš„æ•°æ®ç±»åž‹å’Œè¾“å…¥çš„æ•°æ®ç±»åž‹ä¸€è‡´ã€‚ã€‘ Sharing Memory For Performance: Copy Vs Sharetorch.Tensor() and torch.tensor() copy their input data while torch.as_tensor() and torch.from_numpy() share their input data in memory with the original input object. This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the torch.Tensor and the numpy.ndarray. Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory. ã€torch.Tensor() å’Œ torch.tensor() åœ¨æ ¹æ®dataåˆ›å»ºtensoræ—¶ï¼Œåœ¨å†…å­˜ä¸­é¢å¤–å¤åˆ¶æ•°æ®ã€‘ ã€torch.as_tensor() å’Œ torch.from_numpy() åœ¨æ ¹æ®dataåˆ›å»ºtensoræ—¶ï¼Œæ˜¯å’ŒåŽŸè¾“å…¥æ•°æ®å…±äº«çš„å†…å­˜ï¼Œå³åŽŸnumpy.ndarryçš„æ•°æ®æ”¹å˜ï¼Œç›¸åº”çš„tensorä¹Ÿä¼šæ”¹å˜ã€‚ã€‘ Share Data Copy Data torch.as_tensor() torch.tensor() torch.from_numpy() torch.Tensor() Some things to keep in mind about memory sharing (it works where it can): Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used. ã€åœ¨ä½¿ç”¨GPUæ—¶ï¼Œ as_tensor() ä¹Ÿä¼šå°†ndarrayæ•°æ®ä»ŽCPUå¤åˆ¶åˆ°GPUä¸Šã€‚ã€‘ The memory sharing of as_tensor() doesnâ€™t work with built-in Python data structures like lists. ã€as_tensor() åœ¨Pythonå†…ç½®æ•°æ®ç»“æž„æ—¶ä¸ä¼šå…±äº«å†…å­˜ã€‘ The as_tensor() performance improvement will be greater if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. ã€as_tensor() åœ¨ndarryå’Œtensorä¹‹é—´å¤§é‡è¿žç»­æ“ä½œæ—¶èƒ½æœ‰æ•ˆæé«˜æ€§èƒ½ã€‘ torch.as_tensor() Vs torch.from_numpy()This establishes that torch.as_tensor() and torch.from_numpy() both share memory with their input data. However, which one should we use, and how are they different? The torch.from_numpy() function only accepts numpy.ndarrays, while the torch.as_tensor() function accepts a wide variety of array-like objects, including other PyTorch tensors. ã€è¿™ä¸¤ä¸ªéƒ½æ˜¯å’Œè¾“å…¥æ•°æ®å…±äº«å†…å­˜ï¼Œä½† torch.from_numpy() åªèƒ½æŽ¥å—numpy.ndarrays ç±»åž‹çš„æ•°æ®ï¼Œè€Œtorch.as_tensor() èƒ½æŽ¥å—array-like(åƒlist, tuple)ç­‰ç±»åž‹ï¼Œæ‰€ä»¥ä¸€èˆ¬torch.as_tensor() æ›´å¸¸ç”¨ã€‚ã€‘ If we have a torch.Tensor and we want to convert it to a numpy.ndarray ã€ç”¨torch.numpy() æŠŠtensorè½¬æ¢ä¸ºndarrayã€‘ Creating Tensors without data. ã€è¿˜æœ‰å‡ ç§åˆ›å»ºå¸¸è§tensorçš„æ–¹å¼ã€‘ torch.eyes(n) : åˆ›å»º2-D tensorï¼Œå³n*nçš„å•ä½å‘é‡ã€‚ torch.zeros(shape) : åˆ›å»ºshape=shapeçš„å…¨0tensorã€‚ torch.ones(shape) : åˆ›å»ºå…¨1tensorã€‚ torch.rand(shape) : åˆ›å»ºéšæœºå€¼tensorã€‚ Tensor Operationå…³äºŽTensor æ“ä½œçš„Colabè¿è¡Œç¬”è®°ã€‚å¯¹ç…§ä½¿ç”¨æœ€ä½³ã€‚å¦‚æžœæ‰“ä¸å¼€ä¹Ÿå¯ä»¥çœ‹github Tensor Operations: Reshape Tensor Operations: Element-wise Tensor Operation: Reduction and Access We have the following high-level categories of operations: Reshaping operations Element-wise operations Reduction operations Access operations ã€å¯¹tensorçš„æ“ä½œä¸»è¦åˆ†ä¸º4ç§ï¼šreshape, element-wise, reduction, accessã€‘ ReshapeAs neural network programmers, we have to do the same with our tensors, and usually shaping and reshaping our tensors is a frequent task. ã€reshapeåœ¨NNç¼–ç¨‹ä¸­æ˜¯å¾ˆå¸¸è§çš„æ“ä½œã€‘ ï¼ˆå…·ä½“æ“ä½œè§colabè¿è¡Œç¬”è®°æœ¬:Tensor Operations: Reshape ï¼‰ 12345678import torcht = torch.tensor([ [1,1,1,1], [2,2,2,2], [3,3,3,3]], dtype=torch.float32)t.reshape([2,6])t.reshape(2,2,3) Reshaping changes the tensorâ€™s shape but not the underlying data. Our tensor has 12 elements, so any reshaping must account for exactly 12 elements. ã€reshapeæ“ä½œä¸æ”¹å˜åº•å±‚çš„æ•°æ®ï¼Œåªæ˜¯æ”¹å˜tensorçš„shapeã€‘ In PyTorch, the -1 tells the reshape() function to figure out what the value should be based on the number of elements contained within the tensor. ã€reshapeä¸­ä¼ å…¥çš„-1å‚æ•°ï¼ŒPyTorchå¯ä»¥è‡ªåŠ¨è®¡ç®—è¯¥å€¼ï¼Œå› ä¸ºPyTorchè¦ä¿è¯tensorçš„å…ƒç´ ä¸ªæ•°ä¸å˜ã€‘ Squeezing And Unsqueezing Squeezing a tensor removes the dimensions or axes that have a length of one. ã€Squeezingæ“ä½œï¼šç§»é™¤tensorä¸­axisé•¿åº¦ä¸º1çš„ç»´åº¦ã€‘ Unsqueezing a tensor adds a dimension with a length of one. ã€Unsqueezingæ“ä½œï¼šå¢žåŠ ä¸€ä¸ªaxisé•¿åº¦ä¸º1çš„ç»´åº¦ã€‘ ï¼ˆå…·ä½“æ“ä½œè§colabè¿è¡Œç¬”è®°æœ¬:Tensor Operations: Reshape ï¼‰ 12t.squeeze()t.squeeze().unsqueeze(dim=0) Concatenation TensorsWe combine tensors using the cat() function, and the resulting tensor will have a shape that depends on the shape of the two input tensors. ï¼ˆå…·ä½“æ“ä½œè§colabè¿è¡Œç¬”è®°æœ¬:Tensor Operations: Reshape ï¼‰ 12torch.cat((t1,t2,t3), dim=0)torch.cat((t1,t2,t3), dim=1) Flattenè¿™é‡Œä»ŽCNNçš„ä¾‹å­çœ‹Flattenï¼ŒCNNçš„ç›¸å…³ç»†èŠ‚è§ï¼šè¿™ç¯‡æ–‡ç«  A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input. ã€flattenåœ¨å·ç§¯å±‚ç½‘ç»œå¾ˆå¸¸è§ï¼Œå› ä¸ºè¾“å…¥å¿…é¡»flattenåŽæ‰èƒ½è¿žæŽ¥åˆ°ä¸€ä¸ªå…¨è¿žæŽ¥ç½‘ç»œå±‚ã€‘ å¯¹äºŽMNISTæ•°æ®é›†ä¸­18*18çš„æ‰‹å†™æ•°å­—ï¼Œåœ¨å‰æ–‡è¯´åˆ°CNNçš„è¾“å…¥æ˜¯[Batch Size, Channels, Height, Width] ï¼Œæ€Žä¹ˆæ‰èƒ½flatten tensorçš„éƒ¨åˆ†axisï¼Œè€Œä¸æ˜¯å…¨éƒ¨ç»´åº¦ã€‚ CNNçš„è¾“å…¥ï¼Œéœ€è¦flattençš„axesï¼š(C,H,W) ä»Ždim1ç»´åº¦å¼€å§‹flattenï¼ˆå…·ä½“æ“ä½œè§colabè¿è¡Œç¬”è®°æœ¬:Tensor Operations: Reshape ï¼‰ 1t.flatten(start_dim=1, end_dim=-1) Broadcasting and Element-WiseAn element-wise operation operates on corresponding elements between tensors. ã€element-wiseæ“ä½œä¸¤ä¸ªtensorä¹‹é—´å¯¹åº”çš„å…ƒç´ ã€‚ã€‘ BroadcastingBroadcasting describes how tensors with different shapes are treated during element-wise operations. Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors. ã€broadcastæè¿°äº†ä¸åŒshapeä¹‹é—´çš„tensorå¦‚ä½•è¿›è¡Œelement-wiseæ“ä½œã€‘ ã€broadcastå…è®¸æˆ‘ä»¬å¢žåŠ scalarsåˆ°é«˜ç»´åº¦ã€‘ Letâ€™s think about the t1 + 2 operation. Here, the scaler valued tensor is being broadcasted to the shape of t1, and then, the element-wise operation is carried out. ã€åœ¨t1+2æ—¶ï¼Œscalar 2å®žé™…æ˜¯å…ˆè¢«broadcaståˆ°å’Œt1ç›¸åŒçš„shape, å†æ‰§è¡Œelement-wiseæ“ä½œã€‘ We have two tensors with different shapes. The goal of broadcasting is to make the tensors have the same shape so we can perform element-wise operations on them. ï¼ˆå…·ä½“æ“ä½œè§colabè¿è¡Œç¬”è®°æœ¬:Tensor Operations: Element-wise ï¼‰ Broadcasting Detailsï¼ˆå…·ä½“æ“ä½œè§colabè¿è¡Œç¬”è®°æœ¬:Tensor Operations: Element-wise ï¼‰ Same Shapes: ç›´æŽ¥æ“ä½œ Same Rank, Different Shape: Determine if tensors are compatibleï¼ˆå…¼å®¹ï¼‰. ã€ä¸¤ä¸ªtensorå…¼å®¹ï¼Œæ‰å¯ä»¥å¯¹tensor broadcastï¼Œå†æ‰§è¡Œelement-wiseæ“ä½œã€‘ We compare the shapes of the two tensors, starting at their last dimensions and working backwards. Our goal is to determine whether each dimension between the two tensorsâ€™ shapes is compatible. ã€ä»Žæœ€åŽä¸€ä¸ªç»´åº¦å‘å‰åˆ¤æ–­ï¼Œæ¯ä¸ªç»´åº¦æ˜¯å¦å…¼å®¹ã€‘ ã€åˆ¤æ–­è¯¥ç»´åº¦å…¼å®¹çš„æ¡ä»¶æ˜¯æ»¡è¶³ä¸‹é¢ä¸¤ä¸ªæ¡ä»¶å…¶ä¸€ï¼šç»´åº¦é•¿åº¦ç›¸åŒï¼›æˆ–è€…å…¶ä¸­ä¸€ä¸ªä¸º1ã€‘ The dimensions are compatible when either: Theyâ€™re equal to each other. One of them is 1. Determine the shape of the resulting tensor. ã€æ“ä½œçš„ç»“æžœæ˜¯ä¸€ä¸ªæ–°çš„tensorï¼Œç»“æžœtensorçš„æ¯ä¸ªç»´åº¦é•¿åº¦æ˜¯åŽŸtensorsåœ¨è¯¥ç»´åº¦çš„æœ€å¤§å€¼ã€‘ Different Ranks: Determine if tensors are compatible.(åŒä¸Š) When weâ€™re in a situation where the ranks of the two tensors arenâ€™t the same, like what we have here, then we simply substitute a one in for the missing dimensions of the lower-ranked tensor. ã€å¯¹ä½Žç»´åº¦çš„tensorçš„ç¼ºå¤±ç»´åº¦ï¼Œç”¨1æ¥ä»£æ›¿ï¼Œæ¯”å¦‚shapeä¸º(1,3) å’Œ ()ï¼Œä½Žç»´åº¦çš„shapeå˜ä¸º(1,1)ã€‘ Determine the shape of the resulting tensor. ArgMax and ReductionA reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. ã€reduction æ“ä½œæ˜¯èƒ½å‡å°‘tensorå…ƒç´ æ•°é‡çš„æ“ä½œã€‚ã€‘ Reshaping operations gave us the ability to position our elements along particular axes. Element-wise operations allow us to perform operations on elements between two tensors, and reduction operations allow us to perform operations on elements within a single tensor. ã€Reshapeæ“ä½œè®©æˆ‘ä»¬èƒ½æ²¿ç€æŸä¸€axisæ“çºµtensor ä¸­çš„å…ƒç´ ä½ç½®ï¼›Element-wiseæ“ä½œè®©æˆ‘ä»¬èƒ½å¯¹tensorsä¹‹é—´å¯¹åº”å…ƒç´ è¿›è¡Œæ“ä½œï¼›Reductionæ“ä½œèƒ½è®©æˆ‘ä»¬å¯¹å•ä¸ªtensoré—´çš„å…ƒç´ æ“ä½œã€‚ã€‘ (å…·ä½“æ“ä½œè§colabç¬”è®°æœ¬ï¼šTensor Operation: Reduction and Access ) 1234t.sum()t.prod()t.mean()t.std() Reducing Tensors By Axesåªéœ€è¦å¯¹è¿™äº›æ–¹æ³•ä¼ ä¸€ä¸ªç»´åº¦å¯¹å‚æ•°ã€‚ (å…·ä½“æ“ä½œè§colabç¬”è®°æœ¬ï¼šTensor Operation: Reduction and Access ) 12t.sum(dim=0)t.sum(dim=1) ArgmaxArgmax returns the index location of the maximum value inside a tensor. ã€Argmaxè¿”å›žæœ€å¤§valueçš„indexã€‘ (å…·ä½“æ“ä½œè§colabç¬”è®°æœ¬ï¼šTensor Operation: Reduction and Access ) 1t.argmax(dim=0) Aceessing Elements Inside TensorsThe last type of common operation that we need for tensors is the ability to access data from within the tensor. ã€Accessæ“ä½œèƒ½èŽ·å¾—tensorä¸­çš„æ•°æ®ï¼Œå³å°†tensorä¸­çš„æ•°æ®æ‹¿å‡ºæ¥æ”¾åœ¨Pythonå†…ç½®çš„æ•°æ®ç»“æž„ä¸­ã€‘ (å…·ä½“æ“ä½œè§colabç¬”è®°æœ¬ï¼šTensor Operation: Reduction and Access ) 123t.mean().item()t.mean(dim=0).tolist()t.mean(dim=0).numpy() Advanced Indexing And SlicingPyTorch Tensoræ”¯æŒå¤§å¤šæ•°NumPyçš„indexå’Œslicingæ“ä½œã€‚ å‘ï¼šhttps://numpy.org/doc/stable/reference/arrays.indexing.html Reference æŒ–å‘ï¼šadvanced indexing and slicing: https://numpy.org/doc/stable/reference/arrays.indexing.html","link":"/2020/10/20/pytorch-tensors/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šRecurrent Neural Networkï¼ˆRNNï¼‰","text":"è¿™ç¯‡æ–‡ç« ä¸­é¦–å…ˆä»ŽRNNèƒ½è§£å†³ä»€ä¹ˆé—®é¢˜å…¥æ‰‹ï¼Œåˆ†æžäº†RNNä¸Žä¸€èˆ¬NNçš„åŒºåˆ«ã€‚ç„¶åŽç€é‡è®²è§£äº†åŸºäºŽRNNçš„LSTMæ¨¡åž‹ï¼ŒåŒ…æ‹¬LSTMçš„ç»†èŠ‚ã€å’Œä¸€èˆ¬NNçš„åŒºåˆ«ï¼Œä»¥åŠå¦‚ä½•è®­ç»ƒLSTMæ¨¡åž‹ã€‚å…·ä½“é˜è¿°äº†åœ¨æ¨¡åž‹ï¼ˆRNNç±»æ¨¡åž‹ï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ºä»€ä¹ˆä¼šé‡åˆ°å‰§çƒˆæŠ–åŠ¨é—®é¢˜å’Œå¦‚ä½•è§£å†³æŠ–åŠ¨çš„å·¥ç¨‹è§£å†³æ–¹æ³•ã€‚ Example applicationSolt fillingå…ˆä»ŽRNNçš„åº”ç”¨è¯´èµ·ï¼ŒRNNèƒ½åšä»€ä¹ˆï¼Ÿ RNNå¯ä»¥åšæ™ºæ…§ç³»ç»Ÿï¼š å¦‚ä¸‹å›¾ä¸­ï¼Œç”¨æˆ·å‘Šè¯‰è®¢ç¥¨ç³»ç»Ÿï¼šâ€I would like to arrive Taipei on November 2ndâ€. è®¢ç¥¨ç³»ç»Ÿèƒ½ä»Žè¿™å¥è¯ä¸­å¾—åˆ°Destination: Taipeiï¼Œtime of arrival: November 2nd. è¿™ä¸ªè¿‡ç¨‹ä¹Ÿå°±æ˜¯Solt Filling ï¼ˆæ§½ä½å¡«å……ï¼‰ã€‚ å¦‚æžœç”¨Feedforward networkæ¥è§£å†³solt fillingé—®é¢˜ï¼Œè¾“å…¥å°±æ˜¯å•è¯ï¼Œè¾“å‡ºæ˜¯æ¯ä¸ªæ§½ä½ï¼ˆslotï¼‰çš„å•è¯ï¼Œå¦‚ä¸‹å›¾ã€‚ ä¸Šå›¾ä¸­ï¼Œå¦‚ä½•å°†wordè¡¨ç¤ºä¸ºä¸€ä¸ªvectorï¼Ÿ EncodingHow to represent each word as a vector? 1-of-N encodingæœ€ç®€å•çš„æ–¹å¼æ˜¯1-of-N encodingæ–¹å¼ï¼ˆç‹¬çƒ­æ–¹å¼ï¼‰ã€‚ å‘é‡ç»´åº¦å¤§å°æ˜¯æ•´ä¸ªè¯æ±‡è¡¨çš„å¤§å°ï¼Œæ¯ä¸€ä¸ªç»´åº¦ä»£è¡¨è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªå•è¯ï¼Œå¦‚æžœè¯¥ç»´åº¦ç½®1ï¼Œè¡¨ç¤ºè¿™ä¸ªç»´åº¦ä»£è¡¨çš„å•è¯ã€‚ Beyond 1-of-N encodingå¯¹1-of-N encodingæ–¹å¼æ”¹è¿›ã€‚ ç¬¬ä¸€ç§ï¼šDimension for â€œOtherâ€ åœ¨1-of-Nçš„åŸºç¡€ä¸Šå¢žåŠ ä¸€ç»´åº¦â€”â€”â€˜otherâ€™ç»´åº¦ï¼Œå³å½“å•è¯ä¸åœ¨ç³»ç»Ÿè¯æ±‡è¡¨ä¸­ï¼Œå°†otherç»´åº¦ç½®1ä»£è¡¨è¯¥å•è¯ã€‚ ç¬¬äºŒç§ï¼šWord hashing å³ä¾¿æ˜¯å¢žåŠ äº†â€otherâ€ç»´åº¦ï¼Œç¼–ç vectorçš„ç»´åº¦ä¹Ÿå¾ˆå¤§ï¼Œç”¨word hashingçš„æ–¹å¼å°†å¤§å¹…å‡å°‘ç»´åº¦ã€‚ ä»¥appleä¸ºä¾‹ï¼Œæ‹†æˆapp, ppl, pleä¸‰ä¸ªéƒ¨åˆ†ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œvectorä¸­è¡¨ç¤ºè¿™ä¸‰ä¸ªéƒ¨åˆ†çš„ç»´åº¦ç½®1ã€‚ ç”¨è¿™æ ·çš„word hashingæ–¹å¼ï¼Œvectorçš„ç»´åº¦åªæœ‰ $26\\times 26\\times26$ ï¼Œå¤§å¹…å‡å°‘è¯å‘é‡çš„ç»´åº¦ã€‚ Exampleé€šè¿‡encodingçš„æ–¹å¼ï¼Œå•è¯ç”¨vectoræ¥è¡¨ç¤ºï¼Œç”¨å‰é¦ˆç¥žç»ç½‘ç»œæ¥è§£å†³solt fillingé—®é¢˜ã€‚ å¦‚ä¸‹å›¾. input:ä¸€ä¸ªå•è¯ï¼ˆencodingä¸ºvectorï¼‰ output: inputå•è¯ä¸­å±žäºŽè¯¥æ§½ä½(solts)çš„æ¦‚çŽ‡åˆ†å¸ƒ(vector)ã€‚ ä½†ç”¨æ™®é€šçš„å‰é¦ˆç¥žç»ç½‘ç»œå¤„ç†solt fillingé—®é¢˜ä¼šå‡ºçŽ°ä¸‹å›¾é—®é¢˜ï¼š ä¸Šå›¾ä¸­ï¼Œarrive Taipei on November 2nd å’Œ leave Taipei on November 2ndï¼Œå°†è¿™ä¸¤å¥è¯çš„æ¯ä¸ªå•è¯ï¼ˆvectorï¼‰æ”¾å…¥å‰é¦ˆç¥žç»ç½‘ç»œï¼Œå¾—å‡ºçš„destæ§½ä½éƒ½åº”è¯¥æ˜¯Taipeiã€‚ ä½†ï¼Œé€šè¿‡ä¹‹å‰çš„è¯­æ„ï¼Œarrive Taipeiçš„Taipeiåº”è¯¥æ˜¯ç»ˆç‚¹ï¼Œè€Œleave Taipeiçš„Taipeiæ˜¯èµ·ç‚¹ã€‚ å› æ­¤ï¼Œåœ¨å¤„ç†è¿™ç§é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬çš„ç¥žç»ç½‘ç»œåº”è¯¥éœ€è¦memoryï¼Œå¯¹è¯¥è¾“å…¥çš„ä¸Šä¸‹æ–‡æœ‰ä¸€å®šçš„è®°å¿†å­˜å‚¨ã€‚ Recurrent Neural Network(RNN)Basic structureå› æ­¤ï¼Œæˆ‘ä»¬å¯¹ä¸€èˆ¬çš„å‰é¦ˆç¥žç»ç½‘ç»œåŠ å…¥è®°å¿†å…ƒä»¶a, a å­˜å‚¨hidden layerçš„è¾“å‡ºï¼ŒåŒæ—¶aä¹Ÿä½œä¸ºä¸‹ä¸€æ¬¡è®¡ç®—çš„è¾“å…¥éƒ¨åˆ†,ä¸‹å›¾å°±æ˜¯æœ€åŸºç¡€çš„RNNæ¨¡åž‹ã€‚ ä¸¾ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜Žè¯¥è¿‡ç¨‹ï¼š Input sequence: $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}2 \\ 2 \\end{bmatrix}$ â€¦ RNNæ¨¡åž‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼šæ‰€æœ‰çš„weightéƒ½æ˜¯1ï¼Œæ²¡æœ‰bias; æ‰€æœ‰çš„ç¥žç»å…ƒçš„activation function éƒ½æ˜¯çº¿æ€§çš„ã€‚ input : $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$, è®°å¿†å…ƒä»¶åˆå€¼ a1=0 a2=0. è®°å¿†å…ƒä»¶ä¹Ÿä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼Œhidden layerçš„è¾“å‡ºä¸º 2 2, æ›´æ–°è®°å¿†å…ƒä»¶çš„å€¼. output: $\\begin{bmatrix}4 \\ 4 \\end{bmatrix}$ , è®°å¿†å…ƒä»¶å­˜å‚¨å€¼ a1=2 a2=2. input : $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ , è®°å¿†å…ƒä»¶å­˜å‚¨å€¼ a1=2 a2=2. è®°å¿†å…ƒä»¶ä¹Ÿä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼Œhidden layer çš„è¾“å‡ºä¸º6 6,æ›´æ–°è®°å¿†å…ƒä»¶çš„å€¼ã€‚ output: $\\begin{bmatrix}12 \\ 12 \\end{bmatrix}$ , è®°å¿†å…ƒä»¶å­˜å‚¨å€¼ a1=6 a2=6. è¿™é‡Œå¯ä»¥å‘çŽ°ï¼Œç¬¬ä¸€æ¬¡å’Œç¬¬äºŒæ¬¡çš„è¾“å…¥ç›¸åŒï¼Œä½†æ˜¯ç”±äºŽæœ‰è®°å¿†å…ƒä»¶çš„ç¼˜æ•…ï¼Œä¸¤æ¬¡è¾“å‡ºä¸åŒã€‚ input : $\\begin{bmatrix}2 \\ 2 \\end{bmatrix}$ , è®°å¿†å…ƒä»¶å­˜å‚¨å€¼ a1=6 a2=6. è®°å¿†å…ƒä»¶ä¹Ÿä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼Œhidden layer çš„è¾“å‡ºä¸º16 16,æ›´æ–°è®°å¿†å…ƒä»¶çš„å€¼ã€‚ output: $\\begin{bmatrix}32 \\ 32 \\end{bmatrix}$ , è®°å¿†å…ƒä»¶å­˜å‚¨å€¼ a1=16 a2=16. RNNä¸­ï¼Œç”±äºŽæœ‰memoryï¼Œä¼šå’Œä¸€èˆ¬å‰é¦ˆæ¨¡åž‹æœ‰ä¸¤ä¸ªä¸åŒçš„åœ°æ–¹ï¼šä¸€æ˜¯è¾“å…¥ç›¸åŒçš„vectorï¼Œè¾“å‡ºå¯èƒ½æ˜¯ä¸åŒçš„ï¼›äºŒæ˜¯å°†ä¸€ä¸ªsequenceè¿žç»­æ”¾è¿›RNNæ¨¡åž‹ä¸­ï¼Œå¦‚æžœsequenceä¸­æ”¹å˜é¡ºåºï¼Œè¾“å‡ºä¹Ÿå¤§å¤šä¸åŒã€‚ ç”¨è¿™ä¸ªRNNæ¨¡åž‹æ¥è§£å†³ä¹‹å‰çš„solt fillingé—®é¢˜ï¼Œå°±å¯ä»¥è§£å†³ä¸Šä¸‹æ–‡è¯­æ„ä¸åŒå½±å“soltçš„é—®é¢˜ã€‚ å°†arrive Taipei on November 2ndçš„æ¯ä¸ªå•è¯éƒ½æ”¾å…¥åŒæ ·çš„æ¨¡åž‹ä¸­ã€‚ å› æ­¤å°†RNNå±•å¼€ï¼Œå¦‚ä¸Šå›¾ï¼Œåƒä¸åŒæ—¶é—´ç‚¹çš„æ¨¡åž‹ï¼Œä½†å…¶å®žæ˜¯ä¸åŒæ—¶é—´ç‚¹å¾ªçŽ¯ä½¿ç”¨åŒä¸€ä¸ªæ¨¡åž‹ã€‚ ç”±äºŽå·¦è¾¹çš„å‰æ–‡æ˜¯arriveï¼Œå³è¾¹çš„å‰æ–‡æ˜¯leaveï¼Œæ‰€ä»¥å­˜å‚¨åœ¨memoryä¸­çš„å€¼ä¸åŒï¼ŒTaipeiä½œä¸ºinputçš„è¾“å‡ºï¼ˆæ§½ä½çš„æ¦‚çŽ‡åˆ†å¸ƒï¼‰ä¹Ÿä¸åŒã€‚ Elman Network &amp; Jordan Networkä¸Šæ–‡ä¸­åªæ˜¯RNNæ¨¡åž‹ä¸­çš„ä¸€ç§ï¼Œå³Elman Networkï¼Œè®°å¿†å…ƒä»¶å­˜å‚¨çš„æ˜¯ä¸Šä¸€ä¸ªæ—¶é—´ç‚¹hidden layerçš„è¾“å‡ºã€‚ è€ŒJordan Networkæ¨¡åž‹ä¸­,ä»–çš„è®°å¿†å…ƒä»¶å­˜å‚¨çš„æ˜¯ä¸Šä¸€æ—¶é—´ç‚¹çš„outputã€‚ ï¼ˆæ®è¯´ï¼Œè®°å¿†å…ƒä»¶ä¸­å­˜å‚¨outputçš„å€¼ä¼šæœ‰è¾ƒå¥½çš„performanceï¼Œå› ä¸ºoutputæ˜¯æœ‰target vectorçš„ï¼Œå› æ­¤èƒ½å…·è±¡çš„ä½“çŽ°æ”¾è¿›memoryçš„æ˜¯ä»€ä¹ˆï¼‰ Bidirectional RNNä¸Šæ–‡ä¸­çš„RNNæ¨¡åž‹ï¼Œè®°å¿†å…ƒä»¶ä¸­å­˜å‚¨çš„éƒ½æ˜¯ä¸Šæ–‡çš„ä¿¡æ¯ï¼Œå¦‚æžœè¦åŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå³æ˜¯bidirectional RNN(åŒå‘RNN)ã€‚ æ¨¡åž‹å¦‚ä¸‹å›¾ã€‚ åŒå‘RNNçš„å¥½å¤„æ˜¯çœ‹çš„èŒƒå›´æ¯”è¾ƒå¹¿ï¼Œå½“è®¡ç®—è¾“å‡º $y^t$ æ—¶ï¼Œä¸Šä¸‹æ–‡çš„å†…å®¹éƒ½æœ‰è€ƒè™‘åˆ°ã€‚ Long Short-term Memory(LSTM)çŽ°åœ¨æœ€å¸¸ç”¨çš„RNNæ¨¡åž‹æ˜¯LSTMï¼ŒLong Short-term Memoryï¼Œè¿™é‡Œçš„longæ˜¯ç›¸å½“äºŽä¸Šæ–‡ä¸­çš„RNNæ¨¡åž‹ï¼Œå› ä¸ºä¸Šæ–‡æåˆ°çš„RNNæ¨¡åž‹éƒ½æ˜¯short-term,å³æ¯ä¸€ä¸ªæ—¶é—´ç‚¹ï¼Œéƒ½ä¼šæŠŠmemoryä¸­çš„å€¼æ´—æŽ‰ï¼ŒLSTMçš„longï¼Œå°±æ˜¯ä¼šæŠŠmemoryçš„å€¼ä¿ç•™çš„ç›¸å¯¹äºŽä¹…ä¸€äº›ã€‚ LSTMå¦‚ä¸‹å›¾ï¼Œä¸Žä¸€èˆ¬NNä¸åŒçš„åœ°æ–¹æ˜¯ï¼Œä»–æœ‰4ä¸ªinputs,ä¸€ä¸ªoutputsã€‚ LSTMä¸»è¦æœ‰å››éƒ¨åˆ†ç»„æˆï¼š Input Gateï¼šè¾“å…¥é—¨ï¼Œä¸‹æ–¹ç®­å¤´æ˜¯è¾“å…¥ï¼Œå·¦æ–¹ç®­å¤´æ˜¯è¾“å…¥ä¿¡å·æŽ§åˆ¶è¾“å…¥é—¨çš„æ‰“å¼€ç¨‹åº¦ï¼Œå®Œå…¨æ‰“å¼€LSTMæ‰èƒ½å°†è¾“å…¥å€¼å®Œå…¨è¯»å…¥ï¼Œæ‰“å¼€çš„ç¨‹åº¦ä¹Ÿæ˜¯NNè‡ªå·±å­¦ã€‚ Output Gateï¼šè¾“å‡ºé—¨ï¼Œä¸Šæ–¹ç®­å¤´æ˜¯è¾“å‡ºï¼Œå·¦æ–¹ç®­å¤´æ˜¯è¾“å…¥ä¿¡å·æŽ§åˆ¶è¾“å‡ºé—¨çš„æ‰“å¼€ç¨‹åº¦ï¼ŒåŒç†ï¼Œæ‰“å¼€ç¨‹åº¦ä¹Ÿæ˜¯NNè‡ªå·±å­¦ä¹ ã€‚ Memory Cellï¼šè®°å¿†å…ƒä»¶ã€‚ Forget Gateï¼šé—å¿˜é—¨ï¼Œå³è¾¹çš„ç®­å¤´æ˜¯è¾“å…¥ä¿¡å·æŽ§åˆ¶é—å¿˜é—¨çš„æ‰“å¼€ç¨‹åº¦ï¼ŒæŽ§åˆ¶å°†memory cellæ´—æŽ‰çš„ç¨‹åº¦ã€‚ LSTMæ›´è¯¦ç»†çš„é˜è¿°LSTMçš„å†…éƒ¨æœºåˆ¶ï¼š æ³¨æ„ï¼š $z_o,z_i,z_f$ æ˜¯é—¨çš„signal control,å…¶å®žå°±ç­‰åŒäºŽä¸€èˆ¬NNä¸­neuronçš„è¾“å…¥zï¼Œæ˜¯scalarã€‚ gateå…¶å®žå°±æ˜¯ä¸€ä¸ªneuronï¼Œé€šå¸¸gate neuron çš„activation function få– sigmod,å› ä¸ºå€¼åŸŸåœ¨0åˆ°1ä¹‹é—´ï¼Œå³å¯¹åº”é—¨çš„æ‰“å¼€ç¨‹åº¦ã€‚ input/forget/output gateçš„neuronçš„activation functionæ˜¯f(sigmod function), input neuronçš„activation functionæ˜¯gã€‚ input gateæŽ§åˆ¶è¾“å…¥:$g(z)f(z_i)$ input: z $\\rightarrow$ $g(z)$ input gate signal control: $z_i \\rightarrow f(z_i)$ multiplyï¼š$g(z)f(z_i)$ forget gate æŽ§åˆ¶memoryï¼š$cf(z_f)$ forget gate signal control: $z_f\\rightarrow f(z_f)$ å¦‚æžœ $f(z_f)=1$ ,è¯´æ˜Žmemoryé‡Œçš„å€¼ä¿ç•™ï¼›å¦‚æžœ $f(z_f)=0$ ,è¯´æ˜Žmemoryé‡Œçš„å€¼æ´—æŽ‰ã€‚ æ›´æ–°å½“å‰æ—¶é—´ç‚¹çš„memory(è¾“å…¥+æ—§çš„memoryå€¼) ï¼š$câ€™=g(z)f(z_i)+cf(z_f)$ output gate æŽ§åˆ¶è¾“å‡ºï¼š$h(câ€™)f(z_o)$ output: $câ€™ \\rightarrow h(câ€™)$ output gare signal control: $z_o \\rightarrow f(z_o)$ multiply: $h(câ€™)f(z_o)$ LSTMæ¨¡åž‹ï¼ˆtrainedï¼‰å¦‚ä¸‹å›¾ï¼š è¾“å…¥åºåˆ—ä¸ºï¼š $\\begin{bmatrix}3 \\ 1 \\ 0 \\end{bmatrix}$$\\begin{bmatrix}4 \\ 1 \\ 0 \\end{bmatrix}$ $\\begin{bmatrix}2 \\ 0 \\ 0 \\end{bmatrix}$ $\\begin{bmatrix}1 \\ 0 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}3 \\ -1 \\ 0 \\end{bmatrix}$ è¯¥LSTM activation function: gã€héƒ½ä¸ºlinear functionï¼ˆå³è¾“å‡ºç­‰äºŽè¾“å…¥ï¼‰ï¼Œfä¸ºsigmod. é€šè¿‡è¯¥LSTMçš„è¾“å‡ºåºåˆ—ä¸ºï¼š 0 0 0 7 0 0 ï¼ˆå»ºè®®æ‰‹ç®—ä¸€éï¼‰ Compared with Original Networkoriginal networkå¦‚ä¸‹å›¾ï¼š LSTM çš„NNå³ç”¨LSTMæ›¿æ¢åŽŸæ¥çš„neuronï¼Œè¿™ä¸ªneuronæœ‰å››ä¸ªinputsï¼Œç›¸å¯¹äºŽoriginal networkä¹Ÿæœ‰4å€çš„å‚æ•°ï¼Œå¦‚ä¸‹å›¾ï¼š æ‰€ä»¥åŽŸæ¥RNNçš„neuronæ¢ä¸ºLSTMï¼Œå°±æ˜¯ä¸‹å›¾ï¼š ä¸Šå›¾ä¸­ï¼š è¿™é‡Œçš„ $z^f,z^u,z,z^o$ éƒ½æ˜¯ $x^t \\begin{bmatrix} \\quad\\end{bmatrix}$ çŸ©é˜µè¿ç®—å¾—åˆ°çš„vector, å› ä¸ºä¸Šå›¾ä¸­æœ‰å¤šä¸ªLSTMï¼Œå› æ­¤ $z^i$ çš„ç¬¬kä¸ªå…ƒç´ ï¼Œå°±æ˜¯æŽ§åˆ¶ç¬¬kä¸ªLSTMçš„input signal control scalarã€‚æ‰€ä»¥ï¼Œ$z^f,z^u,z,z^o$ çš„ç»´åº¦ç­‰äºŽä¸‹ä¸€å±‚neuron/LSTMçš„ä¸ªæ•°ã€‚ æ‰€ä»¥è¿™é‡Œmemoryï¼ˆcellï¼‰$c^t$ ä¹Ÿæ˜¯ä¸€ä¸ªvectorï¼Œç¬¬kä¸ªå…ƒç´ æ˜¯ç¬¬kä¸ªLSTMä¸­cellå­˜å‚¨çš„å€¼ã€‚ å‘é‡è¿ç®—å’Œscalarä¸€æ ·ï¼ŒLSTMç»†èŠ‚å¦‚ä¸‹å›¾ï¼š Extensionï¼šâ€œpeepholeâ€ä¸Šå°èŠ‚çš„LSTMæ˜¯simplifiedï¼Œå°†LSTM hidden layerçš„è¾“å‡º $h^t$ å’Œcellä¸­å­˜å‚¨çš„å€¼ $c^t$ å’Œä¸‹ä¸€æ—¶é—´ç‚¹çš„è¾“å…¥ $x^{t+1}$ ä¸€åŒä½œä¸ºä¸‹ä¸€æ—¶é—´ç‚¹çš„è¾“å…¥ï¼Œå°±æ˜¯LSTMçš„æ‰©å±•ç‰ˆâ€peepholeâ€ã€‚ å¦‚ä¸‹å›¾ï¼š Multi-layer LSTMå¤šå±‚çš„peephole LSTMå¦‚ä¸‹å›¾ï¼š ï¼ˆï¼šwtf æˆ‘åˆ°åº•çœ‹åˆ°äº†ä»€ä¹ˆ ä¸è¦æ€•ï¼šKerasã€PyTorchç­‰å¥—ä»¶éƒ½æœ‰ â€œLSTMâ€ï¼Œâ€œGURï¼Œâ€SimpleRNNâ€œ å·²å®žçŽ°å¥½çš„layers. Learningè®­ç»ƒRNNæ—¶ï¼Œè¾“å…¥ä¸Žtargetå¦‚ä¸‹æ‰€ç¤ºï¼š ä¼°æµ‹æ¨¡åž‹çš„å¥½åï¼Œè®¡ç®—RNNçš„Lossæ—¶ï¼Œéœ€è¦çœ‹ä½œä¸€ä¸ªæ•´ä½“ï¼Œè®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹RNNè¾“å‡ºä¸Žtargetçš„crossentropyçš„å’Œã€‚ è®­ç»ƒä¹Ÿå¯åŒæ ·ç”¨Backpropagationï¼Œä½†è€ƒè™‘åˆ°æ—¶é—´ç‚¹ï¼Œæœ‰ä¸€ä¸ªè¿›é˜¶ç‰ˆçš„â€Backpropogation through time(BPTT)â€[1]ã€‚ RNNä¸€èˆ¬å°±ç”¨BPTTè®­ç»ƒã€‚ How to train wellnot easy to trainRNN-based network is not always easy to learn. ä½†åŸºäºŽRNNçš„æ¨¡åž‹å¾€å¾€ä¸å¤ªå¥½è®­ç»ƒï¼Œæ€»æ˜¯ä¼šå‡ºçŽ°ä¸‹å›¾ä¸­çš„ç»¿è‰²çº¿æƒ…å†µï¼ˆå³æŠ–åŠ¨ï¼‰ã€‚ error surface is rougherror surfaceï¼Œå³total lossåœ¨å‚æ•°å˜åŒ–æ—¶çš„å‡½æ•°å›¾ã€‚ ä¼šå‘çŽ°åŸºäºŽRNNçš„æ¨¡åž‹çš„error surfaceä¼šé•¿ä¸‹å›¾è¿™ä¸ªæ ·å­ï¼šæœ‰æ—¶å¾ˆå¹³å¦(flat)æœ‰æ—¶å¾ˆé™¡å³­(steep) æ©™è‰²ç‚¹å‡ºå‘ï¼š èµ·åˆå¤„åœ¨flatçš„ä½ç½®ã€‚ éšç€ä¸€æ¬¡æ¬¡æ›´æ–°ï¼Œgradientåœ¨å˜å°ï¼Œlearning rateå³ä¼šå˜å¤§ã€‚ å¯èƒ½ç¨å¾®ä¸å¹¸ï¼Œå°±ä¼šå‡ºçŽ°è·¨è¿‡æ‚¬å´–ï¼Œå³å‡ºçŽ°äº†å‰§çƒˆéœ‡è¡çš„é—®é¢˜ã€‚ å¦‚æžœåˆšå¥½å½“å‰å¤„åœ¨æ‚¬å´–ä½Žï¼Œè¿™æ—¶çš„gradientå¾ˆå¤§ï¼Œlearning rateä¹Ÿå¾ˆå¤§ï¼Œstepå°±ä¼šå¾ˆå¤§ï¼Œé£žå‡ºåŽ»ï¼Œæžå¯èƒ½å‡ºçŽ°segment fault(NaN). Thomas Mikolv ç”¨å·¥ç¨‹å¸ˆçš„è§’åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå³å½“æ­¤æ—¶çš„gradientå¤§äºŽæŸä¸ªé˜ˆå€¼(threshold)æ—¶ï¼Œå°±ä¸è¦è®©å½“å‰çš„gradientè¶…è¿‡è¿™ä¸ªé˜ˆå€¼ï¼ˆé€šå¸¸å–15ï¼‰ã€‚ è¿™æ ·å¤„åœ¨æ‚¬å´–ä½Žçš„æ©™è‰²ç‚¹ï¼Œï¼ˆClippingè·¯çº¿ï¼‰ï¼Œæ›´æ–°å°±ä¼šåˆ°ç»¿è‰²çš„ï¼Œç»§ç»­æ›´æ–°ã€‚ Whyä¸ºä»€ä¹ˆRNNæ¨¡åž‹ä¼šå‡ºçŽ°æŠ–åŠ¨çš„æƒ…å†µå‘¢ï¼Ÿ ç”¨ä¸‹å›¾è¿™ä¸ªç®€å•ä¾‹å­è¯´æ˜Žï¼ˆä¸€èˆ¬activation functionç”¨sigmod,è€ŒReLuçš„performanceä¸€èˆ¬è¾ƒå·®ï¼‰ï¼š ä¸Šå›¾ä¸­ï¼Œè¾“å…¥åºåˆ—æ˜¯1 0 0 0 â€¦ï¼Œmemoryè¿žæŽ¥ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„æƒé‡æ˜¯wï¼Œå¯ä»¥è½»æ˜“å¾—åˆ°æœ€åŽä¸€ä¸ªæ—¶é—´ç‚¹çš„è¾“å‡º $y^{1000}=w^{999}$ ã€‚ ä¸Šå›¾ä¸­ï¼Œå¾ªçŽ¯è¾“å‡º1000æ¬¡ï¼Œå¦‚æžœwå˜åŒ– $\\Delta w$ ï¼Œçœ‹è¾“å‡º $y^{1000}$ çš„å˜åŒ–ï¼Œæ¥ç›´è§‚ä½“çŽ°gradient çš„å˜åŒ–ï¼š ä¸Šå›¾ä¸­ï¼Œå¯ä»¥çœ‹å‡ºï¼š ç»¿è‰²éƒ¨åˆ†ï¼šå½“wä»Ž1å˜åŒ–ä¸º1.01æ—¶ï¼Œ $y^{1000}$ çš„è¾“å‡ºå˜åŒ–å³å¤§ï¼Œæ—¢æœ‰è¾ƒå¤§çš„gradientï¼Œç†åº”æœ‰å°çš„learning rateã€‚ é»„è‰²éƒ¨åˆ†ï¼šå½“wä»Ž0.99å˜åŒ–ä¸º0.01æ—¶ï¼Œ $y^{1000}$ çš„è¾“å‡ºå‡ ä¹Žä¸å˜åŒ–ï¼Œå³æœ‰è¾ƒå°çš„gradientï¼Œç†åº”æœ‰å¤§å¤§learning rate. åœ¨å¾ˆå°çš„åœ°æ–¹ï¼ˆ0.01 åˆ° 1.01ï¼‰ï¼Œä»–çš„gradientå°±å˜åŒ–å³å¤§ï¼Œå³æŠ–åŠ¨çš„å‡ºçŽ°ã€‚ Reasonï¼šRNNï¼Œè™½ç„¶å¯ä»¥çœ‹ä½œä¸åŒæ—¶é—´ç‚¹çš„å±•å¼€è®¡ç®—ï¼Œä½†å§‹ç»ˆæ˜¯åŒä¸€ä¸ªNNçš„æƒé‡è®¡ç®—ï¼ˆcellè¿žæŽ¥åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„æƒé‡ï¼‰ï¼Œåœ¨ä¸åŒæ—¶é—´ä¸­ï¼Œåå¤å ä¹˜ï¼Œå› æ­¤ä¼šå‡ºçŽ°è¿™ç§æƒ…å†µã€‚ Helpful Techniques LSTMå‡ ä¹Žå·²ç»ç®—RNNçš„ä¸€ä¸ªæ ‡å‡†äº†ï¼Œä¸ºä»€ä¹ˆLSTMçš„performanceæ¯”è¾ƒå¥½å‘¢ã€‚ ä¸ºä»€ä¹ˆç”¨LSTMæ›¿æ¢ä¸ºRNNï¼Ÿ :Can deal with gradient vanishing(not gradient explode). å¯ä»¥è§£å†³gradient vanishçš„é—®é¢˜ï¼ˆgradient vanish problem å…·ä½“è§ è¿™ç¯‡æ–‡ç« 2.1.1ï¼‰ ä¸ºä»€ä¹ˆLSTMå¯ä»¥è§£å†³gradient vanishé—®é¢˜ ï¼šmemory and input are added.ï¼ˆLSTMçš„çš„è¾“å‡ºä¸Žè¾“å…¥å’Œmemoryæœ‰å…³ï¼‰ : The influence never disappears unless forget gate is closed.ï¼ˆmemoryçš„å½±å“å¯ä»¥å¾ˆæŒä¹…ï¼‰ GRU[2]ï¼ˆGated Recurrent Unitï¼‰ï¼šæ˜¯åªæœ‰ä¸¤ä¸ªGateï¼Œæ¯”LSTMç®€å•ï¼Œå‚æ•°æ›´å°‘ï¼Œä¸å®¹æ˜“overfitting çŽ„å­¦äº†å­ More Applicationsã€å¾…æ›´æ–°ã€‘ Many to OneMany to ManyBeyond SequenceSeq2SeqAuto-encoder-TextAuto-encoder-SpeechChat-botReference BPTT GRU","link":"/2020/06/10/rnn/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€:Semi-supervised Learning","text":"è¿™ç¯‡æ–‡ç« å¼€ç¯‡è®²è¿°äº†ä»€ä¹ˆæ˜¯Semi-supervised Learningï¼ˆåŠç›‘ç£å­¦ä¹ ï¼‰ï¼Ÿ å†æ¬¡ï¼Œæ–‡ç« å…·ä½“é˜è¿°äº†å››ç§Semi-supervised Learningï¼ŒåŒ…æ‹¬Generative Modelï¼ŒLow-densityï¼ŒSmoothness Assumptionå’ŒBetter Representationã€‚ å¯¹äºŽGenerative Modelï¼Œæ–‡ç« é‡ç‚¹è®²è¿°äº†å¦‚ä½•ç”¨EMç®—æ³•æ¥è®­ç»ƒæ¨¡åž‹ã€‚ å¯¹äºŽLow-densityï¼Œæ–‡ç« é‡ç‚¹è®²è¿°äº†å¦‚ä½•è®©æ¨¡åž‹è¿›è¡ŒSelf-trainingï¼Œå¹¶ä¸”åœ¨è®­ç»ƒä¸­å¼•å…¥Entropy-based Regularization termæ¥å°½å¯èƒ½low-densityçš„å‡è®¾ã€‚ å¯¹äºŽSmoothness Assumptionï¼Œæ–‡ç« é‡ç‚¹è®²è¿°äº†Graph-based Approachï¼ˆåŸºäºŽå›¾çš„æ–¹æ³•ï¼‰ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒä¸­å¼•å…¥Smoothness Regularization termæ¥å°½å¯èƒ½æ»¡è¶³Smoothness Assumptionçš„å‡è®¾ã€‚ å¯¹äºŽBetter Representationï¼Œæœ¬ç¯‡æ–‡ç« åªæ˜¯ç®€å•é˜è¿°äº†å…¶æ€æƒ³ï¼Œå…·ä½“ä»‹ç»è§è¿™ç¯‡åšå®¢ã€‚ Introductionä»€ä¹ˆæ˜¯Semi-supervised learning(åŠç›‘ç£å­¦ä¹ )ï¼Ÿå’ŒSupervised learningï¼ˆç›‘ç£å¼å­¦ä¹ ï¼‰çš„åŒºåˆ«åœ¨å“ªï¼Ÿ Supervised learningï¼ˆç›‘ç£å¼å­¦ä¹ ï¼‰ï¼š ç”¨æ¥è®­ç»ƒçš„æ•°æ®é›† $R$ ä¸­çš„æ•°æ®labeled dataï¼Œå³ ${(x^r,\\hat{y}^r)}_{r=1}^R$ . æ¯”å¦‚åœ¨å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸­ï¼š $x^r$ æ˜¯imageï¼Œå¯¹åº”çš„target output $y^r$ æ˜¯åˆ†ç±»çš„labelã€‚ è€ŒSemi-supervised learningï¼ˆåŠç›‘ç£å¼å­¦ä¹ ï¼‰ï¼š ç”¨æ¥çš„è®­ç»ƒçš„æ•°æ®é›†ç”±ä¸¤éƒ¨åˆ†ç»„æˆ $\\{(x^r,\\hat{y}^r)\\}_{r=1}^R$ , $\\{x^u\\}_{u=R}^{R+U}$ ï¼Œå³labeled dataå’Œunlabeled dataï¼Œè€Œä¸”é€šå¸¸æƒ…å†µä¸‹ï¼Œunlabeled dataçš„æ•°é‡è¿œè¿œé«˜äºŽlabeled dataçš„æ•°é‡ï¼Œå³ $U&gt;&gt;R$ . å¯¹äºŽä¸€èˆ¬çš„æœºå™¨å­¦ä¹ ï¼Œæœ‰è®­ç»ƒé›†ï¼ˆlabeledï¼‰å’Œæµ‹è¯•é›†ï¼Œæµ‹è¯•é›†æ˜¯ä¸ä¼šå‡ºçŽ°åœ¨è®­ç»ƒé›†ä¸­çš„ï¼Œè¿™ç§æƒ…å†µå°±æ˜¯inductive learningï¼ˆå½’çº³æŽ¨ç†ï¼Œå³é€šè¿‡å·²æœ‰çš„labeledçš„dataåŽ»æŽ¨æ–­æ²¡æœ‰è§è¿‡çš„å…¶ä»–çš„æ•°æ®çš„labelï¼‰ã€‚ è€ŒSemi-supervised learning åˆåˆ†ä¸ºä¸¤ç§ï¼ŒTransductive learning ï¼ˆè½¬å¯¼/æŽ¨è®ºæŽ¨å¯¼ï¼‰å’Œ Inductive learningï¼ˆå½’çº³æŽ¨ç†ï¼‰ Transductive learing: unlabeled data is the testing data. å³è¿™é‡Œç”¨æ¥è®­ç»ƒçš„ $\\{x^u\\}_{u=R}^{R+U}$ å°±æ˜¯æ¥è‡ªæµ‹è¯•æ•°æ®é›†ä¸­çš„æ•°æ®ã€‚ï¼ˆåªä½¿ç”¨ä»–çš„featureï¼Œè€Œä¸ä½¿ç”¨ä»–çš„labelï¼ï¼‰ Inductive learning: unlabeled data is not the testing data.å³ç”¨æ¥è®­ç»ƒçš„ $\\{x^u\\}_{u=R}^{R+U}$ ä¸æ˜¯æ¥è‡ªæµ‹è¯•æ•°æ®é›†ä¸­çš„æ•°æ®ï¼Œæ˜¯å¦å¤–çš„unlabeled dataã€‚ è¿™é‡Œçš„ä½¿ç”¨testing dataæ˜¯æŒ‡ä½¿ç”¨testing dataçš„featureï¼Œå³unlabelè€Œä¸æ˜¯ä½¿ç”¨testing dataçš„labelã€‚ ä¸ºä»€ä¹ˆä¼šæœ‰semi-supervised learningï¼Ÿ Collecting data is easy, but collecting â€œlabelledâ€ data is expensive. ã€æ”¶é›†æ•°æ®å¾ˆç®€å•ï¼Œä½†æ”¶é›†æœ‰labelçš„æ•°æ®å¾ˆéš¾ã€‘ We do semi-supervised learning in our lives ã€åœ¨ç”Ÿæ´»ä¸­ï¼Œæ›´å¤šçš„ä¹Ÿæ˜¯åŠç›‘ç£å¼å­¦ä¹ ï¼Œæˆ‘ä»¬èƒ½æ˜Žç™½å°‘é‡çœ‹åˆ°çš„äº‹ç‰©ï¼Œä½†çœ‹åˆ°äº†æ›´å¤šæˆ‘ä»¬ä¸æ‡‚çš„ï¼Œå³unlabeled dataã€‘ Why Semi-supervised learning helpsä¸ºä»€ä¹ˆåŠç›‘ç£å­¦ä¹ èƒ½å¸®åŠ©è§£å†³ä¸€äº›é—®é¢˜ï¼Ÿ å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¦‚æžœåªæœ‰labeled dataï¼Œåˆ†ç±»æ‰€ç”»çš„boundaryå¯èƒ½æ˜¯ä¸€æ¡ç«–çº¿ã€‚ ä½†å¦‚æžœæœ‰ä¸€äº›unlabeled dataï¼ˆå¦‚ç°è‰²çš„ç‚¹ï¼‰ï¼Œåˆ†ç±»æ‰€ç”»çš„boundaryå¯èƒ½æ˜¯ä¸€æ¡æ–œçº¿ã€‚ The distribution of the unlabeled data tell us something. åŠç›‘ç£å¼å­¦ä¹ ä¹‹æ‰€ä»¥æœ‰ç”¨ï¼Œæ˜¯å› ä¸ºè¿™äº›unlabeled dataçš„åˆ†å¸ƒèƒ½å‘Šè¯‰æˆ‘ä»¬ä¸€äº›ä¸œè¥¿ã€‚ é€šå¸¸è¿™ä¹Ÿä¼´éšç€ä¸€äº›å‡è®¾ï¼Œæ‰€ä»¥åŠç›‘ç£å¼å­¦ä¹ æ˜¯å¦æœ‰ç”¨å¾€å¾€å–å†³äºŽè¿™äº›å‡è®¾æ˜¯å¦åˆç†ã€‚ Semi-supervised Learning for Generative ModelSupervised Generative Modelåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæœ‰è¯¦ç»†è®²è¿°åˆ†ç±»é—®é¢˜ä¸­çš„generative modelã€‚ ç»™å®šä¸€ä¸ªlabelled training data $x^r\\in C_1,C_2$ è®­ç»ƒé›†ã€‚ prior probabilityï¼ˆå…ˆéªŒæ¦‚çŽ‡ï¼‰æœ‰ $P(C_i)$ å’Œ $P(x|C_i)$ ï¼Œå‡è®¾æ˜¯Gaussianæ¨¡åž‹ï¼Œåˆ™ $P(x|C_i)$ ç”±Gaussianæ¨¡åž‹ä¸­çš„ $\\mu^i,\\Sigma$ å‚æ•°å†³å®šã€‚ æ ¹æ®å·²æœ‰çš„labeled dataï¼Œè®¡ç®—å‡ºå‡è®¾çš„Gaussianæ¨¡åž‹çš„å‚æ•°ï¼ˆå¦‚ä¸‹å›¾ï¼‰ï¼Œä»Žè€Œå¾—å‡ºprior probabilityã€‚ å³å¯ç®—å‡ºposterior probability $P\\left(C_{1} \\mid x\\right)=\\frac{P\\left(x \\mid C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x \\mid C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x \\mid C_{2}\\right) P\\left(C_{2}\\right)}$ Semi-supervised Generative Modelåœ¨åªæœ‰labeled dataçš„å›¾ä¸­ï¼Œç®—å‡ºæ¥çš„ $\\mu,\\Sigma$ å‚æ•°å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š ä½†å¦‚æžœæœ‰unlabeled dataï¼ˆç»¿è‰²ç‚¹ï¼‰ï¼Œä¼šå‘çŽ°åˆ†å¸ƒçš„æ¨¡åž‹å‚æ•°æ›´å¯èƒ½æ˜¯æ˜¯ä¸‹å›¾ï¼š The unlabeled data $x^u$ help re-estimate $P(C_1),P(C_2),\\mu^1,\\mu^2,\\Sigma$ . å› æ­¤ï¼Œunlabeled dataä¼šå½±å“åˆ†å¸ƒï¼Œä»Žè€Œå½±å“prior probabilityï¼Œposterior probabilityï¼Œæœ€ç»ˆå½±å“ boundaryã€‚ EMæ‰€ä»¥æœ‰unlabeled data, è¿™ä¸ªSemi-supervised çš„ç®—æ³•æ€Žä¹ˆåšå‘¢ï¼Ÿ å…¶å®žå°±æ˜¯EMï¼ˆExpected-maximization algorithmï¼ŒæœŸæœ›æœ€å¤§åŒ–ç®—æ³•ã€‚ï¼‰ Initialization : $\\theta={P(C_1),P(C_2),\\mu^1,\\mu^2,\\Sigma}$ . åˆå§‹åŒ–Gaussianæ¨¡åž‹å‚æ•°ï¼Œå¯ä»¥éšæœºåˆå§‹ï¼Œä¹Ÿå¯ä»¥é€šè¿‡labeled dataå¾—å‡ºã€‚ è™½ç„¶è¿™ä¸ªç®—æ³•æœ€ç»ˆä¼šæ”¶æ•›ï¼Œä½†æ˜¯åˆå§‹åŒ–çš„å‚æ•°å½±å“æ”¶æ•›ç»“æžœï¼Œå°±åƒgradient descentä¸€æ ·ã€‚ Eï¼šStep 1: compute the posterior probability of unlabeled data $P_\\theta(C_1|x^u)$ (depending on model $\\theta$ ) æ ¹æ®å½“å‰modelçš„å‚æ•°ï¼Œè®¡ç®—å‡ºunlabeled dataçš„posterior probability $P(C_1|x^u)$ .(ä»¥$P(C_1|x^u)$ ä¸ºä¾‹) Mï¼šStep 2: update model. Back to step1 until the algorithm converges enventually. ç”¨Eæ­¥å¾—åˆ°unlabeled dataçš„posterior probabilityæ¥æœ€å¤§åŒ–æžå¤§ä¼¼ç„¶å‡½æ•°ï¼Œæ›´æ–°å¾—åˆ°æ–°çš„æ¨¡åž‹å‚æ•°ï¼Œå…¬å¼å¾ˆç›´è§‰ã€‚(ä»¥ $C_1$ ä¸ºä¾‹) ï¼ˆ$N$ ï¼šdata çš„æ€»æ•°ï¼ŒåŒ…æ‹¬unlabeled data; $N_1$ :label= $C_1$ çš„dataæ•°ï¼‰ $P(C_1)=\\frac{N_1+\\Sigma_{x^u}P(C_1|x^u)}{N}$ å¯¹æ¯”æ²¡æœ‰unlabeled dataä¹‹å‰çš„å¼å­ï¼Œ $P(C_1)=\\frac{N_1}{N}$ ï¼Œé™¤äº†å·²æœ‰label= $C_1$ ï¼Œè¿˜å¤šäº†ä¸€éƒ¨åˆ†ï¼Œå³unlabeled dataä¸­å±žäºŽ $C_1$ çš„æ¦‚çŽ‡å’Œã€‚ $\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}+\\frac{1}{\\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right)} \\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right) x^{u}$ å¯¹æ¯”æ²¡æœ‰unlabeled dataçš„å¼å­ ï¼Œ$\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}$ ï¼Œé™¤äº†å·²æœ‰çš„label= $C_1$ ï¼Œè¿˜å¤šäº†ä¸€éƒ¨åˆ†ï¼Œå³unlabeled dataçš„ $x^u$ çš„åŠ æƒå¹³å‡ï¼ˆæƒé‡ä¸º $P(C_1\\mid x^u)$ ï¼Œå³å±žäºŽ $C_1$ çš„æ¦‚çŽ‡ï¼‰ã€‚ $\\Sigma$ å…¬å¼ä¹ŸåŒ…æ‹¬äº†unlabeled data. æ‰€ä»¥è¿™ä¸ªç®—æ³•çš„Step 1å°±æ˜¯EMç®—æ³•çš„ExpectedæœŸæœ›éƒ¨åˆ†ï¼Œæ ¹æ®å·²æœ‰çš„labeled dataå¾—å‡ºæžå¤§ä¼¼ç„¶å‡½æ•°çš„ä¼°è®¡å€¼ï¼› Step 2å°±æ˜¯EMç®—æ³•çš„Maximuméƒ¨åˆ†ï¼Œåˆ©ç”¨unlabeled dataï¼ˆé€šè¿‡å·²æœ‰æ¨¡åž‹çš„å‚æ•°ï¼‰æœ€å¤§åŒ–Eæ­¥çš„æžå¤§ä¼¼ç„¶å‡½æ•°ï¼Œæ›´æ–°æ¨¡åž‹å‚æ•°ã€‚ æœ€åŽåå¤è¿­ä»£Step 1å’ŒStep 2ï¼Œç›´è‡³æ”¶æ•›ã€‚ Why EM[1]æŒ–å‘EMè¯¦è§£ã€‚ ä¸ºä»€ä¹ˆå¯ä»¥ç”¨EMç®—æ³•æ¥è§£å†³Semi-supervised? åªæœ‰labeled data æžå¤§ä¼¼ç„¶å‡½æ•° $\\log{L(\\theta)}=\\sum_{x^r}\\log{P_\\theta(x^r,\\hat{y}^r)}$ , å…¶ä¸­ $P_\\theta(x^r,\\hat{y}^r)=P_\\theta(x^r\\mid \\hat{y}^r)P(\\hat{y}^r)$ . å¯¹ä¸Šå¼å­æ±‚å¯¼æ˜¯æœ‰closed-form solutionçš„ã€‚ æœ‰labeled dataå’Œunlabeled data æžå¤§ä¼¼ç„¶å‡½æ•°å¢žåŠ äº†ä¸€éƒ¨åˆ† $\\log L(\\theta)=\\sum_{x^{r}} \\log P_{\\theta}\\left(x^{r}, \\hat{y}^{r}\\right)+\\sum_{x^{u}} \\log P_{\\theta}\\left(x^{u}\\right)$ . å°†åŽéƒ¨åˆ†ç”¨å…¨æ¦‚çŽ‡å±•å¼€ï¼Œ $P_{\\theta}\\left(x^{u}\\right)=P_{\\theta}\\left(x^{u} \\mid C_{1}\\right) P\\left(C_{1}\\right)+P_{\\theta}\\left(x^{u} \\mid C_{2}\\right) P\\left(C_{2}\\right)$ . å¦‚æžœè¦æ±‚åŽéƒ¨åˆ†ï¼Œå› ä¸ºæ˜¯unlabeled data, æ‰€ä»¥æ¨¡åž‹ $\\theta$ éœ€è¦å¾—çŸ¥unlabeled dataçš„labelï¼Œå³ $P(C_1\\mid x^u)$ ,è€Œæ±‚è¿™ä¸ªå¼å­ï¼Œä¹Ÿéœ€è¦å¾—åˆ° prior probability $P(x^u\\mid C_1)$ ,ä½†è¿™ä¸ªå¼å­éœ€è¦äº‹å…ˆå¾—çŸ¥æ¨¡åž‹ $\\theta$ ï¼Œå› æ­¤é™·å…¥äº†æ­»å¾ªçŽ¯ã€‚ å› æ­¤è¿™ä¸ªæžå¤§ä¼¼ç„¶å‡½æ•°ä¸æ˜¯convexï¼ˆå‡¸ï¼‰ï¼Œä¸èƒ½ç›´æŽ¥æ±‚è§£ï¼Œå› æ­¤ç”¨è¿­ä»£çš„EMç®—æ³•é€æ­¥maximumæžå¤§ä¼¼ç„¶å‡½æ•°ã€‚ Low-density Separation Assumptionå¦ä¸€ç§å‡è®¾æ˜¯Low-density Separationçš„å‡è®¾ï¼Œå³è¿™ä¸ªä¸–ç•Œæ˜¯éžé»‘å³ç™½çš„â€Black-or-whiteâ€ã€‚ ä¸¤ç§ç±»åˆ«ä¹‹é—´æ˜¯low-densityï¼Œäº¤ç•Œå¤„æœ‰æ˜Žæ˜¾çš„é¸¿æ²Ÿï¼Œå› æ­¤è¦ä¹ˆæ˜¯ç±»åˆ«1ï¼Œè¦ä¹ˆæ˜¯ç±»åˆ«2ï¼Œæ²¡æœ‰ç¬¬ä¸‰ç§æƒ…å†µã€‚ Self-trainingå¯¹äºŽLow-density Separation Assumptionçš„å‡è®¾ï¼Œä½¿ç”¨Self-trainingçš„æ–¹æ³•ã€‚ Givenï¼šlabeled data set $=\\{(x^r,\\hat{y}^r\\}_{r=1}^R$ ,unlabeled data set $ =\\{x^u\\}_{u=R}^{R+U}$ . Repeatï¼š Train model $f^*$ from labeled data set. $f^*$ is independent to the model) ä»Žlabeled data setä¸­è®­ç»ƒå‡ºä¸€ä¸ªæ¨¡åž‹ Apply $f^*$ to the unlabeled data set. Obtain pseudo-label $\\{(x^u,y^u\\}_{u=l}^{R+U}\\}$ ç”¨è¿™ä¸ªæ¨¡åž‹ $f^*$ æ¥é¢„æµ‹unlabeled data setï¼Œ èŽ·å¾—ä¼ªlabel Remove a set of data from unlabeled data set, and add them into the labeled data set. æ‹¿å‡ºä¸€äº›unlabeled data(pseudo-label)ï¼Œæ”¾åˆ°labeled data setä¸­ï¼Œå›žåˆ°æ­¥éª¤1ï¼Œå†è®­ç»ƒã€‚ how to choose the data set remains open å¦‚ä½•é€‰æ‹©unlabeled data æ˜¯è‡ªè®¾è®¡çš„ you can also provide a weight to each data. è®­ç»ƒä¸­å¯ä»¥å¯¹unlabeled data(pseudo-label)å’Œlabeled data èµ‹äºˆä¸åŒçš„æƒé‡. æ³¨æ„ï¼š Regressionæ¨¡åž‹æ˜¯ä¸èƒ½self-trainingçš„ï¼Œå› ä¸ºunlabeled dataå’Œå…¶pseudo-labelæ”¾åœ¨æ¨¡åž‹ä¸­çš„lossä¸º0ï¼Œæ— æ³•å†minimizeã€‚ Hard LabelV.S. semi-supervised learning for generative model Semi-supervised learning for generative modelå’ŒLow-density Separationçš„åŒºåˆ«å…¶å®žæ˜¯soft label å’Œhard labelçš„åŒºåˆ«ã€‚ Generative Modelæ˜¯åˆ©ç”¨æ¥unlabeled dataçš„ $P(C_1|x^u)$ posterior probabilityæ¥è®¡ç®—æ–°çš„prior probabilityï¼Œè¿­ä»£æ›´æ–°æ¨¡åž‹ã€‚ è€Œlow-densityæ˜¯è®¡ç®—å‡ºunlabeled dataçš„pseudo-labelï¼Œé€‰æ‹©æ€§æ‰©å¤§labeled data set(å³åŠ å…¥éƒ¨åˆ†ç”±pseudo-labelçš„unlabeled data)æ¥è¿­ä»£è®­ç»ƒæ¨¡åž‹ã€‚ å› æ­¤ï¼Œå¦‚æžœè€ƒè™‘Neural Networkï¼š ($\\theta^*$ æ˜¯labeled dataè®¡ç®—æ‰€å¾—çš„network parameters) å¦‚ä¸‹å›¾ï¼Œunlabeled data $x^u$ æ”¾å…¥æ¨¡åž‹ä¸­é¢„æµ‹ï¼Œå¾—åˆ° $\\begin{bmatrix} 0.7 \\ 0.3\\end{bmatrix}$ . å¦‚æžœæ˜¯ä½¿ç”¨hard labelï¼Œåˆ™ $x^u$ çš„targetæ˜¯ $\\begin{bmatrix} 1 \\ 0\\end{bmatrix}$ . å¦‚æžœæ˜¯ä½¿ç”¨soft labelï¼Œåˆ™ $x^u$ çš„targetæ˜¯ $\\begin{bmatrix} 0.7 \\ 0.3\\end{bmatrix}$ . å¦‚æžœæ˜¯ä½¿ç”¨soft labelï¼Œåˆ™self-trainingä¸ä¼šæœ‰æ•ˆï¼Œå› ä¸ºæ–°çš„dataå¯¹åŽŸlossçš„æ”¹å˜ä¸º0ï¼Œä¸ä¼šå¢žå¤§æ¨¡åž‹çš„lossï¼Œä¹Ÿå°±æ— æ³•å†å¯¹å…¶minimize. æ‰€ä»¥åŸºäºŽLow-density Separationçš„å‡è®¾ï¼Œæ˜¯éžé»‘å³ç™½çš„ï¼Œéœ€è¦ä½¿ç”¨hard labelæ¥self-trainingã€‚ Entropy-based Regularizationåœ¨è®­ç»ƒæ¨¡åž‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°½é‡ä¿è¯unlabeled dataåœ¨æ¨¡åž‹ä¸­çš„åˆ†å¸ƒæ˜¯low-density separationã€‚ å³ä¸‹å›¾ä¸­ï¼Œunlabeled dataå¾—åˆ°çš„pseudo-labelçš„åˆ†å¸ƒåº”è¯¥å°½é‡é›†ä¸­ï¼Œè€Œä¸åº”è¯¥å¤ªåˆ†æ•£ã€‚ æ‰€ä»¥ï¼Œåœ¨è®­ç»ƒä¸­ï¼Œå¦‚ä½•è¯„ä¼° $y^u$ çš„åˆ†å¸ƒçš„é›†ä¸­åº¦ï¼Ÿ æ ¹æ®ä¿¡æ¯å­¦ï¼Œä½¿ç”¨ $y^u$ çš„entropyï¼Œå³ $E\\left(y^{u}\\right)=-\\sum_{m=1}^{5} y_{m}^{u} \\ln \\left(y_{m}^{u}\\right)$ (æ³¨ï¼šè¿™é‡Œçš„ $y^u_m$ æ˜¯å˜é‡ $y^u=m$ çš„æ¦‚çŽ‡) å½“ $E(y^u)$ è¶Šå°ï¼Œè¯´æ˜Ž $y^u$ åˆ†å¸ƒè¶Šé›†ä¸­ï¼Œå¦‚ä¸‹å›¾ã€‚ å› æ­¤ï¼Œåœ¨self-trainingä¸­ï¼š $L=\\sum_{y^r} C(x^r,\\hat{y}^r)+\\lambda\\sum_{x^u}E(y^u)$ Loss functionçš„å‰ä¸€é¡¹ï¼ˆcross entropyï¼‰minimizeä¿è¯åˆ†ç±»çš„æ­£ç¡®æ€§ï¼ŒåŽä¸€é¡¹ï¼ˆentropy of $y^u$ ) minimizeä¿è¯ unlabeled dataåˆ†å¸ƒå°½é‡é›†ä¸­ï¼Œæœ€å¤§å¯èƒ½æ»¡è¶³low-density separationçš„å‡è®¾ã€‚ trainingï¼šgradient decent. å› ä¸ºè¿™æ ·çš„å½¢å¼å¾ˆåƒä¹‹å‰æåˆ°è¿‡çš„regularization(å…·ä½“è§è¿™ç¯‡æ–‡ç« çš„3.2)ï¼Œæ‰€ä»¥åˆå«entropy-based regularization. Outlook: Semi-supervised SVMSVMä¹Ÿæ˜¯è§£å†³semi-supervised learningçš„æ–¹æ³•. ä¸Šå›¾ä¸­ï¼Œåœ¨æœ‰unlabeled dataçš„æƒ…å†µä¸‹ï¼Œå¸Œæœ›boundary åˆ†çš„è¶Šå¼€è¶Šå¥½ï¼ˆlargest marginï¼‰å’Œæœ‰æ›´å°çš„error. å› æ­¤æžšä¸¾unlabeled dataæ‰€æœ‰å¯èƒ½çš„æƒ…å†µï¼Œä½†æžšä¸¾åœ¨è®¡ç®—é‡ä¸Šæ˜¯å·¨å¤§çš„ï¼Œå› æ­¤SVMï¼ˆSupport Vector Machinesï¼‰å¯ä»¥å®žçŽ°æžšä¸¾çš„ç›®æ ‡ï¼Œä½†ä¸éœ€è¦è¿™ä¹ˆå¤§çš„æžšä¸¾é‡ã€‚ Smoothness AssumptionSmoothness Assumptionçš„æ€æƒ³å¯ä»¥ç”¨ä»¥ä¸‹è¯å½’çº³ï¼š â€œYou are known by the company you keepâ€ è¿‘æœ±è€…èµ¤ï¼Œè¿‘å¢¨è€…é»‘ã€‚ è“¬ç”Ÿéº»ä¸­ï¼Œä¸æ‰¶è€Œç›´ã€‚ç™½æ²™åœ¨æ¶…ï¼Œä¸Žä¹‹ä¿±é»‘ã€‚ Assumptionï¼šâ€œsimilarâ€ $x$ has the same $\\hat{y}$ . ã€æ„æ€å°±æ˜¯è¯´ï¼šç›¸è¿‘çš„ $x$ æœ‰ç›¸åŒçš„label $\\hat{y}$ .ã€‘ More precise assumptionï¼š x is not uniform if $x^1$ and $x^2$ are close in a hign density region, $\\hat{y}^1$ and $\\hat{y}^2$ are the same. Smoothness Assumptionå‡è®¾æ›´å‡†ç¡®çš„è¡¨è¿°æ˜¯ï¼š xä¸æ˜¯å‡åŒ€åˆ†å¸ƒï¼Œå¦‚æžœ $x^1$ å’Œ $x^2$ é€šè¿‡ä¸€ä¸ªhigh density regionçš„åŒºåŸŸè¿žåœ¨ä¸€èµ·ï¼Œä¸”ç¦»å¾—å¾ˆè¿‘ï¼Œåˆ™ $\\hat{y}^1$ å’Œ $\\hat{y}^2$ ç›¸åŒã€‚ å¦‚ä¸‹å›¾ï¼Œ $x^1$ å’Œ $x^2$ é€šè¿‡high density regionè¿žæŽ¥åœ¨ä¸€èµ·ï¼Œæœ‰ç›¸åŒçš„labelï¼Œè€Œ $x^2$ å’Œ $x^3$ æœ‰ä¸åŒçš„label. Smoothness Assumptioné€šè¿‡è§‚å¯Ÿå¤§é‡unlabeled dataï¼Œå¯ä»¥å¾—åˆ°ä¸€äº›ä¿¡æ¯ã€‚ æ¯”å¦‚ä¸‹å›¾ä¸­çš„ä¸¤å¼ äººçš„å·¦è„¸å’Œå³è„¸å›¾ç‰‡ï¼Œéƒ½æ˜¯unlabeledï¼Œä½†å¦‚æžœç»™å¤§é‡çš„è¿‡æ¸¡å½¢æ€ï¼ˆå·¦è„¸è½¬å‘å³è„¸ï¼‰unlabeled dataï¼Œå¯ä»¥å¾—å‡ºè¿™ä¸¤å¼ å›¾ç‰‡æ˜¯ç›¸ä¼¼çš„ç»“è®º. Smoothness Assumptionè¿˜å¯ä»¥ç”¨åœ¨æ–‡ç« åˆ†ç±»ä¸­ï¼Œæ¯”å¦‚åˆ†ç±»å¤©æ–‡å­¦å’Œæ—…æ¸¸å­¦çš„æ–‡ç« ã€‚ å¦‚ä¸‹å›¾ï¼Œ æ–‡ç«  d1å’Œd3æœ‰overlap wordï¼ˆé‡å å•è¯ï¼‰ï¼Œæ‰€ä»¥d1å’Œd3æ˜¯åŒä¸€ç±»ï¼ŒåŒç† d4å’Œd2æ˜¯ä¸€ç±»ã€‚ å¦‚æžœï¼Œä¸‹å›¾ä¸­ï¼Œd1å’Œd3æ²¡æœ‰overlap wordï¼Œå°±æ— æ³•è¯´æ˜Žd1å’Œd3æ˜¯åŒä¸€ç±»ã€‚ ä½†æ˜¯ï¼Œå¦‚æžœæˆ‘ä»¬æ”¶é›†åˆ°è¶³å¤Ÿå¤šä½†unlabeled dataï¼Œå¦‚ä¸‹å›¾ï¼Œé€šè¿‡high density regionçš„è¿žæŽ¥å’Œä¼ é€’ï¼Œä¹Ÿå¯ä»¥å¾—å‡ºd1å’Œd3ä¸€ç±»ï¼Œd2å’Œd4ä¸€ç±»ã€‚ Cluster and then Labelåœ¨Smoothness Assumptionå‡è®¾ä¸‹ï¼Œç›´è§‚çš„å¯ä»¥ç”¨cluster and then labelï¼Œå…ˆç”¨æ‰€æœ‰çš„dataè®­ç»ƒä¸€ä¸ªclassifierã€‚ ç›´æŽ¥èšç±»æ ‡è®°(æ¯”è¾ƒéš¾è®­ç»ƒï¼‰ã€‚ Graph-based Approachå¦ä¸€ç§æ–¹æ³•æ˜¯åˆ©ç”¨å›¾çš„ç»“æž„ï¼ˆGraph structureï¼‰æ¥å¾—çŸ¥ $x^1$ and $x^2$ are close in a high density region (connected by a high density path). Represent the data points as a graph. ã€æŠŠè¿™äº›æ•°æ®ç‚¹çœ‹ä½œä¸€ä¸ªå›¾ã€‘ å»ºå›¾æœ‰äº›æ—¶å€™æ˜¯å¾ˆç›´è§‚çš„ï¼Œæ¯”å¦‚ç½‘é¡µä¸­çš„è¶…é“¾æŽ¥ï¼Œè®ºæ–‡ä¸­çš„å¼•ç”¨ã€‚ ä½†æœ‰çš„æ—¶å€™ä¹Ÿéœ€è¦è‡ªå·±å»ºå›¾ã€‚ æ³¨æ„ï¼š å¦‚æžœæ˜¯å½±åƒç±»ï¼Œbase on pixelï¼Œperformanceå°±ä¸å¤ªå¥½ï¼Œä¸€èˆ¬ä¼šbase on autoencoderï¼Œå°†featureæŠ½è±¡å‡ºæ¥ï¼Œæ•ˆæžœæ›´å¥½ã€‚ Graph Constructionå»ºå›¾è¿‡ç¨‹å¦‚ä¸‹ï¼š Define the similarity $s(x^i, x^j)$ between $x^i$ and $x^j$ . ã€å®šä¹‰data $x^i$ å’Œ $x^j$ çš„ç›¸ä¼¼åº¦ã€‘ Add edgeã€å®šä¹‰æ•°æ®ç‚¹ä¸­åŠ è¾¹ï¼ˆè¿žé€šï¼‰çš„æ¡ä»¶ã€‘ K Nearest Neighborã€å’Œè¯¥ç‚¹æœ€è¿‘çš„kä¸ªç‚¹ç›¸è¿žæŽ¥ã€‘ e-Neighborhoodã€ä¸Žç¦»è¯¥ç‚¹è·ç¦»å°äºŽç­‰äºŽeçš„ç‚¹ç›¸è¿žæŽ¥ã€‘ Edge weight is proportional to $s(x^i, x^j)$ ã€è¾¹ç‚¹æƒé‡å°±æ˜¯æ­¥éª¤1å®šä¹‰çš„è¿žæŽ¥ä¸¤ç‚¹çš„ç›¸ä¼¼åº¦ã€‘ Gaussian Radial Basis Functionï¼š $s\\left(x^{i}, x^{j}\\right)=\\exp \\left(-\\gamma\\left\\|x^{i}-x^{j}\\right\\|^{2}\\right)$ ä¸€èˆ¬é‡‡ç”¨å¦‚ä¸Šå…¬å¼ï¼ˆç»éªŒä¸Šå–å¾—è¾ƒå¥½çš„performanceï¼‰ã€‚ å› ä¸ºåˆ©ç”¨æŒ‡æ•°åŒ–åŽï¼ˆæŒ‡æ•°å†…æ˜¯ä¸¤ç‚¹çš„Euclidean distanceï¼‰ï¼Œå‡½æ•°ä¸‹é™çš„å¾ˆå¿«ï¼Œåªæœ‰å½“ä¸¤ç‚¹ç¦»çš„å¾ˆè¿‘æ—¶ï¼Œè¯¥ç›¸ä¼¼åº¦ $s(x^i,x^j)$ æ‰å¤§ï¼Œå…¶ä»–æ—¶å€™éƒ½è¶‹è¿‘äºŽ0. Graph-based Approachå›¾å»ºå¥½åŽï¼š The labeled data influence their neighbors. Propagate through the graph. ã€label data ä¸ä»…ä¼šå½±å“ä»–ä»¬çš„é‚»å±…ï¼Œè¿˜ä¼šä¸€ç›´ä¼ æ’­ä¸‹åŽ»ã€‘ å¦‚æžœdata pointså¤Ÿå¤šï¼Œå›¾å»ºçš„å¥½ï¼Œå°±ä¼šåƒä¸‹å›¾è¿™æ ·ï¼š ä½†æ˜¯ï¼Œå¦‚æžœdataè¾ƒå°‘ï¼Œå°±å¯èƒ½å‡ºçŽ°ä¸‹å›¾è¿™ç§labelä¼ ä¸åˆ°unlabeled dataçš„æƒ…å†µï¼š Smoothness Definitionå› ä¸ºæ˜¯åŸºäºŽSmoothness Assumptionï¼Œæ‰€ä»¥æœ€åŽè®­ç»ƒå‡ºçš„æ¨¡åž‹åº”è®©å¾—åˆ°çš„å›¾å°½å¯èƒ½æ»¡è¶³smoothnessçš„å‡è®¾ã€‚ æ³¨æ„ï¼š è¿™é‡Œçš„å› æžœå…³ç³»æ˜¯ï¼Œunlabeled dataä½œä¸ºNNçš„è¾“å…¥ï¼Œå¾—åˆ°label $y$ ï¼Œè¯¥label $y$ å’Œlabeled dataçš„ label $\\hat{y}$ ä¸€èµ·å¾—åˆ°çš„å›¾æ˜¯å°½æœ€å¤§å¯èƒ½æ»¡è¶³Smoothness Assumptionçš„ã€‚ ï¼ˆè€Œä¸æ˜¯å»ºå¥½å›¾ï¼Œç„¶åŽunlabeled dataçš„label $y$ æ˜¯labeled dataåŽŸæœ‰çš„ $\\hat{y}$ ç›´æŽ¥ä¼ æ’­è¿‡æ¥çš„ï¼Œä¸ç„¶è®­ç»ƒNNå¹²å˜›ï¼‰ æŠŠunlabeled dataä½œä¸ºNNçš„è¾“å…¥ï¼Œå¾—åˆ°label ï¼Œå¯¹labeled dataå’Œâ€unlabeled dataâ€ å»ºå›¾ã€‚ ä¸ºäº†åœ¨è®­ç»ƒä¸­ä½¿å¾—æœ€åŽçš„å›¾å°½å¯èƒ½æ»¡è¶³å‡è®¾ï¼Œå®šä¹‰smoothness of the labels on the graph. $S=\\frac{1}{2} \\sum_{i,j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}$ ï¼ˆå¯¹äºŽæ‰€æœ‰çš„labeled data å’Œ â€œunlabeled dataâ€ï¼ˆä½œä¸ºNNè¾“å…¥åŽï¼Œæœ‰labelï¼‰ï¼‰ æŒ‰ç…§ä¸Šå¼è®¡ç®—ï¼Œå¾—åˆ°çš„Smoothnesså¦‚ä¸‹å›¾æ‰€ç¤ºï¼š Smaller means smoother. ã€Smoothness $S$ è¶Šå°ï¼Œè¡¨ç¤ºå›¾è¶Šæ»¡è¶³è¿™ä¸ªå‡è®¾ã€‘ è®¡ç®—smoothness $S$ æœ‰ä¸€ç§ç®€ä¾¿çš„æ–¹æ³•ï¼š $S=\\frac{1}{2} \\sum_{i, j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}=y^{T} L y$ (è¿™é‡Œçš„1/2åªæ˜¯ä¸ºäº†è®¡ç®—æ–¹ä¾¿) $y$ : (R+U)-dim vectorï¼Œæ˜¯æ‰€æœ‰label dataå’Œâ€unlabeled dataâ€ çš„labelï¼Œæ‰€ä»¥æ˜¯R+Uç»´ã€‚ $y=\\begin{bmatrix}â€¦y^iâ€¦y^jâ€¦\\end{bmatrix}^T$ $L$ :(R+U) $\\times$ (R+U) matrixï¼Œä¹Ÿå«Graph Laplacianï¼ˆè°ƒå’ŒçŸ©é˜µï¼Œæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼‰ $L$ çš„è®¡ç®—æ–¹æ³•ï¼š$L=D-W$ å…¶ä¸­ $W$ çŸ©é˜µç®—æ˜¯å›¾çš„é‚»æŽ¥çŸ©é˜µï¼ˆåŒºåˆ«æ˜¯æ— ç›´æŽ¥å¯è¾¾è¾¹çš„å€¼æ˜¯0ï¼‰ $D$ çŸ©é˜µæ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´ çš„å€¼ç­‰äºŽ $W$ çŸ©é˜µå¯¹åº”è¡Œçš„å…ƒç´ å’Œ çŸ©é˜µè¡¨ç¤ºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š ï¼ˆè¯æ˜Žæ®è¯´å¾ˆæž¯ç‡¥ï¼Œæš‚æ—¶ç•¥[2]) Smoothness Regularization$S=\\frac{1}{2} \\sum_{i, j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}=y^{T} L y$ $S$ ä¸­çš„ $y$ å…¶å®žæ˜¯å’Œnetwork parametersæœ‰å…³çš„ï¼ˆunlabeled dataçš„labelï¼‰ï¼Œæ‰€ä»¥æŠŠ $S$ ä¹Ÿæ”¾è¿›æŸå¤±å‡½æ•°ä¸­minimizeï¼Œä»¥æ±‚å°½å¯èƒ½æ»¡è¶³smoothness assumption. ä»¥æ»¡è¶³smoothness assumptionçš„æŸå¤±å‡½æ•°ï¼š $L=\\sum_{x^r} C\\left(y^{r}, \\hat{y}^{r}\\right)+\\lambda S$ æŸå¤±å‡½æ•°çš„å‰éƒ¨åˆ†ä½¿labeled dataçš„è¾“å‡ºæ›´è´´è¿‘å…¶labelï¼ŒåŽéƒ¨åˆ† $\\lambda S$ ä½œä¸ºregularization termï¼Œä½¿å¾—labeled dataå’Œunlabeled dataå°½å¯èƒ½æ»¡è¶³smoothness assumption. é™¤äº†è®©NNçš„outputæ»¡è¶³smoothnessçš„å‡è®¾ï¼Œè¿˜å¯ä»¥è®©NNçš„ä»»ä½•ä¸€å±‚çš„è¾“å‡ºæ»¡è¶³smoothness assumptionï¼Œæˆ–è€…è®©æŸå±‚å¤–æŽ¥ä¸€å±‚embedding layerï¼Œä½¿å…¶æ»¡è¶³smoothness assumptionï¼Œå¦‚ä¸‹å›¾ï¼š Better RepresentationBetter Presentationçš„æ€æƒ³å°±æ˜¯ï¼šåŽ»èŠœå­˜èï¼ŒåŒ–ç¹ä¸ºç®€ã€‚ Find the latent(æ½œåœ¨çš„) factors behind the observation. The latent factors (usually simpler) are better representation. ã€æ‰¾åˆ°æ‰€è§‚å¯Ÿäº‹ç‰©çš„æ½œåœ¨ç‰¹å¾ï¼Œå³è¯¥äº‹ç‰©çš„better representationã€‘ è¯¥éƒ¨åˆ†åŽç»­è§è¿™ç¯‡åšå®¢ã€‚ Reference æŒ–å‘ï¼šEMç®—æ³•è¯¦è§£ æŒ–å‘ï¼šGraph Laplacian in smoothness. Olivier Chapelleï¼šSemi-Supervised Learning","link":"/2020/07/02/semi-supervised/"},{"title":"ã€ŒåŒºå—é“¾ã€ï¼šSolidity-advanced","text":"Solidityçš„å®˜æ–¹æ•™ç¨‹ç¬”è®°ï¼šadvancedã€‚ Part 1ç¬¬1ç« : æ™ºèƒ½åè®®çš„æ°¸å›ºæ€§åˆ°çŽ°åœ¨ä¸ºæ­¢ï¼Œæˆ‘ä»¬è®²çš„ Solidity å’Œå…¶ä»–è¯­è¨€æ²¡æœ‰è´¨çš„åŒºåˆ«ï¼Œå®ƒé•¿å¾—ä¹Ÿå¾ˆåƒ JavaScriptã€‚ ä½†æ˜¯ï¼Œåœ¨æœ‰å‡ ç‚¹ä»¥å¤ªåŠä¸Šçš„ DApp è·Ÿæ™®é€šçš„åº”ç”¨ç¨‹åºæœ‰ç€å¤©å£¤ä¹‹åˆ«ã€‚ ç¬¬ä¸€ä¸ªä¾‹å­ï¼Œåœ¨ä½ æŠŠæ™ºèƒ½åè®®ä¼ ä¸Šä»¥å¤ªåŠä¹‹åŽï¼Œå®ƒå°±å˜å¾—ä¸å¯æ›´æ”¹, è¿™ç§æ°¸å›ºæ€§æ„å‘³ç€ä½ çš„ä»£ç æ°¸è¿œä¸èƒ½è¢«è°ƒæ•´æˆ–æ›´æ–°ã€‚ ä½ ç¼–è¯‘çš„ç¨‹åºä¼šä¸€ç›´ï¼Œæ°¸ä¹…çš„ï¼Œä¸å¯æ›´æ”¹çš„ï¼Œå­˜åœ¨ä»¥å¤ªåŠä¸Šã€‚è¿™å°±æ˜¯ Solidity ä»£ç çš„å®‰å…¨æ€§å¦‚æ­¤é‡è¦çš„ä¸€ä¸ªåŽŸå› ã€‚å¦‚æžœä½ çš„æ™ºèƒ½åè®®æœ‰ä»»ä½•æ¼æ´žï¼Œå³ä½¿ä½ å‘çŽ°äº†ä¹Ÿæ— æ³•è¡¥æ•‘ã€‚ä½ åªèƒ½è®©ä½ çš„ç”¨æˆ·ä»¬æ”¾å¼ƒè¿™ä¸ªæ™ºèƒ½åè®®ï¼Œç„¶åŽè½¬ç§»åˆ°ä¸€ä¸ªæ–°çš„ä¿®å¤åŽçš„åˆçº¦ä¸Šã€‚ ä½†è¿™æ°å¥½ä¹Ÿæ˜¯æ™ºèƒ½åˆçº¦çš„ä¸€å¤§ä¼˜åŠ¿ã€‚ä»£ç è¯´æ˜Žä¸€åˆ‡ã€‚å¦‚æžœä½ åŽ»è¯»æ™ºèƒ½åˆçº¦çš„ä»£ç ï¼Œå¹¶éªŒè¯å®ƒï¼Œä½ ä¼šå‘çŽ°ï¼Œä¸€æ—¦å‡½æ•°è¢«å®šä¹‰ä¸‹æ¥ï¼Œæ¯ä¸€æ¬¡çš„è¿è¡Œï¼Œç¨‹åºéƒ½ä¼šä¸¥æ ¼éµç…§å‡½æ•°ä¸­åŽŸæœ‰çš„ä»£ç é€»è¾‘ä¸€ä¸ä¸è‹Ÿåœ°æ‰§è¡Œï¼Œå®Œå…¨ä¸ç”¨æ‹…å¿ƒå‡½æ•°è¢«äººç¯¡æ”¹è€Œå¾—åˆ°æ„å¤–çš„ç»“æžœã€‚ å¤–éƒ¨ä¾èµ–å…³ç³»åœ¨ç¬¬2è¯¾ä¸­ï¼Œæˆ‘ä»¬å°†åŠ å¯†å°çŒ«ï¼ˆCryptoKittiesï¼‰åˆçº¦çš„åœ°å€ç¡¬ç¼–ç åˆ° DApp ä¸­åŽ»äº†ã€‚æœ‰æ²¡æœ‰æƒ³è¿‡ï¼Œå¦‚æžœåŠ å¯†å°çŒ«å‡ºäº†ç‚¹é—®é¢˜ï¼Œæ¯”æ–¹è¯´ï¼Œé›†ä½“æ¶ˆå¤±äº†ä¼šæ€Žä¹ˆæ ·ï¼Ÿ è™½ç„¶è¿™ç§äº‹æƒ…å‡ ä¹Žä¸å¯èƒ½å‘ç”Ÿï¼Œä½†æ˜¯ï¼Œå¦‚æžœå°çŒ«æ²¡äº†ï¼Œæˆ‘ä»¬çš„ DApp ä¹Ÿä¼šéšä¹‹å¤±æ•ˆ â€“ å› ä¸ºæˆ‘ä»¬åœ¨ DApp çš„ä»£ç ä¸­ç”¨â€œç¡¬ç¼–ç â€çš„æ–¹å¼æŒ‡å®šäº†åŠ å¯†å°çŒ«çš„åœ°å€ï¼Œå¦‚æžœè¿™ä¸ªæ ¹æ®åœ°å€æ‰¾ä¸åˆ°å°çŒ«ï¼Œæˆ‘ä»¬çš„åƒµå°¸ä¹Ÿå°±åƒä¸åˆ°å°çŒ«äº†ï¼Œè€ŒæŒ‰ç…§å‰é¢çš„æè¿°ï¼Œæˆ‘ä»¬å´æ²¡æ³•ä¿®æ”¹åˆçº¦åŽ»åº”ä»˜è¿™ä¸ªå˜åŒ–ï¼ æˆ‘ä»¬ä¸èƒ½ç¡¬ç¼–ç ï¼Œè€Œè¦é‡‡ç”¨â€œå‡½æ•°â€ï¼Œä»¥ä¾¿äºŽ DApp çš„å…³é”®éƒ¨åˆ†å¯ä»¥ä»¥å‚æ•°å½¢å¼ä¿®æ”¹ã€‚ æˆ‘ä»¬ä¸å†ä¸€å¼€å§‹å°±æŠŠçŒŽç‰©åœ°å€ç»™å†™å…¥ä»£ç ï¼Œè€Œæ˜¯å†™ä¸ªå‡½æ•° setKittyContractAddress, è¿è¡Œæ—¶å†è®¾å®šçŒŽç‰©çš„åœ°å€ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥éšæ—¶åŽ»é”å®šæ–°çš„çŒŽç‰©ï¼Œä¹Ÿä¸ç”¨æ‹…å¿ƒåŠ å¯†å°çŒ«é›†ä½“æ¶ˆå¤±äº†ã€‚ ç¬¬2ç« : Ownable ContractsOpenZeppelinåº“çš„Ownable åˆçº¦OpenZeppelin æ˜¯ä¸»æ‰“å®‰ä¿å’Œç¤¾åŒºå®¡æŸ¥çš„æ™ºèƒ½åˆçº¦åº“ï¼Œæ‚¨å¯ä»¥åœ¨è‡ªå·±çš„ DAppsä¸­å¼•ç”¨ã€‚ç­‰æŠŠè¿™ä¸€è¯¾å­¦å®Œï¼Œæ‚¨ä¸è¦å‚¬æˆ‘ä»¬å‘å¸ƒä¸‹ä¸€è¯¾ï¼Œæœ€å¥½åˆ©ç”¨è¿™ä¸ªæ—¶é—´æŠŠ OpenZeppelin çš„ç½‘ç«™çœ‹çœ‹ 1234567891011121314151617181920212223242526272829303132333435/** * @title Ownable * @dev The Ownable contract has an owner address, and provides basic authorization control * functions, this simplifies the implementation of &quot;user permissions&quot;. */contract Ownable { address public owner; event OwnershipTransferred(address indexed previousOwner, address indexed newOwner); /** * @dev The Ownable constructor sets the original `owner` of the contract to the sender * account. */ function Ownable() public { owner = msg.sender; } /** * @dev Throws if called by any account other than the owner. */ modifier onlyOwner() { require(msg.sender == owner); _; } /** * @dev Allows the current owner to transfer control of the contract to a newOwner. * @param newOwner The address to transfer ownership to. */ function transferOwnership(address newOwner) public onlyOwner { require(newOwner != address(0)); OwnershipTransferred(owner, newOwner); owner = newOwner; }} ä¸‹é¢æœ‰æ²¡æœ‰æ‚¨æ²¡å­¦è¿‡çš„ä¸œä¸œï¼Ÿ æž„é€ å‡½æ•°ï¼šfunction Ownable()æ˜¯ä¸€ä¸ª constructor (æž„é€ å‡½æ•°)ï¼Œæž„é€ å‡½æ•°ä¸æ˜¯å¿…é¡»çš„ï¼Œå®ƒä¸Žåˆçº¦åŒåï¼Œæž„é€ å‡½æ•°ä¸€ç”Ÿä¸­å”¯ä¸€çš„ä¸€æ¬¡æ‰§è¡Œï¼Œå°±æ˜¯åœ¨åˆçº¦æœ€åˆè¢«åˆ›å»ºçš„æ—¶å€™ã€‚ å‡½æ•°ä¿®é¥°ç¬¦ï¼šmodifier onlyOwner()ã€‚ ä¿®é¥°ç¬¦è·Ÿå‡½æ•°å¾ˆç±»ä¼¼ï¼Œä¸è¿‡æ˜¯ç”¨æ¥ä¿®é¥°å…¶ä»–å·²æœ‰å‡½æ•°ç”¨çš„ï¼Œ åœ¨å…¶ä»–è¯­å¥æ‰§è¡Œå‰ï¼Œä¸ºå®ƒæ£€æŸ¥ä¸‹å…ˆéªŒæ¡ä»¶ã€‚ åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°±å¯ä»¥å†™ä¸ªä¿®é¥°ç¬¦ onlyOwner æ£€æŸ¥ä¸‹è°ƒç”¨è€…ï¼Œç¡®ä¿åªæœ‰åˆçº¦çš„ä¸»äººæ‰èƒ½è¿è¡Œæœ¬å‡½æ•°ã€‚æˆ‘ä»¬ä¸‹ä¸€ç« ä¸­ä¼šè¯¦ç»†è®²è¿°ä¿®é¥°ç¬¦ï¼Œä»¥åŠé‚£ä¸ªå¥‡æ€ªçš„_;ã€‚ indexed å…³é”®å­—ï¼šåˆ«æ‹…å¿ƒï¼Œæˆ‘ä»¬è¿˜ç”¨ä¸åˆ°å®ƒã€‚ æ‰€ä»¥Ownable åˆçº¦åŸºæœ¬éƒ½ä¼šè¿™ä¹ˆå¹²ï¼š åˆçº¦åˆ›å»ºï¼Œæž„é€ å‡½æ•°å…ˆè¡Œï¼Œå°†å…¶ owner è®¾ç½®ä¸ºmsg.senderï¼ˆå…¶éƒ¨ç½²è€…ï¼‰ ä¸ºå®ƒåŠ ä¸Šä¸€ä¸ªä¿®é¥°ç¬¦ onlyOwnerï¼Œå®ƒä¼šé™åˆ¶é™Œç”Ÿäººçš„è®¿é—®ï¼Œå°†è®¿é—®æŸäº›å‡½æ•°çš„æƒé™é”å®šåœ¨ owner ä¸Šã€‚ å…è®¸å°†åˆçº¦æ‰€æœ‰æƒè½¬è®©ç»™ä»–äººã€‚ onlyOwner ç®€ç›´äººè§äººçˆ±ï¼Œå¤§å¤šæ•°äººå¼€å‘è‡ªå·±çš„ Solidity DAppsï¼Œéƒ½æ˜¯ä»Žå¤åˆ¶/ç²˜è´´ Ownable å¼€å§‹çš„ï¼Œä»Žå®ƒå†ç»§æ‰¿å‡ºçš„å­ç±»ï¼Œå¹¶åœ¨ä¹‹ä¸Šè¿›è¡ŒåŠŸèƒ½å¼€å‘ã€‚ æ—¢ç„¶æˆ‘ä»¬æƒ³æŠŠ setKittyContractAddress é™åˆ¶ä¸º onlyOwner ï¼Œæˆ‘ä»¬ä¹Ÿè¦åšåŒæ ·çš„äº‹æƒ…ã€‚ ç¬¬3ç« : onlyOwner å‡½æ•°ä¿®é¥°ç¬¦çŽ°åœ¨æˆ‘ä»¬æœ‰äº†ä¸ªåŸºæœ¬ç‰ˆçš„åˆçº¦ ZombieFactory äº†ï¼Œå®ƒç»§æ‰¿è‡ª Ownable æŽ¥å£ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç»™ ZombieFeeding åŠ ä¸Š onlyOwner å‡½æ•°ä¿®é¥°ç¬¦ã€‚ ZombieFeeding æ˜¯ä¸ª ZombieFactory ZombieFactory æ˜¯ä¸ª Ownable å› æ­¤ ZombieFeeding ä¹Ÿæ˜¯ä¸ª Ownable, å¹¶å¯ä»¥é€šè¿‡ Ownable æŽ¥å£è®¿é—®çˆ¶ç±»ä¸­çš„å‡½æ•°/äº‹ä»¶/ä¿®é¥°ç¬¦ã€‚å¾€åŽï¼ŒZombieFeeding çš„ç»§æ‰¿è€…åˆçº¦ä»¬åŒæ ·ä¹Ÿå¯ä»¥è¿™ä¹ˆå»¶ç»­ä¸‹åŽ»ã€‚ å‡½æ•°ä¿®é¥°ç¬¦å‡½æ•°ä¿®é¥°ç¬¦çœ‹èµ·æ¥è·Ÿå‡½æ•°æ²¡ä»€ä¹ˆä¸åŒï¼Œä¸è¿‡å…³é”®å­—modifier å‘Šè¯‰ç¼–è¯‘å™¨ï¼Œè¿™æ˜¯ä¸ªmodifier(ä¿®é¥°ç¬¦)ï¼Œè€Œä¸æ˜¯ä¸ªfunction(å‡½æ•°)ã€‚å®ƒä¸èƒ½åƒå‡½æ•°é‚£æ ·è¢«ç›´æŽ¥è°ƒç”¨ï¼Œåªèƒ½è¢«æ·»åŠ åˆ°å‡½æ•°å®šä¹‰çš„æœ«å°¾ï¼Œç”¨ä»¥æ”¹å˜å‡½æ•°çš„è¡Œä¸ºã€‚ å’±ä»¬ä»”ç»†è¯»è¯» onlyOwner: 1234567/** * @dev è°ƒç”¨è€…ä¸æ˜¯â€˜ä¸»äººâ€™ï¼Œå°±ä¼šæŠ›å‡ºå¼‚å¸¸ */modifier onlyOwner() { require(msg.sender == owner); _;} onlyOwner å‡½æ•°ä¿®é¥°ç¬¦æ˜¯è¿™ä¹ˆç”¨çš„ï¼š 12345678contract MyContract is Ownable { event LaughManiacally(string laughter); //æ³¨æ„ï¼ `onlyOwner`ä¸Šåœº : function likeABoss() external onlyOwner { LaughManiacally(&quot;Muahahahaha&quot;); }} æ³¨æ„ likeABoss å‡½æ•°ä¸Šçš„ onlyOwner ä¿®é¥°ç¬¦ã€‚ å½“ä½ è°ƒç”¨ likeABoss æ—¶ï¼š é¦–å…ˆæ‰§è¡Œ onlyOwner ä¸­çš„ä»£ç ã€‚ æ‰§è¡Œåˆ° onlyOwner ä¸­çš„ _; è¯­å¥æ—¶ï¼Œç¨‹åºå†è¿”å›žå¹¶æ‰§è¡Œ likeABoss ä¸­çš„ä»£ç ã€‚ å¯è§ï¼Œå°½ç®¡å‡½æ•°ä¿®é¥°ç¬¦ä¹Ÿå¯ä»¥åº”ç”¨åˆ°å„ç§åœºåˆï¼Œä½†æœ€å¸¸è§çš„è¿˜æ˜¯æ”¾åœ¨å‡½æ•°æ‰§è¡Œä¹‹å‰æ·»åŠ å¿«é€Ÿçš„ requireæ£€æŸ¥ã€‚ å› ä¸ºç»™å‡½æ•°æ·»åŠ äº†ä¿®é¥°ç¬¦ onlyOwnerï¼Œä½¿å¾—å”¯æœ‰åˆçº¦çš„ä¸»äººï¼ˆä¹Ÿå°±æ˜¯éƒ¨ç½²è€…ï¼‰æ‰èƒ½è°ƒç”¨å®ƒã€‚ æ³¨æ„ï¼šä¸»äººå¯¹åˆçº¦äº«æœ‰çš„ç‰¹æƒå½“ç„¶æ˜¯æ­£å½“çš„ï¼Œä¸è¿‡ä¹Ÿå¯èƒ½è¢«æ¶æ„ä½¿ç”¨ã€‚æ¯”å¦‚ï¼Œä¸‡ä¸€ï¼Œä¸»äººæ·»åŠ äº†ä¸ªåŽé—¨ï¼Œå…è®¸ä»–å·èµ°åˆ«äººçš„åƒµå°¸å‘¢ï¼Ÿ æ‰€ä»¥éžå¸¸é‡è¦çš„æ˜¯ï¼Œéƒ¨ç½²åœ¨ä»¥å¤ªåŠä¸Šçš„ DAppï¼Œå¹¶ä¸èƒ½ä¿è¯å®ƒçœŸæ­£åšåˆ°åŽ»ä¸­å¿ƒï¼Œä½ éœ€è¦é˜…è¯»å¹¶ç†è§£å®ƒçš„æºä»£ç ï¼Œæ‰èƒ½é˜²æ­¢å…¶ä¸­æ²¡æœ‰è¢«éƒ¨ç½²è€…æ¶æ„æ¤å…¥åŽé—¨ï¼›ä½œä¸ºå¼€å‘äººå‘˜ï¼Œå¦‚ä½•åšåˆ°æ—¢è¦ç»™è‡ªå·±ç•™ä¸‹ä¿®å¤ bug çš„ä½™åœ°ï¼Œåˆè¦å°½é‡åœ°æ”¾æƒç»™ä½¿ç”¨è€…ï¼Œä»¥ä¾¿è®©ä»–ä»¬æ”¾å¿ƒä½ ï¼Œä»Žè€Œæ„¿æ„æŠŠæ•°æ®æ”¾åœ¨ä½ çš„ DApp ä¸­ï¼Œè¿™ç¡®å®žéœ€è¦ä¸ªå¾®å¦™çš„å¹³è¡¡ã€‚ ç¬¬4ç« : GasçŽ°åœ¨æˆ‘ä»¬æ‡‚äº†å¦‚ä½•åœ¨ç¦æ­¢ç¬¬ä¸‰æ–¹ä¿®æ”¹æˆ‘ä»¬çš„åˆçº¦çš„åŒæ—¶ï¼Œç•™ä¸ªåŽé—¨ç»™å’±ä»¬è‡ªå·±åŽ»ä¿®æ”¹ã€‚ è®©æˆ‘ä»¬æ¥çœ‹å¦ä¸€ç§ä½¿å¾— Solidity ç¼–ç¨‹è¯­è¨€ä¸Žä¼—ä¸åŒçš„ç‰¹å¾ï¼š Gas - é©±åŠ¨ä»¥å¤ªåŠDAppsçš„èƒ½æºåœ¨ Solidity ä¸­ï¼Œä½ çš„ç”¨æˆ·æƒ³è¦æ¯æ¬¡æ‰§è¡Œä½ çš„ DApp éƒ½éœ€è¦æ”¯ä»˜ä¸€å®šçš„ gasï¼Œgas å¯ä»¥ç”¨ä»¥å¤ªå¸è´­ä¹°ï¼Œå› æ­¤ï¼Œç”¨æˆ·æ¯æ¬¡è·‘ DApp éƒ½å¾—èŠ±è´¹ä»¥å¤ªå¸ã€‚ ä¸€ä¸ª DApp æ”¶å–å¤šå°‘ gas å–å†³äºŽåŠŸèƒ½é€»è¾‘çš„å¤æ‚ç¨‹åº¦ã€‚æ¯ä¸ªæ“ä½œèƒŒåŽï¼Œéƒ½åœ¨è®¡ç®—å®Œæˆè¿™ä¸ªæ“ä½œæ‰€éœ€è¦çš„è®¡ç®—èµ„æºï¼Œï¼ˆæ¯”å¦‚ï¼Œå­˜å‚¨æ•°æ®å°±æ¯”åšä¸ªåŠ æ³•è¿ç®—è´µå¾—å¤šï¼‰ï¼Œ ä¸€æ¬¡æ“ä½œæ‰€éœ€è¦èŠ±è´¹çš„ gas ç­‰äºŽè¿™ä¸ªæ“ä½œèƒŒåŽçš„æ‰€æœ‰è¿ç®—èŠ±é”€çš„æ€»å’Œã€‚ ç”±äºŽè¿è¡Œä½ çš„ç¨‹åºéœ€è¦èŠ±è´¹ç”¨æˆ·çš„çœŸé‡‘ç™½é“¶ï¼Œåœ¨ä»¥å¤ªåŠä¸­ä»£ç çš„ç¼–ç¨‹è¯­è¨€ï¼Œæ¯”å…¶ä»–ä»»ä½•ç¼–ç¨‹è¯­è¨€éƒ½æ›´å¼ºè°ƒä¼˜åŒ–ã€‚ ä¸ºä»€ä¹ˆè¦ç”¨ gas æ¥é©±åŠ¨ï¼Ÿä»¥å¤ªåŠå°±åƒä¸€ä¸ªå·¨å¤§ã€ç¼“æ…¢ã€ä½†éžå¸¸å®‰å…¨çš„ç”µè„‘ã€‚å½“ä½ è¿è¡Œä¸€ä¸ªç¨‹åºçš„æ—¶å€™ï¼Œç½‘ç»œä¸Šçš„æ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½åœ¨è¿›è¡Œç›¸åŒçš„è¿ç®—ï¼Œä»¥éªŒè¯å®ƒçš„è¾“å‡º â€”â€” è¿™å°±æ˜¯æ‰€è°“çš„â€œåŽ»ä¸­å¿ƒåŒ–â€ ç”±äºŽæ•°ä»¥åƒè®¡çš„èŠ‚ç‚¹åŒæ—¶åœ¨éªŒè¯ç€æ¯ä¸ªåŠŸèƒ½çš„è¿è¡Œï¼Œè¿™å¯ä»¥ç¡®ä¿å®ƒçš„æ•°æ®ä¸ä¼šè¢«è¢«ç›‘æŽ§ï¼Œæˆ–è€…è¢«åˆ»æ„ä¿®æ”¹ã€‚ å¯èƒ½ä¼šæœ‰ç”¨æˆ·ç”¨æ— é™å¾ªçŽ¯å µå¡žç½‘ç»œï¼ŒæŠ‘æˆ–ç”¨å¯†é›†è¿ç®—æ¥å ç”¨å¤§é‡çš„ç½‘ç»œèµ„æºï¼Œä¸ºäº†é˜²æ­¢è¿™ç§äº‹æƒ…çš„å‘ç”Ÿï¼Œä»¥å¤ªåŠçš„åˆ›å»ºè€…ä¸ºä»¥å¤ªåŠä¸Šçš„èµ„æºåˆ¶å®šäº†ä»·æ ¼ï¼Œæƒ³è¦åœ¨ä»¥å¤ªåŠä¸Šè¿ç®—æˆ–è€…å­˜å‚¨ï¼Œä½ éœ€è¦å…ˆä»˜è´¹ã€‚ æ³¨æ„ï¼šå¦‚æžœä½ ä½¿ç”¨ä¾§é“¾ï¼Œå€’æ˜¯ä¸ä¸€å®šéœ€è¦ä»˜è´¹ï¼Œæ¯”å¦‚å’±ä»¬åœ¨ Loom Network ä¸Šæž„å»ºçš„ CryptoZombies å°±å…è´¹ã€‚ä½ ä¸ä¼šæƒ³è¦åœ¨ä»¥å¤ªåŠä¸»ç½‘ä¸ŠçŽ©å„¿â€œé­”å…½ä¸–ç•Œâ€å§ï¼Ÿ - æ‰€éœ€è¦çš„ gas å¯èƒ½ä¼šä¹°åˆ°ä½ ç ´äº§ã€‚ä½†æ˜¯ä½ å¯ä»¥æ‰¾ä¸ªç®—æ³•ç†å¿µä¸åŒçš„ä¾§é“¾æ¥çŽ©å®ƒã€‚æˆ‘ä»¬å°†åœ¨ä»¥åŽçš„è¯¾ç¨‹ä¸­å’±ä»¬ä¼šè®¨è®ºåˆ°ï¼Œä»€ä¹ˆæ ·çš„ DApp åº”è¯¥éƒ¨ç½²åœ¨å¤ªåŠä¸»é“¾ä¸Šï¼Œä»€ä¹ˆåˆæœ€å¥½æ”¾åœ¨ä¾§é“¾ã€‚ çœ gas çš„æ‹›æ•°ï¼šç»“æž„å°è£… ï¼ˆStruct packingï¼‰åœ¨ç¬¬1è¯¾ä¸­ï¼Œæˆ‘ä»¬æåˆ°é™¤äº†åŸºæœ¬ç‰ˆçš„ uint å¤–ï¼Œè¿˜æœ‰å…¶ä»–å˜ç§ uintï¼šuint8ï¼Œuint16ï¼Œuint32ç­‰ã€‚ é€šå¸¸æƒ…å†µä¸‹æˆ‘ä»¬ä¸ä¼šè€ƒè™‘ä½¿ç”¨ uint å˜ç§ï¼Œå› ä¸ºæ— è®ºå¦‚ä½•å®šä¹‰ uintçš„å¤§å°ï¼ŒSolidity ä¸ºå®ƒä¿ç•™256ä½çš„å­˜å‚¨ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ uint8 è€Œä¸æ˜¯uintï¼ˆuint256ï¼‰ä¸ä¼šä¸ºä½ èŠ‚çœä»»ä½• gasã€‚ é™¤éžï¼ŒæŠŠ uint ç»‘å®šåˆ° struct é‡Œé¢ã€‚ å¦‚æžœä¸€ä¸ª struct ä¸­æœ‰å¤šä¸ª uintï¼Œåˆ™å°½å¯èƒ½ä½¿ç”¨è¾ƒå°çš„ uint, Solidity ä¼šå°†è¿™äº› uint æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œä»Žè€Œå ç”¨è¾ƒå°‘çš„å­˜å‚¨ç©ºé—´ã€‚ä¾‹å¦‚ï¼š 123456789101112131415struct NormalStruct { uint a; uint b; uint c;}struct MiniMe { uint32 a; uint32 b; uint c;}// å› ä¸ºä½¿ç”¨äº†ç»“æž„æ‰“åŒ…ï¼Œ`mini` æ¯” `normal` å ç”¨çš„ç©ºé—´æ›´å°‘NormalStruct normal = NormalStruct(10, 20, 30);MiniMe mini = MiniMe(10, 20, 30); æ‰€ä»¥ï¼Œå½“ uint å®šä¹‰åœ¨ä¸€ä¸ª struct ä¸­çš„æ—¶å€™ï¼Œå°½é‡ä½¿ç”¨æœ€å°çš„æ•´æ•°å­ç±»åž‹ä»¥èŠ‚çº¦ç©ºé—´ã€‚ å¹¶ä¸”æŠŠåŒæ ·ç±»åž‹çš„å˜é‡æ”¾ä¸€èµ·ï¼ˆå³åœ¨ struct ä¸­å°†æŠŠå˜é‡æŒ‰ç…§ç±»åž‹ä¾æ¬¡æ”¾ç½®ï¼‰ï¼Œè¿™æ · Solidity å¯ä»¥å°†å­˜å‚¨ç©ºé—´æœ€å°åŒ–ã€‚ä¾‹å¦‚ï¼Œæœ‰ä¸¤ä¸ª structï¼š 1uint c; uint32 a; uint32 b;` å’Œ `uint32 a; uint c; uint32 b; å‰è€…æ¯”åŽè€…éœ€è¦çš„gasæ›´å°‘ï¼Œå› ä¸ºå‰è€…æŠŠuint32æ”¾ä¸€èµ·äº†ã€‚ ç¬¬5ç« : æ—¶é—´å•ä½level å±žæ€§è¡¨ç¤ºåƒµå°¸çš„çº§åˆ«ã€‚ä»¥åŽï¼Œåœ¨æˆ‘ä»¬åˆ›å»ºçš„æˆ˜æ–—ç³»ç»Ÿä¸­ï¼Œæ‰“èƒœä»—çš„åƒµå°¸ä¼šé€æ¸å‡çº§å¹¶èŽ·å¾—æ›´å¤šçš„èƒ½åŠ›ã€‚ readyTime ç¨å¾®å¤æ‚ç‚¹ã€‚æˆ‘ä»¬å¸Œæœ›å¢žåŠ ä¸€ä¸ªâ€œå†·å´å‘¨æœŸâ€ï¼Œè¡¨ç¤ºåƒµå°¸åœ¨ä¸¤æ¬¡çŒŽé£Ÿæˆ–æ”»å‡»ä¹‹ä¹‹é—´å¿…é¡»ç­‰å¾…çš„æ—¶é—´ã€‚å¦‚æžœæ²¡æœ‰å®ƒï¼Œåƒµå°¸æ¯å¤©å¯èƒ½ä¼šæ”»å‡»å’Œç¹æ®–1,000æ¬¡ï¼Œè¿™æ ·æ¸¸æˆå°±å¤ªç®€å•äº†ã€‚ ä¸ºäº†è®°å½•åƒµå°¸åœ¨ä¸‹ä¸€æ¬¡è¿›å‡»å‰éœ€è¦ç­‰å¾…çš„æ—¶é—´ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† Solidity çš„æ—¶é—´å•ä½ã€‚ æ—¶é—´å•ä½Solidity ä½¿ç”¨è‡ªå·±çš„æœ¬åœ°æ—¶é—´å•ä½ã€‚ å˜é‡now: å˜é‡ now å°†è¿”å›žå½“å‰çš„unixæ—¶é—´æˆ³ï¼ˆè‡ª1970å¹´1æœˆ1æ—¥ä»¥æ¥ç»è¿‡çš„ç§’æ•°ï¼‰ã€‚æˆ‘å†™è¿™å¥è¯æ—¶ unix æ—¶é—´æ˜¯ 1604319686ã€‚ æ³¨æ„ï¼šUnixæ—¶é—´ä¼ ç»Ÿç”¨ä¸€ä¸ª32ä½çš„æ•´æ•°è¿›è¡Œå­˜å‚¨ã€‚è¿™ä¼šå¯¼è‡´â€œ2038å¹´â€é—®é¢˜ï¼Œå½“è¿™ä¸ª32ä½çš„unixæ—¶é—´æˆ³ä¸å¤Ÿç”¨ï¼Œäº§ç”Ÿæº¢å‡ºï¼Œä½¿ç”¨è¿™ä¸ªæ—¶é—´çš„é—ç•™ç³»ç»Ÿå°±éº»çƒ¦äº†ã€‚æ‰€ä»¥ï¼Œå¦‚æžœæˆ‘ä»¬æƒ³è®©æˆ‘ä»¬çš„ DApp è·‘å¤Ÿ20å¹´ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨64ä½æ•´æ•°è¡¨ç¤ºæ—¶é—´ï¼Œä½†ä¸ºæ­¤æˆ‘ä»¬çš„ç”¨æˆ·åˆå¾—æ”¯ä»˜æ›´å¤šçš„ gasã€‚çœŸæ˜¯ä¸ªä¸¤éš¾çš„è®¾è®¡å•Šï¼ æ—¶é—´å•ä½ï¼šseconds minutes hours days weeks years Solidity è¿˜åŒ…å«ç§’(seconds)ï¼Œåˆ†é’Ÿ(minutes)ï¼Œå°æ—¶(hours)ï¼Œå¤©(days)ï¼Œå‘¨(weeks) å’Œ å¹´(years) ç­‰æ—¶é—´å•ä½ã€‚å®ƒä»¬éƒ½ä¼šè½¬æ¢æˆå¯¹åº”çš„ç§’æ•°æ”¾å…¥ uint ä¸­ã€‚æ‰€ä»¥ 1åˆ†é’Ÿ å°±æ˜¯ 60ï¼Œ1å°æ—¶æ˜¯ 3600ï¼ˆ60ç§’Ã—60åˆ†é’Ÿï¼‰ï¼Œ1å¤©æ˜¯86400ï¼ˆ24å°æ—¶Ã—60åˆ†é’ŸÃ—60ç§’ï¼‰ 123456789101112uint lastUpdated;// å°†â€˜ä¸Šæ¬¡æ›´æ–°æ—¶é—´â€™ è®¾ç½®ä¸º â€˜çŽ°åœ¨â€™function updateTimestamp() public { lastUpdated = now;}// å¦‚æžœåˆ°ä¸Šæ¬¡`updateTimestamp` è¶…è¿‡5åˆ†é’Ÿï¼Œè¿”å›ž 'true'// ä¸åˆ°5åˆ†é’Ÿè¿”å›ž 'false'function fiveMinutesHavePassed() public view returns (bool) { return (now &gt;= (lastUpdated + 5 minutes));} æœ‰äº†è¿™äº›å·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºåƒµå°¸è®¾å®šâ€œå†·é™æ—¶é—´â€åŠŸèƒ½ã€‚ ç¬¬6ç« : åƒµå°¸å†·å´é¦–å…ˆï¼Œæˆ‘ä»¬è¦å®šä¹‰ä¸€äº›è¾…åŠ©å‡½æ•°ï¼Œè®¾ç½®å¹¶æ£€æŸ¥åƒµå°¸çš„ readyTimeã€‚ å°†ç»“æž„ä½“ä½œä¸ºå‚æ•°ä¼ å…¥ç”±äºŽç»“æž„ä½“çš„å­˜å‚¨æŒ‡é’ˆå¯ä»¥ä»¥å‚æ•°çš„æ–¹å¼ä¼ é€’ç»™ä¸€ä¸ª private æˆ– internal çš„å‡½æ•°ï¼Œå› æ­¤ç»“æž„ä½“å¯ä»¥åœ¨å¤šä¸ªå‡½æ•°ä¹‹é—´ç›¸äº’ä¼ é€’ã€‚ æŠŠç»“æž„ä½“ä½œä¸ºæŒ‡é’ˆï¼Œå› æ­¤ä¼ é€’å‚æ•°å‰éœ€è¦åŠ storage éµå¾ªè¿™æ ·çš„è¯­æ³•ï¼š 123function _doStuff(Zombie storage _zombie) internal { // do stuff with _zombie} è¿™æ ·æˆ‘ä»¬å¯ä»¥å°†æŸåƒµå°¸çš„å¼•ç”¨ç›´æŽ¥ä¼ é€’ç»™ä¸€ä¸ªå‡½æ•°ï¼Œè€Œä¸ç”¨æ˜¯é€šè¿‡å‚æ•°ä¼ å…¥åƒµå°¸IDåŽï¼Œå‡½æ•°å†ä¾æ®IDåŽ»æŸ¥æ‰¾ã€‚ ç¬¬7ç« : å…¬æœ‰å‡½æ•°å’Œå®‰å…¨æ€§ä½ å¿…é¡»ä»”ç»†åœ°æ£€æŸ¥æ‰€æœ‰å£°æ˜Žä¸º public å’Œ externalçš„å‡½æ•°ï¼Œä¸€ä¸ªä¸ªæŽ’é™¤ç”¨æˆ·æ»¥ç”¨å®ƒä»¬çš„å¯èƒ½ï¼Œè°¨é˜²å®‰å…¨æ¼æ´žã€‚è¯·è®°ä½ï¼Œå¦‚æžœè¿™äº›å‡½æ•°æ²¡æœ‰ç±»ä¼¼ onlyOwner è¿™æ ·çš„å‡½æ•°ä¿®é¥°ç¬¦ï¼Œç”¨æˆ·èƒ½åˆ©ç”¨å„ç§å¯èƒ½çš„å‚æ•°åŽ»è°ƒç”¨å®ƒä»¬ã€‚ ç¬¬8ç« : è¿›ä¸€æ­¥äº†è§£å‡½æ•°ä¿®é¥°ç¬¦æˆ‘ä»¬æ‰“ç®—è®©åƒµå°¸åœ¨è¾¾åˆ°ä¸€å®šæ°´å¹³åŽï¼ŒèŽ·å¾—ç‰¹æ®Šèƒ½åŠ›ã€‚ä½†æ˜¯è¾¾åˆ°è¿™ä¸ªå°ç›®æ ‡ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å­¦ä¸€å­¦ä»€ä¹ˆæ˜¯â€œå‡½æ•°ä¿®é¥°ç¬¦â€ã€‚ å¸¦å‚æ•°çš„å‡½æ•°ä¿®é¥°ç¬¦ä¹‹å‰æˆ‘ä»¬å·²ç»è¯»è¿‡ä¸€ä¸ªç®€å•çš„å‡½æ•°ä¿®é¥°ç¬¦äº†ï¼šonlyOwnerã€‚å‡½æ•°ä¿®é¥°ç¬¦ä¹Ÿå¯ä»¥å¸¦å‚æ•°ã€‚ä¾‹å¦‚ï¼š 1234567891011121314// å­˜å‚¨ç”¨æˆ·å¹´é¾„çš„æ˜ å°„mapping (uint =&gt; uint) public age;// é™å®šç”¨æˆ·å¹´é¾„çš„ä¿®é¥°ç¬¦modifier olderThan(uint _age, uint _userId) { require(age[_userId] &gt;= _age); _;}// å¿…é¡»å¹´æ»¡16å‘¨å²æ‰å…è®¸å¼€è½¦ (è‡³å°‘åœ¨ç¾Žå›½æ˜¯è¿™æ ·çš„).// æˆ‘ä»¬å¯ä»¥ç”¨å¦‚ä¸‹å‚æ•°è°ƒç”¨`olderThan` ä¿®é¥°ç¬¦:function driveCar(uint _userId) public olderThan(16, _userId) { // å…¶ä½™çš„ç¨‹åºé€»è¾‘} çœ‹åˆ°äº†å§ï¼Œ olderThan ä¿®é¥°ç¬¦å¯ä»¥åƒå‡½æ•°ä¸€æ ·æŽ¥æ”¶å‚æ•°ï¼Œæ˜¯â€œå®¿ä¸»â€å‡½æ•° driveCar æŠŠå‚æ•°ä¼ é€’ç»™å®ƒçš„ä¿®é¥°ç¬¦çš„ã€‚ æ¥ï¼Œæˆ‘ä»¬è‡ªå·±ç”Ÿäº§ä¸€ä¸ªä¿®é¥°ç¬¦ï¼Œé€šè¿‡ä¼ å…¥çš„levelå‚æ•°æ¥é™åˆ¶åƒµå°¸ä½¿ç”¨æŸäº›ç‰¹æ®ŠåŠŸèƒ½ã€‚ å¸¦å‚æ•°çš„å‡½æ•°ä¿®é¥°ç¬¦ä¹‹å‰æˆ‘ä»¬å·²ç»è¯»è¿‡ä¸€ä¸ªç®€å•çš„å‡½æ•°ä¿®é¥°ç¬¦äº†ï¼šonlyOwnerã€‚å‡½æ•°ä¿®é¥°ç¬¦ä¹Ÿå¯ä»¥å¸¦å‚æ•°ã€‚ä¾‹å¦‚ï¼š 1234567891011121314// å­˜å‚¨ç”¨æˆ·å¹´é¾„çš„æ˜ å°„mapping (uint =&gt; uint) public age;// é™å®šç”¨æˆ·å¹´é¾„çš„ä¿®é¥°ç¬¦modifier olderThan(uint _age, uint _userId) { require(age[_userId] &gt;= _age); _;}// å¿…é¡»å¹´æ»¡16å‘¨å²æ‰å…è®¸å¼€è½¦ (è‡³å°‘åœ¨ç¾Žå›½æ˜¯è¿™æ ·çš„).// æˆ‘ä»¬å¯ä»¥ç”¨å¦‚ä¸‹å‚æ•°è°ƒç”¨`olderThan` ä¿®é¥°ç¬¦:function driveCar(uint _userId) public olderThan(16, _userId) { // å…¶ä½™çš„ç¨‹åºé€»è¾‘} çœ‹åˆ°äº†å§ï¼Œ olderThan ä¿®é¥°ç¬¦å¯ä»¥åƒå‡½æ•°ä¸€æ ·æŽ¥æ”¶å‚æ•°ï¼Œæ˜¯â€œå®¿ä¸»â€å‡½æ•° driveCar æŠŠå‚æ•°ä¼ é€’ç»™å®ƒçš„ä¿®é¥°ç¬¦çš„ã€‚ æ¥ï¼Œæˆ‘ä»¬è‡ªå·±ç”Ÿäº§ä¸€ä¸ªä¿®é¥°ç¬¦ï¼Œé€šè¿‡ä¼ å…¥çš„levelå‚æ•°æ¥é™åˆ¶åƒµå°¸ä½¿ç”¨æŸäº›ç‰¹æ®ŠåŠŸèƒ½ã€‚ è®°ä½ï¼Œä¿®é¥°ç¬¦çš„æœ€åŽä¸€è¡Œä¸º _;ï¼Œè¡¨ç¤ºä¿®é¥°ç¬¦è°ƒç”¨ç»“æŸåŽè¿”å›žï¼Œå¹¶æ‰§è¡Œè°ƒç”¨å‡½æ•°ä½™ä¸‹çš„éƒ¨åˆ†ã€‚ ç¬¬10ç« : åˆ©ç”¨ â€˜Viewâ€™ å‡½æ•°èŠ‚çœ GasçŽ°åœ¨éœ€è¦æ·»åŠ çš„ä¸€ä¸ªåŠŸèƒ½æ˜¯ï¼šæˆ‘ä»¬çš„ DApp éœ€è¦ä¸€ä¸ªæ–¹æ³•æ¥æŸ¥çœ‹æŸçŽ©å®¶çš„æ•´ä¸ªåƒµå°¸å†›å›¢ - æˆ‘ä»¬ç§°ä¹‹ä¸º getZombiesByOwnerã€‚ å®žçŽ°è¿™ä¸ªåŠŸèƒ½åªéœ€ä»ŽåŒºå—é“¾ä¸­è¯»å–æ•°æ®ï¼Œæ‰€ä»¥å®ƒå¯ä»¥æ˜¯ä¸€ä¸ª view å‡½æ•°ã€‚è¿™è®©æˆ‘ä»¬ä¸å¾—ä¸å›žé¡¾ä¸€ä¸‹â€œgasä¼˜åŒ–â€è¿™ä¸ªé‡è¦è¯é¢˜ã€‚ â€œviewâ€ å‡½æ•°ä¸èŠ± â€œgasâ€å½“çŽ©å®¶ä»Žå¤–éƒ¨è°ƒç”¨ä¸€ä¸ªviewå‡½æ•°ï¼Œæ˜¯ä¸éœ€è¦æ”¯ä»˜ä¸€åˆ† gas çš„ã€‚ è¿™æ˜¯å› ä¸º view å‡½æ•°ä¸ä¼šçœŸæ­£æ”¹å˜åŒºå—é“¾ä¸Šçš„ä»»ä½•æ•°æ® - å®ƒä»¬åªæ˜¯è¯»å–ã€‚å› æ­¤ç”¨ view æ ‡è®°ä¸€ä¸ªå‡½æ•°ï¼Œæ„å‘³ç€å‘Šè¯‰ web3.jsï¼Œè¿è¡Œè¿™ä¸ªå‡½æ•°åªéœ€è¦æŸ¥è¯¢ä½ çš„æœ¬åœ°ä»¥å¤ªåŠèŠ‚ç‚¹ï¼Œè€Œä¸éœ€è¦åœ¨åŒºå—é“¾ä¸Šåˆ›å»ºä¸€ä¸ªäº‹åŠ¡ï¼ˆäº‹åŠ¡éœ€è¦è¿è¡Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šï¼Œå› æ­¤èŠ±è´¹ gasï¼‰ã€‚ ç¨åŽæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•åœ¨è‡ªå·±çš„èŠ‚ç‚¹ä¸Šè®¾ç½® web3.jsã€‚ä½†çŽ°åœ¨ï¼Œä½ å…³é”®æ˜¯è¦è®°ä½ï¼Œåœ¨æ‰€èƒ½åªè¯»çš„å‡½æ•°ä¸Šæ ‡è®°ä¸Šè¡¨ç¤ºâ€œåªè¯»â€çš„â€œexternal view å£°æ˜Žï¼Œå°±èƒ½ä¸ºä½ çš„çŽ©å®¶å‡å°‘åœ¨ DApp ä¸­ gas ç”¨é‡ã€‚ æ³¨æ„ï¼šå¦‚æžœä¸€ä¸ª view å‡½æ•°åœ¨å¦ä¸€ä¸ªå‡½æ•°çš„å†…éƒ¨è¢«è°ƒç”¨ï¼Œè€Œè°ƒç”¨å‡½æ•°ä¸Ž view å‡½æ•°çš„ä¸å±žäºŽåŒä¸€ä¸ªåˆçº¦ï¼Œä¹Ÿä¼šäº§ç”Ÿè°ƒç”¨æˆæœ¬ã€‚è¿™æ˜¯å› ä¸ºå¦‚æžœä¸»è°ƒå‡½æ•°åœ¨ä»¥å¤ªåŠåˆ›å»ºäº†ä¸€ä¸ªäº‹åŠ¡ï¼Œå®ƒä»ç„¶éœ€è¦é€ä¸ªèŠ‚ç‚¹åŽ»éªŒè¯ã€‚æ‰€ä»¥æ ‡è®°ä¸º view çš„å‡½æ•°åªæœ‰åœ¨å¤–éƒ¨è°ƒç”¨æ—¶æ‰æ˜¯å…è´¹çš„ã€‚ ç¬¬11ç« : å­˜å‚¨éžå¸¸æ˜‚è´µSolidity ä½¿ç”¨storage(å­˜å‚¨)æ˜¯ç›¸å½“æ˜‚è´µçš„ï¼Œâ€å†™å…¥â€œæ“ä½œå°¤å…¶è´µã€‚ è¿™æ˜¯å› ä¸ºï¼Œæ— è®ºæ˜¯å†™å…¥è¿˜æ˜¯æ›´æ”¹ä¸€æ®µæ•°æ®ï¼Œ è¿™éƒ½å°†æ°¸ä¹…æ€§åœ°å†™å…¥åŒºå—é“¾ã€‚â€æ°¸ä¹…æ€§â€œå•Šï¼éœ€è¦åœ¨å…¨çƒæ•°åƒä¸ªèŠ‚ç‚¹çš„ç¡¬ç›˜ä¸Šå­˜å…¥è¿™äº›æ•°æ®ï¼Œéšç€åŒºå—é“¾çš„å¢žé•¿ï¼Œæ‹·è´ä»½æ•°æ›´å¤šï¼Œå­˜å‚¨é‡ä¹Ÿå°±è¶Šå¤§ã€‚è¿™æ˜¯éœ€è¦æˆæœ¬çš„ï¼ ä¸ºäº†é™ä½Žæˆæœ¬ï¼Œä¸åˆ°ä¸‡ä¸å¾—å·²ï¼Œé¿å…å°†æ•°æ®å†™å…¥å­˜å‚¨ã€‚è¿™ä¹Ÿä¼šå¯¼è‡´æ•ˆçŽ‡ä½Žä¸‹çš„ç¼–ç¨‹é€»è¾‘ - æ¯”å¦‚æ¯æ¬¡è°ƒç”¨ä¸€ä¸ªå‡½æ•°ï¼Œéƒ½éœ€è¦åœ¨ memory(å†…å­˜) ä¸­é‡å»ºä¸€ä¸ªæ•°ç»„ï¼Œè€Œä¸æ˜¯ç®€å•åœ°å°†ä¸Šæ¬¡è®¡ç®—çš„æ•°ç»„ç»™å­˜å‚¨ä¸‹æ¥ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾ã€‚ éåŽ†å¤§æ•°æ®é›†åˆéƒ½æ˜¯æ˜‚è´µçš„ã€‚ä½†æ˜¯åœ¨ Solidity ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ªæ ‡è®°äº†external viewçš„å‡½æ•°ï¼ŒéåŽ†æ¯” storage è¦ä¾¿å®œå¤ªå¤šï¼Œå› ä¸º view å‡½æ•°ä¸ä¼šäº§ç”Ÿä»»ä½•èŠ±é”€ã€‚ ï¼ˆgaså¯æ˜¯çœŸé‡‘ç™½é“¶å•Šï¼ï¼‰ã€‚ æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« è®¨è®ºforå¾ªçŽ¯ï¼ŒçŽ°åœ¨æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹çœ‹å¦‚ä½•å¦‚ä½•åœ¨å†…å­˜ä¸­å£°æ˜Žæ•°ç»„ã€‚ åœ¨å†…å­˜ä¸­å£°æ˜Žæ•°ç»„åœ¨æ•°ç»„åŽé¢åŠ ä¸Š memoryå…³é”®å­—ï¼Œ è¡¨æ˜Žè¿™ä¸ªæ•°ç»„æ˜¯ä»…ä»…åœ¨å†…å­˜ä¸­åˆ›å»ºï¼Œä¸éœ€è¦å†™å…¥å¤–éƒ¨å­˜å‚¨ï¼Œå¹¶ä¸”åœ¨å‡½æ•°è°ƒç”¨ç»“æŸæ—¶å®ƒå°±è§£æ•£äº†ã€‚ä¸Žåœ¨ç¨‹åºç»“æŸæ—¶æŠŠæ•°æ®ä¿å­˜è¿› storage çš„åšæ³•ç›¸æ¯”ï¼Œå†…å­˜è¿ç®—å¯ä»¥å¤§å¤§èŠ‚çœgaså¼€é”€ â€“ æŠŠè¿™æ•°ç»„æ”¾åœ¨viewé‡Œç”¨ï¼Œå®Œå…¨ä¸ç”¨èŠ±é’±ã€‚ 12345678910function getArray() external pure returns(uint[]) { // åˆå§‹åŒ–ä¸€ä¸ªé•¿åº¦ä¸º3çš„å†…å­˜æ•°ç»„ uint[] memory values = new uint[](3); // èµ‹å€¼ values.push(1); values.push(2); values.push(3); // è¿”å›žæ•°ç»„ return values;} è¿™ä¸ªå°ä¾‹å­å±•ç¤ºäº†ä¸€äº›è¯­æ³•è§„åˆ™ï¼Œä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®žé™…ç”¨ä¾‹ï¼Œå±•ç¤ºå®ƒå’Œ for å¾ªçŽ¯ç»“åˆçš„åšæ³•ã€‚ æ³¨æ„ï¼šå†…å­˜æ•°ç»„ å¿…é¡» ç”¨é•¿åº¦å‚æ•°ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º3ï¼‰åˆ›å»ºã€‚ç›®å‰ä¸æ”¯æŒ array.push()ä¹‹ç±»çš„æ–¹æ³•è°ƒæ•´æ•°ç»„å¤§å°ï¼Œåœ¨æœªæ¥çš„ç‰ˆæœ¬å¯èƒ½ä¼šæ”¯æŒé•¿åº¦ä¿®æ”¹ã€‚ ç¬¬12ç« : For å¾ªçŽ¯æˆ‘ä»¬æåˆ°è¿‡ï¼Œå‡½æ•°ä¸­ä½¿ç”¨çš„æ•°ç»„æ˜¯è¿è¡Œæ—¶åœ¨å†…å­˜ä¸­é€šè¿‡ for å¾ªçŽ¯å®žæ—¶æž„å»ºï¼Œè€Œä¸æ˜¯é¢„å…ˆå»ºç«‹åœ¨å­˜å‚¨ä¸­çš„ã€‚ ä¸ºä»€ä¹ˆè¦è¿™æ ·åšå‘¢ï¼Ÿ ä¸ºäº†å®žçŽ° getZombiesByOwner å‡½æ•°ï¼Œä¸€ç§â€œæ— è„‘å¼â€çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨ ZombieFactory ä¸­å­˜å…¥â€ä¸»äººâ€œå’Œâ€åƒµå°¸å†›å›¢â€œçš„æ˜ å°„ã€‚ 1mapping (address =&gt; uint[]) public ownerToZombies ç„¶åŽæˆ‘ä»¬æ¯æ¬¡åˆ›å»ºæ–°åƒµå°¸æ—¶ï¼Œæ‰§è¡Œ ownerToZombies [owner] .pushï¼ˆzombieIdï¼‰ å°†å…¶æ·»åŠ åˆ°ä¸»äººçš„åƒµå°¸æ•°ç»„ä¸­ã€‚è€Œ getZombiesByOwner å‡½æ•°ä¹Ÿéžå¸¸ç®€å•ï¼š 123function getZombiesByOwner(address _owner) external view returns (uint[]) { return ownerToZombies[_owner];} è¿™ä¸ªåšæ³•æœ‰é—®é¢˜åšæ³•å€’æ˜¯ç®€å•ã€‚å¯æ˜¯å¦‚æžœæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥æŠŠä¸€å¤´åƒµå°¸è½¬ç§»åˆ°å¦ä¸€ä¸ªä¸»äººåä¸‹ï¼ˆæˆ‘ä»¬ä¸€å®šä¼šåœ¨åŽé¢çš„è¯¾ç¨‹ä¸­å®žçŽ°çš„ï¼‰ï¼Œåˆä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ è¿™ä¸ªâ€œæ¢ä¸»â€å‡½æ•°è¦åšåˆ°ï¼š 1.å°†åƒµå°¸pushåˆ°æ–°ä¸»äººçš„ ownerToZombies æ•°ç»„ä¸­ï¼Œ 2.ä»Žæ—§ä¸»çš„ ownerToZombies æ•°ç»„ä¸­ç§»é™¤åƒµå°¸ï¼Œ 3.å°†æ—§ä¸»åƒµå°¸æ•°ç»„ä¸­â€œæ¢ä¸»åƒµå°¸â€ä¹‹åŽçš„çš„æ¯å¤´åƒµå°¸éƒ½å¾€å‰æŒªä¸€ä½ï¼ŒæŠŠæŒªèµ°â€œæ¢ä¸»åƒµå°¸â€åŽç•™ä¸‹çš„â€œç©ºæ§½â€å¡«ä¸Šï¼Œ 4.å°†æ•°ç»„é•¿åº¦å‡1ã€‚ ä½†æ˜¯ç¬¬ä¸‰æ­¥å®žåœ¨æ˜¯å¤ªè´µäº†ï¼å› ä¸ºæ¯æŒªåŠ¨ä¸€å¤´åƒµå°¸ï¼Œæˆ‘ä»¬éƒ½è¦æ‰§è¡Œä¸€æ¬¡å†™æ“ä½œã€‚å¦‚æžœä¸€ä¸ªä¸»äººæœ‰20å¤´åƒµå°¸ï¼Œè€Œç¬¬ä¸€å¤´è¢«æŒªèµ°äº†ï¼Œé‚£ä¸ºäº†ä¿æŒæ•°ç»„çš„é¡ºåºï¼Œæˆ‘ä»¬å¾—åš19ä¸ªå†™æ“ä½œã€‚ ç”±äºŽå†™å…¥å­˜å‚¨æ˜¯ Solidity ä¸­æœ€è´¹ gas çš„æ“ä½œä¹‹ä¸€ï¼Œä½¿å¾—æ¢ä¸»å‡½æ•°çš„æ¯æ¬¡è°ƒç”¨éƒ½éžå¸¸æ˜‚è´µã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œæ¯æ¬¡è°ƒç”¨çš„æ—¶å€™èŠ±è´¹çš„ gas éƒ½ä¸åŒï¼å…·ä½“è¿˜å–å†³äºŽç”¨æˆ·åœ¨åŽŸä¸»å†›å›¢ä¸­çš„åƒµå°¸å¤´æ•°ï¼Œä»¥åŠç§»èµ°çš„åƒµå°¸æ‰€åœ¨çš„ä½ç½®ã€‚ä»¥è‡³äºŽç”¨æˆ·éƒ½ä¸çŸ¥é“åº”è¯¥æ”¯ä»˜å¤šå°‘ gasã€‚ æ³¨æ„ï¼šå½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠæ•°ç»„ä¸­æœ€åŽä¸€ä¸ªåƒµå°¸å¾€å‰æŒªæ¥å¡«è¡¥ç©ºæ§½ï¼Œå¹¶å°†æ•°ç»„é•¿åº¦å‡å°‘ä¸€ã€‚ä½†è¿™æ ·æ¯åšä¸€ç¬”äº¤æ˜“ï¼Œéƒ½ä¼šæ”¹å˜åƒµå°¸å†›å›¢çš„ç§©åºã€‚ ç”±äºŽä»Žå¤–éƒ¨è°ƒç”¨ä¸€ä¸ª view å‡½æ•°æ˜¯å…è´¹çš„ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨ getZombiesByOwner å‡½æ•°ä¸­ç”¨ä¸€ä¸ªforå¾ªçŽ¯éåŽ†æ•´ä¸ªåƒµå°¸æ•°ç»„ï¼ŒæŠŠå±žäºŽæŸä¸ªä¸»äººçš„åƒµå°¸æŒ‘å‡ºæ¥æž„å»ºå‡ºåƒµå°¸æ•°ç»„ã€‚é‚£ä¹ˆæˆ‘ä»¬çš„ transfer å‡½æ•°å°†ä¼šä¾¿å®œå¾—å¤šï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦æŒªåŠ¨å­˜å‚¨é‡Œçš„åƒµå°¸æ•°ç»„é‡æ–°æŽ’åºï¼Œæ€»ä½“ä¸Šè¿™ä¸ªæ–¹æ³•ä¼šæ›´ä¾¿å®œï¼Œè™½ç„¶æœ‰ç‚¹åç›´è§‰ã€‚ ä½¿ç”¨ for å¾ªçŽ¯forå¾ªçŽ¯çš„è¯­æ³•åœ¨ Solidity å’Œ JavaScript ä¸­ç±»ä¼¼ã€‚ æ¥çœ‹ä¸€ä¸ªåˆ›å»ºå¶æ•°æ•°ç»„çš„ä¾‹å­ï¼š 12345678910111213141516function getEvens() pure external returns(uint[]) { uint[] memory evens = new uint[](5); // åœ¨æ–°æ•°ç»„ä¸­è®°å½•åºåˆ—å· uint counter = 0; // åœ¨å¾ªçŽ¯ä»Ž1è¿­ä»£åˆ°10ï¼š for (uint i = 1; i &lt;= 10; i++) { // å¦‚æžœ `i` æ˜¯å¶æ•°... if (i % 2 == 0) { // æŠŠå®ƒåŠ å…¥å¶æ•°æ•°ç»„ evens[counter] = i; //ç´¢å¼•åŠ ä¸€ï¼Œ æŒ‡å‘ä¸‹ä¸€ä¸ªç©ºçš„â€˜evenâ€™ counter++; } } return evens;} è¿™ä¸ªå‡½æ•°å°†è¿”å›žä¸€ä¸ªå½¢ä¸º [2,4,6,8,10] çš„æ•°ç»„ã€‚ part 2ç¬¬1ç« : å¯æ”¯ä»˜æˆªè‡³ç›®å‰ï¼Œæˆ‘ä»¬åªæŽ¥è§¦åˆ°å¾ˆå°‘çš„ å‡½æ•°ä¿®é¥°ç¬¦ã€‚ è¦è®°ä½æ‰€æœ‰çš„ä¸œè¥¿å¾ˆéš¾ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥ä¸ªæ¦‚è§ˆï¼š æˆ‘ä»¬æœ‰å†³å®šå‡½æ•°ä½•æ—¶å’Œè¢«è°è°ƒç”¨çš„å¯è§æ€§ä¿®é¥°ç¬¦: private æ„å‘³ç€å®ƒåªèƒ½è¢«åˆçº¦å†…éƒ¨è°ƒç”¨ï¼› internal å°±åƒ private ä½†æ˜¯ä¹Ÿèƒ½è¢«ç»§æ‰¿çš„åˆçº¦è°ƒç”¨ï¼› external åªèƒ½ä»Žåˆçº¦å¤–éƒ¨è°ƒç”¨ï¼›æœ€åŽ public å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹è°ƒç”¨ï¼Œä¸ç®¡æ˜¯å†…éƒ¨è¿˜æ˜¯å¤–éƒ¨ã€‚ æˆ‘ä»¬ä¹Ÿæœ‰çŠ¶æ€ä¿®é¥°ç¬¦ï¼Œ å‘Šè¯‰æˆ‘ä»¬å‡½æ•°å¦‚ä½•å’ŒåŒºå—é“¾äº¤äº’: view å‘Šè¯‰æˆ‘ä»¬è¿è¡Œè¿™ä¸ªå‡½æ•°ä¸ä¼šæ›´æ”¹å’Œä¿å­˜ä»»ä½•æ•°æ®ï¼› pure å‘Šè¯‰æˆ‘ä»¬è¿™ä¸ªå‡½æ•°ä¸ä½†ä¸ä¼šå¾€åŒºå—é“¾å†™æ•°æ®ï¼Œå®ƒç”šè‡³ä¸ä»ŽåŒºå—é“¾è¯»å–æ•°æ®ã€‚è¿™ä¸¤ç§åœ¨è¢«ä»Žåˆçº¦å¤–éƒ¨è°ƒç”¨çš„æ—¶å€™éƒ½ä¸èŠ±è´¹ä»»ä½•gasï¼ˆä½†æ˜¯å®ƒä»¬åœ¨è¢«å†…éƒ¨å…¶ä»–å‡½æ•°è°ƒç”¨çš„æ—¶å€™å°†ä¼šè€—è´¹gasï¼‰ã€‚ ç„¶åŽæˆ‘ä»¬æœ‰äº†è‡ªå®šä¹‰çš„ modifiersï¼Œä¾‹å¦‚åœ¨ç¬¬ä¸‰è¯¾å­¦ä¹ çš„: onlyOwner å’Œ aboveLevelã€‚ å¯¹äºŽè¿™äº›ä¿®é¥°ç¬¦æˆ‘ä»¬å¯ä»¥è‡ªå®šä¹‰å…¶å¯¹å‡½æ•°çš„çº¦æŸé€»è¾‘ã€‚ è¿™äº›ä¿®é¥°ç¬¦å¯ä»¥åŒæ—¶ä½œç”¨äºŽä¸€ä¸ªå‡½æ•°å®šä¹‰ä¸Šï¼š 1function test() external view onlyOwner anotherModifier { /* ... */ } åœ¨è¿™ä¸€ç« ï¼Œæˆ‘ä»¬æ¥å­¦ä¹ ä¸€ä¸ªæ–°çš„ä¿®é¥°ç¬¦ payable. payable ä¿®é¥°ç¬¦payable æ–¹æ³•æ˜¯è®© Solidity å’Œä»¥å¤ªåŠå˜å¾—å¦‚æ­¤é…·çš„ä¸€éƒ¨åˆ† â€”â€” å®ƒä»¬æ˜¯ä¸€ç§å¯ä»¥æŽ¥æ”¶ä»¥å¤ªçš„ç‰¹æ®Šå‡½æ•°ã€‚ å½“ä½ åœ¨è°ƒç”¨ä¸€ä¸ªæ™®é€šç½‘ç«™æœåŠ¡å™¨ä¸Šçš„APIå‡½æ•°çš„æ—¶å€™ï¼Œä½ æ— æ³•ç”¨ä½ çš„å‡½æ•°ä¼ é€ç¾Žå…ƒâ€”â€”ä½ ä¹Ÿä¸èƒ½ä¼ é€æ¯”ç‰¹å¸ã€‚ ä½†æ˜¯åœ¨ä»¥å¤ªåŠä¸­ï¼Œ å› ä¸ºé’± (ä»¥å¤ª), æ•°æ® (äº‹åŠ¡è´Ÿè½½)ï¼Œ ä»¥åŠåˆçº¦ä»£ç æœ¬èº«éƒ½å­˜åœ¨äºŽä»¥å¤ªåŠã€‚ä½ å¯ä»¥åœ¨åŒæ—¶è°ƒç”¨å‡½æ•° å¹¶ä»˜é’±ç»™å¦å¤–ä¸€ä¸ªåˆçº¦ã€‚ è¿™å°±å…è®¸å‡ºçŽ°å¾ˆå¤šæœ‰è¶£çš„é€»è¾‘ï¼Œ æ¯”å¦‚å‘ä¸€ä¸ªåˆçº¦è¦æ±‚æ”¯ä»˜ä¸€å®šçš„é’±æ¥è¿è¡Œä¸€ä¸ªå‡½æ•°ã€‚ 12345678contract OnlineStore { function buySomething() external payable { // æ£€æŸ¥ä»¥ç¡®å®š0.001ä»¥å¤ªå‘é€å‡ºåŽ»æ¥è¿è¡Œå‡½æ•°: require(msg.value == 0.001 ether); // å¦‚æžœä¸ºçœŸï¼Œä¸€äº›ç”¨æ¥å‘å‡½æ•°è°ƒç”¨è€…å‘é€æ•°å­—å†…å®¹çš„é€»è¾‘ transferThing(msg.sender); }} msg.value å¯ä»¥æŸ¥çœ‹msg.sender å‘åˆçº¦å‘é€äº†å¤šå°‘ä»¥å¤ªçš„æ–¹æ³•ï¼Œå¦å¤– ether æ˜¯ä¸€ä¸ªå†…ç½®å•ä½ã€‚ è¿™é‡Œå‘ç”Ÿçš„äº‹æ˜¯ï¼Œä¸€äº›äººä¼šä»Ž web3.js è°ƒç”¨è¿™ä¸ªå‡½æ•° (ä»ŽDAppçš„å‰ç«¯)ï¼Œ åƒè¿™æ · : 12// å‡è®¾ `OnlineStore` åœ¨ä»¥å¤ªåŠä¸ŠæŒ‡å‘ä½ çš„åˆçº¦:OnlineStore.buySomething().send(from: web3.eth.defaultAccount, value: web3.utils.toWei(0.001)) æ³¨æ„è¿™ä¸ª value å­—æ®µï¼Œ JavaScript è°ƒç”¨æ¥æŒ‡å®šå‘é€å¤šå°‘(0.001)ä»¥å¤ªã€‚å¦‚æžœæŠŠäº‹åŠ¡æƒ³è±¡æˆä¸€ä¸ªä¿¡å°ï¼Œä½ å‘é€åˆ°å‡½æ•°çš„å‚æ•°å°±æ˜¯ä¿¡çš„å†…å®¹ã€‚ æ·»åŠ ä¸€ä¸ª value å¾ˆåƒåœ¨ä¿¡å°é‡Œé¢æ”¾é’± â€”â€” ä¿¡ä»¶å†…å®¹å’Œé’±åŒæ—¶å‘é€ç»™äº†æŽ¥æ”¶è€…ã€‚ æ³¨æ„ï¼š å¦‚æžœä¸€ä¸ªå‡½æ•°æ²¡æ ‡è®°ä¸ºpayableï¼Œ è€Œä½ å°è¯•åˆ©ç”¨ä¸Šé¢çš„æ–¹æ³•å‘é€ä»¥å¤ªï¼Œå‡½æ•°å°†æ‹’ç»ä½ çš„äº‹åŠ¡ã€‚ ç¬¬2ç« : æçŽ°åœ¨ä½ å‘é€ä»¥å¤ªä¹‹åŽï¼Œå®ƒå°†è¢«å­˜å‚¨è¿›è¯¥åˆçº¦çš„ä»¥å¤ªåŠè´¦æˆ·ä¸­ï¼Œ å¹¶å†»ç»“åœ¨å“ªé‡Œ â€”â€” é™¤éžä½ æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥ä»Žåˆçº¦ä¸­æŠŠä»¥å¤ªæçŽ°ã€‚ ä½ å¯ä»¥å†™ä¸€ä¸ªå‡½æ•°æ¥ä»Žåˆçº¦ä¸­æçŽ°ä»¥å¤ªï¼Œç±»ä¼¼è¿™æ ·ï¼š 12345contract GetPaid is Ownable { function withdraw() external onlyOwner { owner.transfer(this.balance); }} æ³¨æ„æˆ‘ä»¬ä½¿ç”¨ Ownable åˆçº¦ä¸­çš„ owner å’Œ onlyOwnerï¼Œå‡å®šå®ƒå·²ç»è¢«å¼•å…¥äº†ã€‚ ä½ å¯ä»¥é€šè¿‡ transfer å‡½æ•°å‘ä¸€ä¸ªåœ°å€å‘é€ä»¥å¤ªï¼Œ ç„¶åŽ this.balance å°†è¿”å›žå½“å‰åˆçº¦å­˜å‚¨äº†å¤šå°‘ä»¥å¤ªã€‚ æ‰€ä»¥å¦‚æžœ100ä¸ªç”¨æˆ·æ¯äººå‘æˆ‘ä»¬æ”¯ä»˜1ä»¥å¤ªï¼Œ this.balance å°†æ˜¯100ä»¥å¤ªã€‚ ä½ å¯ä»¥é€šè¿‡ transfer å‘ä»»ä½•ä»¥å¤ªåŠåœ°å€ä»˜é’±ã€‚ æ¯”å¦‚ï¼Œä½ å¯ä»¥æœ‰ä¸€ä¸ªå‡½æ•°åœ¨ msg.sender è¶…é¢ä»˜æ¬¾çš„æ—¶å€™ç»™ä»–ä»¬é€€é’±ï¼š 12uint itemFee = 0.001 ether;msg.sender.transfer(msg.value - itemFee); æˆ–è€…åœ¨ä¸€ä¸ªæœ‰ä¹°å®¶å’Œå–å®¶çš„åˆçº¦ä¸­ï¼Œ ä½ å¯ä»¥æŠŠå–å®¶çš„åœ°å€å­˜å‚¨èµ·æ¥ï¼Œ å½“æœ‰äººä¹°äº†å®ƒçš„ä¸œè¥¿çš„æ—¶å€™ï¼ŒæŠŠä¹°å®¶æ”¯ä»˜çš„é’±å‘é€ç»™å®ƒ seller.transfer(msg.value)ã€‚ æœ‰å¾ˆå¤šä¾‹å­æ¥å±•ç¤ºä»€ä¹ˆè®©ä»¥å¤ªåŠç¼–ç¨‹å¦‚æ­¤ä¹‹é…· â€”â€” ä½ å¯ä»¥æ‹¥æœ‰ä¸€ä¸ªä¸è¢«ä»»ä½•äººæŽ§åˆ¶çš„åŽ»ä¸­å¿ƒåŒ–å¸‚åœºã€‚ ç¬¬3ç« : åƒµå°¸æˆ˜æ–—ç¬¬4ç« : éšæœºæ•°ä¼˜ç§€çš„æ¸¸æˆéƒ½éœ€è¦ä¸€äº›éšæœºå…ƒç´ ï¼Œé‚£ä¹ˆæˆ‘ä»¬åœ¨ Solidity é‡Œå¦‚ä½•ç”Ÿæˆéšæœºæ•°å‘¢ï¼Ÿ çœŸæ­£çš„ç­”æ¡ˆæ˜¯ä½ ä¸èƒ½ï¼Œæˆ–è€…æœ€èµ·ç ï¼Œä½ æ— æ³•å®‰å…¨åœ°åšåˆ°è¿™ä¸€ç‚¹ã€‚ ç”¨ keccak256 æ¥åˆ¶é€ éšæœºæ•°ã€‚SHA-3ç¬¬ä¸‰ä»£å®‰å…¨æ•£åˆ—ç®—æ³•(Secure Hash Algorithm 3)ï¼Œä¹‹å‰åä¸ºKeccakï¼ˆå¿µä½œ/ËˆkÉ›tÊƒÃ¦k/æˆ–/kÉ›tÊƒÉ‘Ëk/)ï¼‰ç®—æ³•. Solidity ä¸­æœ€å¥½çš„éšæœºæ•°ç”Ÿæˆå™¨æ˜¯ keccak256 å“ˆå¸Œå‡½æ•°. æˆ‘ä»¬å¯ä»¥è¿™æ ·æ¥ç”Ÿæˆä¸€äº›éšæœºæ•° 12345// ç”Ÿæˆä¸€ä¸ª0åˆ°100çš„éšæœºæ•°:uint randNonce = 0;uint random = uint(keccak256(now, msg.sender, randNonce)) % 100;randNonce++;uint random2 = uint(keccak256(now, msg.sender, randNonce)) % 100; è¿™ä¸ªæ–¹æ³•é¦–å…ˆæ‹¿åˆ° now çš„æ—¶é—´æˆ³ã€ msg.senderã€ ä»¥åŠä¸€ä¸ªè‡ªå¢žæ•° nonce ï¼ˆä¸€ä¸ªä»…ä¼šè¢«ä½¿ç”¨ä¸€æ¬¡çš„æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸ä¼šå¯¹ç›¸åŒçš„è¾“å…¥å€¼è°ƒç”¨ä¸€æ¬¡ä»¥ä¸Šå“ˆå¸Œå‡½æ•°äº†ï¼‰ã€‚ ç„¶åŽåˆ©ç”¨ keccak æŠŠè¾“å…¥çš„å€¼è½¬å˜ä¸ºä¸€ä¸ªå“ˆå¸Œå€¼, å†å°†å“ˆå¸Œå€¼è½¬æ¢ä¸º uint, ç„¶åŽåˆ©ç”¨ % 100 æ¥å–æœ€åŽä¸¤ä½, å°±ç”Ÿæˆäº†ä¸€ä¸ª0åˆ°100ä¹‹é—´éšæœºæ•°äº†ã€‚ è¿™ä¸ªæ–¹æ³•å¾ˆå®¹æ˜“è¢«ä¸è¯šå®žçš„èŠ‚ç‚¹æ”»å‡»åœ¨ä»¥å¤ªåŠä¸Š, å½“ä½ åœ¨å’Œä¸€ä¸ªåˆçº¦ä¸Šè°ƒç”¨å‡½æ•°çš„æ—¶å€™, ä½ ä¼šæŠŠå®ƒå¹¿æ’­ç»™ä¸€ä¸ªèŠ‚ç‚¹æˆ–è€…åœ¨ç½‘ç»œä¸Šçš„ transaction\\ èŠ‚ç‚¹ä»¬ã€‚ ç½‘ç»œä¸Šçš„èŠ‚ç‚¹å°†æ”¶é›†å¾ˆå¤šäº‹åŠ¡, è¯•ç€æˆä¸ºç¬¬ä¸€ä¸ªè§£å†³è®¡ç®—å¯†é›†åž‹æ•°å­¦é—®é¢˜çš„äººï¼Œä½œä¸ºâ€œå·¥ä½œè¯æ˜Žâ€ï¼Œç„¶åŽå°†â€œå·¥ä½œè¯æ˜Žâ€(Proof of Work, PoW)å’Œäº‹åŠ¡ä¸€èµ·ä½œä¸ºä¸€ä¸ª block\\ å‘å¸ƒåœ¨ç½‘ç»œä¸Šã€‚ ä¸€æ—¦ä¸€ä¸ªèŠ‚ç‚¹è§£å†³äº†ä¸€ä¸ªPoW, å…¶ä»–èŠ‚ç‚¹å°±ä¼šåœæ­¢å°è¯•è§£å†³è¿™ä¸ª PoW, å¹¶éªŒè¯å…¶ä»–èŠ‚ç‚¹çš„äº‹åŠ¡åˆ—è¡¨æ˜¯æœ‰æ•ˆçš„ï¼Œç„¶åŽæŽ¥å—è¿™ä¸ªèŠ‚ç‚¹è½¬è€Œå°è¯•è§£å†³ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ã€‚ è¿™å°±è®©æˆ‘ä»¬çš„éšæœºæ•°å‡½æ•°å˜å¾—å¯åˆ©ç”¨äº† æˆ‘ä»¬å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¡¬å¸ç¿»è½¬åˆçº¦â€”â€”æ­£é¢ä½ èµ¢åŒå€é’±ï¼Œåé¢ä½ è¾“æŽ‰æ‰€æœ‰çš„é’±ã€‚å‡å¦‚å®ƒä½¿ç”¨ä¸Šé¢çš„æ–¹æ³•æ¥å†³å®šæ˜¯æ­£é¢è¿˜æ˜¯åé¢ (random &gt;= 50 ç®—æ­£é¢, random &lt; 50 ç®—åé¢)ã€‚ å¦‚æžœæˆ‘æ­£è¿è¡Œä¸€ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘å¯ä»¥ åªå¯¹æˆ‘è‡ªå·±çš„èŠ‚ç‚¹ å‘å¸ƒä¸€ä¸ªäº‹åŠ¡ï¼Œä¸”ä¸åˆ†äº«å®ƒã€‚ æˆ‘å¯ä»¥è¿è¡Œç¡¬å¸ç¿»è½¬æ–¹æ³•æ¥å·çª¥æˆ‘çš„è¾“èµ¢ â€” å¦‚æžœæˆ‘è¾“äº†ï¼Œæˆ‘å°±ä¸æŠŠè¿™ä¸ªäº‹åŠ¡åŒ…å«è¿›æˆ‘è¦è§£å†³çš„ä¸‹ä¸€ä¸ªåŒºå—ä¸­åŽ»ã€‚æˆ‘å¯ä»¥ä¸€ç›´è¿è¡Œè¿™ä¸ªæ–¹æ³•ï¼Œç›´åˆ°æˆ‘èµ¢å¾—äº†ç¡¬å¸ç¿»è½¬å¹¶è§£å†³äº†ä¸‹ä¸€ä¸ªåŒºå—ï¼Œç„¶åŽèŽ·åˆ©ã€‚ æ‰€ä»¥æˆ‘ä»¬è¯¥å¦‚ä½•åœ¨ä»¥å¤ªåŠä¸Šå®‰å…¨åœ°ç”Ÿæˆéšæœºæ•°å‘¢å› ä¸ºåŒºå—é“¾çš„å…¨éƒ¨å†…å®¹å¯¹æ‰€æœ‰å‚ä¸Žè€…æ¥è¯´æ˜¯é€æ˜Žçš„ï¼Œ è¿™å°±è®©è¿™ä¸ªé—®é¢˜å˜å¾—å¾ˆéš¾ï¼Œå®ƒçš„è§£å†³æ–¹æ³•ä¸åœ¨æœ¬è¯¾ç¨‹è®¨è®ºèŒƒå›´ï¼Œä½ å¯ä»¥é˜…è¯» è¿™ä¸ª StackOverflow ä¸Šçš„è®¨è®º æ¥èŽ·å¾—ä¸€äº›ä¸»æ„ã€‚ ä¸€ä¸ªæ–¹æ³•æ˜¯åˆ©ç”¨ oracle\\ æ¥è®¿é—®ä»¥å¤ªåŠåŒºå—é“¾ä¹‹å¤–çš„éšæœºæ•°å‡½æ•°ã€‚ å› ä¸ºç½‘ç»œä¸Šæˆåƒä¸Šä¸‡çš„ä»¥å¤ªåŠèŠ‚ç‚¹éƒ½åœ¨ç«žäº‰è§£å†³ä¸‹ä¸€ä¸ªåŒºå—ï¼Œæˆ‘èƒ½æˆåŠŸè§£å†³ä¸‹ä¸€ä¸ªåŒºå—çš„å‡ çŽ‡éžå¸¸ä¹‹ä½Žã€‚ è¿™å°†èŠ±è´¹æˆ‘ä»¬å·¨å¤§çš„è®¡ç®—èµ„æºæ¥å¼€å‘è¿™ä¸ªèŽ·åˆ©æ–¹æ³• â€” ä½†æ˜¯å¦‚æžœå¥–åŠ±å¼‚å¸¸åœ°é«˜(æ¯”å¦‚æˆ‘å¯ä»¥åœ¨ç¡¬å¸ç¿»è½¬å‡½æ•°ä¸­èµ¢å¾— 1ä¸ªäº¿)ï¼Œ é‚£å°±å¾ˆå€¼å¾—åŽ»æ”»å‡»äº†ã€‚ æ‰€ä»¥å°½ç®¡è¿™ä¸ªæ–¹æ³•åœ¨ä»¥å¤ªåŠä¸Šä¸å®‰å…¨ï¼Œåœ¨å®žé™…ä¸­ï¼Œé™¤éžæˆ‘ä»¬çš„éšæœºå‡½æ•°æœ‰ä¸€å¤§ç¬”é’±åœ¨ä¸Šé¢ï¼Œä½ æ¸¸æˆçš„ç”¨æˆ·ä¸€èˆ¬æ˜¯æ²¡æœ‰è¶³å¤Ÿçš„èµ„æºåŽ»æ”»å‡»çš„ã€‚ ç¬¬5ç« : åƒµå°¸å¯¹æˆ˜ç¬¬6ç« : é‡æž„é€šç”¨é€»è¾‘ç¬¬7ç« : æ›´å¤šé‡æž„ç¬¬8ç« : å›žåˆ°æ”»å‡»ï¼ç¬¬9ç« : åƒµå°¸çš„è¾“èµ¢å¯¹æˆ‘ä»¬çš„åƒµå°¸æ¸¸æˆæ¥è¯´ï¼Œæˆ‘ä»¬å°†è¦è¿½è¸ªæˆ‘ä»¬çš„åƒµå°¸è¾“èµ¢äº†å¤šå°‘åœºã€‚æœ‰äº†è¿™ä¸ªæˆ‘ä»¬å¯ä»¥åœ¨æ¸¸æˆé‡Œç»´æŠ¤ä¸€ä¸ª â€œåƒµå°¸æŽ’è¡Œæ¦œâ€ã€‚ æœ‰å¤šç§æ–¹æ³•åœ¨æˆ‘ä»¬çš„DAppé‡Œé¢ä¿å­˜ä¸€ä¸ªæ•°å€¼ â€” ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„æ˜ å°„ï¼Œä½œä¸ºä¸€ä¸ªâ€œæŽ’è¡Œæ¦œâ€ç»“æž„ä½“ï¼Œæˆ–è€…ä¿å­˜åœ¨ Zombie ç»“æž„ä½“å†…ã€‚ ç¬¬10ç« : åƒµå°¸èƒœåˆ©äº† ðŸ˜„ç¬¬11ç« : åƒµå°¸å¤±è´¥ ðŸ˜žåœ¨æˆ‘ä»¬çš„æ¸¸æˆä¸­ï¼Œåƒµå°¸è¾“äº†åŽå¹¶ä¸ä¼šé™çº§ â€”â€” åªæ˜¯ç®€å•åœ°ç»™ lossCount åŠ ä¸€ï¼Œå¹¶è§¦å‘å†·å´ï¼Œç­‰å¾…ä¸€å¤©åŽæ‰èƒ½å†æ¬¡å‚æˆ˜ã€‚ å®žçŽ°è¿™ä¸ªé€»è¾‘ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª else è¯­å¥ã€‚ else è¯­å¥å’Œ JavaScript ä»¥åŠå¾ˆå¤šå…¶ä»–è¯­è¨€çš„ else è¯­å¥ä¸€æ ·ã€‚ 12345if (zombieCoins[msg.sender] &gt; 100000000) { // ä½ å¥½æœ‰é’±!!!} else { // æˆ‘ä»¬éœ€è¦æ›´å¤šçš„åƒµå°¸å¸...} Part 3ç¬¬1ç« : ä»¥å¤ªåŠä¸Šçš„ä»£å¸è®©æˆ‘ä»¬æ¥èŠèŠ ä»£å¸.(tokens) å¦‚æžœä½ å¯¹ä»¥å¤ªåŠçš„ä¸–ç•Œæœ‰ä¸€äº›äº†è§£ï¼Œä½ å¾ˆå¯èƒ½å¬è¿‡äººä»¬èŠåˆ°ä»£å¸â€”â€”å°¤å…¶æ˜¯ ERC20 ä»£å¸\\. A token\\ on Ethereum is basically just a smart contract that follows some common rules â€” namely it implements a standard set of functions that all other token contracts share, such as transferFrom(address _from, address _to, uint256 _tokenId) and balanceOf(address _owner). ä¸€ä¸ªä»£å¸å°±æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆçº¦ã€‚ ä¸€ä¸ª ä»£å¸ åœ¨ä»¥å¤ªåŠåŸºæœ¬ä¸Šå°±æ˜¯ä¸€ä¸ªéµå¾ªä¸€äº›å…±åŒè§„åˆ™çš„æ™ºèƒ½åˆçº¦â€”â€”å³å®ƒå®žçŽ°äº†æ‰€æœ‰å…¶ä»–ä»£å¸åˆçº¦å…±äº«çš„ä¸€ç»„æ ‡å‡†å‡½æ•°ï¼Œä¾‹å¦‚ transfer(address _to, uint256 _value) å’Œ balanceOf(address _owner). åœ¨æ™ºèƒ½åˆçº¦å†…éƒ¨ï¼Œé€šå¸¸æœ‰ä¸€ä¸ªæ˜ å°„ï¼Œ mapping(address =&gt; uint256) balancesï¼Œç”¨äºŽè¿½è¸ªæ¯ä¸ªåœ°å€è¿˜æœ‰å¤šå°‘ä½™é¢ã€‚ æ‰€ä»¥åŸºæœ¬ä¸Šä¸€ä¸ªä»£å¸åªæ˜¯ä¸€ä¸ªè¿½è¸ªè°æ‹¥æœ‰å¤šå°‘è¯¥ä»£å¸çš„åˆçº¦ï¼Œå’Œä¸€äº›å¯ä»¥è®©é‚£äº›ç”¨æˆ·å°†ä»–ä»¬çš„ä»£å¸è½¬ç§»åˆ°å…¶ä»–åœ°å€çš„å‡½æ•°ã€‚ å®ƒä¸ºä»€ä¹ˆé‡è¦å‘¢ï¼Ÿç”±äºŽæ‰€æœ‰ ERC20 ä»£å¸å…±äº«å…·æœ‰ç›¸åŒåç§°çš„åŒä¸€ç»„å‡½æ•°ï¼Œå®ƒä»¬éƒ½å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼è¿›è¡Œäº¤äº’ã€‚ è¿™æ„å‘³ç€å¦‚æžœä½ æž„å»ºçš„åº”ç”¨ç¨‹åºèƒ½å¤Ÿä¸Žä¸€ä¸ª ERC20 ä»£å¸è¿›è¡Œäº¤äº’ï¼Œé‚£ä¹ˆå®ƒå°±ä¹Ÿèƒ½å¤Ÿä¸Žä»»ä½• ERC20 ä»£å¸è¿›è¡Œäº¤äº’ã€‚ è¿™æ ·ä¸€æ¥ï¼Œå°†æ¥ä½ å°±å¯ä»¥è½»æ¾åœ°å°†æ›´å¤šçš„ä»£å¸æ·»åŠ åˆ°ä½ çš„åº”ç”¨ä¸­ï¼Œè€Œæ— éœ€è¿›è¡Œè‡ªå®šä¹‰ç¼–ç ã€‚ ä½ å¯ä»¥ç®€å•åœ°æ’å…¥æ–°çš„ä»£å¸åˆçº¦åœ°å€ï¼Œç„¶åŽå“—å•¦ï¼Œä½ çš„åº”ç”¨ç¨‹åºæœ‰å¦ä¸€ä¸ªå®ƒå¯ä»¥ä½¿ç”¨çš„ä»£å¸äº†ã€‚ å…¶ä¸­ä¸€ä¸ªä¾‹å­å°±æ˜¯äº¤æ˜“æ‰€ã€‚ å½“äº¤æ˜“æ‰€æ·»åŠ ä¸€ä¸ªæ–°çš„ ERC20 ä»£å¸æ—¶ï¼Œå®žé™…ä¸Šå®ƒåªéœ€è¦æ·»åŠ ä¸Žä¹‹å¯¹è¯çš„å¦ä¸€ä¸ªæ™ºèƒ½åˆçº¦ã€‚ ç”¨æˆ·å¯ä»¥è®©é‚£ä¸ªåˆçº¦å°†ä»£å¸å‘é€åˆ°äº¤æ˜“æ‰€çš„é’±åŒ…åœ°å€ï¼Œç„¶åŽäº¤æ˜“æ‰€å¯ä»¥è®©åˆçº¦åœ¨ç”¨æˆ·è¦æ±‚å–æ¬¾æ—¶å°†ä»£å¸å‘é€å›žç»™ä»–ä»¬ã€‚ äº¤æ˜“æ‰€åªéœ€è¦å®žçŽ°è¿™ç§è½¬ç§»é€»è¾‘ä¸€æ¬¡ï¼Œç„¶åŽå½“å®ƒæƒ³è¦æ·»åŠ ä¸€ä¸ªæ–°çš„ ERC20 ä»£å¸æ—¶ï¼Œåªéœ€å°†æ–°çš„åˆçº¦åœ°å€æ·»åŠ åˆ°å®ƒçš„æ•°æ®åº“å³å¯ã€‚ å…¶ä»–ä»£å¸æ ‡å‡†å¯¹äºŽåƒè´§å¸ä¸€æ ·çš„ä»£å¸æ¥è¯´ï¼ŒERC20 ä»£å¸éžå¸¸é…·ã€‚ ä½†æ˜¯è¦åœ¨æˆ‘ä»¬åƒµå°¸æ¸¸æˆä¸­ä»£è¡¨åƒµå°¸å°±å¹¶ä¸æ˜¯ç‰¹åˆ«æœ‰ç”¨ã€‚ é¦–å…ˆï¼Œåƒµå°¸ä¸åƒè´§å¸å¯ä»¥åˆ†å‰² â€”â€” æˆ‘å¯ä»¥å‘ç»™ä½  0.237 ä»¥å¤ªï¼Œä½†æ˜¯è½¬ç§»ç»™ä½  0.237 çš„åƒµå°¸å¬èµ·æ¥å°±æœ‰äº›æžç¬‘ã€‚ å…¶æ¬¡ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰åƒµå°¸éƒ½æ˜¯å¹³ç­‰çš„ã€‚ ä½ çš„2çº§åƒµå°¸â€Steveâ€œå®Œå…¨ä¸èƒ½ç­‰åŒäºŽæˆ‘732çº§çš„åƒµå°¸â€H4XF13LD MORRIS ðŸ’¯ðŸ’¯ðŸ˜ŽðŸ’¯ðŸ’¯â€œã€‚ï¼ˆä½ å·®å¾—è¿œå‘¢ï¼ŒSteveï¼‰ã€‚ æœ‰å¦ä¸€ä¸ªä»£å¸æ ‡å‡†æ›´é€‚åˆå¦‚ CryptoZombies è¿™æ ·çš„åŠ å¯†æ”¶è—å“â€”â€”å®ƒä»¬è¢«ç§°ä¸ºERC721 ä»£å¸.\\ ERC721 ä»£å¸\\æ˜¯ä¸èƒ½äº’æ¢çš„ï¼Œå› ä¸ºæ¯ä¸ªä»£å¸éƒ½è¢«è®¤ä¸ºæ˜¯å”¯ä¸€ä¸”ä¸å¯åˆ†å‰²çš„ã€‚ ä½ åªèƒ½ä»¥æ•´ä¸ªå•ä½äº¤æ˜“å®ƒä»¬ï¼Œå¹¶ä¸”æ¯ä¸ªå•ä½éƒ½æœ‰å”¯ä¸€çš„ IDã€‚ è¿™äº›ç‰¹æ€§æ­£å¥½è®©æˆ‘ä»¬çš„åƒµå°¸å¯ä»¥ç”¨æ¥äº¤æ˜“ã€‚ è¯·æ³¨æ„ï¼Œä½¿ç”¨åƒ ERC721 è¿™æ ·çš„æ ‡å‡†çš„ä¼˜åŠ¿å°±æ˜¯ï¼Œæˆ‘ä»¬ä¸å¿…åœ¨æˆ‘ä»¬çš„åˆçº¦ä¸­å®žçŽ°æ‹å–æˆ–æ‰˜ç®¡é€»è¾‘ï¼Œè¿™å†³å®šäº†çŽ©å®¶èƒ½å¤Ÿå¦‚ä½•äº¤æ˜“ï¼å‡ºå”®æˆ‘ä»¬çš„åƒµå°¸ã€‚ å¦‚æžœæˆ‘ä»¬ç¬¦åˆè§„èŒƒï¼Œå…¶ä»–äººå¯ä»¥ä¸ºåŠ å¯†å¯äº¤æ˜“çš„ ERC721 èµ„äº§æ­å»ºä¸€ä¸ªäº¤æ˜“æ‰€å¹³å°ï¼Œæˆ‘ä»¬çš„ ERC721 åƒµå°¸å°†å¯ä»¥åœ¨è¯¥å¹³å°ä¸Šä½¿ç”¨ã€‚ æ‰€ä»¥ä½¿ç”¨ä»£å¸æ ‡å‡†ç›¸è¾ƒäºŽä½¿ç”¨ä½ è‡ªå·±çš„äº¤æ˜“é€»è¾‘æœ‰æ˜Žæ˜¾çš„å¥½å¤„ã€‚ ç¬¬2ç« : ERC721 æ ‡å‡†, å¤šé‡ç»§æ‰¿è®©æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹ ERC721 æ ‡å‡†ï¼š 12345678910contract ERC721 { event Transfer(address indexed _from, address indexed _to, uint256 _tokenId); event Approval(address indexed _owner, address indexed _approved, uint256 _tokenId); function balanceOf(address _owner) public view returns (uint256 _balance); function ownerOf(uint256 _tokenId) public view returns (address _owner); function transfer(address _to, uint256 _tokenId) public; function approve(address _to, uint256 _tokenId) public; function takeOwnership(uint256 _tokenId) public;} è¿™æ˜¯æˆ‘ä»¬éœ€è¦å®žçŽ°çš„æ–¹æ³•åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†åœ¨æŽ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­é€ä¸ªå­¦ä¹ ã€‚ æ³¨æ„ï¼š ERC721ç›®å‰æ˜¯ä¸€ä¸ª è‰ç¨¿**ï¼Œè¿˜æ²¡æœ‰æ­£å¼å•†å®šçš„å®žçŽ°ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ OpenZeppelin åº“ä¸­çš„å½“å‰ç‰ˆæœ¬ï¼Œä½†åœ¨æœªæ¥æ­£å¼å‘å¸ƒä¹‹å‰å®ƒå¯èƒ½ä¼šæœ‰æ›´æ”¹ã€‚ æ‰€ä»¥æŠŠè¿™ ä¸€ä¸ª å¯èƒ½çš„å®žçŽ°å½“ä½œè€ƒè™‘ï¼Œä½†ä¸è¦æŠŠå®ƒä½œä¸º ERC721 ä»£å¸çš„å®˜æ–¹æ ‡å‡†ã€‚ å®žçŽ°ä¸€ä¸ªä»£å¸åˆçº¦åœ¨å®žçŽ°ä¸€ä¸ªä»£å¸åˆçº¦çš„æ—¶å€™ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšçš„æ˜¯å°†æŽ¥å£å¤åˆ¶åˆ°å®ƒè‡ªå·±çš„ Solidity æ–‡ä»¶å¹¶å¯¼å…¥å®ƒï¼Œimport &quot;./erc721.sol&quot;;ã€‚ æŽ¥ç€ï¼Œè®©æˆ‘ä»¬çš„åˆçº¦ç»§æ‰¿å®ƒï¼Œç„¶åŽæˆ‘ä»¬ç”¨ä¸€ä¸ªå‡½æ•°å®šä¹‰æ¥é‡å†™æ¯ä¸ªæ–¹æ³•ã€‚ å¹¸è¿çš„æ˜¯åœ¨Solidityï¼Œä½ çš„åˆçº¦å¯ä»¥ç»§æ‰¿è‡ªå¤šä¸ªåˆçº¦ï¼Œå‚è€ƒå¦‚ä¸‹ï¼š 123contract SatoshiNakamoto is NickSzabo, HalFinney { // å•§å•§å•§ï¼Œå®‡å®™çš„å¥¥ç§˜æ³„éœ²äº†} æ­£å¦‚ä½ æ‰€è§ï¼Œå½“ä½¿ç”¨å¤šé‡ç»§æ‰¿çš„æ—¶å€™ï¼Œä½ åªéœ€è¦ç”¨é€—å· , æ¥éš”å¼€å‡ ä¸ªä½ æƒ³è¦ç»§æ‰¿çš„åˆçº¦ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„åˆçº¦ç»§æ‰¿è‡ª NickSzabo å’Œ HalFinneyã€‚ ç¬¬3ç« : balanceOf å’Œ ownerOfbalanceOf1function balanceOf(address _owner) public view returns (uint256 _balance); è¿™ä¸ªå‡½æ•°åªéœ€è¦ä¸€ä¸ªä¼ å…¥ address å‚æ•°ï¼Œç„¶åŽè¿”å›žè¿™ä¸ª address æ‹¥æœ‰å¤šå°‘ä»£å¸ã€‚ åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„â€œä»£å¸â€æ˜¯åƒµå°¸ã€‚ä½ è¿˜è®°å¾—åœ¨æˆ‘ä»¬ DApp çš„å“ªé‡Œå­˜å‚¨äº†ä¸€ä¸ªä¸»äººæ‹¥æœ‰å¤šå°‘åªåƒµå°¸å—ï¼Ÿ ownerOf1function ownerOf(uint256 _tokenId) public view returns (address _owner); è¿™ä¸ªå‡½æ•°éœ€è¦ä¼ å…¥ä¸€ä¸ªä»£å¸ ID ä½œä¸ºå‚æ•° (æˆ‘ä»¬çš„æƒ…å†µå°±æ˜¯ä¸€ä¸ªåƒµå°¸ ID)ï¼Œç„¶åŽè¿”å›žè¯¥ä»£å¸æ‹¥æœ‰è€…çš„ addressã€‚ åŒæ ·çš„ï¼Œå› ä¸ºåœ¨æˆ‘ä»¬çš„ DApp é‡Œå·²ç»æœ‰ä¸€ä¸ª mapping (æ˜ å°„) å­˜å‚¨äº†è¿™ä¸ªä¿¡æ¯ï¼Œæ‰€ä»¥å¯¹æˆ‘ä»¬æ¥è¯´è¿™ä¸ªå®žçŽ°éžå¸¸ç›´æŽ¥æ¸…æ™°ã€‚æˆ‘ä»¬å¯ä»¥åªç”¨ä¸€è¡Œ return è¯­å¥æ¥å®žçŽ°è¿™ä¸ªå‡½æ•°ã€‚ æ³¨æ„ï¼šè¦è®°å¾—ï¼Œ uint256 ç­‰åŒäºŽuintã€‚æˆ‘ä»¬ä»Žè¯¾ç¨‹çš„å¼€å§‹ä¸€ç›´åœ¨ä»£ç ä¸­ä½¿ç”¨ uintï¼Œä½†ä»ŽçŽ°åœ¨å¼€å§‹æˆ‘ä»¬å°†åœ¨è¿™é‡Œç”¨ uint256ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æŽ¥ä»Žè§„èŒƒä¸­å¤åˆ¶ç²˜è´´ã€‚ ç¬¬4ç« : é‡æž„å¦‚æžœä½ å°è¯•ç¼–è¯‘è¿™æ®µä»£ç ï¼Œç¼–è¯‘å™¨ä¼šç»™ä½ ä¸€ä¸ªé”™è¯¯è¯´ä½ ä¸èƒ½æœ‰ç›¸åŒåç§°çš„ä¿®é¥°ç¬¦å’Œå‡½æ•°ã€‚ æ‰€ä»¥æˆ‘ä»¬åº”è¯¥æŠŠåœ¨ ZombieOwnership é‡Œçš„å‡½æ•°åç§°æ”¹æˆåˆ«çš„å—ï¼Ÿ ä¸ï¼Œæˆ‘ä»¬ä¸èƒ½é‚£æ ·åšï¼ï¼ï¼è¦è®°å¾—ï¼Œæˆ‘ä»¬æ­£åœ¨ç”¨ ERC721 ä»£å¸æ ‡å‡†ï¼Œæ„å‘³ç€å…¶ä»–åˆçº¦å°†æœŸæœ›æˆ‘ä»¬çš„åˆçº¦ä»¥è¿™äº›ç¡®åˆ‡çš„åç§°æ¥å®šä¹‰å‡½æ•°ã€‚è¿™å°±æ˜¯è¿™äº›æ ‡å‡†å®žç”¨çš„åŽŸå› â€”â€”å¦‚æžœå¦ä¸€ä¸ªåˆçº¦çŸ¥é“æˆ‘ä»¬çš„åˆçº¦ç¬¦åˆ ERC721 æ ‡å‡†ï¼Œå®ƒå¯ä»¥ç›´æŽ¥ä¸Žæˆ‘ä»¬äº¤äº’ï¼Œè€Œæ— éœ€äº†è§£ä»»ä½•å…³äºŽæˆ‘ä»¬å†…éƒ¨å¦‚ä½•å®žçŽ°çš„ç»†èŠ‚ã€‚ æ‰€ä»¥ï¼Œé‚£æ„å‘³ç€æˆ‘ä»¬å°†å¿…é¡»é‡æž„æˆ‘ä»¬ç¬¬4è¯¾ä¸­çš„ä»£ç ï¼Œå°† modifier çš„åç§°æ¢æˆåˆ«çš„ã€‚ ç¬¬5ç« : ERC721: Transfer Logicæ³¨æ„ ERC721 è§„èŒƒæœ‰ä¸¤ç§ä¸åŒçš„æ–¹æ³•æ¥è½¬ç§»ä»£å¸ï¼š 1234function transfer(address _to, uint256 _tokenId) public;function approve(address _to, uint256 _tokenId) public;function takeOwnership(uint256 _tokenId) public; ç¬¬ä¸€ç§æ–¹æ³•æ˜¯ä»£å¸çš„æ‹¥æœ‰è€…è°ƒç”¨transfer æ–¹æ³•ï¼Œä¼ å…¥ä»–æƒ³è½¬ç§»åˆ°çš„ address å’Œä»–æƒ³è½¬ç§»çš„ä»£å¸çš„ _tokenIdã€‚ ç¬¬äºŒç§æ–¹æ³•æ˜¯ä»£å¸æ‹¥æœ‰è€…é¦–å…ˆè°ƒç”¨ approveï¼Œç„¶åŽä¼ å…¥ä¸Žä»¥ä¸Šç›¸åŒçš„å‚æ•°ã€‚æŽ¥ç€ï¼Œè¯¥åˆçº¦ä¼šå­˜å‚¨è°è¢«å…è®¸æå–ä»£å¸ï¼Œé€šå¸¸å­˜å‚¨åˆ°ä¸€ä¸ª mapping (uint256 =&gt; address) é‡Œã€‚ç„¶åŽï¼Œå½“æœ‰äººè°ƒç”¨ takeOwnership æ—¶ï¼Œåˆçº¦ä¼šæ£€æŸ¥ msg.sender æ˜¯å¦å¾—åˆ°æ‹¥æœ‰è€…çš„æ‰¹å‡†æ¥æå–ä»£å¸ï¼Œå¦‚æžœæ˜¯ï¼Œåˆ™å°†ä»£å¸è½¬ç§»ç»™ä»–ã€‚ ä½ æ³¨æ„åˆ°äº†å—ï¼Œtransfer å’Œ takeOwnership éƒ½å°†åŒ…å«ç›¸åŒçš„è½¬ç§»é€»è¾‘ï¼Œåªæ˜¯ä»¥ç›¸åçš„é¡ºåºã€‚ ï¼ˆä¸€ç§æƒ…å†µæ˜¯ä»£å¸çš„å‘é€è€…è°ƒç”¨å‡½æ•°ï¼›å¦ä¸€ç§æƒ…å†µæ˜¯ä»£å¸çš„æŽ¥æ”¶è€…è°ƒç”¨å®ƒï¼‰ã€‚ ç¬¬7ç« : ERC721: Approvalè®°ä½ï¼Œä½¿ç”¨ approve æˆ–è€… takeOwnership çš„æ—¶å€™ï¼Œè½¬ç§»æœ‰2ä¸ªæ­¥éª¤ï¼š ä½ ï¼Œä½œä¸ºæ‰€æœ‰è€…ï¼Œç”¨æ–°ä¸»äººçš„ address å’Œä½ å¸Œæœ›ä»–èŽ·å–çš„ _tokenId æ¥è°ƒç”¨ approve æ–°ä¸»äººç”¨ _tokenId æ¥è°ƒç”¨ takeOwnershipï¼Œåˆçº¦ä¼šæ£€æŸ¥ç¡®ä¿ä»–èŽ·å¾—äº†æ‰¹å‡†ï¼Œç„¶åŽæŠŠä»£å¸è½¬ç§»ç»™ä»–ã€‚ å› ä¸ºè¿™å‘ç”Ÿåœ¨2ä¸ªå‡½æ•°çš„è°ƒç”¨ä¸­ï¼Œæ‰€ä»¥åœ¨å‡½æ•°è°ƒç”¨ä¹‹é—´ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®ç»“æž„æ¥å­˜å‚¨ä»€ä¹ˆäººè¢«æ‰¹å‡†èŽ·å–ä»€ä¹ˆã€‚ ç¬¬8ç« : ERC721: takeOwnershipæœ€åŽä¸€ä¸ªå‡½æ•° takeOwnershipï¼Œ åº”è¯¥åªæ˜¯ç®€å•åœ°æ£€æŸ¥ä»¥ç¡®ä¿ msg.sender å·²ç»è¢«æ‰¹å‡†æ¥æå–è¿™ä¸ªä»£å¸æˆ–è€…åƒµå°¸ã€‚è‹¥ç¡®è®¤ï¼Œå°±è°ƒç”¨ _transferï¼› ç¬¬9ç« : é¢„é˜²æº¢å‡ºä¸è¿‡è¦è®°ä½é‚£åªæ˜¯æœ€ç®€å•çš„å®žçŽ°ã€‚è¿˜æœ‰å¾ˆå¤šçš„ç‰¹æ€§æˆ‘ä»¬ä¹Ÿè®¸æƒ³åŠ å…¥åˆ°æˆ‘ä»¬çš„å®žçŽ°ä¸­æ¥ï¼Œæ¯”å¦‚ä¸€äº›é¢å¤–çš„æ£€æŸ¥ï¼Œæ¥ç¡®ä¿ç”¨æˆ·ä¸ä¼šä¸å°å¿ƒæŠŠä»–ä»¬çš„åƒµå°¸è½¬ç§»ç»™0 åœ°å€ï¼ˆè¿™è¢«ç§°ä½œ â€œçƒ§å¸â€, åŸºæœ¬ä¸Šå°±æ˜¯æŠŠä»£å¸è½¬ç§»åˆ°ä¸€ä¸ªè°ä¹Ÿæ²¡æœ‰ç§é’¥çš„åœ°å€ï¼Œè®©è¿™ä¸ªä»£å¸æ°¸è¿œä¹Ÿæ— æ³•æ¢å¤ï¼‰ã€‚ æˆ–è€…åœ¨ DApp ä¸­åŠ å…¥ä¸€äº›åŸºæœ¬çš„æ‹å–é€»è¾‘ã€‚ï¼ˆä½ èƒ½æƒ³å‡ºä¸€äº›å®žçŽ°çš„æ–¹æ³•ä¹ˆï¼Ÿï¼‰ ä½†æ˜¯ä¸ºäº†è®©æˆ‘ä»¬çš„è¯¾ç¨‹ä¸è‡³äºŽç¦»é¢˜å¤ªè¿œï¼Œæ‰€ä»¥æˆ‘ä»¬åªä¸“æ³¨äºŽä¸€äº›åŸºç¡€å®žçŽ°ã€‚å¦‚æžœä½ æƒ³å­¦ä¹ ä¸€äº›æ›´æ·±å±‚æ¬¡çš„å®žçŽ°ï¼Œå¯ä»¥åœ¨è¿™ä¸ªæ•™ç¨‹ç»“æŸåŽï¼ŒåŽ»çœ‹çœ‹ OpenZeppelin çš„ ERC721 åˆçº¦ã€‚ åˆçº¦å®‰å…¨å¢žå¼º: æº¢å‡ºå’Œä¸‹æº¢æˆ‘ä»¬å°†æ¥å­¦ä¹ ä½ åœ¨ç¼–å†™æ™ºèƒ½åˆçº¦çš„æ—¶å€™éœ€è¦æ³¨æ„çš„ä¸€ä¸ªä¸»è¦çš„å®‰å…¨ç‰¹æ€§ï¼šé˜²æ­¢æº¢å‡ºå’Œä¸‹æº¢ã€‚ ä»€ä¹ˆæ˜¯ æº¢å‡º (overflow\\)? å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª uint8, åªèƒ½å­˜å‚¨8 bitæ•°æ®ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬èƒ½å­˜å‚¨çš„æœ€å¤§æ•°å­—å°±æ˜¯äºŒè¿›åˆ¶ 11111111 (æˆ–è€…è¯´åè¿›åˆ¶çš„ 2^8 - 1 = 255). æ¥çœ‹çœ‹ä¸‹é¢çš„ä»£ç ã€‚æœ€åŽ number å°†ä¼šæ˜¯ä»€ä¹ˆå€¼ï¼Ÿ 12uint8 number = 255;number++; åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯¼è‡´äº†æº¢å‡º â€” è™½ç„¶æˆ‘ä»¬åŠ äº†1ï¼Œ ä½†æ˜¯ number å‡ºä¹Žæ„æ–™åœ°ç­‰äºŽ 0äº†ã€‚ (å¦‚æžœä½ ç»™äºŒè¿›åˆ¶ 11111111 åŠ 1, å®ƒå°†è¢«é‡ç½®ä¸º 00000000ï¼Œå°±åƒé’Ÿè¡¨ä»Ž 23:59 èµ°å‘ 00:00)ã€‚ ä¸‹æº¢(underflow)ä¹Ÿç±»ä¼¼ï¼Œå¦‚æžœä½ ä»Žä¸€ä¸ªç­‰äºŽ 0 çš„ uint8 å‡åŽ» 1, å®ƒå°†å˜æˆ 255 (å› ä¸º uint æ˜¯æ— ç¬¦å·çš„ï¼Œå…¶ä¸èƒ½ç­‰äºŽè´Ÿæ•°)ã€‚ è™½ç„¶æˆ‘ä»¬åœ¨è¿™é‡Œä¸ä½¿ç”¨ uint8ï¼Œè€Œä¸”æ¯æ¬¡ç»™ä¸€ä¸ª uint256 åŠ  1 ä¹Ÿä¸å¤ªå¯èƒ½æº¢å‡º (2^256 çœŸçš„æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æ•°äº†)ï¼Œåœ¨æˆ‘ä»¬çš„åˆçº¦ä¸­æ·»åŠ ä¸€äº›ä¿æŠ¤æœºåˆ¶ä¾ç„¶æ˜¯éžå¸¸æœ‰å¿…è¦çš„ï¼Œä»¥é˜²æˆ‘ä»¬çš„ DApp ä»¥åŽå‡ºçŽ°ä»€ä¹ˆå¼‚å¸¸æƒ…å†µã€‚ ä½¿ç”¨ SafeMathä¸ºäº†é˜²æ­¢è¿™äº›æƒ…å†µï¼ŒOpenZeppelin å»ºç«‹äº†ä¸€ä¸ªå«åš SafeMath çš„ åº“(library\\)ï¼Œé»˜è®¤æƒ…å†µä¸‹å¯ä»¥é˜²æ­¢è¿™äº›é—®é¢˜ã€‚ ä¸è¿‡åœ¨æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰â€¦â€¦ ä»€ä¹ˆå«åšåº“? ä¸€ä¸ªåº“ æ˜¯ Solidity ä¸­ä¸€ç§ç‰¹æ®Šçš„åˆçº¦ã€‚å…¶ä¸­ä¸€ä¸ªæœ‰ç”¨çš„åŠŸèƒ½æ˜¯ç»™åŽŸå§‹æ•°æ®ç±»åž‹å¢žåŠ ä¸€äº›æ–¹æ³•ã€‚ æ¯”å¦‚ï¼Œä½¿ç”¨ SafeMath åº“çš„æ—¶å€™ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ using SafeMath for uint256 è¿™æ ·çš„è¯­æ³•ã€‚ SafeMath åº“æœ‰å››ä¸ªæ–¹æ³• â€” addï¼Œ subï¼Œ mulï¼Œ ä»¥åŠ divã€‚çŽ°åœ¨æˆ‘ä»¬å¯ä»¥è¿™æ ·æ¥è®© uint256 è°ƒç”¨è¿™äº›æ–¹æ³•ï¼š 12345using SafeMath for uint256;uint256 a = 5;uint256 b = a.add(3); // 5 + 3 = 8uint256 c = a.mul(2); // 5 * 2 = 10","link":"/2020/11/04/solidity-advanced/"},{"title":"ã€ŒåŒºå—é“¾ã€ï¼šSolidity-basic","text":"Solidityçš„å®˜æ–¹æ•™ç¨‹ç¬”è®°ï¼šbasicã€‚ Part 1ç¬¬2ç« : åˆçº¦ä»Žæœ€åŸºæœ¬çš„å¼€å§‹å…¥æ‰‹: Solidity çš„ä»£ç éƒ½åŒ…è£¹åœ¨åˆçº¦é‡Œé¢. ä¸€ä»½åˆçº¦å°±æ˜¯ä»¥å¤ªåº”å¸åº”ç”¨çš„åŸºæœ¬æ¨¡å—ï¼Œ æ‰€æœ‰çš„å˜é‡å’Œå‡½æ•°éƒ½å±žäºŽä¸€ä»½åˆçº¦, å®ƒæ˜¯ä½ æ‰€æœ‰åº”ç”¨çš„èµ·ç‚¹. ä¸€ä»½åä¸º HelloWorld çš„ç©ºåˆçº¦å¦‚ä¸‹: 123contract HelloWorld {} ç‰ˆæœ¬æŒ‡ä»¤æ‰€æœ‰çš„ Solidity æºç éƒ½å¿…é¡»å† ä»¥ â€œversion pragmaâ€ â€” æ ‡æ˜Ž Solidity ç¼–è¯‘å™¨çš„ç‰ˆæœ¬. ä»¥é¿å…å°†æ¥æ–°çš„ç¼–è¯‘å™¨å¯èƒ½ç ´åä½ çš„ä»£ç ã€‚ ä¾‹å¦‚: pragma solidity ^0.4.19; (å½“å‰ Solidity çš„æœ€æ–°ç‰ˆæœ¬æ˜¯ 0.4.19). è¦æœ‰åˆ†å·ï¼ 1234pragma solidity ^0.4.19;contract HelloWorld {} ç¬¬3ç« : çŠ¶æ€å˜é‡å’Œæ•´æ•°çŠ¶æ€å˜é‡æ˜¯è¢«æ°¸ä¹…åœ°ä¿å­˜åœ¨åˆçº¦ä¸­ã€‚ä¹Ÿå°±æ˜¯è¯´å®ƒä»¬è¢«å†™å…¥ä»¥å¤ªå¸åŒºå—é“¾ä¸­. æƒ³è±¡æˆå†™å…¥ä¸€ä¸ªæ•°æ®åº“ã€‚ æ— ç¬¦å·æ•´æ•°: uintuint æ— ç¬¦å·æ•°æ®ç±»åž‹ï¼Œ æŒ‡å…¶å€¼ä¸èƒ½æ˜¯è´Ÿæ•°ï¼Œå¯¹äºŽæœ‰ç¬¦å·çš„æ•´æ•°å­˜åœ¨åä¸º int çš„æ•°æ®ç±»åž‹ã€‚ æ³¨: Solidityä¸­ï¼Œ uint å®žé™…ä¸Šæ˜¯ uint256ä»£åè¯ï¼Œ ä¸€ä¸ª256ä½çš„æ— ç¬¦å·æ•´æ•°ã€‚ä½ ä¹Ÿå¯ä»¥å®šä¹‰ä½æ•°å°‘çš„uints â€” uint8ï¼Œ uint16ï¼Œ uint32ï¼Œ ç­‰â€¦â€¦ ä½†ä¸€èˆ¬æ¥è®²ä½ æ„¿æ„ä½¿ç”¨ç®€å•çš„ uintï¼Œ é™¤éžåœ¨æŸäº›ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œè¿™æˆ‘ä»¬åŽé¢ä¼šè®²ã€‚ ç¬¬4ç« : æ•°å­¦è¿ç®—åœ¨ Solidity ä¸­ï¼Œæ•°å­¦è¿ç®—å¾ˆç›´è§‚æ˜Žäº†ï¼Œä¸Žå…¶å®ƒç¨‹åºè®¾è®¡è¯­è¨€ç›¸åŒ: åŠ æ³•: x + y å‡æ³•: x - y, ä¹˜æ³•: x * y é™¤æ³•: x / y å–æ¨¡ / æ±‚ä½™: x % y (ä¾‹å¦‚, 13 % 5 ä½™ 3, å› ä¸º13é™¤ä»¥5ï¼Œä½™3) Solidity è¿˜æ”¯æŒ ä¹˜æ–¹æ“ä½œ\\ (å¦‚ï¼šx çš„ yæ¬¡æ–¹ï¼‰ å¦‚ï¼š 5 ** 2 = 25 1uint x = 5 ** 2; // equal to 5^2 = 25 ç¬¬5ç« : ç»“æž„ä½“1234struct Person { uint age; string name;} ç¬¬6ç« : æ•°ç»„å¦‚æžœä½ æƒ³å»ºç«‹ä¸€ä¸ªé›†åˆï¼Œå¯ä»¥ç”¨ æ•°ç»„è¿™æ ·çš„æ•°æ®ç±»åž‹. Solidity æ”¯æŒä¸¤ç§æ•°ç»„: é™æ€ æ•°ç»„å’ŒåŠ¨æ€ æ•°ç»„: 123456// å›ºå®šé•¿åº¦ä¸º2çš„é™æ€æ•°ç»„:uint[2] fixedArray;// å›ºå®šé•¿åº¦ä¸º5çš„stringç±»åž‹çš„é™æ€æ•°ç»„:string[5] stringArray;// åŠ¨æ€æ•°ç»„ï¼Œé•¿åº¦ä¸å›ºå®šï¼Œå¯ä»¥åŠ¨æ€æ·»åŠ å…ƒç´ :uint[] dynamicArray; è®°ä½ï¼šçŠ¶æ€å˜é‡è¢«æ°¸ä¹…ä¿å­˜åœ¨åŒºå—é“¾ä¸­ã€‚æ‰€ä»¥åœ¨ä½ çš„åˆçº¦ä¸­åˆ›å»ºåŠ¨æ€æ•°ç»„æ¥ä¿å­˜æˆç»“æž„çš„æ•°æ®æ˜¯éžå¸¸æœ‰æ„ä¹‰çš„ã€‚ å…¬å…±æ•°ç»„ä½ å¯ä»¥å®šä¹‰ public æ•°ç»„, Solidity ä¼šè‡ªåŠ¨åˆ›å»º getter æ–¹æ³•. è¯­æ³•å¦‚ä¸‹: 1Person[] public people; å…¶å®ƒçš„åˆçº¦å¯ä»¥ä»Žè¿™ä¸ªæ•°ç»„è¯»å–æ•°æ®ï¼ˆä½†ä¸èƒ½å†™å…¥æ•°æ®ï¼‰ï¼Œæ‰€ä»¥è¿™åœ¨åˆçº¦ä¸­æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ä¿å­˜å…¬å…±æ•°æ®çš„æ¨¡å¼ã€‚ ç¬¬7ç« : å®šä¹‰å‡½æ•°åœ¨ Solidity ä¸­å‡½æ•°å®šä¹‰çš„å¥æ³•å¦‚ä¸‹: 123function eatHamburgers(string _name, uint _amount) {} æ³¨ï¼šä¹ æƒ¯ä¸Šå‡½æ•°é‡Œçš„å˜é‡éƒ½æ˜¯ä»¥(*_)å¼€å¤´ (ä½†ä¸æ˜¯ç¡¬æ€§è§„å®š) ä»¥åŒºåˆ«å…¨å±€å˜é‡ã€‚(æ•´ä¸ªæ•™ç¨‹éƒ½ä¼šæ²¿ç”¨è¿™ä¸ªä¹ æƒ¯ã€‚) ç¬¬8ç« : ä½¿ç”¨ç»“æž„ä½“å’Œæ•°ç»„çŽ°åœ¨æˆ‘ä»¬å­¦ä¹ åˆ›å»ºæ–°çš„ Person ç»“æž„ï¼Œç„¶åŽæŠŠå®ƒåŠ å…¥åˆ°åä¸º people çš„æ•°ç»„ä¸­. 12345// åˆ›å»ºä¸€ä¸ªæ–°çš„Person:Person satoshi = Person(172, &quot;Satoshi&quot;);// å°†æ–°åˆ›å»ºçš„satoshiæ·»åŠ è¿›peopleæ•°ç»„:people.push(satoshi); ä½ ä¹Ÿå¯ä»¥ä¸¤æ­¥å¹¶ä¸€æ­¥ï¼Œç”¨ä¸€è¡Œä»£ç æ›´ç®€æ´: 1people.push(Person(16, &quot;Vitalik&quot;)); æ³¨ï¼šarray.push() åœ¨æ•°ç»„çš„ å°¾éƒ¨ åŠ å…¥æ–°å…ƒç´  ï¼Œæ‰€ä»¥å…ƒç´ åœ¨æ•°ç»„ä¸­çš„é¡ºåºå°±æ˜¯æˆ‘ä»¬æ·»åŠ çš„é¡ºåºï¼Œ å¦‚: ç¬¬9ç« : ç§æœ‰ / å…¬å…±å‡½æ•°Solidity å®šä¹‰çš„å‡½æ•°çš„å±žæ€§é»˜è®¤ä¸ºå…¬å…±ã€‚ è¿™å°±æ„å‘³ç€ä»»ä½•ä¸€æ–¹ (æˆ–å…¶å®ƒåˆçº¦) éƒ½å¯ä»¥è°ƒç”¨ä½ åˆçº¦é‡Œçš„å‡½æ•°ã€‚ æ˜¾ç„¶ï¼Œä¸æ˜¯ä»€ä¹ˆæ—¶å€™éƒ½éœ€è¦è¿™æ ·ï¼Œè€Œä¸”è¿™æ ·çš„åˆçº¦æ˜“äºŽå—åˆ°æ”»å‡»ã€‚ æ‰€ä»¥å°†è‡ªå·±çš„å‡½æ•°å®šä¹‰ä¸ºç§æœ‰æ˜¯ä¸€ä¸ªå¥½çš„ç¼–ç¨‹ä¹ æƒ¯ï¼Œåªæœ‰å½“ä½ éœ€è¦å¤–éƒ¨ä¸–ç•Œè°ƒç”¨å®ƒæ—¶æ‰å°†å®ƒè®¾ç½®ä¸ºå…¬å…±ã€‚ å¦‚ä½•å®šä¹‰ä¸€ä¸ªç§æœ‰çš„å‡½æ•°å‘¢ï¼Ÿ 12345uint[] numbers;function _addToArray(uint _number) private { numbers.push(_number);} è¿™æ„å‘³ç€åªæœ‰æˆ‘ä»¬åˆçº¦ä¸­çš„å…¶å®ƒå‡½æ•°æ‰èƒ½å¤Ÿè°ƒç”¨è¿™ä¸ªå‡½æ•°ï¼Œç»™ numbers æ•°ç»„æ·»åŠ æ–°æˆå‘˜ã€‚ å¯ä»¥çœ‹åˆ°ï¼Œåœ¨å‡½æ•°åå­—åŽé¢ä½¿ç”¨å…³é”®å­— private å³å¯ã€‚å’Œå‡½æ•°çš„å‚æ•°ç±»ä¼¼ï¼Œç§æœ‰å‡½æ•°çš„åå­—ç”¨(_)èµ·å§‹ã€‚ ç¬¬10ç« : å‡½æ•°çš„æ›´å¤šå±žæ€§æœ¬ç« ä¸­æˆ‘ä»¬å°†å­¦ä¹ å‡½æ•°çš„è¿”å›žå€¼å’Œä¿®é¥°ç¬¦ã€‚ è¿”å›žå€¼è¦æƒ³å‡½æ•°è¿”å›žä¸€ä¸ªæ•°å€¼ï¼ŒæŒ‰å¦‚ä¸‹å®šä¹‰ï¼š 12345string greeting = &quot;What's up dog&quot;;function sayHello() public returns (string) { return greeting;} Solidity é‡Œï¼Œå‡½æ•°çš„å®šä¹‰é‡Œå¯åŒ…å«è¿”å›žå€¼çš„æ•°æ®ç±»åž‹(å¦‚æœ¬ä¾‹ä¸­ string)ã€‚ å‡½æ•°çš„ä¿®é¥°ç¬¦ä¸Šé¢çš„å‡½æ•°å®žé™…ä¸Šæ²¡æœ‰æ”¹å˜ Solidity é‡Œçš„çŠ¶æ€ï¼Œå³ï¼Œå®ƒæ²¡æœ‰æ”¹å˜ä»»ä½•å€¼æˆ–è€…å†™ä»»ä½•ä¸œè¥¿ã€‚ è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬å¯ä»¥æŠŠå‡½æ•°å®šä¹‰ä¸º view, æ„å‘³ç€å®ƒåªèƒ½è¯»å–æ•°æ®ä¸èƒ½æ›´æ”¹æ•°æ®: 1function sayHello() public view returns (string) { Solidity è¿˜æ”¯æŒ pure å‡½æ•°, è¡¨æ˜Žè¿™ä¸ªå‡½æ•°ç”šè‡³éƒ½ä¸è®¿é—®åº”ç”¨é‡Œçš„æ•°æ®ï¼Œä¾‹å¦‚ï¼š 123function _multiply(uint a, uint b) private pure returns (uint) { return a * b;} è¿™ä¸ªå‡½æ•°ç”šè‡³éƒ½ä¸è¯»å–åº”ç”¨é‡Œçš„çŠ¶æ€ â€” å®ƒçš„è¿”å›žå€¼å®Œå…¨å–å†³äºŽå®ƒçš„è¾“å…¥å‚æ•°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æŠŠå‡½æ•°å®šä¹‰ä¸º pure. æ³¨ï¼šå¯èƒ½å¾ˆéš¾è®°ä½ä½•æ—¶æŠŠå‡½æ•°æ ‡è®°ä¸º pure/viewã€‚ å¹¸è¿çš„æ˜¯ï¼Œ Solidity ç¼–è¾‘å™¨ä¼šç»™å‡ºæç¤ºï¼Œæé†’ä½ ä½¿ç”¨è¿™äº›ä¿®é¥°ç¬¦ã€‚ ç¬¬11ç« : Keccak256 å’Œ ç±»åž‹è½¬æ¢å¦‚ä½•è®© _generateRandomDna å‡½æ•°è¿”å›žä¸€ä¸ªå…¨(åŠ) éšæœºçš„ uint? Ethereum å†…éƒ¨æœ‰ä¸€ä¸ªæ•£åˆ—å‡½æ•°keccak256ï¼Œå®ƒç”¨äº†SHA3ç‰ˆæœ¬ã€‚ä¸€ä¸ªæ•£åˆ—å‡½æ•°åŸºæœ¬ä¸Šå°±æ˜¯æŠŠä¸€ä¸ªå­—ç¬¦ä¸²è½¬æ¢ä¸ºä¸€ä¸ª256ä½çš„16è¿›åˆ¶æ•°å­—ã€‚å­—ç¬¦ä¸²çš„ä¸€ä¸ªå¾®å°å˜åŒ–ä¼šå¼•èµ·æ•£åˆ—æ•°æ®æžå¤§å˜åŒ–ã€‚ ä¾‹å­: 1234//6e91ec6b618bb462a4a6ee5aa2cb0e9cf30f7a052bb467b0ba58b8748c00d2e5keccak256(&quot;aaaab&quot;);//b1f078126895a1424524de5321b339ab00408010b7cf0e6ed451514981e58aa9keccak256(&quot;aaaac&quot;); æ³¨: åœ¨åŒºå—é“¾ä¸­å®‰å…¨åœ°äº§ç”Ÿä¸€ä¸ªéšæœºæ•°æ˜¯ä¸€ä¸ªå¾ˆéš¾çš„é—®é¢˜ï¼Œ æœ¬ä¾‹çš„æ–¹æ³•ä¸å®‰å…¨ï¼Œä½†æ˜¯åœ¨æˆ‘ä»¬çš„Zombie DNAç®—æ³•é‡Œä¸æ˜¯é‚£ä¹ˆé‡è¦ï¼Œå·²ç»å¾ˆå¥½åœ°æ»¡è¶³æˆ‘ä»¬çš„éœ€è¦äº†ã€‚* ç±»åž‹è½¬æ¢æœ‰æ—¶ä½ éœ€è¦å˜æ¢æ•°æ®ç±»åž‹ã€‚ä¾‹å¦‚: 123456uint8 a = 5;uint b = 6;// å°†ä¼šæŠ›å‡ºé”™è¯¯ï¼Œå› ä¸º a * b è¿”å›ž uint, è€Œä¸æ˜¯ uint8:uint8 c = a * b;// æˆ‘ä»¬éœ€è¦å°† b è½¬æ¢ä¸º uint8:uint8 c = a * uint8(b); ä¸Šé¢, a * b è¿”å›žç±»åž‹æ˜¯ uint, ä½†æ˜¯å½“æˆ‘ä»¬å°è¯•ç”¨ uint8 ç±»åž‹æŽ¥æ”¶æ—¶, å°±ä¼šé€ æˆæ½œåœ¨çš„é”™è¯¯ã€‚å¦‚æžœæŠŠå®ƒçš„æ•°æ®ç±»åž‹è½¬æ¢ä¸º uint8, å°±å¯ä»¥äº†ï¼Œç¼–è¯‘å™¨ä¹Ÿä¸ä¼šå‡ºé”™ã€‚ ç¬¬13ç« : äº‹ä»¶äº‹ä»¶ æ˜¯åˆçº¦å’ŒåŒºå—é“¾é€šè®¯çš„ä¸€ç§æœºåˆ¶ã€‚ä½ çš„å‰ç«¯åº”ç”¨â€œç›‘å¬â€æŸäº›äº‹ä»¶ï¼Œå¹¶åšå‡ºååº”ã€‚ ä¾‹å­: 123456789// è¿™é‡Œå»ºç«‹äº‹ä»¶event IntegersAdded(uint x, uint y, uint result);function add(uint _x, uint _y) public { uint result = _x + _y; //è§¦å‘äº‹ä»¶ï¼Œé€šçŸ¥app IntegersAdded(_x, _y, result); return result;} ä½ çš„ app å‰ç«¯å¯ä»¥ç›‘å¬è¿™ä¸ªäº‹ä»¶ã€‚JavaScript å®žçŽ°å¦‚ä¸‹: 123YourContract.IntegersAdded(function(error, result) { // å¹²äº›äº‹} array.push() è¿”å›žæ•°ç»„çš„é•¿åº¦ç±»åž‹æ˜¯uint - å› ä¸ºæ•°ç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç´¢å¼•æ˜¯ 0ï¼Œ array.push() - 1 å°†æ˜¯æˆ‘ä»¬åŠ å…¥çš„åƒµå°¸çš„ç´¢å¼•ã€‚ zombies.push() - 1 å°±æ˜¯ idï¼Œæ•°æ®ç±»åž‹æ˜¯ uintã€‚ ç¬¬14ç« : Web3.jsä»¥å¤ªåŠæœ‰ä¸€ä¸ª JavaScript åº“ï¼Œåä¸ºWeb3.jsã€‚ åœ¨åŽé¢çš„è¯¾ç¨‹é‡Œï¼Œæˆ‘ä»¬ä¼šè¿›ä¸€æ­¥åœ°æ•™ä½ å¦‚ä½•å®‰è£…ä¸€ä¸ªåˆçº¦ï¼Œå¦‚ä½•è®¾ç½®Web3.jsã€‚ ä½†æ˜¯çŽ°åœ¨æˆ‘ä»¬é€šè¿‡ä¸€æ®µä»£ç æ¥äº†è§£ Web3.js æ˜¯å¦‚ä½•å’Œæˆ‘ä»¬å‘å¸ƒçš„åˆçº¦äº¤äº’çš„å§ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// ä¸‹é¢æ˜¯è°ƒç”¨åˆçº¦çš„æ–¹å¼:var abi = /* abiæ˜¯ç”±ç¼–è¯‘å™¨ç”Ÿæˆçš„ */var ZombieFactoryContract = web3.eth.contract(abi)var contractAddress = /* å‘å¸ƒä¹‹åŽåœ¨ä»¥å¤ªåŠä¸Šç”Ÿæˆçš„åˆçº¦åœ°å€ */var ZombieFactory = ZombieFactoryContract.at(contractAddress)// `ZombieFactory` èƒ½è®¿é—®å…¬å…±çš„å‡½æ•°ä»¥åŠäº‹ä»¶// æŸä¸ªç›‘å¬æ–‡æœ¬è¾“å…¥çš„ç›‘å¬å™¨:$(&quot;#ourButton&quot;).click(function(e) { var name = $(&quot;#nameInput&quot;).val() //è°ƒç”¨åˆçº¦çš„ `createRandomZombie` å‡½æ•°: ZombieFactory.createRandomZombie(name)})// ç›‘å¬ `NewZombie` äº‹ä»¶, å¹¶ä¸”æ›´æ–°UIvar event = ZombieFactory.NewZombie(function(error, result) { if (error) return generateZombie(result.zombieId, result.name, result.dna)})// èŽ·å– Zombie çš„ dna, æ›´æ–°å›¾åƒfunction generateZombie(id, name, dna) { let dnaStr = String(dna) // å¦‚æžœdnaå°‘äºŽ16ä½,åœ¨å®ƒå‰é¢ç”¨0è¡¥ä¸Š while (dnaStr.length &lt; 16) dnaStr = &quot;0&quot; + dnaStr let zombieDetails = { // å‰ä¸¤ä½æ•°æž„æˆå¤´éƒ¨.æˆ‘ä»¬å¯èƒ½æœ‰7ç§å¤´éƒ¨, æ‰€ä»¥ % 7 // å¾—åˆ°çš„æ•°åœ¨0-6,å†åŠ ä¸Š1,æ•°çš„èŒƒå›´å˜æˆ1-7 // é€šè¿‡è¿™æ ·è®¡ç®—ï¼š headChoice: dnaStr.substring(0, 2) % 7 + 1ï¼Œ // æˆ‘ä»¬å¾—åˆ°çš„å›¾ç‰‡åç§°ä»Žhead1.png åˆ° head7.png // æŽ¥ä¸‹æ¥çš„ä¸¤ä½æ•°æž„æˆçœ¼ç›, çœ¼ç›å˜åŒ–å°±å¯¹11å–æ¨¡: eyeChoice: dnaStr.substring(2, 4) % 11 + 1, // å†æŽ¥ä¸‹æ¥çš„ä¸¤ä½æ•°æž„æˆè¡£æœï¼Œè¡£æœå˜åŒ–å°±å¯¹6å–æ¨¡: shirtChoice: dnaStr.substring(4, 6) % 6 + 1, //æœ€åŽ6ä½æŽ§åˆ¶é¢œè‰². ç”¨cssé€‰æ‹©å™¨: hue-rotateæ¥æ›´æ–° // 360åº¦: skinColorChoice: parseInt(dnaStr.substring(6, 8) / 100 * 360), eyeColorChoice: parseInt(dnaStr.substring(8, 10) / 100 * 360), clothesColorChoice: parseInt(dnaStr.substring(10, 12) / 100 * 360), zombieName: name, zombieDescription: &quot;A Level 1 CryptoZombie&quot;, } return zombieDetails} æˆ‘ä»¬çš„ JavaScript æ‰€åšçš„å°±æ˜¯èŽ·å–ç”±zombieDetails äº§ç”Ÿçš„æ•°æ®, å¹¶ä¸”åˆ©ç”¨æµè§ˆå™¨é‡Œçš„ JavaScript ç¥žå¥‡åŠŸèƒ½ (æˆ‘ä»¬ç”¨ Vue.js)ï¼Œç½®æ¢å‡ºå›¾åƒä»¥åŠä½¿ç”¨CSSè¿‡æ»¤å™¨ã€‚åœ¨åŽé¢çš„è¯¾ç¨‹ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°å…¨éƒ¨çš„ä»£ç ã€‚ Part 2ç¬¬2ç« : æ˜ å°„ï¼ˆMappingï¼‰å’Œåœ°å€ï¼ˆAddressï¼‰å¦‚æ­¤ä¸€æ¥ï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥2ä¸ªæ–°çš„æ•°æ®ç±»åž‹ï¼šmappingï¼ˆæ˜ å°„ï¼‰ å’Œ addressï¼ˆåœ°å€ï¼‰ã€‚ Addresses ï¼ˆåœ°å€ï¼‰ä»¥å¤ªåŠåŒºå—é“¾ç”± account ** (è´¦æˆ·)ç»„æˆï¼Œä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆé“¶è¡Œè´¦æˆ·ã€‚ä¸€ä¸ªå¸æˆ·çš„ä½™é¢æ˜¯ **ä»¥å¤ª ï¼ˆåœ¨ä»¥å¤ªåŠåŒºå—é“¾ä¸Šä½¿ç”¨çš„å¸ç§ï¼‰ï¼Œä½ å¯ä»¥å’Œå…¶ä»–å¸æˆ·ä¹‹é—´æ”¯ä»˜å’ŒæŽ¥å—ä»¥å¤ªå¸ï¼Œå°±åƒä½ çš„é“¶è¡Œå¸æˆ·å¯ä»¥ç”µæ±‡èµ„é‡‘åˆ°å…¶ä»–é“¶è¡Œå¸æˆ·ä¸€æ ·ã€‚ æ¯ä¸ªå¸æˆ·éƒ½æœ‰ä¸€ä¸ªâ€œåœ°å€â€ï¼Œä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆé“¶è¡Œè´¦å·ã€‚è¿™æ˜¯è´¦æˆ·å”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼Œå®ƒçœ‹èµ·æ¥é•¿è¿™æ ·ï¼š 10x0cE446255506E92DF41614C46F1d6df9Cc969183 çŽ°åœ¨ä½ åªéœ€è¦äº†è§£åœ°å€å±žäºŽç‰¹å®šç”¨æˆ·ï¼ˆæˆ–æ™ºèƒ½åˆçº¦ï¼‰çš„ã€‚ Mappingï¼ˆæ˜ å°„ï¼‰åœ¨ç¬¬1è¯¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº† ç»“æž„ä½“ ** å’Œ **æ•°ç»„ ã€‚ æ˜ å°„ æ˜¯å¦ä¸€ç§åœ¨ Solidity ä¸­å­˜å‚¨æœ‰ç»„ç»‡æ•°æ®çš„æ–¹æ³•ã€‚ æ˜ å°„æ˜¯è¿™æ ·å®šä¹‰çš„ï¼š 1234//å¯¹äºŽé‡‘èžåº”ç”¨ç¨‹åºï¼Œå°†ç”¨æˆ·çš„ä½™é¢ä¿å­˜åœ¨ä¸€ä¸ª uintç±»åž‹çš„å˜é‡ä¸­ï¼šmapping (address =&gt; uint) public accountBalance;//æˆ–è€…å¯ä»¥ç”¨æ¥é€šè¿‡userId å­˜å‚¨/æŸ¥æ‰¾çš„ç”¨æˆ·åmapping (uint =&gt; string) userIdToName; æ˜ å°„æœ¬è´¨ä¸Šæ˜¯å­˜å‚¨å’ŒæŸ¥æ‰¾æ•°æ®æ‰€ç”¨çš„é”®-å€¼å¯¹ã€‚ ç¬¬3ç« : Msg.sendermsg.senderåœ¨ Solidity ä¸­ï¼Œæœ‰ä¸€äº›å…¨å±€å˜é‡å¯ä»¥è¢«æ‰€æœ‰å‡½æ•°è°ƒç”¨ã€‚ å…¶ä¸­ä¸€ä¸ªå°±æ˜¯ msg.senderï¼Œå®ƒæŒ‡çš„æ˜¯å½“å‰è°ƒç”¨è€…ï¼ˆæˆ–æ™ºèƒ½åˆçº¦ï¼‰çš„ addressã€‚ æ³¨æ„ï¼šåœ¨ Solidity ä¸­ï¼ŒåŠŸèƒ½æ‰§è¡Œå§‹ç»ˆéœ€è¦ä»Žå¤–éƒ¨è°ƒç”¨è€…å¼€å§‹ã€‚ ä¸€ä¸ªåˆçº¦åªä¼šåœ¨åŒºå—é“¾ä¸Šä»€ä¹ˆä¹Ÿä¸åšï¼Œé™¤éžæœ‰äººè°ƒç”¨å…¶ä¸­çš„å‡½æ•°ã€‚æ‰€ä»¥ msg.senderæ€»æ˜¯å­˜åœ¨çš„ã€‚ 12345678910111213mapping (address =&gt; uint) favoriteNumber;function setMyNumber(uint _myNumber) public { // æ›´æ–°æˆ‘ä»¬çš„ `favoriteNumber` æ˜ å°„æ¥å°† `_myNumber`å­˜å‚¨åœ¨ `msg.sender`åä¸‹ favoriteNumber[msg.sender] = _myNumber; // å­˜å‚¨æ•°æ®è‡³æ˜ å°„çš„æ–¹æ³•å’Œå°†æ•°æ®å­˜å‚¨åœ¨æ•°ç»„ç›¸ä¼¼}function whatIsMyNumber() public view returns (uint) { // æ‹¿åˆ°å­˜å‚¨åœ¨è°ƒç”¨è€…åœ°å€åä¸‹çš„å€¼ // è‹¥è°ƒç”¨è€…è¿˜æ²¡è°ƒç”¨ setMyNumberï¼Œ åˆ™å€¼ä¸º `0` return favoriteNumber[msg.sender];} åœ¨è¿™ä¸ªå°å°çš„ä¾‹å­ä¸­ï¼Œä»»ä½•äººéƒ½å¯ä»¥è°ƒç”¨ setMyNumber åœ¨æˆ‘ä»¬çš„åˆçº¦ä¸­å­˜ä¸‹ä¸€ä¸ª uint å¹¶ä¸”ä¸Žä»–ä»¬çš„åœ°å€ç›¸ç»‘å®šã€‚ ç„¶åŽï¼Œä»–ä»¬è°ƒç”¨ whatIsMyNumber å°±ä¼šè¿”å›žä»–ä»¬å­˜å‚¨çš„ uintã€‚ ä½¿ç”¨ msg.sender å¾ˆå®‰å…¨ï¼Œå› ä¸ºå®ƒå…·æœ‰ä»¥å¤ªåŠåŒºå—é“¾çš„å®‰å…¨ä¿éšœ â€”â€” é™¤éžçªƒå–ä¸Žä»¥å¤ªåŠåœ°å€ç›¸å…³è”çš„ç§é’¥ï¼Œå¦åˆ™æ˜¯æ²¡æœ‰åŠžæ³•ä¿®æ”¹å…¶ä»–äººçš„æ•°æ®çš„ã€‚ è·Ÿåœ¨ JavaScript ä¸­ä¸€æ ·ï¼Œ åœ¨ Solidity ä¸­ä½ ä¹Ÿå¯ä»¥ç”¨ ++ ä½¿ uint é€’å¢žã€‚ 123uint number = 0;number++;// `number` çŽ°åœ¨æ˜¯ `1`äº† ç¬¬4ç« : Require requireä½¿å¾—å‡½æ•°åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå½“ä¸æ»¡è¶³æŸäº›æ¡ä»¶æ—¶æŠ›å‡ºé”™è¯¯ï¼Œå¹¶åœæ­¢æ‰§è¡Œï¼š 12345678function sayHiToVitalik(string _name) public returns (string) { // æ¯”è¾ƒ _name æ˜¯å¦ç­‰äºŽ &quot;Vitalik&quot;. å¦‚æžœä¸æˆç«‹ï¼ŒæŠ›å‡ºå¼‚å¸¸å¹¶ç»ˆæ­¢ç¨‹åº // (æ•²é»‘æ¿: Solidity å¹¶ä¸æ”¯æŒåŽŸç”Ÿçš„å­—ç¬¦ä¸²æ¯”è¾ƒ, æˆ‘ä»¬åªèƒ½é€šè¿‡æ¯”è¾ƒ // ä¸¤å­—ç¬¦ä¸²çš„ keccak256 å“ˆå¸Œå€¼æ¥è¿›è¡Œåˆ¤æ–­) require(keccak256(_name) == keccak256(&quot;Vitalik&quot;)); // å¦‚æžœè¿”å›ž true, è¿è¡Œå¦‚ä¸‹è¯­å¥ return &quot;Hi!&quot;;} å¦‚æžœä½ è¿™æ ·è°ƒç”¨å‡½æ•° sayHiToVitalikï¼ˆâ€œVitalikâ€ï¼‰ ,å®ƒä¼šè¿”å›žâ€œHiï¼â€ã€‚è€Œå¦‚æžœè°ƒç”¨çš„æ—¶å€™ä½¿ç”¨äº†å…¶ä»–å‚æ•°ï¼Œå®ƒåˆ™ä¼šæŠ›å‡ºé”™è¯¯å¹¶åœæ­¢æ‰§è¡Œã€‚ å› æ­¤ï¼Œåœ¨è°ƒç”¨ä¸€ä¸ªå‡½æ•°ä¹‹å‰ï¼Œç”¨ require éªŒè¯å‰ç½®æ¡ä»¶æ˜¯éžå¸¸æœ‰å¿…è¦çš„ã€‚ ç¬¬5ç« : ç»§æ‰¿ï¼ˆInheritanceï¼‰ å½“ä»£ç è¿‡äºŽå†—é•¿çš„æ—¶å€™ï¼Œæœ€å¥½å°†ä»£ç å’Œé€»è¾‘åˆ†æ‹†åˆ°å¤šä¸ªä¸åŒçš„åˆçº¦ä¸­ï¼Œä»¥ä¾¿äºŽç®¡ç†ã€‚ æœ‰ä¸ªè®© Solidity çš„ä»£ç æ˜“äºŽç®¡ç†çš„åŠŸèƒ½ï¼Œå°±æ˜¯åˆçº¦ inheritance (ç»§æ‰¿)ï¼š 1234567891011contract Doge { function catchphrase() public returns (string) { return &quot;So Wow CryptoDoge&quot;; }}contract BabyDoge is Doge { function anotherCatchphrase() public returns (string) { return &quot;Such Moon BabyDoge&quot;; }} ç”±äºŽ BabyDoge æ˜¯ä»Ž Doge é‚£é‡Œ inherits ï¼ˆç»§æ‰¿)è¿‡æ¥çš„ã€‚ è¿™æ„å‘³ç€å½“ä½ ç¼–è¯‘å’Œéƒ¨ç½²äº† BabyDogeï¼Œå®ƒå°†å¯ä»¥è®¿é—® catchphrase() å’Œ anotherCatchphrase()å’Œå…¶ä»–æˆ‘ä»¬åœ¨ Doge ä¸­å®šä¹‰çš„å…¶ä»–å…¬å…±å‡½æ•°ã€‚ è¿™å¯ä»¥ç”¨äºŽé€»è¾‘ç»§æ‰¿ï¼ˆæ¯”å¦‚è¡¨è¾¾å­ç±»çš„æ—¶å€™ï¼ŒCat æ˜¯ä¸€ç§ Animalï¼‰ã€‚ ä½†ä¹Ÿå¯ä»¥ç®€å•åœ°å°†ç±»ä¼¼çš„é€»è¾‘ç»„åˆåˆ°ä¸åŒçš„åˆçº¦ä¸­ä»¥ç»„ç»‡ä»£ç ã€‚ ç¬¬6ç« : å¼•å…¥ï¼ˆImportï¼‰åœ¨ Solidity ä¸­ï¼Œå½“ä½ æœ‰å¤šä¸ªæ–‡ä»¶å¹¶ä¸”æƒ³æŠŠä¸€ä¸ªæ–‡ä»¶å¯¼å…¥å¦ä¸€ä¸ªæ–‡ä»¶æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ import è¯­å¥ï¼š 12345import &quot;./someothercontract.sol&quot;;contract newContract is SomeOtherContract {} è¿™æ ·å½“æˆ‘ä»¬åœ¨åˆçº¦ï¼ˆcontractï¼‰ç›®å½•ä¸‹æœ‰ä¸€ä¸ªåä¸º someothercontract.sol çš„æ–‡ä»¶ï¼ˆ ./ å°±æ˜¯åŒä¸€ç›®å½•çš„æ„æ€ï¼‰ï¼Œå®ƒå°±ä¼šè¢«ç¼–è¯‘å™¨å¯¼å…¥ã€‚ ç¬¬7ç« : Storageä¸ŽMemoryåœ¨ Solidity ä¸­ï¼Œæœ‰ä¸¤ä¸ªåœ°æ–¹å¯ä»¥å­˜å‚¨å˜é‡ â€”â€” storage æˆ– memoryã€‚ Storage å˜é‡æ˜¯æŒ‡æ°¸ä¹…å­˜å‚¨åœ¨åŒºå—é“¾ä¸­çš„å˜é‡ã€‚ Memory å˜é‡åˆ™æ˜¯ä¸´æ—¶çš„ï¼Œå½“å¤–éƒ¨å‡½æ•°å¯¹æŸåˆçº¦è°ƒç”¨å®Œæˆæ—¶ï¼Œå†…å­˜åž‹å˜é‡å³è¢«ç§»é™¤ã€‚ ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆå­˜å‚¨åœ¨ä½ ç”µè„‘çš„ç¡¬ç›˜æˆ–æ˜¯RAMä¸­æ•°æ®çš„å…³ç³»ã€‚ å¤§å¤šæ•°æ—¶å€™ä½ éƒ½ç”¨ä¸åˆ°è¿™äº›å…³é”®å­—ï¼Œé»˜è®¤æƒ…å†µä¸‹ Solidity ä¼šè‡ªåŠ¨å¤„ç†å®ƒä»¬ã€‚ çŠ¶æ€å˜é‡ï¼ˆåœ¨å‡½æ•°ä¹‹å¤–å£°æ˜Žçš„å˜é‡ï¼‰é»˜è®¤ä¸ºâ€œå­˜å‚¨â€å½¢å¼ï¼Œå¹¶æ°¸ä¹…å†™å…¥åŒºå—é“¾ï¼›è€Œåœ¨å‡½æ•°å†…éƒ¨å£°æ˜Žçš„å˜é‡æ˜¯â€œå†…å­˜â€åž‹çš„ï¼Œå®ƒä»¬å‡½æ•°è°ƒç”¨ç»“æŸåŽæ¶ˆå¤±ã€‚ ç„¶è€Œä¹Ÿæœ‰ä¸€äº›æƒ…å†µä¸‹ï¼Œä½ éœ€è¦æ‰‹åŠ¨å£°æ˜Žå­˜å‚¨ç±»åž‹ï¼Œä¸»è¦ç”¨äºŽå¤„ç†å‡½æ•°å†…çš„ ç»“æž„ä½“ å’Œ æ•°ç»„ æ—¶ï¼š 1234567891011121314151617181920212223242526272829303132contract SandwichFactory { struct Sandwich { string name; string status; } Sandwich[] sandwiches; function eatSandwich(uint _index) public { // Sandwich mySandwich = sandwiches[_index]; // ^ çœ‹ä¸ŠåŽ»å¾ˆç›´æŽ¥ï¼Œä¸è¿‡ Solidity å°†ä¼šç»™å‡ºè­¦å‘Š // å‘Šè¯‰ä½ åº”è¯¥æ˜Žç¡®åœ¨è¿™é‡Œå®šä¹‰ `storage` æˆ–è€… `memory`ã€‚ // æ‰€ä»¥ä½ åº”è¯¥æ˜Žç¡®å®šä¹‰ `storage`:æŠŠä»–å½“æŒ‡é’ˆ Sandwich storage mySandwich = sandwiches[_index]; // ...è¿™æ · `mySandwich` æ˜¯æŒ‡å‘ `sandwiches[_index]`çš„æŒ‡é’ˆ // åœ¨å­˜å‚¨é‡Œï¼Œå¦å¤–... mySandwich.status = &quot;Eaten!&quot;; // ...è¿™å°†æ°¸ä¹…æŠŠ `sandwiches[_index]` å˜ä¸ºåŒºå—é“¾ä¸Šçš„å­˜å‚¨ // å¦‚æžœä½ åªæƒ³è¦ä¸€ä¸ªå‰¯æœ¬ï¼Œå¯ä»¥ä½¿ç”¨`memory`:åœ¨å†…å­˜çš„ä¸´æ—¶å‰¯æœ¬ Sandwich memory anotherSandwich = sandwiches[_index + 1]; // ...è¿™æ · `anotherSandwich` å°±ä»…ä»…æ˜¯ä¸€ä¸ªå†…å­˜é‡Œçš„å‰¯æœ¬äº† // å¦å¤– anotherSandwich.status = &quot;Eaten!&quot;; // ...å°†ä»…ä»…ä¿®æ”¹ä¸´æ—¶å˜é‡ï¼Œå¯¹ `sandwiches[_index + 1]` æ²¡æœ‰ä»»ä½•å½±å“ // ä¸è¿‡ä½ å¯ä»¥è¿™æ ·åš: sandwiches[_index + 1] = anotherSandwich; // ...å¦‚æžœä½ æƒ³æŠŠå‰¯æœ¬çš„æ”¹åŠ¨ä¿å­˜å›žåŒºå—é“¾å­˜å‚¨ }} å½“ä½ ä¸å¾—ä¸ä½¿ç”¨åˆ°è¿™äº›å…³é”®å­—çš„æ—¶å€™ï¼ŒSolidity ç¼–è¯‘å™¨ä¼šå‘è­¦ç¤ºæé†’ä½ çš„ã€‚ ç¬¬9ç« : æ›´å¤šå…³äºŽå‡½æ•°å¯è§æ€§é”™è¯¯åœ¨äºŽï¼Œæˆ‘ä»¬å°è¯•ä»Ž ZombieFeeding ä¸­è°ƒç”¨ _createZombie å‡½æ•°ï¼Œä½† _createZombie å´æ˜¯ ZombieFactory çš„ private ï¼ˆç§æœ‰ï¼‰å‡½æ•°ã€‚è¿™æ„å‘³ç€ä»»ä½•ç»§æ‰¿è‡ª ZombieFactory çš„å­åˆçº¦éƒ½ä¸èƒ½è®¿é—®å®ƒã€‚ internal å’Œ externalé™¤ public å’Œ private å±žæ€§ä¹‹å¤–ï¼ŒSolidity è¿˜ä½¿ç”¨äº†å¦å¤–ä¸¤ä¸ªæè¿°å‡½æ•°å¯è§æ€§çš„ä¿®é¥°è¯ï¼šinternalï¼ˆå†…éƒ¨ï¼‰ å’Œ externalï¼ˆå¤–éƒ¨ï¼‰ã€‚ internal å’Œ private ç±»ä¼¼ï¼Œä¸è¿‡ï¼Œ å¦‚æžœæŸä¸ªåˆçº¦ç»§æ‰¿è‡ªå…¶çˆ¶åˆçº¦ï¼Œè¿™ä¸ªåˆçº¦å³å¯ä»¥è®¿é—®çˆ¶åˆçº¦ä¸­å®šä¹‰çš„â€œå†…éƒ¨â€å‡½æ•°ã€‚è€Œprivateåªå…è®¸åˆçº¦å†…éƒ¨å‡½æ•°è®¿é—®ã€‚ external ä¸Žpublic ç±»ä¼¼ï¼Œåªä¸è¿‡è¿™äº›å‡½æ•°åªèƒ½åœ¨åˆçº¦ä¹‹å¤–è°ƒç”¨ï¼Œå®ƒä»¬ä¸èƒ½è¢«åˆçº¦å†…çš„å…¶ä»–å‡½æ•°è°ƒç”¨ã€‚ç¨åŽæˆ‘ä»¬å°†è®¨è®ºä»€ä¹ˆæ—¶å€™ä½¿ç”¨ external å’Œ publicã€‚è€Œpublicå…è®¸åˆçº¦å†…å’Œåˆçº¦å¤–çš„å‡½æ•°è°ƒç”¨ã€‚ å£°æ˜Žå‡½æ•° internal æˆ– external ç±»åž‹çš„è¯­æ³•ï¼Œä¸Žå£°æ˜Ž private å’Œ publicç±» åž‹ç›¸åŒï¼š 1234567891011121314151617contract Sandwich { uint private sandwichesEaten = 0; function eat() internal { sandwichesEaten++; }}contract BLT is Sandwich { uint private baconSandwichesEaten = 0; function eatWithBacon() public returns (string) { baconSandwichesEaten++; // å› ä¸ºeat() æ˜¯internal çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬èƒ½åœ¨è¿™é‡Œè°ƒç”¨ eat(); }} ä¸Žå…¶ä»–åˆçº¦çš„äº¤äº’å¦‚æžœæˆ‘ä»¬çš„åˆçº¦éœ€è¦å’ŒåŒºå—é“¾ä¸Šçš„å…¶ä»–çš„åˆçº¦ä¼šè¯ï¼Œåˆ™éœ€å…ˆå®šä¹‰ä¸€ä¸ª interface (æŽ¥å£)ã€‚ å…ˆä¸¾ä¸€ä¸ªç®€å•çš„æ —å­ã€‚ å‡è®¾åœ¨åŒºå—é“¾ä¸Šæœ‰è¿™ä¹ˆä¸€ä¸ªåˆçº¦ï¼š 1234567891011contract LuckyNumber { mapping(address =&gt; uint) numbers; function setNum(uint _num) public { numbers[msg.sender] = _num; } function getNum(address _myAddress) public view returns (uint) { return numbers[_myAddress]; }} è¿™æ˜¯ä¸ªå¾ˆç®€å•çš„åˆçº¦ï¼Œæ‚¨å¯ä»¥ç”¨å®ƒå­˜å‚¨è‡ªå·±çš„å¹¸è¿å·ç ï¼Œå¹¶å°†å…¶ä¸Žæ‚¨çš„ä»¥å¤ªåŠåœ°å€å…³è”ã€‚ è¿™æ ·å…¶ä»–äººå°±å¯ä»¥é€šè¿‡æ‚¨çš„åœ°å€æŸ¥æ‰¾æ‚¨çš„å¹¸è¿å·ç äº†ã€‚ çŽ°åœ¨å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤–éƒ¨åˆçº¦ï¼Œä½¿ç”¨ getNum å‡½æ•°å¯è¯»å–å…¶ä¸­çš„æ•°æ®ã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ LuckyNumber åˆçº¦çš„ interface ï¼š 123contract NumberInterface { function getNum(address _myAddress) public view returns (uint);} è¯·æ³¨æ„ï¼Œè¿™ä¸ªè¿‡ç¨‹è™½ç„¶çœ‹èµ·æ¥åƒåœ¨å®šä¹‰ä¸€ä¸ªåˆçº¦ï¼Œä½†å…¶å®žå†…é‡Œä¸åŒï¼š é¦–å…ˆï¼Œæˆ‘ä»¬åªå£°æ˜Žäº†è¦ä¸Žä¹‹äº¤äº’çš„å‡½æ•° ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º getNum ï¼‰ï¼Œåœ¨å…¶ä¸­æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨åˆ°ä»»ä½•å…¶ä»–çš„å‡½æ•°æˆ–çŠ¶æ€å˜é‡ã€‚ å…¶æ¬¡ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰ä½¿ç”¨å¤§æ‹¬å·ï¼ˆ{ å’Œ }ï¼‰å®šä¹‰å‡½æ•°ä½“ï¼Œæˆ‘ä»¬å•å•ç”¨åˆ†å·ï¼ˆ;ï¼‰ç»“æŸäº†å‡½æ•°å£°æ˜Žã€‚è¿™ä½¿å®ƒçœ‹èµ·æ¥åƒä¸€ä¸ªåˆçº¦æ¡†æž¶ã€‚ ç¼–è¯‘å™¨å°±æ˜¯é è¿™äº›ç‰¹å¾è®¤å‡ºå®ƒæ˜¯ä¸€ä¸ªæŽ¥å£çš„ã€‚ åœ¨æˆ‘ä»¬çš„ app ä»£ç ä¸­ä½¿ç”¨è¿™ä¸ªæŽ¥å£ï¼Œåˆçº¦å°±çŸ¥é“å…¶ä»–åˆçº¦çš„å‡½æ•°æ˜¯æ€Žæ ·çš„ï¼Œåº”è¯¥å¦‚ä½•è°ƒç”¨ï¼Œä»¥åŠå¯æœŸå¾…ä»€ä¹ˆç±»åž‹çš„è¿”å›žå€¼ã€‚ æˆ‘ä»¬å·²ç»ä¸ºä½ æŸ¥çœ‹è¿‡äº† CryptoKitties çš„æºä»£ç ï¼Œå¹¶ä¸”æ‰¾åˆ°äº†ä¸€ä¸ªåä¸º getKittyçš„å‡½æ•°ï¼Œå®ƒè¿”å›žæ‰€æœ‰çš„åŠ å¯†çŒ«çš„æ•°æ®ï¼ŒåŒ…æ‹¬å®ƒçš„â€œåŸºå› â€ï¼ˆæˆ‘ä»¬çš„åƒµå°¸æ¸¸æˆè¦ç”¨å®ƒç”Ÿæˆæ–°çš„åƒµå°¸ï¼‰ã€‚ è¯¥å‡½æ•°å¦‚ä¸‹æ‰€ç¤ºï¼š 1234567891011121314151617181920212223242526function getKitty(uint256 _id) external view returns ( bool isGestating, bool isReady, uint256 cooldownIndex, uint256 nextActionAt, uint256 siringWithId, uint256 birthTime, uint256 matronId, uint256 sireId, uint256 generation, uint256 genes) { Kitty storage kit = kitties[_id]; // if this variable is 0 then it's not gestating isGestating = (kit.siringWithId != 0); isReady = (kit.cooldownEndBlock &lt;= block.number); cooldownIndex = uint256(kit.cooldownIndex); nextActionAt = uint256(kit.cooldownEndBlock); siringWithId = uint256(kit.siringWithId); birthTime = uint256(kit.birthTime); matronId = uint256(kit.matronId); sireId = uint256(kit.sireId); generation = uint256(kit.generation); genes = kit.genes;} è¿™ä¸ªå‡½æ•°çœ‹èµ·æ¥è·Ÿæˆ‘ä»¬ä¹ æƒ¯çš„å‡½æ•°ä¸å¤ªä¸€æ ·ã€‚ å®ƒç«Ÿç„¶è¿”å›žäº†â€¦ä¸€å †ä¸åŒçš„å€¼ï¼ åœ¨ Solidityä¸­ï¼Œå¯ä»¥è®©ä¸€ä¸ªå‡½æ•°è¿”å›žå¤šä¸ªå€¼ã€‚ ç¬¬11ç« : ä½¿ç”¨æŽ¥å£ç»§ç»­å‰é¢ NumberInterface çš„ä¾‹å­ï¼Œæˆ‘ä»¬æ—¢ç„¶å°†æŽ¥å£å®šä¹‰ä¸ºï¼š 123contract NumberInterface { function getNum(address _myAddress) public view returns (uint);} æˆ‘ä»¬å¯ä»¥åœ¨åˆçº¦ä¸­è¿™æ ·ä½¿ç”¨ï¼š èŽ·å¾—åˆçº¦åœ°å€ï¼ˆè¯¥åˆçº¦å¿…é¡»ä¸ºexternalæˆ–è€…publicï¼‰ å®šä¹‰ä¸€ä¸ªå˜é‡ï¼šæŒ‡å‘è¯¥åˆçº¦åœ°å€çš„åˆçº¦å¯¹è±¡ï¼ˆä¹‹å‰å®šä¹‰çš„ä¸Žä¹‹äº¤äº’çš„æŽ¥å£å¯¹è±¡ï¼‰ è°ƒç”¨è¯¥åˆçº¦å¯¹è±¡ä¸­çš„å‡½æ•° 123456789101112contract MyContract { address NumberInterfaceAddress = 0xab38...; // ^ è¿™æ˜¯FavoriteNumberåˆçº¦åœ¨ä»¥å¤ªåŠä¸Šçš„åœ°å€ NumberInterface numberContract = NumberInterface(NumberInterfaceAddress); // çŽ°åœ¨å˜é‡ `numberContract` æŒ‡å‘å¦ä¸€ä¸ªåˆçº¦å¯¹è±¡ function someFunction() public { // çŽ°åœ¨æˆ‘ä»¬å¯ä»¥è°ƒç”¨åœ¨é‚£ä¸ªåˆçº¦ä¸­å£°æ˜Žçš„ `getNum`å‡½æ•°: uint num = numberContract.getNum(msg.sender); // ...åœ¨è¿™å„¿ä½¿ç”¨ `num`å˜é‡åšäº›ä»€ä¹ˆ }} é€šè¿‡è¿™ç§æ–¹å¼ï¼Œåªè¦å°†æ‚¨åˆçº¦çš„å¯è§æ€§è®¾ç½®ä¸ºpublic(å…¬å…±)æˆ–external(å¤–éƒ¨)ï¼Œå®ƒä»¬å°±å¯ä»¥ä¸Žä»¥å¤ªåŠåŒºå—é“¾ä¸Šçš„ä»»ä½•å…¶ä»–åˆçº¦è¿›è¡Œäº¤äº’ã€‚ ç¬¬12ç« : å¤„ç†å¤šè¿”å›žå€¼getKitty æ˜¯æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ç¬¬ä¸€ä¸ªè¿”å›žå¤šä¸ªå€¼çš„å‡½æ•°ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹æ˜¯å¦‚ä½•å¤„ç†çš„ï¼š 123456789101112131415161718function multipleReturns() internal returns(uint a, uint b, uint c) { return (1, 2, 3);}function processMultipleReturns() external { uint a; uint b; uint c; // è¿™æ ·æ¥åšæ‰¹é‡èµ‹å€¼: (a, b, c) = multipleReturns();}// æˆ–è€…å¦‚æžœæˆ‘ä»¬åªæƒ³è¿”å›žå…¶ä¸­ä¸€ä¸ªå˜é‡:function getLastReturnValue() external { uint c; // å¯ä»¥å¯¹å…¶ä»–å­—æ®µç•™ç©º: (,,c) = multipleReturns();} ç¬¬13ç« : å¥–åŠ±: Kitty åŸºå› if è¯­å¥ifè¯­å¥çš„è¯­æ³•åœ¨ Solidity ä¸­ï¼Œä¸Žåœ¨ JavaScript ä¸­å·®ä¸å¤šï¼š 123456function eatBLT(string sandwich) public { // çœ‹æ¸…æ¥šäº†ï¼Œå½“æˆ‘ä»¬æ¯”è¾ƒå­—ç¬¦ä¸²çš„æ—¶å€™ï¼Œéœ€è¦æ¯”è¾ƒä»–ä»¬çš„ keccak256 å“ˆå¸Œç  if (keccak256(sandwich) == keccak256(&quot;BLT&quot;)) { eat(); }} ç¬¬14ç« : æ”¾åœ¨ä¸€èµ·JavaScript å®žçŽ°æˆ‘ä»¬åªç”¨ç¼–è¯‘å’Œéƒ¨ç½² ZombieFeedingï¼Œå°±å¯ä»¥å°†è¿™ä¸ªåˆçº¦éƒ¨ç½²åˆ°ä»¥å¤ªåŠäº†ã€‚æˆ‘ä»¬æœ€ç»ˆå®Œæˆçš„è¿™ä¸ªåˆçº¦ç»§æ‰¿è‡ª ZombieFactoryï¼Œå› æ­¤å®ƒå¯ä»¥è®¿é—®è‡ªå·±å’Œçˆ¶è¾ˆåˆçº¦ä¸­çš„æ‰€æœ‰ public æ–¹æ³•ã€‚ æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¸Žæˆ‘ä»¬çš„åˆšéƒ¨ç½²çš„åˆçº¦è¿›è¡Œäº¤äº’çš„ä¾‹å­ï¼Œ è¿™ä¸ªä¾‹å­ä½¿ç”¨äº† JavaScript å’Œ web3.jsï¼š 123456789101112131415161718192021222324252627282930var abi = /* abi generated by the compiler */var ZombieFeedingContract = web3.eth.contract(abi)var contractAddress = /* our contract address on Ethereum after deploying */var ZombieFeeding = ZombieFeedingContract.at(contractAddress)// å‡è®¾æˆ‘ä»¬æœ‰æˆ‘ä»¬çš„åƒµå°¸IDå’Œè¦æ”»å‡»çš„çŒ«å’ªIDlet zombieId = 1;let kittyId = 1;// è¦æ‹¿åˆ°çŒ«å’ªçš„DNAï¼Œæˆ‘ä»¬éœ€è¦è°ƒç”¨å®ƒçš„APIã€‚è¿™äº›æ•°æ®ä¿å­˜åœ¨å®ƒä»¬çš„æœåŠ¡å™¨ä¸Šè€Œä¸æ˜¯åŒºå—é“¾ä¸Šã€‚// å¦‚æžœä¸€åˆ‡éƒ½åœ¨åŒºå—é“¾ä¸Šï¼Œæˆ‘ä»¬å°±ä¸ç”¨æ‹…å¿ƒå®ƒä»¬çš„æœåŠ¡å™¨æŒ‚äº†ï¼Œæˆ–è€…å®ƒä»¬ä¿®æ”¹äº†APIï¼Œ// æˆ–è€…å› ä¸ºä¸å–œæ¬¢æˆ‘ä»¬çš„åƒµå°¸æ¸¸æˆè€Œå°æ€äº†æˆ‘ä»¬let apiUrl = &quot;https://api.cryptokitties.co/kitties/&quot; + kittyId$.get(apiUrl, function(data) { let imgUrl = data.image_url // ä¸€äº›æ˜¾ç¤ºå›¾ç‰‡çš„ä»£ç })// å½“ç”¨æˆ·ç‚¹å‡»ä¸€åªçŒ«å’ªçš„æ—¶å€™:$(&quot;.kittyImage&quot;).click(function(e) { // è°ƒç”¨æˆ‘ä»¬åˆçº¦çš„ `feedOnKitty` å‡½æ•° ZombieFeeding.feedOnKitty(zombieId, kittyId)})// ä¾¦å¬æ¥è‡ªæˆ‘ä»¬åˆçº¦çš„æ–°åƒµå°¸äº‹ä»¶å¥½æ¥å¤„ç†ZombieFactory.NewZombie(function(error, result) { if (error) return // è¿™ä¸ªå‡½æ•°ç”¨æ¥æ˜¾ç¤ºåƒµå°¸: generateZombie(result.zombieId, result.name, result.dna)})","link":"/2020/11/02/solidity-basic/"},{"title":"ã€Œç®—æ³•å¯¼è®ºã€:æŽ’åº-æ€»ç»“","text":"æœ¬ç¯‡æ–‡ç« reviewäº†ç®—æ³•ä¸­çš„æŽ’åºç®—æ³•ï¼ŒåŒ…æ‹¬å†’æ³¡æŽ’åºã€æ’å…¥æŽ’åºã€å½’å¹¶æŽ’åºã€å †æŽ’åºï¼ˆä»¥åŠç”¨å †å®žçŽ°ä¼˜å…ˆé˜Ÿåˆ—ï¼‰ã€å¿«é€ŸæŽ’åºå’Œè®¡æ•°æŽ’åºã€‚ åˆ†åˆ«ä»Žç®—æ³•æ€è·¯ã€ç®—æ³•ä¼ªä»£ç å®žçŽ°ã€ç®—æ³•æµç¨‹ã€ç®—æ³•æ—¶é—´å¤æ‚åº¦å››ä¸ªæ–¹é¢é˜è¿°æ¯ä¸ªç®—æ³•ã€‚ æŽ’åºæŽ’åºé—®é¢˜ï¼š è¾“å…¥ï¼šä¸€ä¸ª $n$ä¸ªæ•°çš„åºåˆ— $&lt;a_1,a_1,â€¦,a_n&gt;$ è¾“å‡ºï¼šè¾“å…¥åºåˆ—çš„ä¸€ä¸ªé‡æ‹ $&lt;a_1â€™,a_2â€™,â€¦,a_nâ€™&gt;$ ï¼Œä½¿å¾— $a_1â€™\\leq a_2â€™ \\leqâ€¦\\leq a_nâ€™$ . åœ¨å®žé™…ä¸­ï¼Œå¾…æŽ’åºçš„æ•°å¾ˆå°‘æ˜¯å•ç‹¬çš„æ•°å€¼ï¼Œå®ƒä»¬é€šå¸¸æ˜¯ä¸€ç»„æ•°æ®ï¼Œç§°ä¸ºè®°å½•(record)ã€‚æ¯ä¸ªè®°å½•ä¸­åŒ…å«ä¸€ä¸ªå…³é”®å­—(key)ï¼Œè¿™å°±æ˜¯éœ€è¦æŽ’åºçš„å€¼ã€‚è®°å½•å‰©ä¸‹çš„æ•°æ®éƒ¨åˆ†ç§°ä¸ºå«æ˜Ÿæ•°æ®(satellite data)ï¼Œé€šå¸¸å’Œå…³é”®å­—ä¸€åŒå­˜å–ã€‚ åŽŸå€æŽ’åºï¼šè¾“å…¥æ•°ç»„ä¸­ä»…æœ‰å¸¸æ•°ä¸ªå…ƒç´ éœ€è¦åœ¨æŽ’åºè¿‡ç¨‹ä¸­å­˜å‚¨åœ¨æ•°ç»„ä¹‹å¤–ã€‚ å…¸åž‹çš„åŽŸå€æŽ’åºæœ‰ï¼šæ’å…¥æŽ’åºã€å †æŽ’åºã€å¿«é€ŸæŽ’åºã€‚ ç¬¦å·è¯´æ˜Žï¼š $\\Theta$ è®°å·ï¼š $\\Theta$ è®°å·æ¸è¿›ç»™å‡ºä¸€ä¸ªå‡½æ•°çš„ä¸Šç•Œå’Œä¸‹ç•Œã€‚ $\\Theta(g(n))=\\left\\{f(n): \\text { there exist positive constants } c_{1}, c_{2}, \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq c_{1} g(n) \\leq f(n) \\leq c_{2} g(n) \\text { for all } n \\geq n_{0}\\right\\}$ $g(n)$ ç§°ä¸º $f(n)$ çš„ä¸€ä¸ªæ¸è¿›ç´§ç¡®ç•Œ(asymptotically tight bound) $O$ è®°å· $O$ è®°å·åªç»™å‡ºäº†å‡½æ•°çš„æ¸è¿›ä¸Šç•Œã€‚ $O(g(n))=\\left\\{f(n): \\text { there exist positive constants } c \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq f(n) \\leq c g(n) \\text { for all } n \\geq n_{0}\\right\\}$ $\\Omega$ è®°å· $\\Omega$ è®°å·ç»™å‡ºäº†å‡½æ•°çš„æ¸è¿›ä¸‹ç•Œã€‚ $\\Omega(g(n))=\\left\\{f(n): \\text { there exist positive constants } c \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq c g(n) \\leq f(n) \\text { for all } n \\geq n_{0}\\right\\}$ ç¬¦å·æ¯”è¾ƒå¦‚ä¸‹å›¾ï¼š æŽ’åºç®—æ³•è¿è¡Œæ—¶é—´ä¸€è§ˆ ï¼š ç®—æ³• æœ€åæƒ…å†µè¿è¡Œæ—¶é—´ å¹³å‡æƒ…å†µ/æœŸæœ›è¿è¡Œæ—¶é—´ æ’å…¥æŽ’åº $\\Theta (n^2)$ $\\Theta(n^2)$ å½’å¹¶æŽ’åº $\\Theta(n\\lg{n})$ $\\Theta(n\\lg{n})$ å †æŽ’åº $O(n\\lg{n})$ - å¿«é€ŸæŽ’åº $\\Theta(n^2)$ $\\Theta(n\\lg{n})$ (expected) è®¡æ•°æŽ’åº $\\Theta(k+n)$ $\\Theta(k+n)$ åŸºæ•°æŽ’åº $\\Theta(d(n+k))$ $\\Theta(d(n+k))$ æ¡¶æŽ’åº $\\Theta(n^2)$ $\\Theta(n)$ (average-case) å†’æ³¡æŽ’åºåå¤äº¤äº’ç›¸é‚»æœªæŒ‰æ¬¡åºæŽ’åˆ—çš„å…ƒç´ ã€‚ BUBBLESORT(A) å‚æ•°ï¼šAå¾…æŽ’åºæ•°ç»„ 1234for i = 1 to A.lengh-1 for j = A.length downto i+1 //æ¯æ¬¡è¿­ä»£æ‰¾å‡ºA[i..j]ä¸­æœ€å°çš„å…ƒç´ æ”¾åœ¨A[i]ä½ç½® if A[j] &lt; A[j-1] exchange A[j] with A[j-1] å†’æ³¡æŽ’åºæ˜¯åŽŸå€æŽ’åºï¼Œæµè¡Œä½†ä½Žæ•ˆã€‚ æ’å…¥æŽ’åºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ’å…¥æŽ’åºå°±åƒæ‰“ç‰Œæ—¶æŽ’åºä¸€æ‰‹æ‰‘å…‹ç‰Œã€‚ å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬çš„å·¦æ‰‹ä¸ºç©ºï¼Œæ¡Œå­ä¸Šçš„ç‰Œé¢å‘ä¸‹ã€‚ ç„¶åŽï¼Œæˆ‘ä»¬æ¯æ¬¡ä»Žæ¡Œå­ä¸Šæ‹¿èµ°ä¸€å¼ ç‰Œï¼Œæƒ³æŠŠå®ƒæ”¾åœ¨å·¦æ‰‹ä¸­çš„æ­£ç¡®ä½ç½®ã€‚ ä¸ºäº†æ‰¾åˆ°è¿™å¼ ç‰Œçš„æ­£ç¡®ä½ç½®ï¼Œæˆ‘ä»¬ä»Žå³åˆ°å·¦å°†è¿™å¼ ç‰Œå’Œå·¦æ‰‹é‡Œçš„ç‰Œä¾æ¬¡æ¯”è¾ƒï¼Œæ”¾å…¥æ­£ç¡®çš„ä½ç½®ã€‚ å·¦æ‰‹éƒ½æ˜¯å·²æŽ’åºå¥½çš„ç‰Œã€‚ INSERTION-SORT(A) Aï¼šå¾…æŽ’åºæ•°ç»„ 12345678for j = 2 to A.length key = A[j] //å°†keyæ’å…¥åˆ°å·²æŽ’åºå¥½çš„A[1..j-1] i = j - 1 //pointer for sorted sequence A[1..j-1] while i &gt; 0 and A[i] &gt; key A[i+1] = A[i] i-- A[i+1] = key æ’å…¥æŽ’åºæ˜¯åŽŸå€æŽ’åºï¼Œå¯¹äºŽå°‘é‡å…ƒç´ æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç®—æ³•ã€‚ æœ€åæƒ…å†µçš„è¿è¡Œæ—¶é—´ï¼š $\\Theta(n^2)$ . å½’å¹¶æŽ’åºåˆ†æ²»åˆ†æ²»ï¼š å°†åŽŸé—®é¢˜åˆ†è§£ä¸ºå‡ ä¸ªè§„æ¨¡è¾ƒå°ä½†ç±»ä¼¼äºŽåŽŸé—®é¢˜çš„å­é—®é¢˜ï¼Œé€’å½’åœ°æ±‚è§£è¿™äº›å­é—®é¢˜ï¼Œç„¶åŽå†åˆå¹¶è¿™äº›å­é—®é¢˜çš„è§£æ¥å»ºç«‹åŽŸé—®é¢˜çš„è§£ã€‚ åˆ†æ²»çš„æ­¥éª¤ï¼š åˆ†è§£ï¼šåˆ†è§£åŽŸé—®é¢˜ä¸ºè‹¥å¹²è§„æ¨¡è¾ƒå°çš„å­é—®é¢˜ã€‚ è§£å†³ï¼šé€’å½’åœ°æ±‚è§£å„å­é—®é¢˜ï¼Œè§„æ¨¡è¾ƒå°ï¼Œå¯ç›´æŽ¥æ±‚è§£ã€‚ åˆå¹¶ï¼šåˆå¹¶è¿™äº›å­é—®é¢˜çš„è§£æˆåŽŸé—®é¢˜çš„è§£ã€‚ ç®—æ³•æ ¸å¿ƒå½’å¹¶æŽ’åºä¸­çš„åˆ†æ²» ï¼š åˆ†è§£ï¼šåˆ†è§£å¾…æŽ’åºçš„nä¸ªå…ƒç´ åºåˆ—æˆå„n/2ä¸ªå…ƒç´ åºåˆ—çš„ä¸¤ä¸ªå­åºåˆ—ã€‚ è§£å†³ï¼šä½¿ç”¨å½’å¹¶æŽ’åºé€’å½’åœ°æŽ’åºä¸¤ä¸ªå­åºåˆ—ï¼Œå½“åºåˆ—é•¿åº¦ä¸º1æ—¶ï¼Œé€’å½’åˆ°è¾¾å°½å¤´ã€‚ åˆå¹¶ï¼šåˆå¹¶ä¸¤ä¸ªå·²ç»æŽ’åºå¥½çš„å­åºåˆ—ä»¥äº§ç”ŸæŽ’åºå¥½çš„åŽŸåºåˆ—ã€‚ æ ¸å¿ƒ ï¼šåˆå¹¶ä¸¤ä¸ªå·²ç»æŽ’åºå¥½çš„å­åºåˆ—â€”â€”MERGE(A, p, q, r) A: å¾…æŽ’åºåŽŸæ•°ç»„ã€‚ p, q, r: æ•°ç»„ä¸‹æ ‡ï¼Œæ»¡è¶³ $p\\leq q&lt;r$ ã€‚ å‡è®¾å­æ•°ç»„ A[p..q] å’ŒA[q+1..r]éƒ½å·²ç»æŽ’å¥½åºï¼Œåˆå¹¶è¿™ä¸¤ä¸ªæ•°ç»„ä»£æ›¿åŽŸæ¥çš„A[p..r]å­æ•°ç»„ã€‚ MERGEç®—æ³•ç†è§£ï¼š ç‰Œæ¡Œä¸Šæœ‰ä¸¤å †ç‰Œé¢æœä¸Šï¼Œæ¯å †éƒ½å·²æŽ’å¥½åºï¼Œæœ€å°çš„ç‰Œåœ¨é¡¶ä¸Šã€‚å¸Œæœ›å°†ä¸¤å †ç‰Œåˆå¹¶æˆæŽ’åºå¥½çš„è¾“å‡ºç‰Œå †ï¼Œä¸”ç‰Œé¢æœä¸‹ã€‚ æ¯”è¾ƒä¸¤å †ç‰Œé¡¶é¡¶ç‰Œï¼Œé€‰å–è¾ƒå°çš„é‚£å¼ ï¼Œç‰Œé¢æœä¸‹çš„æ”¾åœ¨è¾“å‡ºç‰Œå †ã€‚ é‡å¤æ­¥éª¤2ç›´è‡³æŸä¸€ç‰Œå †ä¸ºç©ºã€‚ å°†å‰©ä¸‹çš„å¦ä¸€å †ç‰Œé¢æœä¸‹æ”¾åœ¨è¾“å‡ºå †ã€‚ MERGEåˆå¹¶çš„è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š MERGEç®—æ³•åˆ†æžï¼š åœ¨ä¸Šè¿°è¿‡ç¨‹ä¸­ï¼Œnä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬æœ€å¤šæ‰§è¡Œnä¸ªæ­¥éª¤ï¼Œæ‰€ä»¥MERGEåˆå¹¶éœ€è¦ $\\Theta(n)$ çš„æ—¶é—´ã€‚ ä¼ªä»£ç MERGE(A, p, q, r) åŠŸèƒ½ï¼šåˆå¹¶å·²æŽ’åºå¥½çš„å­æ•°ç»„A[p..q]å’ŒA[q+1..r] å‚æ•°ï¼šAä¸ºå¾…æŽ’åºæ•°ç»„ï¼Œp, q, rä¸ºæ•°ç»„ä¸‹æ ‡ï¼Œä¸”æ»¡è¶³ $p\\leq q&lt;r$ 12345678910Let S[p..r] be new arraysk = p //pointer for S[]i = p, j = q+1 //pointer for subarraywhile k &lt;= r while ( i &lt;= q and j &lt;= r and A[i] &lt;= A[j] ) or j &gt; r // å–A[p..q]ç‰Œå † S[k++] = A[i++] while ( i &lt;= q and j &lt;= r and A[i] &gt;= A[j] ) or i &gt; q //å–A[q+1..r]ç‰Œå † A[p..r] = S[p..r] MERGE-SORT(A, p, r) åŠŸèƒ½ï¼šæŽ’åºå­æ•°ç»„A[p..r] 12345if p &lt; r q = (p+r)/2 MERGE-SORT(A, p, q) MERGE-SORT(A, q+1, r) MERGE(A, p, q, r) ç®—æ³•åˆ†æžåˆ†æ²»ç®—æ³•è¿è¡Œæ—¶é—´åˆ†æžåˆ†æ²»ç®—æ³•è¿è¡Œæ—¶é—´é€’å½’å¼æ¥è‡ªä¸‰ä¸ªéƒ¨åˆ†ã€‚ å‡è®¾ $T(n)$ æ˜¯è§„æ¨¡ä¸º $n$ çš„ä¸€ä¸ªé—®é¢˜çš„è¿è¡Œæ—¶é—´ã€‚è‹¥è§„æ¨¡é—®é¢˜è¶³å¤Ÿå°ï¼Œåˆ™ç›´æŽ¥æ±‚è§£éœ€è¦å¸¸é‡æ—¶é—´ï¼Œå°†å…¶å†™ä½œ $\\Theta(1)$ ã€‚ å‡è®¾æŠŠåŽŸé—®é¢˜åˆ†è§£æˆ $a$ ä¸ªå­é—®é¢˜ï¼Œæ¯ä¸ªå­é—®é¢˜çš„è§„æ¨¡æ˜¯åŽŸé—®é¢˜çš„ $1/b$ (åœ¨å½’å¹¶æŽ’åºä¸­ï¼Œ $a$ å’Œ $b$ éƒ½ä¸º2ï¼Œä½†å¾ˆå¤šåˆ†æ²»ç®—æ³•ä¸­ $a\\neq b$ )ã€‚ä¸ºäº†æ±‚è§£ä¸€ä¸ªè§„æ¨¡ä¸º $n/b$ è§„æ¨¡çš„å­é—®é¢˜ï¼Œéœ€è¦ $T(n/b)$ çš„æ—¶é—´ï¼Œæ‰€ä»¥éœ€è¦ $aT(n/b)$ çš„æ—¶é—´æ±‚è§£ $a$ ä¸ªå­é—®é¢˜ã€‚ å¦‚æžœåˆ†è§£å­é—®é¢˜éœ€è¦ $D(n)$ æ—¶é—´ï¼Œåˆå¹¶å­é—®é¢˜éœ€è¦ $C(n)$ æ—¶é—´ã€‚ é€’å½’å¼ï¼š $$ T(n)=\\left\\{\\begin{array}{ll}\\Theta(1) & \\text { if } n \\leq c \\\\ a T(n / b)+D(n)+C(n) & \\text { otherwise }\\end{array}\\right. $$ å½’å¹¶æŽ’åºåˆ†æžå‰æ–‡åˆ†æžäº†MERGE(A, p, q, r) åˆå¹¶ä¸¤ä¸ªå­æ•°ç»„çš„æ—¶é—´å¤æ‚åº¦æ˜¯ $\\Theta(n)$ ï¼Œå³ $C(n)=\\Theta(n)$ ï¼Œä¸” $D(n)=\\Theta(n)$ . å½’å¹¶æŽ’åºçš„æœ€åæƒ…å†µè¿è¡Œæ—¶é—´ $T(n)$ : $$ T(n)=\\left\\{\\begin{array}{ll}\\Theta(1) & \\text { if } n=1 \\\\ 2 T(n / 2)+\\Theta(n) & \\text { if } n>1\\end{array}\\right. $$ ç”¨é€’å½’æ ‘çš„æ€æƒ³æ±‚è§£é€’å½’å¼ï¼š å³é€’å½’æ ‘æ¯å±‚çš„ä»£ä»·ä¸º $\\Theta(n)=cn$ ï¼Œå…±æœ‰ $\\lg{n}+1$ å±‚ï¼Œæ‰€ä»¥å½’å¹¶æŽ’åºçš„è¿è¡Œæ—¶é—´ç»“æžœæ˜¯ $\\Theta(n\\lg{n})$ . å †æŽ’åºå †è‡ªç”±æ ‘ ï¼šè¿žé€šçš„ã€æ— çŽ¯çš„æ— å‘å›¾ã€‚ æœ‰æ ¹æ•° ï¼šæ˜¯ä¸€æ£µè‡ªç”±æ ‘ï¼Œå…¶é¡¶ç‚¹å­˜åœ¨ä¸€ä¸ªä¸Žå…¶ä»–é¡¶ç‚¹ä¸åŒçš„é¡¶ç‚¹ï¼Œç§°ä¸ºæ ‘çš„æ ¹ã€‚ åº¦ ï¼šæœ‰æ ¹æ ‘ä¸­ä¸€ä¸ªç»“ç‚¹ $x$ å­©å­çš„æ•°ç›®ç§°ä¸º $x$ çš„åº¦ã€‚ æ·±åº¦ :ä»Žæ ¹ $r$ åˆ°ç»“ç‚¹ $x$ çš„ç®€å•è·¯å¾„ã€‚ äºŒå‰æ ‘ ï¼šä¸åŒ…æ‹¬ä»»ä½•ç»“ç‚¹ï¼Œæˆ–è€…åŒ…æ‹¬ä¸‰ä¸ªä¸ç›¸äº¤çš„ç»“ç‚¹é›†åˆï¼šä¸€ä¸ªæ ¹ç»“ç‚¹ï¼Œä¸€æ£µç§°ä¸ºå·¦å­æ ‘çš„äºŒå‰æ ‘å’Œä¸€æ£µç§°ä¸ºå³å­æ ‘çš„äºŒå‰æ ‘ã€‚ å®Œå…¨kå‰æ ‘ ï¼šæ‰€æœ‰å¶ç»“ç‚¹æ·±åº¦ç›¸åŒï¼Œä¸”æ‰€æœ‰å†…éƒ¨ç»“ç‚¹åº¦ä¸ºkçš„kå‰æ ‘ã€‚ ï¼ˆäºŒå‰ï¼‰å † ï¼šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå®ƒå¯ä»¥è¢«çœ‹æˆä¸€ä¸ªè¿‘ä¼¼çš„å®Œå…¨äºŒå‰æ ‘ï¼Œæ ‘ä¸Šçš„æ¯ä¸ªç»“ç‚¹å¯¹åº”æ•°ç»„ä¸­çš„ä¸€ä¸ªå…ƒç´ ã€‚é™¤äº†æœ€åº•å±‚å¤–ï¼Œè¯¥æ ‘è¢«å®Œå…¨å¡«æ»¡ï¼Œå¹¶ä¸”æ˜¯ä»Žå·¦åˆ°å³å¡«å……ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ å †çš„æ•°ç»„$A$ æœ‰ä¸¤ä¸ªå±žæ€§ï¼š $A.length$ ï¼šæ•°ç»„å…ƒç´ çš„ä¸ªæ•°ï¼ŒA[1..A.length]ä¸­éƒ½å­˜æœ‰å€¼ã€‚ $A.heap-size$ ï¼šæœ‰å¤šå°‘ä¸ªå †å…ƒç´ åœ¨æ•°ç»„ï¼ŒA[1..heap-size]ä¸­å­˜æ”¾çš„æ˜¯å †çš„æœ‰æ•ˆå…ƒç´ ã€‚ ï¼ˆ$0\\leq A.heap-size\\leq A.lengh$ ) å †çš„æ€§è´¨ï¼š $A[1]$ :å­˜æ”¾çš„æ˜¯æ ‘çš„æ ¹ç»“ç‚¹ã€‚ å¯¹äºŽç»™å®šçš„ä¸€ä¸ªç»“ç‚¹ $i$ ï¼Œå¾ˆå®¹æ˜“è®¡ç®—ä»–çš„çˆ¶ç»“ç‚¹ã€å·¦å­©å­å’Œå³å­©å­çš„ä¸‹æ ‡ã€‚ PARENT(i) 1return i/2 //i&gt;&gt;&gt;1 LEFT(i) 1return 2*i //i&lt;&lt;&lt;1 RIGHT(i) 1return 2*i+1 //i&lt;&lt;&lt;1 | 1 åŒ…å«$n$ ä¸ªå…ƒç´ çš„å †çš„é«˜åº¦ä¸º $\\Theta(\\lg{n})$ å †ç»“æž„ä¸Šçš„åŸºæœ¬æ“ä½œçš„è¿è¡Œæ—¶é—´è‡³å¤šå’Œå †çš„é«˜åº¦æˆæ­£æ¯”ï¼Œå³æ—¶é—´å¤æ‚åº¦ä¸º $O(\\lg{n})$ . å¶å­ç»“ç‚¹ï¼šn/2+1 , n/2+2 , â€¦ , n å †çš„åˆ†ç±»ï¼š æœ€å¤§å †ï¼š é™¤äº†æ ¹ä»¥å¤–çš„ç»“ç‚¹ $i$ éƒ½æ»¡è¶³ $A[\\text{PARENT}(i)]\\geq A[i]$ . æŸä¸ªç»“ç‚¹æœ€å¤šå’Œå…¶çˆ¶ç»“ç‚¹ä¸€æ ·å¤§ã€‚ å †çš„æœ€å¤§å…ƒç´ å­˜æ”¾åœ¨æ ¹ç»“ç‚¹ä¸­ã€‚ æœ€å°å †ï¼š é™¤äº†æ ¹ä»¥å¤–çš„ç»“ç‚¹ $i$ éƒ½æ»¡è¶³ $A[\\text{PARENT}(i)]\\leq A[i]$ . å †çš„æœ€å°å…ƒç´ å­˜æ”¾åœ¨æ ¹ç»“ç‚¹ä¸­ã€‚ å †çš„åŸºæœ¬è¿‡ç¨‹ : MAX-HEAPIFYï¼šç»´æŠ¤æœ€å¤§å †çš„è¿‡ç¨‹ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(\\lg{n})$ BUILD-MAX-HEAPï¼šå°†æ— åºçš„è¾“å…¥æ•°æ®æ•°ç»„æž„é€ ä¸€ä¸ªæœ€å¤§å †ï¼Œå…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦ $O(n\\lg{n})$ ã€‚ HEAPSORTï¼šå¯¹ä¸€ä¸ªæ•°ç»„è¿›è¡ŒåŽŸå€æŽ’åºï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(n\\lg{n})$ MAX-HEAP-INSERTã€HEAP-EXTRACT-MAXã€HEAP-INCREASE-KEYå’ŒHEAP-MAXIMUMï¼šåˆ©ç”¨å †å®žçŽ°ä¸€ä¸ªä¼˜å…ˆé˜Ÿåˆ—ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(\\lg{n})$ . ç»´æŠ¤ï¼šMAX-HEAPIFYè°ƒç”¨MAX-HEAPIFYçš„æ—¶å€™ï¼Œå‡å®šæ ¹ç»“ç‚¹LEFT(i)å’ŒRIGHT(i)çš„äºŒå‰æ ‘éƒ½æ˜¯æœ€å¤§å †ï¼Œä½†A[i]å¯èƒ½å°äºŽå…¶å·¦å³å­©å­ï¼Œå› æ­¤è¿èƒŒäº†å †çš„æ€§è´¨ã€‚ MAX-HEAPIFYé€šè¿‡è®© A[i]â€œé€çº§ä¸‹é™â€ï¼Œä»Žè€Œä½¿ä¸‹æ ‡ä¸ºiçš„æ ¹ç»“ç‚¹çš„å­æ ‘æ»¡è¶³æœ€å¤§å †çš„æ€§è´¨ã€‚ MAX-HEAPIFY(A, i) åŠŸèƒ½ï¼šç»´æŠ¤ä¸‹æ ‡ä¸ºiçš„æ ¹ç»“ç‚¹çš„å­æ ‘ï¼Œä½¿å…¶æ»¡è¶³æœ€å¤§å †çš„æ€§è´¨ã€‚ å‚æ•°ï¼ši æ˜¯è¯¥å­æ ‘çš„æ ¹ç»“ç‚¹ï¼Œå…¶å·¦å­æ ‘å³å­æ ‘å‡æ»¡è¶³æœ€å¤§å †çš„æ€§è´¨ã€‚ 12345678910l = LEFT(i)r = RIGHT(i)if l &lt;= A.heap-size and A[l] &gt; A[i] largest = lelse largest = iif r &lt;= A.heap-size and A[r] &gt; A[i] largest = rif largest != i exchange A[i] with A[largest] MAX-HEAPIFY(A, largest) ä¸‹å›¾æ˜¯æ‰§è¡Œ MAX-HEAPIFY(A, 2)çš„æ‰§è¡Œè¿‡ç¨‹ã€‚A.heap-size=10, å›¾(a)(b)(c)ä¾æ¬¡ä½“çŽ°äº†å€¼ä¸º4çš„ç»“ç‚¹ä¾æ¬¡ä¸‹é™çš„è¿‡ç¨‹ã€‚ æ—¶é—´å¤æ‚åº¦åˆ†æž ï¼š MAX-HEAPIFYçš„æ—¶é—´å¤æ‚åº¦ä¸º $O(lg{n})$. å»ºå †ï¼šBUILD-MAX-HEAPå †çš„æ€§è´¨ï¼š å­æ•°ç»„A[n/2+1..n]ä¸­çš„å…ƒç´ éƒ½æ˜¯æ ‘çš„å¶å­ç»“ç‚¹ã€‚å› ä¸ºä¸‹æ ‡æœ€å¤§çš„çˆ¶ç»“ç‚¹æ˜¯n/2ï¼Œæ‰€ä»¥n/2ä»¥åŽçš„ç»“ç‚¹éƒ½æ²¡æœ‰å­©å­ã€‚ å»ºå † ï¼šæ¯ä¸ªå¶ç»“ç‚¹éƒ½å¯ä»¥çœ‹æˆåªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„å †ï¼Œåˆ©ç”¨è‡ªåº•å‘ä¸Šçš„æ–¹æ³•ï¼Œå¯¹æ ‘ä¸­å…¶ä»–ç»“ç‚¹éƒ½è°ƒç”¨ä¸€æ¬¡MAX-HEAPIFYï¼ŒæŠŠä¸€ä¸ªå¤§å°ä¸ºn = A.lengthçš„æ•°ç»„A[1..n]è½¬æ¢ä¸ºæœ€å¤§å †ã€‚ BUILD-MAX-HEAP(A) åŠŸèƒ½ï¼šæŠŠA[1..n]æ•°ç»„è½¬æ¢ä¸ºæœ€å¤§å † 123A.heap-size = A.lengthfor i = A.length/2 downto 1 MAX-HEAPIFY(A, i) ä¸‹å›¾æ˜¯æŠŠAæ•°ç»„æž„é€ æˆæœ€å¤§å †çš„è¿‡ç¨‹ï¼š æ—¶é—´å¤æ‚åº¦åˆ†æž ï¼š BUILD-MAX-HEAPéœ€è¦ $O(n)$ æ¬¡è°ƒç”¨MAX-HEAPIFYï¼Œå› æ­¤æž„é€ æœ€å¤§å †çš„æ—¶é—´å¤æ‚åº¦æ˜¯ $O(n\\lg{n})$ . æŽ’åºï¼šHEAPSORTç®—æ³•æ€è·¯ï¼š åˆå§‹åŒ–æ—¶ï¼Œè°ƒç”¨BUILD-MAX-HEAPå°†è¾“å…¥æ•°ç»„A[1..n]å»ºæˆæœ€å¤§å †ï¼Œå…¶ä¸­ n = A.lengthã€‚ è°ƒç”¨åŽï¼Œæœ€å¤§çš„å…ƒç´ åœ¨A[1]ï¼Œå°†A[1]å’ŒA[n]äº’æ¢ï¼Œå¯ä»¥æŠŠå…ƒç´ æ”¾åœ¨æ­£ç¡®çš„ä½ç½®ã€‚ å°†nç»“ç‚¹ä»Žå †ä¸­åŽ»æŽ‰(é€šè¿‡å‡å°‘A.heap-sizeå®žçŽ°)ï¼Œå‰©ä½™ç»“ç‚¹ä¸­ï¼ŒåŽŸæ¥æ ¹çš„å­©å­ä»æ˜¯æœ€å¤§å †ï¼Œä½†æ ¹ç»“ç‚¹å¯èƒ½ä¼šè¿èƒŒå †çš„æ€§è´¨ï¼Œè°ƒç”¨MAX-HEAPIFY(A, 1)ï¼Œä»Žè€Œæž„é€ ä¸€ä¸ªæ–°çš„æœ€å¤§å †ã€‚ é‡å¤æ­¥éª¤3ï¼Œç›´åˆ°å †çš„å¤§å°ä»Žn-1é™ä¸º2. HEAPSORT(A) åŠŸèƒ½ï¼šåˆ©ç”¨å †å¯¹æ•°ç»„æŽ’åº 12345BUILD-MAX-HEAP(A)for i = A.length downto 2 exchange A[1] with A[i] A.heap-size = A.heap-size - 1 MAX-HEAPIFY(A, 1) ä¸‹å›¾ä¸ºè°ƒç”¨HEAPSORTçš„è¿‡ç¨‹å›¾ï¼š æ—¶é—´å¤æ‚åº¦åˆ†æž ï¼š å»ºå †BUILD-MAX-HEAPçš„æ—¶é—´å¤æ‚åº¦ä¸º $O(n\\lg{n})$ ï¼Œn-1æ¬¡è°ƒç”¨MAX-HEAPIFYçš„æ—¶é—´å¤æ‚åº¦ä¸º $O(n\\lg{n})$ ï¼Œæ‰€ä»¥å †æŽ’åºçš„æ—¶é—´å¤æ‚åº¦ä¸º $O(n\\lg{n})$ . å †çš„åº”ç”¨ï¼šä¼˜å…ˆé˜Ÿåˆ—è¿™é‡Œå…³æ³¨å¦‚ä½•ç”¨æœ€å¤§å †å®žçŽ°æœ€å¤§ä¼˜å…ˆé˜Ÿåˆ—ã€‚ ä¼˜å…ˆé˜Ÿåˆ—(priority queue)ï¼š ä¸€ç§ç”¨æ¥ç»´æŠ¤ç”±ä¸€ç»„å…ƒç´ æž„æˆçš„é›†åˆSçš„æ•°æ®ç»“æž„ï¼Œå…¶ä¸­æ¯ä¸€ä¸ªå…ƒç´ éƒ½æœ‰ä¸€ä¸ªç›¸å…³çš„å€¼ï¼Œç§°ä¸ºå…³é”®å­—(key)ã€‚ ï¼ˆæœ€å¤§ï¼‰ä¼˜å…ˆé˜Ÿåˆ—æ”¯æŒçš„æ“ä½œ ï¼š INSERT(S, x)ï¼šæŠŠå…ƒç´  $x$ æ’å…¥é›†åˆSä¸­ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(\\lg{n})$ ã€‚ MAXIMUM(S)ï¼šè¿”å›žSä¸­å…·æœ‰æœ€å¤§å…³é”®å­—çš„å…ƒç´ ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(1)$ ã€‚ EXTRACT-MAX(S)ï¼šåŽ»æŽ‰å¹¶è¿”å›žSä¸­çš„å…·æœ‰æœ€å¤§å…³é”®å­—çš„å…ƒç´ ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(\\lg{n})$ ã€‚ INCREASE-KEY(S, x, k)ï¼šå°†å…ƒç´  $x$ çš„å…³é”®å­—å€¼å¢žåŠ åˆ°kï¼Œè¿™é‡Œå‡è®¾kçš„å¤§å°ä¸å°äºŽå…ƒç´  $x$ çš„åŽŸå…³é”®å­—å€¼ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(\\lg{n})$ ã€‚ MAXIMUMå°†é›†åˆSå·²å»ºç«‹æœ€å¤§å †çš„å‰æä¸‹ï¼Œè°ƒç”¨HEAP-MAXIMUMåœ¨ $\\Theta(1)$ å®žçŽ°MAXIMUMçš„æ“ä½œã€‚ HEAP-MAXIMUM(A) åŠŸèƒ½ï¼šå®žçŽ°æœ€å¤§ä¼˜å…ˆé˜Ÿåˆ—MAXIMUMçš„æ“ä½œï¼Œå³è¿”å›žé›†åˆä¸­æœ€å¤§å…³é”®å­—çš„å…ƒç´ ã€‚ 1return A[1] æ—¶é—´å¤æ‚åº¦åˆ†æž ï¼š$\\Theta(1)$ EXTRACT-MAXç±»ä¼¼äºŽHEAPSORTçš„è¿‡ç¨‹ã€‚ A[1]ä¸ºæœ€å¤§çš„å…ƒç´ ï¼ŒA[1]çš„å­©å­éƒ½æ˜¯æœ€å¤§å †ã€‚ å°†A[1]å’ŒA[heap-size]äº¤æ¢ï¼Œå‡å°‘å †çš„å¤§å°(heap-size)ã€‚ æ­¤æ—¶æ ¹ç»“ç‚¹çš„å­©å­æ»¡è¶³æœ€å¤§å †ï¼Œè€Œæ ¹ä¸ä¸€å®šæ»¡è¶³æœ€å¤§å †æ€§è´¨ï¼Œç»´æŠ¤ä¸€ä¸‹å½“å‰å †ã€‚ HEAP-EXTRACT-MAX(A) åŠŸèƒ½ï¼šå®žçŽ°æœ€å¤§ä¼˜å…ˆé˜Ÿåˆ—EXTRACT-MAXçš„æ“ä½œï¼Œå³åŽ»æŽ‰å¹¶è¿”å›žé›†åˆä¸­æœ€å¤§å…³é”®å­—çš„å…ƒç´ ã€‚ 1234567if A.heap-size &lt; 1 error &quot;heap underflow&quot;max = A[1]A[1] = A[A.heap-size]A.heap-size = A.heap-size - 1MAX-HEAPIFY(A, 1)return max æ—¶é—´å¤æ‚åº¦åˆ†æž ï¼š$O(\\lg{n})$ . INCREASE-KEYå¦‚æžœå¢žåŠ A[i]çš„å…³é”®è¯ï¼Œå¯èƒ½ä¼šè¿åæœ€å¤§å †çš„æ€§è´¨ï¼Œæ‰€ä»¥å®žçŽ°HEAP-INCREASE-KEYçš„è¿‡ç¨‹ç±»ä¼¼æ’å…¥æŽ’åºï¼šä»Žå½“å‰iç»“ç‚¹åˆ°æ ¹ç»“ç‚¹çš„è·¯å¾„ä¸Šä¸ºæ–°å¢žçš„å…³é”®è¯å¯»æ‰¾æ°å½“çš„æ’å…¥ä½ç½®ã€‚ å½“å‰å…ƒç´ ä¸æ–­å’Œå…¶çˆ¶ç»“ç‚¹æ¯”è¾ƒï¼Œå¦‚æžœå½“å‰å…ƒç´ çš„å…³é”®å­—æ›´å¤§ï¼Œåˆ™å’Œçˆ¶ç»“ç‚¹è¿›è¡Œäº¤æ¢ã€‚ æ­¥éª¤1ä¸æ–­é‡å¤ï¼Œç›´è‡³å½“å‰å…ƒç´ çš„å…³é”®å­—æ¯”çˆ¶ç»“ç‚¹å°ã€‚ HEAP-INCREASE-KEY(A, i, key) åŠŸèƒ½ï¼šå®žçŽ°æœ€å¤§ä¼˜å…ˆé˜Ÿåˆ—INCREASE-KEYçš„åŠŸèƒ½ï¼Œå³å°†A[i]çš„å…³é”®å­—å€¼å¢žåŠ ä¸ºkey. å‚æ•°ï¼šiä¸ºå¾…å¢žåŠ å…ƒç´ çš„ä¸‹æ ‡ï¼Œkeyä¸ºæ–°å…³é”®å­—å€¼ã€‚ 123456if key &lt; A[i] error &quot;new key is smaller than current key&quot;A[i] = keywhile i &gt; 1 and A[PARENT(i)] &lt; A[i] exchange A[i] with A[PARENT(i)] i = PARENT(i) ä¸‹å›¾å±•ç¤ºäº†HEAP-INCREASE-KEYçš„è¿‡ç¨‹ï¼š æ—¶é—´å¤æ‚åº¦åˆ†æž ï¼š$O(\\lg{n})$ INSERTå¦‚ä½•æ’å…¥ä¸€ä¸ªå…ƒç´ æ‰©å±•æœ€å¤§å †ï¼Ÿ å…ˆé€šè¿‡å¢žåŠ ä¸€ä¸ªå…³é”®å­—å€¼ä¸º $-\\infin$ çš„å¶å­ç»“ç‚¹æ‰©å±•æœ€å¤§å †ã€‚ å†è°ƒç”¨HEAP-INCREASE-KEYè¿‡ç¨‹ä¸ºæ–°çš„ç»“ç‚¹è®¾ç½®å¯¹åº”çš„å…³é”®å­—å€¼ã€‚ MAX-HEAP-INSERT(A, key) åŠŸèƒ½ï¼šå®žçŽ°æœ€å¤§ä¼˜å…ˆé˜Ÿåˆ—çš„INSERTåŠŸèƒ½ï¼Œå³å°†å…³é”®å­—å€¼ä¸ºkeyçš„æ–°å…ƒç´ æ’å…¥åˆ°æœ€å¤§å †ä¸­ã€‚ å‚æ•°ï¼škeyæ˜¯å¾…æ’å…¥å…ƒç´ çš„å…³é”®å­—å€¼ã€‚ 123A.heap-size = A.heap-size + 1A[A.heap-size] = -âˆžHEAP-INCREASE-KEY(A, A.heap-size, key) æ—¶é—´å¤æ‚åº¦åˆ†æž ï¼š$O(\\lg{n})$ . å¿«é€ŸæŽ’åºå¯¹äºŽåŒ…å« $n$ä¸ªæ•°çš„è¾“å…¥æ•°ç»„æ¥è¯´ï¼Œå¿«é€ŸæŽ’åºæ˜¯ä¸€ä¸ªæœ€åæƒ…å†µæ—¶é—´å¤æ‚åº¦ä¸º $\\Theta(n^2)$ çš„æŽ’åºç®—æ³•ã€‚ è™½ç„¶æœ€åæƒ…å†µæ—¶é—´å¤æ‚åº¦å¾ˆå·®ï¼Œä½†æ˜¯å¿«é€ŸæŽ’åºé€šå¸¸æ˜¯å®žé™…æŽ’åºåº”ç”¨ä¸­æœ€å¥½çš„é€‰æ‹©ï¼Œå› ä¸ºä»–çš„å¹³å‡æ€§èƒ½éžå¸¸å¥½ï¼šä»–çš„æœŸæœ›æ—¶é—´å¤æ‚åº¦ä¸º $\\Theta(n\\lg{n})$ ï¼Œè€Œä¸” $\\Theta(n\\lg{n})$ ä¸­éšå«çš„å¸¸æ•°å› å­éžå¸¸å°ã€‚ å¦å¤–ï¼Œå®ƒè¿˜èƒ½è¿›è¡ŒåŽŸå€æŽ’åºã€‚ åˆ†æ²»å¯¹A[p..r]å­æ•°ç»„è¿›è¡Œå¿«é€ŸæŽ’åºçš„åˆ†æ²»è¿‡ç¨‹ï¼š åˆ†è§£ï¼š æ•°ç»„A[p..r]è¢«åˆ’åˆ†ä¸ºä¸¤ä¸ªï¼ˆå¯èƒ½ä¸ºç©ºï¼‰çš„å­æ•°ç»„A[p..q-1]å’ŒA[q+1..r]ã€‚ ä½¿å¾—A[p..q-1]ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½å°äºŽç­‰äºŽA[q]ï¼ŒA[q+1..r]ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½å¤§äºŽç­‰äºŽA[q]ã€‚ å…¶ä¸­è®¡ç®—ä¸‹æ ‡qä¹Ÿæ˜¯åˆ†è§£è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚ è§£å†³ï¼šé€šè¿‡é€’å½’è°ƒç”¨å¿«é€ŸæŽ’åºï¼Œå¯¹å­æ•°ç»„A[p..q-1]å’ŒA[q+1..r]è¿›è¡ŒæŽ’åºã€‚ åˆå¹¶ï¼šå› ä¸ºå­æ•°ç»„éƒ½æ˜¯åŽŸå€æŽ’åºçš„ï¼Œæ‰€ä»¥ä¸éœ€è¦åˆå¹¶æ“ä½œï¼ŒA[p..r]å·²ç»æŽ’å¥½åºã€‚ å¿«é€ŸæŽ’åºï¼šQUICKSORTæŒ‰ç…§åˆ†æ²»çš„è¿‡ç¨‹ã€‚ QUICKSORT(A, p, r) åŠŸèƒ½ï¼šå¿«é€ŸæŽ’åºå­æ•°ç»„A[p..r] 1234if p &lt; r q = PARTITION(A, p, r) QUICKSORT(A, p, q-1) QUICKSORT(A, q+1, r) æ•°ç»„çš„åˆ’åˆ†ï¼šPARTITIONå¿«é€ŸæŽ’åºçš„å…³é”®éƒ¨åˆ†å°±åœ¨äºŽå¦‚ä½•å¯¹æ•°ç»„A[p..r]è¿›è¡Œåˆ’åˆ†ï¼Œå³æ‰¾åˆ°ä½ç½®qã€‚ PARTITION(A, p, r) åŠŸèƒ½ï¼šå¯¹å­æ•°ç»„A[p..r] åˆ’åˆ†ä¸ºä¸¤ä¸ªå­æ•°ç»„A[p..q-1]å’Œå­æ•°ç»„A[q+1..r]ï¼Œå…¶ä¸­A[p..q-1] å°äºŽç­‰äºŽA[q]å°äºŽç­‰äºŽA[q+1..r] è¿”å›žï¼šæ•°ç»„çš„åˆ’åˆ†ä¸‹æ ‡q 12345678x = A[r]i = p - 1for j = p to r - 1 // j is pointer for comparation if A[j] &lt;= x i = i+1 exchange A[i] with A[j]exchange A[i+1] with A[r]return i+1 PARTITIONæ€»æ˜¯é€‰æ‹©ä¸€ä¸ª $x=A[r]$ ä½œä¸ºä¸»å…ƒ(pivot element)ï¼Œå¹¶å›´ç»•å®ƒæ¥åˆ’åˆ†å­æ•°ç»„A[p..r]ã€‚ åœ¨å¾ªçŽ¯ä¸­ï¼Œæ•°ç»„è¢«åˆ’åˆ†ä¸ºä¸‹å›¾å››ä¸ªï¼ˆå¯èƒ½ä¸ºç©ºçš„ï¼‰åŒºåŸŸï¼š $p\\leq k\\leq i$ ï¼Œåˆ™ $A[k]\\leq x$ . $i+1\\leq k \\leq j-1$ ï¼Œåˆ™ $A[k]&gt;x$. $k = r$ ï¼Œåˆ™ $A[k]=x$ . $j\\leq k\\leq r-1$ ï¼Œåˆ™ $A[k]$ ä¸Ž $x$ æ— å…³ã€‚ ä¸‹å›¾æ˜¯å°†æ ·ä¾‹æ•°ç»„PARTITIONçš„è¿‡ç¨‹ï¼š å¿«é€ŸæŽ’åºçš„æ€§èƒ½[*]å¾…è¡¥å…… å¿«é€ŸæŽ’åºçš„éšæœºåŒ–ç‰ˆæœ¬ä¸Žå§‹ç»ˆé‡‡ç”¨ $A[r]$ ä½œä¸ºä¸»å…ƒçš„æ–¹æ³•ä¸åŒï¼ŒéšæœºæŠ½æ ·æ˜¯ä»Žå­æ•°ç»„A[p..r]éšæœºé€‰æ‹©ä¸€ä¸ªå…ƒç´ ä½œä¸ºä¸»å…ƒã€‚ åŠ å…¥éšæœºæŠ½æ ·ï¼Œåœ¨å¹³å‡æƒ…å†µä¸‹ï¼Œå¯¹å­æ•°ç»„A[p..r]çš„åˆ’åˆ†æ˜¯æ¯”è¾ƒå‡åŒ€çš„ã€‚ RANDOMIZED-PEARTITION(A, p, r) åŠŸèƒ½ï¼šæ•°ç»„åˆ’åˆ†PARTITIONçš„éšæœºåŒ–ä¸»å…ƒç‰ˆæœ¬ 123i = RANDOM(p, r)exchange A[r] with A[i]return PARTITION(A, p, r) RANDOMIZED-QUICKSORT(A, p, r) åŠŸèƒ½ï¼šä½¿ç”¨éšæœºåŒ–ä¸»å…ƒçš„å¿«é€ŸæŽ’åº 1234if p &lt; r q = RANDOMIZED-PARTITION(A, p, r) RANDOMIZED-QUICKSORT(A, p, q-1) RANDOMIZED-QUICKSORT(A, q+1, r) è®¡æ•°æŽ’åºè®¡æ•°æŽ’åº ï¼š å‡è®¾ $n$ ä¸ªè¾“å…¥å…ƒç´ ä¸­çš„æ¯ä¸€ä¸ªéƒ½æ˜¯åœ¨ 0åˆ° $k$ åŒºé—´å†…åˆ°ä¸€ä¸ªæ•´æ•°ï¼Œå…¶ä¸­ $k$ ä¸ºæŸä¸ªæ•´æ•°ã€‚å½“ $k = O(n)$ æ—¶ï¼ŒæŽ’åºçš„è¿è¡Œæ—¶é—´ä¸º $\\Theta(n)$ . è®¡æ•°æŽ’åºçš„æ€æƒ³ ï¼š å¯¹æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´  $x$ï¼Œç¡®å®šå°äºŽ $x$ çš„å…ƒç´ ä¸ªæ•°ã€‚åˆ©ç”¨è¿™ä¸ªä¿¡æ¯ï¼Œå°±å¯ä»¥ç›´æŽ¥æŠŠ $x$ æ”¾åœ¨è¾“å‡ºæ•°ç»„æ­£ç¡®çš„ä½ç½®äº†ã€‚ COUNTING-SORT(A, B, k) åŠŸèƒ½ï¼šè®¡æ•°æŽ’åº å‚æ•°ï¼š A[1..n]è¾“å…¥çš„å¾…æŽ’åºæ•°ç»„ï¼ŒA.length = n B[1..n] å­˜æ”¾æŽ’åºåŽçš„è¾“å‡ºæ•°ç»„ï¼› ä¸´æ—¶å­˜å‚¨ç©ºé—´ C[0..k] ï¼ŒA[1..n]ä¸­çš„å…ƒç´ å¤§å°ä¸å¤§äºŽk. 123456789101112let C[0..k] be a new arrayfor i = 0 to k C[i] = 0for j = 1 to A.length C[A[j]] = C[A[j]] + 1//C[i] now contains the number of elements equal to i.for i = 1 to k C[i] = C[i] + C[i-1]//C[i] now contains the number of elements less than or equal to i.for j = A.length downto 1 B[C[A[j]]] = A[j] C[A[j]] = C[A[j]] - 1 ä¸‹å›¾æ˜¯è®¡æ•°æŽ’åºçš„è¿‡ç¨‹ï¼š Reference Introduction to Algorithms. ç®—æ³•å¯¼è®º","link":"/2020/06/28/sort-preview/"},{"title":"ã€ŒCryptography-Bonehã€:Block Cipher 1","text":"è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å—å¯†ç ã€‚ æ–‡ä¸­ä¸»è¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ã€‚ ç¬¬ä¸€ä¸ªéƒ¨åˆ†è§£é‡Šäº†å—å¯†ç çš„åŸºç¡€æ¦‚å¿µï¼ŒåŒ…æ‹¬æŠ½è±¡æ¦‚å¿µPRFï¼ˆä¼ªéšæœºå‡½æ•°ï¼‰å’ŒPRPï¼ˆä¼ªéšæœºç½®æ¢ï¼‰çš„å®šä¹‰åŠå…¶å®‰å…¨å®šä¹‰ã€‚ ç¬¬äºŒä¸ªéƒ¨åˆ†ä»‹ç»äº†ç»å…¸å—å¯†ç DESï¼ŒåŒ…æ‹¬DESçš„Feistelç½‘ç»œï¼Œæ”¯æ’‘Feistelç½‘ç»œçš„Luby-Rackoffå®šç†ï¼Œtriple-DESå’Œå¯¹DESçš„ä¸€äº›æ”»å‡»æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯æœ‰æ•ˆçš„ä¸­é—´ç›¸é‡æ”»å‡»ã€‚ ç¬¬ä¸‰ä¸ªéƒ¨åˆ†ä»‹ç»äº†ç›®å‰æµè¡Œçš„å—å¯†ç AESï¼ŒåŒ…æ‹¬AESçš„ç»“æž„å’Œä¸€äº›æ”»å‡»æ–¹æ³•ç­‰ã€‚ æœ€åŽä¸€å°éƒ¨åˆ†ä»‹ç»äº†ç”¨PRGæž„é€ PRFï¼Œå†åˆ©ç”¨Feistelç½‘ç»œå˜æˆå—å¯†ç ã€‚ Overviewå—å¯†ç ï¼Œç®€å•æ¥è¯´ï¼Œå°±æ˜¯åœ¨ç»™å®šå¯†é’¥ (k bits)ä¸‹ï¼Œå°†è¾“å…¥å—(n bits)æ˜ å°„åˆ°è¾“å‡ºå—(n bits)ã€‚ æ¯”å¦‚ï¼Œåœ¨3DESä¸­ï¼Œn = 64 bits, k = 168 bitsï¼›åœ¨AESä¸­ï¼Œn = 128 bits, k = 128, 192, 256 bitsã€‚ å—å¯†ç ç®—æ³•ä¸€èˆ¬æ˜¯é€šè¿‡è¿­ä»£ï¼ˆiterationï¼‰å®žçŽ°çš„ï¼Œå¦‚ä¸‹å›¾çš„ç»“æž„ï¼š å¯†é’¥ké€šè¿‡å¯†é’¥æ‰©å±•(key expansion)å¾—åˆ°ä¸€ç³»åˆ—è½®å¯†é’¥ã€‚ æ˜Žæ–‡å—mé€šè¿‡è¿­ä»£è¿ç®—ï¼Œå¾—åˆ°æœ€ç»ˆçš„å¯†æ–‡cï¼Œå…¶ä¸­æ¯ä¸€æ¬¡è¿ç®—ç§°ä¸ºè½®å‡½æ•°R(k, m)ã€‚ å¯¹äºŽ3DESæ¥è¯´ï¼Œè½®æ•°ä¸ºn = 48ï¼Œå¯¹äºŽAES-128æ¥è¯´ï¼Œè½®æ•°ä¸ºn = 10ã€‚ è™½ç„¶è¿­ä»£è¿ç®—ä¼šè®©å—å¯†ç æ¯”æµå¯†ç æ…¢å¾ˆå¤šï¼Œä½†å—å¯†ç æœ‰å¾ˆå¤šåº”ç”¨ã€‚ PRPs and PRFsè¿™é‡Œä»‹ç»ä¸¤ä¸ªæŠ½è±¡çš„æ¦‚å¿µï¼Œä¼ªéšæœºç½®æ¢ï¼ˆPRPï¼‰å’Œä¼ªéšæœºå‡½æ•°ï¼ˆPRFï¼‰ã€‚ Pseudo Random Function(PRF): defined over(K, X, Y): $F:K\\times X \\rightarrow Y$ such that exists â€œefficientâ€ algorithm to evaluate F(k, x) ã€ä¼ªéšæœºå‡½æ•°å®šä¹‰åœ¨(K, X, Y)ä¸Šï¼Œåªè¦æ±‚æ˜¯å¯è®¡ç®—çš„ï¼Œæ²¡æœ‰è¦æ±‚æ˜¯å¯é€†çš„ã€‘ Pseudo Random Permutations(PRP): defined over(K, X): $E:K\\times X\\rightarrow X$ such that: Exists â€œefficientâ€ deterministic algorithm to evaluate E(k, x) The function E(k, Â·) is one to one Exists â€œefficientâ€ inversion algorithm D(k, y) ã€è€Œä¼ªéšæœºç½®æ¢å®šä¹‰åœ¨(K, X)ä¸Šï¼Œé™¤äº†è¦æ±‚æ˜¯å¯è®¡ç®—çš„ï¼Œè¿˜è¦æ±‚æ˜¯ä¸€å¯¹ä¸€æ˜ å°„ï¼ˆå•å°„ï¼‰ï¼Œå³å­˜åœ¨å¯é€†è¿ç®—ã€‘ å—å¯†ç éƒ½æ˜¯PRPsï¼ˆä¸ç„¶æ€Žä¹ˆè§£å¯†ï¼‰ï¼Œå¸¸è§çš„PRPæœ‰3DESï¼ŒAESç­‰ï¼š AES: $K \\times X \\rightarrow X$ where K = X = $\\{0, 1\\}^{128}$ 3DES: $K\\times X \\rightarrow X$ where X = $\\{0, 1\\}^{64}$ , K = $\\{0, 1\\}^{168}$ ä»Žå®šä¹‰ä¸Šæ¥è¯´ï¼Œä»»ä½•PRPéƒ½æ˜¯PRFï¼›ä½†ä¸€ä¸ªPRFæ˜¯PRPçš„å……åˆ†æ¡ä»¶æ˜¯X=Yï¼Œä¸”å­˜åœ¨å¯é€†è¿ç®—(efficiently invertible)ã€‚ Secure PRFsåœ¨ä»‹ç»PRFçš„å®‰å…¨æ€§ä¹‹å‰ï¼Œå…ˆå®šä¹‰ä¸€äº›ç¬¦å·ï¼š $F: K \\times X \\rightarrow Y$ is a PRF. Funs[X, Y]: the set of all functions from X to Y ã€Xåˆ°Yçš„æ‰€æœ‰æ˜ å°„å…³ç³»ã€‘ $S_F={ F(k, \\cdot)}$ s.t. $k\\in K$ $\\subseteq$ Funs[X, Y] ã€ç¡®å®škå°±ç¡®å®šäº†ä¸€ç§Xåˆ°Yçš„æ˜ å°„å…³ç³»ã€‘ ç›´è§‰ä¸Šæ¥è¯´ï¼Œå½“ä»ŽFuns[X, Y]ä¸­éšæœºé‡‡æ ·çš„å‡½æ•°ä¸Žä»Ž $S_F$ ä¸­éšæœºé‡‡æ ·çš„å‡½æ•°ä¸å¯åŒºåˆ†æ—¶ï¼ŒPRFæ˜¯å®‰å…¨çš„ã€‚ Intuition: a PRF is secure if a random function in Funs[X, Y] is indistinguishable from a random function in $S_F$ . å®šä¹‰ä¸‹å›¾çš„æ¸¸æˆï¼š äº‘ä»ŽFuns[X, Y]ä¸­éšæœºé€‰å–ä¸€ä¸ªå‡½æ•°fï¼Œå¹¶éšæœºé€‰æ‹©ä¸€ä¸ªkï¼Œå³ç¡®å®šäº†ä¸€ä¸ªåœ¨ $S_F$ ä¸­å‡½æ•°ã€‚ æ”»å‡»è€…å‘äº‘å¤šæ¬¡è¯¢é—®xçš„è®¡ç®—ç»“æžœï¼Œå¯¹äº‘æ¥è¯´ï¼Œå¦‚æžœæ˜¯b = 0ï¼Œä»–è¿”å›žf(x)ï¼Œå¦‚æžœb = 1ï¼Œä»–è¿”å›žF(k, x)ã€‚ å½“æ”»å‡»è€…è¾“å…¥ä»»æ„è¾“å…¥xæ—¶ï¼Œéƒ½æ— æ³•åŒºåˆ†è®¡ç®—ç»“æžœæ˜¯f(x)è®¡ç®—çš„è¿˜æ˜¯F(k, x)è®¡ç®—çš„ï¼ŒPRFæ˜¯å®‰å…¨çš„ã€‚ Secure PRPs/Block Cipherså—å¯†ç éƒ½æ˜¯PRPï¼Œæ‰€ä»¥è¿™é‡Œä¹Ÿåœ¨å®šä¹‰ä»€ä¹ˆæ˜¯å®‰å…¨çš„å—å¯†ç ã€‚ åŒæ ·å®šä¹‰ä¸€äº›ç¬¦å·ï¼š $E: K\\times X\\rightarrow Y$ is a PRP ( X = Y). Perms[X, Y]: the set of all one-to-one functions from X to Y ã€Xåˆ°Yçš„æ‰€æœ‰çš„ä¸€å¯¹ä¸€æ˜ å°„å…³ç³»ã€‘ $S_F={ E(k, \\cdot)}$ s.t. $k\\in K$ $\\subseteq$ Perms[X, Y] ã€ç¡®å®škå°±ç¡®å®šäº†ä¸€ç§Xåˆ°Yçš„ä¸€å¯¹ä¸€æ˜ å°„å…³ç³»ã€‘ åŒæ ·ï¼Œå½“ä»ŽPerms[X, Y]ä¸­éšæœºé‡‡æ ·çš„å‡½æ•°ä¸Žä»Ž $S_F$ ä¸­éšæœºé‡‡æ ·çš„å‡½æ•°ä¸å¯åŒºåˆ†æ—¶ï¼ŒPRPæ˜¯å®‰å…¨çš„ã€‚ Intuition: a PRP is secure if a random function in Perms[X, Y] is indistinguishable from a random function in $S_F$ . å®šä¹‰ä¸‹å›¾çš„æ¸¸æˆï¼š äº‘ä»ŽPerms[X, Y]ä¸­éšæœºé€‰å–ä¸€ä¸ªå‡½æ•° $\\pi$ ï¼Œå¹¶éšæœºé€‰æ‹©ä¸€ä¸ªkï¼Œå³ç¡®å®šäº†ä¸€ä¸ªåœ¨ $S_F$ ä¸­å‡½æ•°ã€‚ æ”»å‡»è€…å‘äº‘å¤šæ¬¡è¯¢é—®xçš„è®¡ç®—ç»“æžœï¼Œå¯¹äº‘æ¥è¯´ï¼Œå¦‚æžœæ˜¯b = 0ï¼Œä»–è¿”å›ž$\\pi(x)$ï¼Œå¦‚æžœb = 1ï¼Œä»–è¿”å›žF(k, x)ã€‚ å½“æ”»å‡»è€…è¾“å…¥ä»»æ„è¾“å…¥xæ—¶ï¼Œéƒ½æ— æ³•åŒºåˆ†è®¡ç®—ç»“æžœæ˜¯$\\pi(x)$è®¡ç®—çš„è¿˜æ˜¯F(k, x)è®¡ç®—çš„ï¼ŒPRPæ˜¯å®‰å…¨çš„ã€‚ æ³¨æ„ï¼šè¿™é‡Œæ˜¯å¯¹ä»»æ„è¾“å…¥ï¼Œå¦‚æžœå¯¹æŸä¸€ä¸ªè¾“å…¥ï¼Œæ”»å‡»è€…èƒ½åŒºåˆ†ï¼Œè¿™ä¸ªPRPéƒ½æ˜¯ä¸å®‰å…¨çš„ã€‚ PRF $\\rightarrow$ PRGPRFçš„ä¸€ä¸ªç®€å•åº”ç”¨æ˜¯æŠŠå®ƒå˜æˆä¸€ä¸ªPRGã€‚ Let $F: K\\times \\{0, 1\\}^n\\rightarrow \\{0, 1\\}^n$ be a secure PRF. The following $G: K\\rightarrow \\{0,1\\}^{nt}$ is a secure PRF: $$ G(k) = F(k, 0)\\mid\\mid F(k, 1) \\mid\\mid \\dots \\mid\\mid F(k, t-1) $$ è¿™æ˜¯ä¸€ç§è®¡æ•°å™¨æ¨¡å¼ï¼Œè®¡æ•°å™¨æ¨¡å¼æœ€æ˜¾è‘—çš„ä¼˜ç‚¹æ˜¯å¯å¹¶è¡Œè®¡ç®—ã€‚ æ­¤å¤–è¯¥PRGçš„å®‰å…¨æ€§æ¥è‡ªäºŽPRFçš„å®‰å…¨æ€§ï¼Œå³F(k, Â·)ä¸Žf(Â·)ä¸å¯åŒºåˆ†ã€‚ DESData Encryption Standardï¼ˆDESï¼‰ DESçš„æ ¸å¿ƒå°±æ˜¯Feistelç½‘ç»œç»“æž„ï¼š $R_i = f_i(R_{i-1})\\oplus L_{i-1}$ $L_i = R_{i-1}$ å¯¹äºŽç»™å®šçš„å‡½æ•°çš„è½®å‡½æ•° f1, â€¦, fd: $ \\{0,1\\}^n\\rightarrow \\{0,1\\}$ ï¼ŒFeistelç½‘ç»œç»“æž„éƒ½å¯ä»¥æž„é€ å‡º2n bitsçš„å¯é€†å‡½æ•°: $ F: \\{0,1\\}^{2n}\\rightarrow \\{0,1\\}^{2n}$ Claim: for all f1, â€¦, fd: $ \\{0,1\\}^n\\rightarrow \\{0,1\\}$ , Feistel network $ F: \\{0,1\\}^{2n}\\rightarrow \\{0,1\\}^{2n}$ is invertible. è¯æ˜Žï¼šæž„é€ å‡ºå¯é€†å‡½æ•°å³å¯ $R_{i-1} = L_i$ $L_{i-1}=R_i\\oplus f_i(L_i)$ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è§£å¯†ç”µè·¯ä¸­ï¼Œè½®å‡½æ•°çš„ä½¿ç”¨å¦‚ä¸‹å›¾æ˜¯é€†åºä½¿ç”¨çš„ã€‚ Luby-Rackoffâ€™85 Thmæ”¯æ’‘Feistel Networkå®‰å…¨æ€§çš„ä¸€ä¸ªç†è®ºæ˜¯Luby-Rackoffçš„ä¸€ä¸ªå®šç†ï¼Œè¿™å°†å®‰å…¨çš„PRFå˜æˆå®‰å…¨çš„PRPï¼Œè¿™ä¹Ÿæ˜¯å—å¯†ç ï¼ˆä½¿ç”¨Feistelç»“æž„çš„å—å¯†ç ï¼‰å®‰å…¨æ€§çš„æ”¯æ’‘ã€‚ Thm: $f:K\\times \\{0,1\\}^n\\rightarrow \\{0,1\\}^n$ a secure PRF then, 3-round Feistel $F: K^3\\times{0,1}^{2n}\\rightarrow {0,1}^{2n}$ a secure PRP. DES StructureDESçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ª16è½®çš„Feistelçš„ç½‘ç»œã€‚ DESçš„è¾“å…¥æ˜¯64 bitså—ï¼Œé€šè¿‡ä¸€ä¸ªåˆå§‹ç½®æ¢ï¼ˆIPï¼‰ï¼Œè¯¥ç½®æ¢å¹¶ä¸æ˜¯ä¸ºå®‰å…¨æ€§æœåŠ¡çš„ï¼Œåªæ˜¯DESæ ‡å‡†é‡Œçš„è¦æ±‚ã€‚ ç„¶åŽå†é€šè¿‡16è½®çš„Feistelç½‘ç»œï¼Œåœ¨Feistelç½‘ç»œä¸­ä¾æ¬¡è°ƒç”¨è½®å‡½æ•°ï¼Œè½®å‡½æ•°ä¸­çš„è½®å¯†é’¥æ˜¯é€šè¿‡åˆè¯•å¯†é’¥æ‰©å±•å¾—åˆ°çš„16ä¸ªä¸åŒå¯†é’¥ã€‚ æœ€åŽå†é€šè¿‡é€†ç½®æ¢å¾—åˆ°64 bitsçš„è¾“å‡ºå—ã€‚ The function F(ki, x)DESçš„è½®å‡½æ•°ç»“æž„å¦‚ä¸‹å›¾ï¼š è½®å‡½æ•°çš„è¾“å…¥æ˜¯32 bits xå’Œ48 bitsçš„å¯†é’¥kiï¼š 32 bits xé€šè¿‡expansion boxï¼Œæ‰©å±•ä¸º48 bitsã€‚æ‰©å±•æ–¹å¼åªæ˜¯å¤åˆ¶ä¸€äº›ä½ã€‚ æ‰©å±•åŽçš„xä¸Žè½®å¯†é’¥kiè¿›è¡Œå¼‚æˆ–è¿ç®—ã€‚ å¼‚æˆ–è¿ç®—åŽçš„ç»“æžœä¸º48 bitsï¼Œå†è¢«åˆ’åˆ†ä¸º8ç»„6 bitsï¼Œæ¯ä¸€ç»„éƒ½é€šè¿‡ä¸€ä¸ªS-boxï¼Œæ¯ä¸€ä¸ªS-boxå®žåˆ™æ˜¯ä¸€ä¸ªéžçº¿å½¢æ˜ å°„ ${0,1}^6\\rightarrow {0,1}^4$ ï¼Œåœ¨å®žçŽ°ä¸­å¾€å¾€ä½¿ç”¨æŸ¥è¡¨çš„å½¢å¼ä½¿ç”¨ã€‚ é€šè¿‡S-boxåŽï¼Œåˆå¾—åˆ°äº†32 bitsï¼Œæœ€åŽé€šè¿‡ä¸€ä¸ªç½®æ¢ï¼ˆPermutationï¼‰ï¼Œå¾—åˆ°è¯¥è½®çš„æœ€ç»ˆè¾“å‡ºã€‚ S-boxS-boxä¹Ÿæ˜¯DESä¸­æœ€é‡è¦çš„ä¸€éƒ¨åˆ†ï¼Œå› ä¸ºä»–æ˜¯DESä¸­å”¯ä¸€çš„éžçº¿æ€§éƒ¨åˆ†ï¼Œå®ƒä¿è¯äº†æ•´ä¸ªDESçš„éžçº¿å½¢æ˜ å°„å…³ç³»ã€‚ æ¥çœ‹ä¸€ä¸ªç³Ÿç³•çš„S-boxçš„ä¾‹å­ï¼š $\\mathrm{S}_{\\mathrm{i}}\\left(\\mathrm{x}_{1}, \\mathrm{x}_{2}, \\ldots, \\mathrm{x}_{6}\\right)=\\left(\\mathrm{x}_{2} \\oplus \\mathrm{x}_{3}, \\mathrm{x}_{1} \\oplus \\mathrm{x}_{4} \\oplus \\mathrm{x}_{5}, \\quad \\mathrm{x}_{1} \\oplus \\mathrm{x}_{6}, \\quad \\mathrm{x}_{2} \\oplus \\mathrm{x}_{3} \\oplus \\mathrm{x}_{6}\\right)$ ä¸Šå¼ä¹Ÿå¯ä»¥å†™åšï¼š $\\mathrm{S}{\\mathrm{i}}(\\mathbf{x})=\\mathrm{A}{\\mathrm{i}} \\cdot x(\\bmod 2)$ æˆ‘ä»¬ç§°ä¸Šé¢çš„Sç›’æ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚ è¿™ä¼šå¸¦æ¥ä»€ä¹ˆå½±å“å‘¢ï¼Ÿå¦‚æžœSç›’æ˜¯çº¿æ€§çš„ï¼Œé‚£ä¹ˆæ•´ä¸ªDESéƒ½ä¼šå˜æˆçº¿æ€§çš„ï¼Œå³å­˜åœ¨ä¸€ä¸ªç¡®å®šçš„äºŒå…ƒçŸ©é˜µ $\\mathbf{B}$ æ»¡è¶³ï¼š è€Œè¯¥çº¿æ€§å…³ç³»è¿˜æœ‰ä¸€ä¸ªç®€å•çš„æ£€éªŒæ–¹æ³•ï¼š Exhaustive Search AttacksDESçš„å¯†é’¥ç©ºé—´æ¯”è¾ƒå°ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨ç©·ä¸¾å¯†é’¥çš„æ–¹æ³•æ”»å‡»ã€‚ å¯¹äºŽå·²çŸ¥çš„ä¸€äº›æ˜Žæ–‡-å¯†æ–‡å¯¹ï¼Œæˆ‘ä»¬å¸Œæœ›ç©·ä¸¾æ”»å‡»èƒ½å¾—åˆ°å”¯ä¸€çš„ä¸€ä¸ªå¯†é’¥ï¼Œè€Œä¸æ˜¯è¯´å¯¹äºŽä¸€ä¸ªæ˜Žæ–‡-å¯†æ–‡å¯¹ï¼Œæœ‰å¤šä¸ªå¯†é’¥ï¼ˆæ˜ å°„å…³ç³»ï¼‰éƒ½ç¬¦åˆã€‚ å› æ­¤æ”¯æ’‘ç©·ä¸¾æ”»å‡»DESçš„ä¸€ä¸ªå®šç†æ˜¯ï¼š å‡è®¾DESæ˜¯ä¸€ä¸ªç†æƒ³çš„å¯†ç ï¼Œå³ $2^{56}$ ä¸ªå¯†é’¥å¯¹åº” $2^{56}$ ä¸ªä¸åŒçš„éšæœºå¯é€†å‡½æ•°ï¼Œé‚£ä¹ˆå¯¹äºŽä»»æ„çš„æ˜Žæ–‡-å¯†æ–‡å¯¹ï¼ˆ$c = \\mathrm{DES}(k,m)$ï¼‰ ï¼Œæœ€å¤šåªæœ‰ä¸€ä¸ªå¯†é’¥çš„æ¦‚çŽ‡å¤§äºŽ 99.5%ã€‚ è¯æ˜Žï¼š å‡è®¾å­˜åœ¨ä¸€ä¸ªå¯†é’¥kâ€™ï¼Œä¸æ˜¯çœŸæ­£çš„å¯†é’¥kï¼Œä½†æ»¡è¶³ $c = \\mathrm{DES}(kâ€™, m)=\\mathrm{DES}(k,m)$ ã€‚ è¿™æ ·çš„æ¦‚çŽ‡æ˜¯å°äºŽ æ‰€æœ‰ $2^{56}$ ä¸ªå¯†é’¥éƒ½æ»¡è¶³è¯¥æ˜Žæ–‡-å¯†æ–‡å¯¹çš„æ¦‚çŽ‡ã€‚ å› æ­¤ï¼Œå¦‚æžœå·²çŸ¥ä¸¤ä¸ªæ˜Žæ–‡-å¯†æ–‡å¯¹ï¼Œé‚£ä¹ˆé€šè¿‡è¿™ä¸¤ä¸ªDES pairså°±å¯ä»¥ä»¥ $1-1/2^{71}$ çš„æ¦‚çŽ‡ç¡®å®šå‡ºæ­£ç¡®çš„å¯†é’¥ã€‚ åŒæ ·ï¼Œå¯¹äºŽAES-128æ¥è¯´ï¼Œé€šè¿‡ä¸¤ä¸ªAES pairså°±å¯ä»¥ä»¥ $1-1/2^{128}$ çš„æ¦‚çŽ‡ç¡®å®šå‡ºæ­£ç¡®çš„å¯†é’¥ã€‚ Strength DES against ex. searchå› ä¸ºDESçš„å¯†é’¥ç©ºé—´å®žåœ¨æ˜¯å¤ªå°äº†ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªDES challengeçš„æŒ‘æˆ˜ï¼Œéšç€è®¡ç®—èƒ½åŠ›çš„å‘å±•ï¼Œç ´è§£DESçš„é€Ÿåº¦è¶Šæ¥è¶Šå¿«ã€‚ 1: Triple-DESä¸€ä¸ªæŠµå¾¡DESç©·ä¸¾æ”»å‡»çš„ç®—æ³•æ˜¯Triple- DESã€‚ Let $\\mathrm{E}:\\mathrm{K}\\times \\mathrm{M}\\rightarrow \\mathrm{M}$ be a block cipher. Define $\\mathrm{3E}: \\mathrm{K}^3\\times \\mathrm{M}\\rightarrow \\mathrm{M}$ as $\\mathrm{3E}((\\mathrm{k_1},\\mathrm{k_2},\\mathrm{k_3}),\\mathrm{m})=\\mathrm{E}(\\mathrm{k_1},\\mathrm{D}(\\mathrm{k_2},\\mathrm{E}(\\mathrm{k_3},\\mathrm{m})))$ triple- DESä¸ºä»€ä¹ˆæ˜¯Eã€Dã€Eçš„è¿ç®—é¡ºåºï¼Œè€Œä¸æ˜¯Eã€Eã€Eçš„è¿ç®—é¡ºåºï¼Ÿ å› ä¸ºå¯¹äºŽTriple- DESæ¥è¯´ï¼Œå¦‚æžœ $k_1=k_2=k_3$ ï¼Œå°±å˜æˆäº†single DESï¼ˆè™½ç„¶åœ¨è¿ç®—é€Ÿåº¦ä¸Šï¼Œæ²¡æœ‰ä»€ä¹ˆå¿…è¦ï¼‰ Triple- DESçš„å¯†é’¥ç©ºé—´å¤§å°æ˜¯ $3\\times 56=168$ bitsï¼Œåœ¨è¿ç®—ä¸Šä¼šæ¯”DESæ…¢ä¸‰å€ï¼Œä½†æ”»å‡»triple- DESåªéœ€è¦ $O(2^{118})$ çš„æ—¶é—´å¤æ‚åº¦ã€‚ï¼ˆä¸‹æ–‡ä»‹ç»çš„ä¸­é—´ç›¸é‡æ”»å‡»ï¼‰ Meet in the middle attackä¸ºä»€ä¹ˆä¸ç”¨double DES? å› ä¸ºæœ‰ä¸€ç§ä¸­é—´ç›¸é‡æ”»å‡»(meet in the middle attack)ï¼Œè®©æ”»å‡»double DESçš„æ—¶é—´å’Œç©·ä¸¾æ”»å‡»single DESçš„æ—¶é—´å·®ä¸å¤šã€‚ å¦‚æžœæˆ‘ä»¬ä½¿ç”¨double DESï¼Œ$\\mathrm{2E}((\\mathrm{k_1},\\mathrm{k_2}),\\mathrm{m})=\\mathrm{E}(\\mathrm{k_1},\\mathrm{E}(\\mathrm{k_2},\\mathrm{m}))$ double DESçš„å¯†é’¥ç©ºé—´ä¸º112 bitsï¼Œä½†åœ¨ä¸­é—´ç›¸é‡æ”»å‡»ä¸‹ï¼Œç©·ä¸¾èŒƒå›´å¯ä»¥å°‘ä¸€åŠæ¯”ç‰¹ã€‚ ä¸­é—´ç›¸é‡æ”»å‡»çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ° $(k_1, k_2)$ æ»¡è¶³ $E(k_1, E(k_2, M))=C$ ï¼Œå³æ»¡è¶³ï¼š$E(k_2,M)=D(k_1, C)$ ã€‚ Attack: M = (m1, â€¦, m10), C = (c1, â€¦, c10) build table and sort on 2nd column. ä»Ždouble DES çš„å·¦è¾¹çš„æ˜Žæ–‡å¼€å§‹ï¼Œæžšä¸¾ $k_2$ ï¼Œè®¡ç®— $E(k^i, M)$ çš„å€¼ï¼Œæœ€åŽæŒ‰ç…§ $E(k^i, M)$ çš„å€¼æŽ’åºã€‚ for all $k\\in {0,1}^{56} do:$ test if $D(k,C)$ is in 2nd column. ä»Ždouble DESçš„å³è¾¹çš„å¯†æ–‡å¼€å§‹æ£€æµ‹ï¼Œæ˜¯å¦ $D(k,C)$ å­˜åœ¨åœ¨ä¸Šè¿°è¡¨ä¸­ï¼Œå¦‚æžœ $E(k^i, M)=D(k,C)$ ï¼Œé‚£ä¹ˆå°±å¾—åˆ°äº† $(k^i, k)=(k_2, k_1)$ åˆ†æžä¸€ä¸‹æ”»å‡»æ—¶é—´ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯å»ºè¡¨æŽ’åºï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯è®¡ç®—äºŒåˆ†æŸ¥è¡¨çš„æ—¶é—´ã€‚ Time = $2^{56} \\log \\left(2^{56}\\right)+2^{56} \\log \\left(2^{56}\\right)&lt;2^{63}&lt;&lt;2^{112}, \\quad \\text { space } \\approx 2^{56}$ åŒæ ·çš„ï¼Œä¸­é—´ç›¸é‡æ”»å‡»åŒæ ·å¯ä»¥åº”ç”¨äºŽ3DESä¸­ï¼š$\\text{Time}=2^{118},\\quad \\text{space}\\approx 2^{56}$ 2: DESX$\\mathrm{E}: \\mathrm{K} \\times{0,1}^{n} \\rightarrow{0,1}^{\\mathrm{n}} \\text { a block cipher }$ Define EX as $\\operatorname{EX}\\left(\\left(\\mathrm{k}{1}, \\mathrm{k}{2}, \\mathrm{k}{3}\\right), \\mathrm{m}\\right)=\\mathrm{k}{1} \\oplus \\mathrm{E}\\left(\\mathrm{k}{2}, \\mathrm{~m} \\oplus \\mathrm{k}{3}\\right)$ å¯¹äºŽDESXæ¥è¯´ï¼Œå¯†é’¥ç©ºé—´é•¿åº¦ä¸º: 64+56+64 = 184 bitsï¼Œä½†åŒæ ·çš„é€šè¿‡ä¸­é—´ç›¸é‡æ”»å‡»ï¼Œtime = $2^{64+56}=2^{120}$ . éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨å®žé™…ä½¿ç”¨ä¸­ï¼Œæœ‰äººä½¿ç”¨ $\\mathrm{k}{1} \\oplus \\mathrm{E}\\left(\\mathrm{k}{2}, \\mathrm{m}\\right) \\text { and } \\mathrm{E}\\left(\\mathrm{k}_{2}, \\mathrm{m} \\oplus \\mathrm{k}_{1}\\right)$ çš„è®¾è®¡ï¼Œè¿™æ ·çš„è®¾è®¡å’ŒåŽŸå§‹DESä¸€æ ·æ˜“æ”¶ex. searchçš„æ”»å‡»ã€‚ï¼ˆé€šè¿‡ä¸­é—´ç›¸é‡æ”»å‡»ï¼‰ More attacks on block ciphersAttacks on the implementationå¯¹äºŽDESï¼Œè¿˜æœ‰ä¸€äº›é’ˆå¯¹å®žçŽ°ä¸Šçš„æ”»å‡»ã€‚ ç¬¬ä¸€ç§æ˜¯ä¾§ä¿¡é“æ”»å‡»ï¼ˆSide channel attacksï¼‰ï¼Œé€šè¿‡æµ‹é‡åŠ å¯†è§£å¯†æ—¶çš„æ—¶é—´å·®å¼‚ã€èƒ½æºæ¶ˆè€—å·®å¼‚æ¥å¾—åˆ°ä¸€äº›é¢å¤–çš„ä¿¡æ¯ã€‚ å¦ä¸€ç§æ˜¯Fault attacksï¼Œé€šè¿‡æœ€åŽä¸€è½®çš„è®¡ç®—é”™è¯¯å¯èƒ½ä¼šæš´éœ²å¯†é’¥kã€‚ å¯¹äºŽå®žçŽ°ä¸Šçš„æ”»å‡»çš„ç»éªŒæ•™è®­æ˜¯ï¼šé™¤äº†ä¸è¦è‡ªå·±è®¾è®¡å¯†ç å­¦ç®—æ³•ï¼Œç”šè‡³éƒ½ä¸è¦è‡ªå·±åŽ»å®žçŽ°ã€‚ Linear and differential attacksè¯¥æ”»å‡»çš„ç›®æ ‡æ˜¯ï¼Œç»™å®šè®¸å¤šinp/out pairsï¼Œå¸Œæœ›èƒ½åœ¨æ¯” $2^{56}$ æ›´å°‘çš„æ—¶é—´å†…æ”»å‡»æˆåŠŸã€‚ Linear cryptanalysis (overview): let c = DES(k, m), suppose for random k, m: ç¬¬ä¸€éƒ¨åˆ†æ˜¯msg bitsçš„å­é›†åˆï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯å¯†æ–‡bits çš„å­é›†åˆï¼Œç¬¬ä¸‰éƒ¨åˆ†æ˜¯key bitsçš„å­é›†åˆã€‚ å¦‚æžœè¿™äº›éƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆè¿™ä¸ªæ¦‚çŽ‡åº”è¯¥ä¸º1/2ï¼Œä½†å› ä¸ºæœ‰slightly çº¿æ€§çš„å…³ç³»ï¼Œæ‰€ä»¥è¿™ä¸ªæ¦‚çŽ‡ä¼šæœ‰ä¸€ç‚¹biasã€‚å¯¹äºŽDESæ¥è¯´ï¼Œå› ä¸ºç¬¬äº”ä¸ªS-boxçš„ä¸€ç‚¹bugï¼Œ$\\varepsilon=1/2^{21}\\approx 0.0000000477$ . Thm: åŸºäºŽä¸Šè¿°çº¿æ€§çš„å…³ç³»ï¼Œè¯¥å®šç†è¡¨ç¤ºå¦‚æžœç»™äº† $1/ \\varepsilon^2$ ä¸ªéšæœºæ˜Žæ–‡å¯†æ–‡å¯¹ï¼Œå¤§éƒ¨åˆ†éƒ½æ»¡è¶³ä¸Šå¼æ¯”ç‰¹é›†åˆçš„çº¿æ€§å…³ç³»ã€‚ æ‰€ä»¥ï¼Œå¯¹äºŽDESæ¥è¯´ï¼Œ$\\varepsilon=1/2^{21}$ ï¼Œé€šè¿‡ $2^{42}$ ä¸ªinp/out pairsèƒ½åœ¨ $O(2^{42})$ æ—¶é—´å†…å¾—åˆ° $k[l_1,â€¦,k_u]$ æ¯”ç‰¹é›†å¼‚æˆ–çš„ç»“æžœã€‚ ä½†å…¶å®žä¸æ­¢æ˜¯xor of key bitsï¼Œå®žé™…ä¸Š: can find 14ä¸ªkey bits in time $2^{42}$ ã€‚ï¼ˆtodo) è¿™æ ·å°±å°†ç©·ä¸¾æ”»å‡»çš„å¤æ‚åº¦é™ä½Žåˆ°56-14=42 bits in time $2^{42}$ Total attack time $\\approx 2^{43}&lt;&lt;2^{56}$ with $2^{42}$ random inp/out pairs. Lesson: A tiny bit of linearly in $S_5$ lead to a $2^{42}$ time attack. So donâ€™t design ciphers yourself. Quantum attackså¦ä¸€ä¸ªé’ˆå¯¹ç©·ä¸¾æ”»å‡»çš„è§£å†³æ–¹æ³•æ˜¯é‡å­æ”»å‡»ã€‚ å¯¹äºŽä¸€ä¸ªä¸€èˆ¬çš„æœç´¢é—®é¢˜ï¼š ä¸€èˆ¬è®¡ç®—æœºè§£å†³è¯¥ç±»é—®é¢˜çš„æœ€ä¼˜æ—¶é—´æ˜¯ $O(|X|)$ ï¼Œè€Œé‡å­è®¡ç®—æœºè§£å†³è¯¥ç±»é—®é¢˜çš„æœ€ä¼˜æ—¶é—´æ˜¯ $O(|X|^{1/2})$ ã€‚ ä¸è¿‡ï¼Œé‡å­è®¡ç®—æœºæ˜¯å¦è¢«å®žçŽ°è¿˜æ˜¯ä¸€ä¸ªç–‘é—®ã€‚ ç”¨é‡å­è®¡ç®—æœºæ¥ç©·ä¸¾å¯†é’¥ç©ºé—´ï¼Œåªéœ€è¦ $O(|K|^{1/2})$ çš„æ—¶é—´å³å¯æ‰¾åˆ°å¯†é’¥ã€‚ AESAESçš„å‘å±•è¿‡ç¨‹å¦‚å›¾ï¼š AESä¸æ˜¯Feistelç½‘ç»œç»“æž„çš„å¯†ç ï¼Œä»–æ˜¯ä¸€ä¸ªæ›¿æ¢-ç½®æ¢ç±»ç½‘ç»œ(Subs-Perm network): AES-128 schematicAES-128çš„ç»“æž„å¦‚ä¸‹å›¾ï¼š è¾“å…¥å—æ˜¯16 bytesçš„4*4çŸ©é˜µï¼Œé€šè¿‡10è½®è¿ç®—å¾—åˆ°æœ€åŽçš„è¾“å‡ºå—ã€‚ æ¯ä¸€è½®éƒ½åŒ…æ‹¬åŒæ ·çš„æ“ä½œï¼ˆé™¤äº†æœ€åŽä¸€è½®ä¸å¤ªä¸€æ ·ï¼‰ï¼Œå¯†é’¥åŠ ï¼Œå­—èŠ‚æ›¿æ¢(ByteSub)ï¼Œè¡Œç§»ä½(ShiftRow)ï¼Œåˆ—æ··æ·†(MixColumn)ã€‚å…¶ä¸­æ¯ä¸€è½®çš„è½®å¯†é’¥æ˜¯åˆå§‹16 bytesçš„å¯†é’¥é€šè¿‡å¯†é’¥æ‰©å±•å¾—åˆ°çš„ã€‚ è½®å¯†é’¥ä¸­çš„å­—èŠ‚æ›¿æ¢å¯ä»¥é€šè¿‡é¢„è®¡ç®—æŸ¥è¡¨çš„å½¢å¼ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç›´æŽ¥è¿ç®—çš„å½¢å¼ã€‚å› æ­¤åœ¨å®žé™…åº”ç”¨ä¸­å¯ä»¥æ ¹æ®éœ€æ±‚åšå‡ºæƒè¡¡ã€‚ AES in hardwareä¸ºäº†åŠ å¿«AESçš„è¿ç®—é€Ÿåº¦ï¼Œå„å…¬å¸æœ‰è®¾è®¡ç›¸å…³æŒ‡ä»¤ã€‚ AES instructions in Intel Westmere: aesenc, aesenclast: æ‰§è¡ŒAESä¸­çš„ä¸€è½®è¿ç®— 128-bit registers: xmm1=state, xmm2=round key aesenc xmm1, xmm2 ï¼šæ‰§è¡Œä¸€è½®è¿ç®—ï¼Œå¹¶æŠŠè®¡ç®—ç»“æžœæ”¾åœ¨xmm1ä¸­ aeskeygenassist: æ‰§è¡ŒAESçš„å¯†é’¥æ‰©å±•ã€‚ åŒæ ·çš„ç¡¬ä»¶ä¸Šï¼Œç›¸å¯¹äºŽOpenSSLçš„å®žçŽ°ï¼Œå¯ä»¥å¿«14å€ã€‚ åŒæ ·ï¼Œåœ¨ AMD Bulldozerä¸Šä¹Ÿæœ‰ç±»ä¼¼çš„æŒ‡ä»¤è®¾è®¡ã€‚ AttacksAESä¹Ÿå­˜åœ¨æ¯”æš´åŠ›æœç´¢å¯†é’¥ç©ºé—´æ›´å¥½çš„ç®—æ³•ã€‚ Best key recovery attack: four times better than ex. search [BKRâ€™11] Related key apack on AES-256: Given $2^{99}$ inp/out pairs from four related keys in AES-256 can recover keys in time â‰ˆ299[BKâ€™09] Block ciphers from PRGsæˆ‘ä»¬èƒ½å¦ä»ŽPRGä¸­æž„å»ºPRFï¼Ÿ ç­”æ¡ˆæ˜¯ï¼šæ˜¯çš„ã€‚ Let G: $K\\rightarrow K^2$ be a secure PRG. Define 1-bit PRF F: $K\\times {0,1} \\rightarrow K$ as$$F(k,x\\in {0,1})=G(k)[x]$$æ ¹æ®å®šç†ï¼Œå¦‚æžœGæ˜¯ä¸€ä¸ªsecure PRG,é‚£ä¹ˆFå°±æ˜¯ä¸€ä¸ªsecure PRFã€‚ Thm: If G is a secure PRG then F is a secure PRF. åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™æ ·çš„æ–¹å¼æž„å»ºæ›´å¤§åŸŸçš„PRFã€‚ å¯¹å¾—åˆ°çš„ç»“æžœå†åšä¸€æ¬¡Gè¿ç®—ï¼Œå¾—åˆ°çš„G1å°±æ˜¯ä¸€ä¸ª2-bit PRFï¼š Proof: G1 is a secure PRG è¯æ˜Žæ€è·¯å°±æ˜¯ï¼Œå› ä¸ºGæ˜¯å®‰å…¨çš„PRGï¼Œæ‰€ä»¥ké€šè¿‡Gè¿ç®—å¾—åˆ°çš„ç»“æžœä¸Žéšæœºä¸²ä¸å¯åŒºåˆ†ï¼ŒåŒæ ·çš„ï¼Œæœ€åŽæ‰©å±•å‡ºçš„åœ¨ $K^4$ ä¸­çš„å­—ç¬¦ä¸²ä¸Žéšæœºä¸²æ˜¯ä¸å¯åŒºåˆ†çš„ã€‚ åŒæ ·çš„ï¼Œå¯ä»¥æ‰©å±•å‡º3-bit çš„PRFï¼Œå¯¹äºŽç»™å®šçš„æ¯”ç‰¹è¾“å…¥ï¼Œä¹Ÿæ˜¯æ˜“äºŽè®¡ç®—çš„ï¼š è€Œæ‰©å±•å‡ºæ›´å¤šbitæ—¶ï¼Œå°±å¾—åˆ°äº†GGM PRFã€‚ GGM PRFå°±æ˜¯é€šè¿‡ä¸Šè¿°çš„æ‰©å±•æ–¹å¼å¾—åˆ° ${0,1}^n$ åŸŸä¸Šçš„PRFï¼š è¿™ç§æ–¹å¼ï¼ŒPRFçš„å®‰å…¨æ€§ä¾èµ–äºŽPRGçš„å®‰å…¨æ€§ã€‚ ä½†ç”±äºŽè¿™æ ·çš„æ–¹å¼è¿è¡Œæ•ˆçŽ‡è¾ƒä½Žï¼Œæ‰€ä»¥åœ¨å®žé™…åº”ç”¨ä¸­å¾ˆå°‘ã€‚ è¯´åˆ°çŽ°åœ¨ï¼Œæˆ‘ä»¬å¥½åƒè¿˜æ²¡æœ‰æåˆ°å¦‚ä½•ä»ŽPRGä¸­æž„å»ºå—å¯†ç ï¼Œå› ä¸ºå—å¯†ç æ—¶PRPsã€‚ å¦‚æžœè®°æ€§æ¯”è¾ƒå¥½çš„è¯»è€…åº”è¯¥è¿˜è®°å¾—æˆ‘ä»¬åˆšåˆšæåˆ°çš„Luby-Rackoffå®šç†ï¼Œå°†ä¸€ä¸ªå®‰å…¨çš„PRFä½œä¸ºFeistelç½‘ç»œä¸­çš„è½®å‡½æ•°ï¼Œ3-round Feistelå°±å¯ä»¥æž„å»ºä¸€ä¸ªå®‰å…¨çš„PRPï¼Œå³å®‰å…¨çš„å—å¯†ç ã€‚ Thm: $f:K\\times \\{0,1\\}^n\\rightarrow \\{0,1\\}^n$ a secure PRF then, 3-round Feistel $F: K^3\\times{0,1}^{2n}\\rightarrow {0,1}^{2n}$ a secure PRP.","link":"/2021/09/08/stanford-crypto-blockcipher1/"},{"title":"ã€ŒCryptography-Bonehã€:Block Cipher 2","text":"ä½œä¸ºBlockCipherçš„ç¬¬äºŒç¯‡æ–‡ç« ã€‚ç¬¬ä¸€éƒ¨åˆ†ä»‹ç»äº†å—å¯†ç ä¸­çš„æŠ½è±¡æ¦‚å¿µPRFå’ŒPRPçš„å®‰å…¨å®šä¹‰ã€‚ç¬¬äºŒéƒ¨åˆ†ä»‹ç»äº†ä¸¤ä¸ªæ¦‚å¿µï¼Œä¸€ä¸ªæ˜¯æŠµæŠ—one-time keyçš„è¯­ä¹‰å®‰å…¨ï¼Œå¦ä¸€ä¸ªæ˜¯æŠµæŠ—many-time key(CPA)çš„è¯­ä¹‰å®‰å…¨ã€‚ åœ¨one-time keyä¸­ï¼Œæ¯æ¡æ¶ˆæ¯éƒ½ä½¿ç”¨æ–°çš„å¯†é’¥ï¼Œç±»ä¼¼äºŽæµå¯†ç ä¸­çš„OTPã€‚ä»‹ç»äº†ä¸èƒ½æŠµæŠ—CPAçš„ECBæ¨¡å¼ï¼Œè¿˜é˜è¿°äº†èƒ½æŠµæŠ—CPAçš„det. CTRæ¨¡å¼ã€‚ åœ¨many-time keyä¸­ï¼ŒåŒä¸€æ¡å¯†é’¥å¯ä»¥ç”¨äºŽåŠ å¯†å¤šæ¡æ¶ˆæ¯ï¼Œæ”»å‡»è€…å¯ä»¥è½»æ˜“å…·å¤‡CPAèƒ½åŠ›ï¼Œæ–‡ä¸­è¯´æ˜Žäº†å¦‚æžœç¡®å®šæ€§çš„åŠ å¯†ç®—æ³•ï¼Œåˆ™ä¸èƒ½æŠµæŠ—CPAï¼Œè€Œrandom IVæˆ–è€…unique nonceçš„æ–¹å¼åˆ™å¯ä»¥æŠµæŠ—CPAã€‚ PRPs and PRFsåœ¨å‰æ–‡ä¸­ï¼Œæœ‰æåˆ°PRPå’ŒPRFçš„ç›´è§‰æ€§çš„å®šä¹‰ã€‚çŽ°åœ¨æˆ‘ä»¬æ¥çœ‹ä»–ä»¬çš„å®‰å…¨å®šä¹‰ã€‚ PRFï¼šå¦‚ä¸‹å›¾å®šä¹‰è¿™æ ·ä¸€ä¸ªæ¸¸æˆï¼Œæœ‰æŒ‘æˆ˜è€…å’Œæ”»å‡»è€…ï¼ŒæŒ‘æˆ˜è€…æœ‰ä¸¤ä¸ªå®žéªŒEXP(b)ï¼Œ$b=0,1$ã€‚ åœ¨EXP(0)ä¸­ï¼ŒæŒ‘æˆ˜è€…éšæœºé€‰æ‹©ä¸€ä¸ªå¯†é’¥Kï¼Œå³ç¡®å®šäº†ä¸€ä¸ªä¼ªéšæœºå‡½æ•° $f \\leftarrow \\mathrm{F(k, Â·)}$ ï¼›è€Œåœ¨EXP(1)ä¸­ï¼ŒæŒ‘æˆ˜è€…éšæœºé€‰æ‹©äº†ä¸€ä¸ªå‡½æ•° $f\\leftarrow \\mathrm{Funs[X, Y]} $ ã€‚ æ”»å‡»è€…ä¼šå‘æŒ‘æˆ˜è€…å‘å‡ºè®¸å¤šè¾“å…¥è¯¢é—® $x_1,x_2,â€¦,x_q$ ï¼ŒæŒ‘æˆ˜è€…å¯¹äºŽæ”»å‡»è€…å‘å‡ºçš„è¯¢é—®ï¼Œä¼šéšæœºé€‰æ‹© $b=0,1$ ï¼Œç”¨ $f(x_1),f(x_2), â€¦,f(x_q)$ å“åº”ã€‚ æ”»å‡»è€…å¯¹äºŽæ”¶åˆ°çš„å›žå¤ï¼Œä»–ä¼šè¾“å‡º $bâ€™$ ï¼Œè¡¨ç¤ºæ”»å‡»è€…è§†è§’ä¸‹ï¼Œä»–è®¤ä¸ºè¯¥è¾“å‡ºæ˜¯ç”¨ä¼ªéšæœºå‡½æ•°è®¡ç®—çš„(b=0)ï¼Œè¿˜æ˜¯ç”¨éšæœºå‡½æ•°è®¡ç®—çš„(b=1)ï¼›å®šä¹‰å®žéªŒçš„è¾“å‡ºä¸ºEXP(b)=bâ€™â€‹ ã€‚ å› æ­¤å®‰å…¨å®šä¹‰å¦‚ä¸‹ï¼Œæ”»å‡»è€…åŒºåˆ†è¾“å‡ºæ˜¯ä¼ªéšæœºå‡½æ•°è®¡ç®—çš„è¿˜æ˜¯éšæœºå‡½æ•°è®¡ç®—çš„ä¼˜åŠ¿æ˜¯negã€‚ Def(PRF): F is a secure PRF if for all â€œefficientâ€ A:$$\\operatorname{Adv}_{\\mathrm{PRF}}[\\mathrm{A}, \\mathrm{F}]:=\\mid \\operatorname{Pr}[\\operatorname{EXP}(0)=1]-\\operatorname{Pr}[\\operatorname{EXP}(1)=1] \\quad \\text{is neg.}$$ PRP:åŒç†ï¼Œå¯¹PRPæ¥è¯´ï¼Œå®šä¹‰çš„æ¸¸æˆå¦‚ä¸‹ï¼š åŒç†ï¼ŒPRPçš„å®‰å…¨å®šä¹‰ä¸ºï¼Œæ”»å‡»è€…åŒºåˆ†æ˜¯ä¼ªéšæœºç½®æ¢è®¡ç®—çš„è¿˜æ˜¯éšæœºç½®æ¢è®¡ç®—çš„ä¼˜åŠ¿ä¸ºnegã€‚ Def(PRP): E is a secure PRP if for all â€œefficientâ€ A:$$\\operatorname{Adv}_{\\mathrm{PRP}}[\\mathrm{A}, \\mathrm{E}]:=\\mid \\operatorname{Pr}[\\operatorname{EXP}(0)=1]-\\operatorname{Pr}[\\operatorname{EXP}(1)=1] \\quad \\text{is neg.}$$ ä¾‹é¢˜ï¼š Let X = {0,1}. Perms[X] contains two functions. Consider the following PRP: key space K={0,1}, PRP defined as:$\\mathrm{E(k,x)=x\\oplus k}$ Is this a secure PRP? ç­”æ¡ˆï¼šYes å½“X = {0,1}ä¸‹ï¼Œéšæœºç½®æ¢å¦‚ä¸‹ï¼š è€Œé€‰æ‹©Kçš„åˆ†å¸ƒå’Œé€‰æ‹©éšæœºç½®æ¢çš„åˆ†å¸ƒç›¸åŒï¼Œæ‰€ä»¥æ”»å‡»è€…ä¸èƒ½åŒºåˆ†ã€‚ è€Œå¦‚æžœé—®æ˜¯å¦æ˜¯secure PRFï¼Œç­”æ¡ˆåˆ™æ˜¯ï¼šNoã€‚ æ”»å‡»è€…å¯ä»¥å®šä¹‰è¿™æ ·ä¸€ä¸ªç»Ÿè®¡ç®—æ³•ï¼Œå¦‚æžœf(0)=f(1)åˆ™è¾“å‡º1ï¼Œå¦åˆ™è¾“å‡º0ã€‚ åˆ™æ”»å‡»è€…æœ‰ $\\operatorname{Adv}_{\\mathrm{PRF}}[\\mathrm{A}, \\mathrm{E}]:=ï½œ0-1/2ï½œ=1/2$ çš„ä¼˜åŠ¿æ”»å‡»æˆåŠŸã€‚ çŽ°å®žä¸­å¸¸è§å®‰å…¨PRPsï¼Œæœ‰3DES, AESç­‰ï¼Œæ¯”å¦‚AES-128çš„ç½®æ¢æ˜¯ï¼š$\\mathrm{K\\times X\\rightarrow X}$ where $\\mathrm{K=X={0,1}^{128}}$ ã€‚ å¦‚æžœå¯¹AESçš„å®‰å…¨æ€§å®šä¹‰çš„æ›´å…·ä½“ï¼Œåˆ™æ˜¯æ‰€æœ‰ $2^{80}$ çš„ç®—æ³•Aå¯¹AESæ”»å‡»çš„ä¼˜åŠ¿ $\\mathrm{Adv_{PRP}[A,AES]}&lt;2^{-40}$ . PRF Switching LemmaAny Secure PRP is also a secure PRF, if |X| is sufficiently large. ã€å¯¹äºŽä»»æ„çš„å®‰å…¨PRPï¼Œå½“å®šä¹‰åŸŸ $X$ è¶³å¤Ÿå¤§æ—¶ï¼Œå®ƒä¹Ÿæ˜¯å®‰å…¨PRFï¼Œæ‰€ä»¥AESä¹Ÿæ˜¯å®‰å…¨çš„PRFã€‚ã€‘ è¯æ˜Žå¦‚ä¸‹ï¼š Lemma: Let E be a PRP over (K, X). Then for any q-query adversay A:$$|\\mathrm{Adv_{PRF}[A,E]-\\mathrm{Adv_{PRP}[A, E]}}|&lt;\\mathrm{q^2/2|X|}$$è¯¥å¼•ç†çš„ç»“æžœæ˜¯ï¼Œå½“ ï½œXï½œå¾ˆå¤§æ—¶ï¼Œ$\\mathrm{q^2/2|X|}$ æ˜¯neg.ï¼Œå› æ­¤å·¦éƒ¨åˆ†ä¹Ÿæ˜¯neg.ã€‚ è€Œå½“Eæ˜¯å®‰å…¨PRPæ—¶ï¼Œ$\\mathrm{Adv_{PRP}[A, E]}$ æ˜¯neg.ï¼Œå› æ­¤ $\\mathrm{Adv_{PRF}[A,E]}$ ä¹Ÿæ˜¯neg.ï¼Œæ‰€ä»¥Eä¹Ÿæ˜¯å®‰å…¨çš„PRFã€‚ è¯æ¯•ã€‚ One Time Keyä½¿ç”¨å—å¯†ç æœ‰å¾ˆå¤šç§æ–¹å¼ï¼Œè€Œæˆ‘ä»¬çš„ç›®çš„å°±æ˜¯å¸Œæœ›èƒ½ä»Žå®‰å…¨çš„PRPæž„å»ºå®‰å…¨çš„åŠ å¯†æ–¹å¼ã€‚ æœ¬å°èŠ‚ä¸»è¦é˜è¿°one-time keysçš„æ–¹å¼ï¼Œå³æ¯æ¡æ¶ˆæ¯éƒ½ä½¿ç”¨æ–°çš„å¯†é’¥ï¼Œæ‰€ä»¥æ”»å‡»è€…åªèƒ½å¾—åˆ°è¿™ä¸€æ¡æœ‰å…³è¯¥å¯†é’¥çš„å¯†æ–‡ï¼Œå³ä¸€æ¬¡ä¸€å¯†çš„å”¯å¯†æ–‡æ”»å‡»èƒ½åŠ›ï¼Œä½†æ”»å‡»è€…çš„æ”»å‡»ç›®æ ‡æ˜¯å¸Œæœ›èƒ½ä»Žå¯†æ–‡ä¸­å¾—åˆ°å…³äºŽæ˜Žæ–‡çš„ä¿¡æ¯ï¼Œå³ç ´åè¯­ä¹‰å®‰å…¨ã€‚ one-time key: Adversaryâ€™s power: Adv sees only one ciphertext (one-time key) Adversaryâ€™s goal: Learn info about PT from CT (semantic security) åœ¨ä½¿ç”¨AESçš„ECB(Electronic Code Book)æ–¹å¼ï¼Œå½“æ¶ˆæ¯ä¸­çš„ä¸åŒå— $\\mathrm{m_1=m_2}$ æ—¶ï¼Œ$\\mathrm{c_1=c_2}$ ï¼Œæ”»å‡»è€…èƒ½ä»Žå¯†æ–‡ä¸­å¾—åˆ°ä¸€äº›ä»–ä¸åº”è¯¥å¾—åˆ°çš„ä¿¡æ¯ã€‚ æ¯”å¦‚ç”¨ECBçš„æ–¹å¼åŠ å¯†å›¾ç‰‡ï¼Œå¯ä»¥çœ‹åˆ°åŠ å¯†åŽçš„å›¾ç‰‡ä¹Ÿæ˜¯å¯è§çš„ï¼š Semantic Security(one-time key)è¿™é‡Œçš„one-time keyæ–¹å¼å¯¹äºŽæ”»å‡»è€…æ¥è¯´ï¼Œå°±æ˜¯æ”»å‡»è€…åªèƒ½çœ‹åˆ°ä¸€æ¡ä¸Žå¯†é’¥ç›¸å…³çš„å¯†æ–‡ã€‚ å®šä¹‰å¦‚ä¸‹ä¸¤ä¸ªå®žéªŒï¼ŒæŒ‘æˆ˜è€…éšæœºé€‰æ‹©ä¸€ä¸ªå¯†é’¥ï¼Œä»¥æ­¤ç¡®å®šç”¨äºŽåŠ å¯†çš„ä¼ªéšæœºç½®æ¢ï¼Œæ”»å‡»è€…å‘é€ä»»æ„ä¸¤ä¸ªæ¶ˆæ¯ $\\mathrm{m_0, m_1}$ ï¼Œé•¿åº¦ç›¸åŒã€‚ åœ¨EXP(0)ä¸­ï¼ŒæŒ‘æˆ˜è€…å¯¹ $\\mathrm{m_0}$ åŠ å¯†ï¼›è€Œåœ¨EXP(1)ä¸­ï¼ŒæŒ‘æˆ˜è€…å¯¹ $\\mathrm{m_1}$ åŠ å¯†ã€‚ æ”»å‡»è€…æ”¶åˆ°æŒ‘æˆ˜è€…å“åº”çš„å¯†æ–‡ï¼Œé€šè¿‡ä¸€å®šçš„ç®—æ³•æ¥åˆ¤æ–­è¯¥å¯†æ–‡æ˜¯å¯¹ $\\mathrm{m_0}$ åŠ å¯†çš„ï¼Œè¿˜æ˜¯å¯¹ $\\mathrm{m_1}$ åŠ å¯†çš„ã€‚ å¦‚æžœone-time keyæ–¹å¼æ˜¯è¯­ä¹‰å®‰å…¨çš„ï¼Œåˆ™ï¼š$$\\mathrm{Adv_{SS}[A,OTP]=|Pr[EXP(0)=1]-Pr[EXP(1)=1]} \\quad \\text{should be â€œnegâ€.}$$ è€Œæˆ‘ä»¬ä¸Šæ–‡æåˆ°çš„ECBï¼Œå°±ä¸æ˜¯è¯­ä¹‰å®‰å…¨çš„ã€‚ å› ä¸ºä¸€æ¡æ¶ˆæ¯å®žåˆ™æœ‰è®¸å¤šå—ï¼Œè€Œè¿™äº›å—ä¸­å¦‚æžœæœ‰ç›¸åŒçš„æ˜Žæ–‡ï¼Œåˆ™å¯¹åº”çš„å¯†æ–‡ç›¸åŒã€‚ æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€ç‚¹æ¥åŒºåˆ† $\\mathrm{m_0,m_1}$ ï¼š æ”»å‡»è€…å¯ä»¥ç”¨ç®—æ³•åˆ¤æ–­å¯†æ–‡å—æ˜¯å¦ç›¸ç­‰æ¥åˆ¤æ–­æ˜¯å¯¹å“ªä¸€æ¡æ¶ˆæ¯åŠ å¯†çš„ã€‚ è¯¥åœºæ™¯ä¸‹ï¼Œ æ”»å‡»è€…æ”»å‡»æˆåŠŸçš„ä¼˜åŠ¿ä¸ºï¼š $\\mathrm{Adv_{SS}[A,ECB]}=|0-1|=1$ ã€‚ Det. CTRè¿™é‡Œä»‹ç»ä¸€ç§ä»ŽPRFæž„é€ çš„å®‰å…¨æµå¯†ç ï¼šç¡®å®šçš„è®¡æ•°å™¨æ¨¡å¼ï¼ˆDeterministic counter modeï¼‰ è¯æ˜Žè¯¥ç§æ¨¡å¼çš„å®‰å…¨æ€§ï¼š Theorem: For any L&gt;0, if F is a secure PRF over (K,X,X) then $\\mathrm{E_{DETCTR}}$ is sem. sec. cipher over $\\mathrm{(K,X^L,X^L)}$ . In particular, for any eff. adversary A attacking $\\mathrm{E_{DETCTR}}$ there exists an eff. PRF adversary B s.t.:$$\\mathrm{Adv_{SS}[A,E_{DETCTR}]=2\\cdot Adv_{PRF}[B,F]}$$è¯¥å®šç†å‘Šè¯‰æˆ‘ä»¬ï¼Œå¦‚æžœFæ˜¯ä¸€ä¸ª(K,X,X)ä¸Šå®‰å…¨çš„PRFï¼Œé‚£ä¹ˆ $\\mathrm{E_{DETCTR}}$ å°±æ˜¯ $\\mathrm{(K,X^L,X^L)}$ ä¸Šæ»¡è¶³è¯­ä¹‰å®‰å…¨çš„å¯†ç ï¼ˆLæ˜¯æ¶ˆæ¯å—æ•°ï¼‰ã€‚ æ­¤å¤–ï¼Œå¯¹äºŽä»»æ„çš„æœ‰æ•ˆæ”»å‡»è€…Aæ”»å‡» $\\mathrm{E_{DETCTR}}$ å¯†ç ï¼Œéƒ½å­˜åœ¨ä¸€ä¸ªæœ‰æ•ˆçš„PRFæ”»å‡»è€…Bæ”»å‡» $\\mathrm{E_{DETCTR}}$ åº•å±‚çš„PRFï¼Œä»–ä»¬å„è‡ªæ”»å‡»çš„ä¼˜åŠ¿æ»¡è¶³ï¼š $\\mathrm{Adv_{SS}[A,E_{DETCTR}]=2\\cdot Adv_{PRF}[B,F]}$ ã€‚ å½“PRFæ˜¯å®‰å…¨çš„PRFæ—¶ï¼Œå³ $\\mathrm{Adv_{PRF}[B,F]}$ is negligibleã€‚å› æ­¤ï¼Œ$\\mathrm{Adv_{SS}[A,E_{DETCTR}]}$ ä¹Ÿå¿…é¡»æ˜¯negligibleçš„ã€‚ ä¹Ÿå¯ä»¥é€šè¿‡ä¸‹å›¾ç›´è§‚çš„è¯æ˜Žï¼š Many-time Keyå—å¯†ç çš„å¦ä¸€ç§ä½¿ç”¨æ–¹å¼æ˜¯many-time keyæ–¹å¼ï¼Œå¸¸ç”¨äºŽæ–‡ä»¶ç³»ç»Ÿï¼Œæ¯”å¦‚ç”¨åŒæ ·çš„AESå¯†é’¥åŠ å¯†è®¸å¤šæ–‡ä»¶ï¼›ä»–ä¹Ÿæ˜¯IPsecçš„åŠ å¯†æ–¹å¼ï¼Œç›¸åŒçš„AESå¯†é’¥ç”¨äºŽåŠ å¯†è®¸å¤šIPåŒ…ã€‚ many-time keyçš„æ–¹å¼æ˜¯è¯´ï¼ŒåŒä¸€æ¡å¯†é’¥ç”¨äºŽåŠ å¯†å¤šæ¡æ¶ˆæ¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ”»å‡»è€…æœ‰é€‰æ‹©æ˜Žæ–‡æ”»å‡»çš„èƒ½åŠ›ï¼Œå¯ä»¥èŽ·å¾—ä»–ä»»æ„é€‰æ‹©æ˜Žæ–‡æ¶ˆæ¯å¯¹åº”çš„å¯†æ–‡ã€‚è€Œæ”»å‡»è€…çš„ç›®æ ‡åŒæ ·æ˜¯ç ´åè¯­ä¹‰å®‰å…¨ã€‚ many-time key: Adversaryâ€™s power: chosen-palintext attack(CPA): can obtain the encryption of arbitrary message of his choice. Adversaryâ€™s goal: break semantic security. Semantic Security(many-time key)æŒ‘æˆ˜è€…éšæœºé€‰æ‹©å¯†é’¥ï¼Œä»¥æ­¤ç¡®å®šåŠ å¯†è§£å¯†å¯†ç ã€‚å› ä¸ºæ˜¯many-time keyçš„åŠ å¯†æ–¹å¼ï¼Œæ”»å‡»è€…å¯ä»¥å‘é€å¤šç»„æ¶ˆæ¯ï¼š$\\mathrm{m_{i,0},m_{i,1}}$ ã€‚å®šä¹‰å¦‚ä¸‹ä¸¤ä¸ªå®žéªŒï¼Œå¯¹äºŽæ”»å‡»è€…å‘å‡ºçš„æ¯ä¸€ä¸ªè¯¢é—®ä¸­ï¼Œåœ¨EXP(0)ä¸­ï¼ŒæŒ‘æˆ˜è€…åŠ å¯† $\\mathrm{m_{i,0}}$ ï¼Œåœ¨EXP(1)ä¸­ï¼ŒæŒ‘æˆ˜è€…åŠ å¯† $\\mathrm{m_{i,1}}$ ã€‚æ”»å‡»è€…å¯¹äºŽæ”¶åˆ°çš„å¯†æ–‡ï¼Œæ¥åˆ¤æ–­è¯¥å¯†æ–‡æ˜¯å¯¹ $\\mathrm{m_{i,0},m_{i,1}}$ é‚£æ¡æ¶ˆæ¯åŠ å¯†çš„ã€‚ å½“æ”»å‡»è€…æƒ³è¦æŒ‡å®šæ˜Žæ–‡ $\\mathrm{m}$ çš„å¯†æ–‡æ—¶ï¼Œæ”»å‡»è€…å¯ä»¥å‘æŒ‘æˆ˜è€…å‘å‡ºè¯¢é—®ï¼Œè¯¢é—®ä¸­ $\\mathrm{m_{j,0}=m_{j,1}=m}$ ã€‚å› æ­¤åœ¨many-time keyçš„åŠ å¯†æ–¹å¼ä¸‹ï¼Œæ”»å‡»è€…æ‹¥æœ‰é€‰æ‹©æ˜Žæ–‡æ”»å‡»çš„èƒ½åŠ›ã€‚ å®šä¹‰many-time key çš„è¯­ä¹‰å®‰å…¨ï¼š Def: E is sem. sec. under CPA if for all â€œefficientâ€ A:$$\\mathrm{Adv_{CPA}[A,E]=|Pr[EXP(0)=1]-EXP(1)=1|}\\quad \\text{is neg.}$$ CPA Securityå¦‚æžœ $\\mathrm{E(k,m)}$ å¯¹æ¶ˆæ¯ $\\mathrm{m}$ æ€»æ˜¯è¾“å‡ºç›¸åŒçš„å¯†æ–‡ $\\mathrm{m}$ ï¼Œé‚£è¯¥å¯†ç å°±ä¸æ˜¯è¯­ä¹‰å®‰å…¨çš„ã€‚ æ”»å‡»è¿‡ç¨‹å¦‚ä¸‹å›¾ï¼Œæ”»å‡»è€…åœ¨ç¬¬ä¸€ä¸ªè¯¢é—®å‘é€ $\\mathrm{m_0,m_0}$ ï¼Œå¾—åˆ° $\\mathrm{m_0}$ çš„å¯†æ–‡ï¼Œåœ¨ç¬¬äºŒè¯¢é—®ä¸­å‘é€ $\\mathrm{m_0,m_1}$ ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡ç®—æ³• A(output 0 if $\\mathrm{c=c_0}$) ç ´åå…¶è¯­ä¹‰å®‰å…¨ã€‚ åœ¨çŽ°å®žç”Ÿæ´»ä¸­ï¼Œè¿™æ ·ç¡®å®šçš„åŠ å¯†ç®—æ³•(deterministic encryption)éƒ½ä¸èƒ½è¾¾åˆ°è¯­ä¹‰å®‰å…¨ï¼Œæ¯”å¦‚ä¸€ä¸ªæ”»å‡»è€…å¯ä»¥é€šè¿‡ä¸¤ä¸ªåŠ å¯†æ–‡ä»¶çš„å†…å®¹æ˜¯ç›¸åŒçš„ï¼Œæ¥åˆ¤æ–­è¿™ä¸¤ä»½æ–‡ä»¶æ—¶ç›¸åŒçš„ã€‚ æ‰€ä»¥ï¼Œå¯¹äºŽmany-time keyçš„åŠ å¯†æ–¹å¼æ¥è¯´ï¼Œå¦‚æžœç»™å®šç›¸åŒçš„å¯†æ–‡æ¶ˆæ¯ï¼ŒåŠ å¯†ç®—æ³•å¿…é¡»è¾“å‡ºä¸åŒçš„å¯†æ–‡ï¼Œæ‰èƒ½ä¿è¯è¯­ä¹‰å®‰å…¨ã€‚ Solution 1: randomized encryptionç¬¬ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ $\\mathrm{E(k,m)}$ æ˜¯ä¸€ä¸ªéšæœºç®—æ³•(randomized algorithm)ï¼š éšæœºç®—æ³•çš„å¥½å¤„æ˜¯ï¼Œæ¯æ¬¡åŠ å¯†ç›¸åŒçš„æ¶ˆæ¯éƒ½èƒ½å¾—åˆ°ä¸åŒçš„å¯†æ–‡ã€‚ ä½†è¿™æ ·çš„æ€§è´¨éœ€è¦æ»¡è¶³ï¼Œå¯†æ–‡æ¶ˆæ¯çš„é•¿åº¦æ¯”æ˜Žæ–‡æ¶ˆæ¯çš„é•¿åº¦é•¿ï¼Œå³ CT-size = PT-size + #random bits ä¾‹å¦‚è¿™æ ·çš„åŠ å¯†ç®—æ³•ï¼š$\\mathrm{F:K\\times R \\rightarrow M}$ æ˜¯ä¸€ä¸ªå®‰å…¨çš„PRFï¼Œ$\\mathrm{E(k,m)=[r\\stackrel{R}\\longrightarrow, \\text{output}(r, F(k,r)\\oplus m)]}$ å°±æ˜¯ä¸€ä¸ªåœ¨CPAæ”»å‡»ä¸‹è¯­ä¹‰å®‰å…¨çš„åŠ å¯†ç®—æ³•ã€‚ï¼ˆä¸è¿‡éœ€è¦ä¿è¯Rçš„ç©ºé—´è¶³å¤Ÿå¤§ï¼Œä»¥æ­¤ä¿è¯rä¸ä¼šé‡å¤ï¼‰ Solution 2: nonce-based Encryptionç¬¬äºŒç§è§£å†³æ–¹æ¡ˆæ˜¯åŸºäºŽnonceçš„åŠ å¯†ç®—æ³•ã€‚å¯¹äºŽæ¯ä¸€æ¡æ¶ˆæ¯nonce néƒ½åº”è¯¥æ˜¯ä¸åŒçš„ï¼Œå³ $\\mathrm{(k,n)}$ ä¸ä¼šé‡å¤ã€‚ nonceå¯ä»¥æ˜¯ä¸€ä¸ªè®¡æ•°å™¨(counter)ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªéšæœºå€¼(random nonce)ã€‚ å½“nonceæ˜¯æ”»å‡»è€…é€‰æ‹©æ—¶ï¼ˆæ¯ä¸ªè¯¢é—®çš„nonceéƒ½æ˜¯ä¸åŒçš„ï¼‰ï¼Œç³»ç»Ÿä¿è¯è¯­ä¹‰å®‰å…¨çš„å®šä¹‰ï¼š Def : nonce-based E is sem. sec. under CPA if for all â€œefficientâ€ A:$$\\mathrm{Adv_{nCPA}=|Pr[EXP(0)=1]-Pr[EXP(1)=1]|}\\quad \\text{is neg.}$$ CBCä¸€ç§many-time keyæ–¹å¼çš„å—å¯†ç æ˜¯CBCï¼ˆCipher Block Chainingï¼‰ã€‚ CBC with random IVLet $\\mathrm{(E,D)}$ be a PRP. $\\mathrm{E_{CBC}(k,m)}$ : choose random $\\mathrm{IV\\in X}$ and do: IVæ˜¯éšæœºçš„ï¼Œå› æ­¤è¦æŠŠIVçš„å€¼æ”¾åœ¨å¯†æ–‡ä¸­ä¸€èµ·å‘é€ç»™è§£å¯†æ–¹ã€‚ è§£å¯†çš„ç”µè·¯å¦‚ä¸‹ï¼š CBC: CPA Analysiså®šç†ï¼šå¯¹äºŽä»»æ„å— L&gt;0ï¼ˆLå°±æ˜¯æ¯ä¸ªæ¶ˆæ¯èƒ½å…·å¤‡çš„æœ€å¤§å—æ•°ï¼‰ï¼Œå¦‚æžœEåœ¨(K,X)ä¸Šæ˜¯ä¸€ä¸ªå®‰å…¨çš„PRPï¼Œé‚£ä¹ˆåœ¨CPAæ”»å‡»ä¸‹ $\\mathrm{E_{CBC}} $ $\\mathrm{=(K,X^L,X^{L+1})}$ å°±æ˜¯ä¸€ä¸ªå…·å¤‡è¯­ä¹‰å®‰å…¨çš„å¯†ç ã€‚å¦‚æžœæœ‰ä¸€ä¸ªæ”»å‡» $\\mathrm{E_{CBC}}$ çš„æ”»å‡»è€…Aï¼Œå‘èµ·qä¸ªè¯¢é—®ï¼ˆqå°±æ˜¯æœ€å¤§æ¶ˆæ¯æ•°ï¼‰ï¼Œé‚£ä¹ˆå­˜åœ¨ä¸€ä¸ªæ”»å‡»å…¶ PRPçš„æ”»å‡»è€…Bï¼Œæ»¡è¶³ï¼š$$\\mathrm{Adv_{CPA}[A,E_{CBC}]\\le2\\cdot Adv_{PRP}[B,E] +2q^2L^2/|X|}$$ CBC Theorem: For any L&gt;0, if E is a secure PRP over (K,X) then $\\mathrm{E_{CBC}} $ is a sem. sec. under CPA over $\\mathrm{(K,X^L,X^{L+1})}$ . In particular, for a q-query adversary A attacking $\\mathrm{E_{CBC}} $ , there exists a PRP adversary B s.t.:$$\\mathrm{Adv_{CPA}[A,E_{CBC}]\\le2\\cdot Adv_{PRP}[B,E] +2q^2L^2/|X|}$$q = # messages encrypted with k, L = length of max message PRPæ˜¯å®‰å…¨çš„ï¼Œ$\\mathrm{ Adv_{PRP}[B,E]}$ æ˜¯neg.ï¼Œå› æ­¤åªæœ‰å½“ $\\mathrm{q^2L^2&lt;&lt;|X|}$ æ—¶ï¼ŒCBCæ‰æ˜¯å®‰å…¨çš„ã€‚ Exampleï¼š å‡è®¾æˆ‘ä»¬å¸Œæœ› $\\mathrm{Adv_{CPA}[A,E_{CBC}]}\\le1/2^{32}$ ï¼Œå³éœ€è¦æ»¡è¶³ $\\mathrm{2q^2L^2/|X|\\le1/2^{32}}$ ï¼š å¯¹äºŽAESæ¥è¯´ï¼Œ$\\mathrm{|X|=2^{128}}$ ï¼Œæ‰€ä»¥ $\\mathrm{qL\\lt 2^{48}}$ ã€‚ å› æ­¤ï¼Œåœ¨ $2^{48}$ ä¸ªAESå—åŽï¼Œå¿…é¡»æ”¹å˜å¯†é’¥ã€‚ è€Œå¯¹äºŽDESæ¥è¯´ï¼Œ$\\mathrm{|X|=2^{64}}$ ï¼Œæ‰€ä»¥ $\\mathrm{qL\\lt 2^{16}}$ ã€‚ Warning: an attack on CBC with random IVå½“æ”»å‡»è€…å¯ä»¥é¢„æµ‹IVæ—¶ï¼ŒCBCå°±ä¸å†å…·å¤‡CPA-secureäº†ã€‚ å‡è®¾æ”»å‡»è€…å¯¹äºŽç»™å®šçš„å¯†æ–‡$\\mathrm{c\\leftarrow E_{CBC}(k,m)}$å¯ä»¥é¢„æµ‹ä¸‹ä¸€æ¡æ¶ˆæ¯çš„IVã€‚ æ¯”å¦‚åœ¨SSL/TLS1.1ä¸­çš„ä¸€ä¸ªbugï¼šç¬¬i-1æ¡æŠ¥æ–‡çš„æœ€åŽä¸€ä¸ªå¯†æ–‡å—å°±æ˜¯ç¬¬iæ¡æŠ¥æ–‡çš„IVã€‚ CPA attack: ç¬¬ä¸€æ¬¡è¯¢é—® æ”»å‡»è€…å‘é€ä¸¤æ¡æ¶ˆæ¯ $\\mathrm{m_0=m_1=0}\\in \\mathrm{X}$ æŒ‘æˆ˜è€…ç”¨ $\\mathrm{c_1=[IV_1, E(k,0\\oplus IV_1)]}$ å“åº” æ”»å‡»è€…é€šè¿‡å¯†æ–‡é¢„æµ‹ä¸‹ä¸€æ¡æ¶ˆæ¯çš„IV ç¬¬äºŒæ¬¡è¯¢é—® æ”»å‡»è€…å‘é€ä¸¤æ¡æ¶ˆæ¯ $\\mathrm{m_0=IV\\oplus IV_1,m_1\\ne m_0}$ æŒ‘æˆ˜è€…çš„å¯†æ–‡ $\\mathrm{c=[IV,E(k,IV_1)]\\quad or \\quad c=[IV,E(k, m_1\\oplus IV)]}$ æ”»å‡»è€…çš„ç»Ÿè®¡ç®—æ³• A: output 0 if $\\mathrm{c[1]=c_1[1]}$ æ‰€ä»¥IVå¿…é¡»æ˜¯éšæœºçš„ã€‚ nonce-based CBCCBCçš„å¦ä¸€ç§ä½¿ç”¨æ–¹å¼æ˜¯åŸºäºŽunique nonceã€‚ åœ¨è¿™ç§æ–¹å¼ä¸­ï¼Œå¯†é’¥ç”±ä¸€ç»„å¯†é’¥å¯¹ç»„æˆ key = (k, k1)ï¼Œk1æ˜¯ç”¨æ¥åŠ å¯†nonceçš„ã€‚ unique nonceçš„å«ä¹‰æ˜¯ï¼Œnonceå¯ä»¥ä¸æ˜¯éšæœºçš„ï¼Œä½†å¯¹äºŽæ¯ä¸€æ¡æ¶ˆæ¯ï¼Œ(key, nonce)å¯¹éƒ½å¿…é¡»æ˜¯ä¸åŒçš„ã€‚ æ‰€ä»¥ï¼Œå¯¹äºŽç”¨æˆ·æä¾›çš„éžéšæœºnonceï¼Œå¿…é¡»ç»è¿‡åŠ å¯†ã€‚ åœ¨è¿™ç§æ–¹å¼ä¸­ï¼Œä¸€ä¸ªéžå¸¸é‡è¦ä½†æ­¥éª¤å°±æ˜¯ä½¿ç”¨ k1æ¥åŠ å¯†nonceã€‚ å¦‚æžœæŠŠnonceå½“ä½œIVä½¿ç”¨ï¼ˆå¿˜æŽ‰k1åŠ å¯†nonceçš„æ­¥éª¤ï¼‰ï¼ŒCBCå°±æ²¡æœ‰CPA-secureã€‚ å¦‚æžœk1=kï¼ŒCBCä¹Ÿæ²¡æœ‰CPA-secureï¼ˆIV=0çš„æ¨¡å¼ï¼‰ã€‚ A CBC technicality: paddingå¯¹äºŽæ¶ˆæ¯é•¿åº¦æœªèƒ½è¾¾åˆ°å—çš„æ•´æ•°å€ï¼Œéœ€è¦paddingæŠ€æœ¯è¿›è¡Œå¡«å……æ¶ˆæ¯ã€‚ åœ¨TLSä¸­çš„å¡«å……æ–¹æ³•æ˜¯ï¼Œå¯¹äºŽéœ€è¦å¡«å……n(n&gt;0)ä¸ªbytesçš„æ¶ˆæ¯å—ï¼Œn-byte padä¸­çš„æ¯ä¸ªå­—èŠ‚éƒ½æ˜¯nã€‚ å¦‚æžœä¸éœ€è¦å¡«å……ï¼Œå°±éœ€è¦æ·»åŠ ä¸€ä¸ªdummy blockï¼Œè¯¥å—ä¸­çš„æ¯ä¸ªå­—èŠ‚éƒ½æ˜¯16ã€‚ CTRå¦ä¸€ç§æ–¹å¼æ˜¯CTRã€‚ rand ctr-modeLet F be a secure PRF. $\\mathrm{E_{CBC}(k,m)}$ : choose random $\\mathrm{IV\\in X}$ and do: CTRæ¨¡å¼çš„æœ€å¤§ä¼˜ç‚¹æ˜¯å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚ å…¶æ¬¡ï¼Œå› ä¸ºCTRçš„åŠ å¯†å…¶å®žæ˜¯æµå¯†ç çš„å½¢å¼ï¼Œæ‰€ä»¥åªéœ€è¦è¦æ±‚Fæ˜¯ä¸€ä¸ªå®‰å…¨çš„PRFï¼Œä¸éœ€è¦å…¶å¯é€†ã€‚ nonce ctr-modeä¸ºä¾‹ä¿è¯F(k,x)åªä¼šä½¿ç”¨ä¸€æ¬¡ï¼Œå¯¹æ¯æ¡æ¶ˆæ¯é€‰æ‹©ä¸€ä¸ª64 bitsçš„nonceï¼ŒåŒæ—¶ä½¿ç”¨ä¸€ä¸ª64 bitsçš„counterã€‚ CTR: CPA analysisCTR Theorem: For any L&gt;0, if F is a secure PRF over (K, X, X) then $\\mathrm{E_{CTR}} $ is a sem. sec. under CPA over $\\mathrm{(K,X^L,X^{L+1})}$ . In particular, for a q-query adversary A attacking $\\mathrm{E_{CTR}} $ , there exists a PRF adversary B s.t.:$$\\mathrm{Adv_{CPA}[A,E_{CTR}]\\le2\\cdot Adv_{PRF}[B,F] +2q^2L/|X|}$$q = # messages encrypted with k, L = length of max message CTR-modeä¸­ï¼Œåªæœ‰å½“ $\\mathrm{q^2L}&lt;&lt;|X|$ ï¼Œæ‰å…·å¤‡CPA-secureï¼Œæ¯”CBCå¥½ã€‚ Exampleï¼š å‡è®¾æˆ‘ä»¬å¸Œæœ› $\\mathrm{Adv_{CPA}[A,E_{CTR}]}\\le1/2^{32}$ ï¼Œå³éœ€è¦æ»¡è¶³ $\\mathrm{2q^2L/|X|\\le1/2^{32}}$ ï¼š å¯¹äºŽAESæ¥è¯´ï¼Œ$\\mathrm{|X|=2^{128}}$ ï¼Œæ‰€ä»¥ $\\mathrm{qL^{1/2}\\lt 2^{48}}$ ã€‚ å› æ­¤ï¼Œåœ¨ $2^{32}$ æ¡æ¶ˆæ¯åŽï¼ˆæ¯æ¡æ¶ˆæ¯æœ‰ $2^{32}$ä¸ª AESå—ï¼‰ï¼Œå¿…é¡»æ”¹å˜å¯†é’¥ã€‚ CTR v.s. CBC","link":"/2021/09/12/stanford-crypto-blockcipher2/"},{"title":"ã€ŒCryptography-Bonehã€ï¼šIntroduction","text":"æœ¬ç³»åˆ—æ˜¯å­¦ä¹ Dan Bonehæ•™æŽˆçš„Online Cryptography Courseã€‚ è¿™æ˜¯Danæ•™æŽˆçš„ç¬¬ä¸€è®²ï¼šå¯¹å¯†ç å­¦çš„ä¸€äº›Introductionã€‚ What is cryptography?Crypto coreï¼šå®‰å…¨é€šä¿¡ Secret key establishment (å¯†é’¥çš„å»ºç«‹)ï¼š Alice å’Œ Bob ä¼šå¾—åˆ°ä¸€ä¸ªshared secret keyï¼Œè€Œä¸”Alice çŸ¥é“å¥¹æ˜¯åœ¨å’ŒBobé€šä¿¡ï¼ŒBobä¹ŸçŸ¥é“ä»–æ˜¯åœ¨å’ŒAliceé€šä¿¡ã€‚è€Œattackerä¸èƒ½ä»Žé€šä¿¡ä¸­èŽ·å–keyã€‚ Secure communication ï¼ˆå®‰å…¨é€šä¿¡ï¼‰ï¼š åœ¨é€šä¿¡ä¸­ï¼ŒAliceã€Bobç”¨keyå°†ä¿¡æ¯åŠ å¯†ï¼Œä¿è¯äº†é€šä¿¡çš„confidentialityï¼ˆæœºå¯†æ€§ï¼‰ï¼›åŒæ—¶attackerä¹Ÿæ— æ³•ç¯¡æ”¹é€šä¿¡çš„ä¿¡æ¯ï¼Œä¿è¯äº†é€šä¿¡çš„integrityï¼ˆå®Œæ•´æ€§ï¼‰ã€‚ Crypto can do much moreå¯†ç å­¦é™¤äº†èƒ½ä¿è¯å®‰å…¨é€šä¿¡ï¼Œå¯†ç å­¦è¿˜èƒ½åšå¾ˆå¤šå…¶ä»–çš„äº‹ã€‚ Digital signature &amp; Anonymous Digital signaturesï¼ˆæ•°å­—ç­¾åï¼‰ï¼š çŽ°å®žä¸­ï¼Œäººä»¬å¯¹ä¸åŒçš„æ–‡æ¡£è¿›è¡Œç­¾åï¼Œè™½ç„¶æ˜¯ä¸åŒçš„æ–‡æ¡£ï¼Œä½†æ˜¯ç­¾åçš„å­—æ˜¯ç›¸åŒçš„ã€‚ å¦‚æžœè¿™åº”ç”¨åœ¨ç½‘ç»œçš„æ–‡æ¡£ç­¾åä¸­ï¼Œè¿™ä¼šå¾ˆå±é™©çš„ã€‚æ”»å‡»è€…åªéœ€è¦å°†ç­¾åè¿›è¡Œå¤åˆ¶ã€ç²˜è´´ï¼Œå°±å¯ä»¥å°†ä½ çš„ç­¾åç­¾åœ¨ä½ å¹¶ä¸æƒ³ç­¾çš„æ–‡æ¡£ä¸­ã€‚ æ•°å­—ç­¾åçš„ä¸»è¦æ€æƒ³ï¼šæ•°å­—ç­¾åå…¶å®žæ˜¯ä»£ç­¾å†…å®¹çš„å‡½æ•°å€¼ï¼Œæ‰€ä»¥å¦‚æžœæ”»å‡»è€…åªæ˜¯å¤åˆ¶æ•°å­—ç­¾åï¼ˆåŽŸç­¾åçš„å‡½æ•°å€¼ï¼‰ï¼Œé‚£ä¹ˆæ”»å‡»è€…å¾—åˆ°çš„æ•°å­—ç­¾åä¹Ÿæ˜¯æ— æ•ˆçš„ï¼ˆå‡½æ•°å€¼ä¸åŒï¼‰ã€‚ Anonymous communicationï¼ˆåŒ¿åé€šä¿¡ï¼‰ï¼š åŒ¿åé€šä¿¡çš„å®žçŽ°ï¼Œæœ‰Mix network ï¼ˆwikiè¯¦ç»†ä»‹ç»ï¼‰åè®®ï¼Œè¿™æ˜¯ä¸€ç§è·¯ç”±åè®®ï¼Œé€šè¿‡ä½¿ç”¨æ··åˆçš„ä»£ç†æœåŠ¡å™¨é“¾æ¥å®žçŽ°éš¾ä»¥è¿½è¸ªçš„é€šä¿¡ã€‚ é€šè¿‡è¿™äº›ä»£ç†çš„ä¸æ–­åŠ å¯†è§£å¯†å¯ä»¥å®žçŽ°ï¼š Bobä¸çŸ¥é“ä¸Žä¹‹é€šä¿¡çš„æ˜¯Aliceã€‚ ä»£ç†ä¹Ÿä¸çŸ¥é“æ˜¯Aliceå’ŒBobåœ¨é€šä¿¡ã€‚ åŒå‘é€šä¿¡ï¼šè™½ç„¶Bobä¸çŸ¥ä¸Žä¹‹é€šä¿¡çš„æ˜¯Aliceï¼Œä½†ä¹Ÿèƒ½respondã€‚ Anonymous digital cashï¼ˆåŒ¿åæ•°å­—çŽ°é‡‘ï¼‰ï¼š çŽ°å®žä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åŽ»è¶…å¸‚èŠ±æŽ‰ä¸€å…ƒé’±ï¼Œè€Œè¶…å¸‚ä¸çŸ¥é“æˆ‘æ˜¯è°ã€‚ åœ¨ç½‘ç»œä¸­ï¼Œå¦‚æžœAliceæƒ³åŽ»ç½‘ä¸Šå•†åº—èŠ±æŽ‰æ•°å­—çŽ°é‡‘ä¸€å…ƒé’±ï¼Œç½‘ä¸Šå•†åº—å¯ä»¥ä¸çŸ¥é“æ˜¯è°èŠ±æŽ‰çš„è¿™ä¸€å…ƒé’±å—ï¼Ÿ è¿™å°±æ˜¯åŒ¿åæ•°å­—çŽ°é‡‘éœ€è¦è§£å†³çš„é—®é¢˜ï¼š å¯ä»¥åœ¨åŒ¿åçš„æƒ…å†µä¸‹èŠ±æŽ‰æ•°å­—çŽ°é‡‘å—ï¼Ÿ å¦‚æžœå¯ä»¥ï¼Œå½“Aliceå°†è¿™ä¸€å…ƒé’±å¤åˆ¶å¤šæ¬¡ï¼ˆæ•°å­—çŽ°é‡‘éƒ½æ˜¯æ•°æ®ä¸²ï¼‰ï¼Œå¾—åˆ°äº†ä¸‰å…ƒé’±ï¼Œå†åŽ»æŠŠå®ƒèŠ±æŽ‰ï¼Œç”±äºŽåŒ¿åçš„åŽŸå› ï¼Œæ²¡äººçŸ¥é“æ˜¯è°èŠ±æŽ‰çš„è¿™ä¸‰å…ƒé’±ï¼Œå•†åº—æ‰¾ä¸åˆ°è´£ä»»äººã€‚ è¿™æ˜¯åŒ¿åæ•°å­—çŽ°é‡‘éœ€è¦è§£å†³çš„ç¬¬äºŒä¸ªé—®é¢˜ï¼š å¦‚ä½•é˜²æ­¢ double spendingæƒ…å†µçš„å‘ç”Ÿï¼Ÿ å¯ä»¥ç”¨è¿™æ ·çš„æœºåˆ¶åŽ»å®žçŽ°åŒ¿åæ•°å­—çŽ°é‡‘ï¼šå½“AliceèŠ±è´¹è¿™ä¸€å— onceæ—¶ï¼Œç³»ç»Ÿä¿è¯Aliceçš„åŒ¿åæ€§ï¼›ä½†å½“AliceèŠ±è´¹è¿™ä¸€å— more than once ,ç³»ç»Ÿç«‹å³æ­éœ²Aliceçš„å…¨éƒ¨ä¿¡æ¯ã€‚ Protocolsåœ¨ä»‹ç»ä»€ä¹ˆæ˜¯Protocolsä¹‹å‰ï¼Œå…ˆä»‹ç»ä¸¤ç§åº”ç”¨åœºæ™¯ã€‚ Elections æœ‰5ä¸ªäººè¦è¿›è¡ŒæŠ•ç¥¨é€‰ä¸¾0å’Œ1å·å€™é€‰äººï¼Œä½†æ˜¯éœ€è¦ä¿è¯ï¼šæ¯ä¸ªäººé™¤äº†çŸ¥é“è‡ªå·±çš„æŠ•ç¥¨ç»“æžœï¼Œäº’ç›¸ä¸çŸ¥é“å…¶ä»–äººçš„æŠ•ç¥¨æƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹æ€Žä¹ˆçŸ¥é“æœ€åŽçš„winneræ˜¯è°å—ï¼Ÿ å¦‚ä¸Šå›¾ï¼Œå¯ä»¥å¼•å…¥ä¸€ä¸ªç¬¬ä¸‰æ–¹â€”â€”election centerï¼Œç¬¬ä¸‰æ–¹éªŒè¯æ¯ä¸€ä¸ªäººåªèƒ½æŠ•ä¸€æ¬¡ï¼Œæœ€åŽç»Ÿè®¡ç¥¨æ•°å†³ç­–å‡ºæœ€åŽçš„winnerã€‚ Private auctions ä»‹ç»ä¸€ç§æ‹å–æœºåˆ¶ï¼ŒVickery auctionï¼šå¯¹ä¸€ä¸ªæ‹å–å“ï¼Œæ¯ä¸ªæŠ•æ ‡è€…åœ¨ä¸çŸ¥é“å…¶ä»–äººæŠ•æ ‡ä»·æ ¼çš„æƒ…å†µä¸‹è¿›è¡ŒæŠ•æ ‡ï¼Œæœ€åŽçš„acution winnerï¼š highest bidder &amp; pays 2nd highers bidã€‚å³æ˜¯æ ‡ä»·æœ€é«˜è€…å¾—æ ‡ï¼Œä½†ä»–åªéœ€è¦ä»˜ç¬¬äºŒé«˜çš„æ ‡ä»·ã€‚ æ‰€ä»¥publicçŸ¥é“çš„ä¿¡æ¯åªæœ‰ï¼šä¸­æ ‡è€…å’Œç¬¬äºŒé«˜æŠ•æ ‡è€…çš„æ ‡ä»·ã€‚ éœ€è¦å®žçŽ°è¿™ç§æœºåˆ¶ï¼Œä¹Ÿå¯ä»¥å¼•å…¥ä¸€ä¸ªç¬¬ä¸‰æ–¹â€”â€”auction centerã€‚ ä½†æ˜¯å¼•å…¥ç¬¬ä¸‰æ–¹çœŸçš„å®‰å…¨å—ï¼Ÿå®‰å…¨ç¬¬ä¸‰æ–¹ä¹Ÿä¸å®‰å…¨ã€‚ å†çœ‹ä¸Šé¢é‚£ä¸ªElectionçš„ä¾‹å­ï¼Œå¦‚æžœæŠŠä¸Šé¢å››ä¸ªäººçš„æŠ•ç¥¨æƒ…å†µä½œä¸ºè¾“å…¥ï¼Œç¬¬ä¸‰æ–¹çš„ä»»åŠ¡å…¶å®žæ˜¯è¾“å‡ºä¸€ä¸ªå‡½æ•° $f(x_1,x_2,x_3,x_4)$ è€Œä¸å…¬å¼€å…¶ä»–ä¿¡æ¯ã€‚ å› ä¸ºå®‰å…¨ç¬¬ä¸‰æ–¹ä¹Ÿè®¸å¹¶ä¸å®‰å…¨ï¼Œæ‰€ä»¥å¦‚æžœåŽ»æŽ‰ç¬¬ä¸‰æ–¹ï¼Œä¸Šé¢å››ä¸ªäººéµä»ŽæŸç§åè®®ï¼Œç›¸äº’é€šä¿¡ï¼Œæœ€åŽèƒ½å¦å¾—å‡ºè¿™ä¸ª $f(x_1,x_2,x_3,x_4)$ è¿™ä¸ªç»“æžœå‡½æ•°ï¼Œè€Œä¸é€éœ²å…¶æŠ•ç¥¨ä¿¡æ¯ï¼Ÿ ç­”æ¡ˆæ˜¯ â€œYesâ€ã€‚ æœ‰ä¸€ä¸ªæƒŠäººçš„å®šç†ï¼šä»»ä½•èƒ½é€šè¿‡ç¬¬ä¸‰æ–¹åšåˆ°çš„äº‹ï¼Œä¹Ÿèƒ½ä¸é€šè¿‡ç¬¬ä¸‰æ–¹åšåˆ°ã€‚ Thm: anythong that can done with trusted auth. can also be done without. æ€Žä¹ˆåšåˆ°ï¼Ÿç­”æ¡ˆæ˜¯ Secure multi-party computationï¼ˆå®‰å…¨å¤šæ–¹è®¡ç®—ï¼‰ã€‚ åœ¨MPCä¸“æ  æœ‰ç›¸å…³ä»‹ç»ã€‚ Crypto magic Privately outsourcing computation (å®‰å…¨å¤–åŒ…è®¡ç®—) Aliceæƒ³è¦åœ¨GoogleæœåŠ¡å™¨æŸ¥è¯¢ä¿¡æ¯ï¼Œä¸ºäº†ä¸è®©åˆ«äººçŸ¥é“å¥¹æŸ¥è¯¢çš„æ˜¯ä»€ä¹ˆï¼Œå¥¹æŠŠsearch queryè¿›è¡ŒåŠ å¯†ã€‚ GoogleæœåŠ¡å™¨æŽ¥æ”¶åˆ°åŠ å¯†çš„æŸ¥è¯¢è¯·æ±‚ï¼Œè™½ç„¶Googleä¸çŸ¥é“å¥¹å®žé™…æƒ³æŸ¥è¯¢ä»€ä¹ˆä¿¡æ¯ï¼Œä½†æ˜¯æœåŠ¡å™¨èƒ½æ ¹æ®E[query]è¿”å›žE[results]ã€‚ æœ€åŽAliceå°†æ”¶åˆ°çš„E[results]è§£å¯†ï¼Œå¾—åˆ°çœŸæ­£çš„resultsã€‚ è¿™å°±æ˜¯å®‰å…¨å¤–åŒ…è®¡ç®—çš„ç®€å•è¿‡ç¨‹ï¼šEncryptionã€Searchã€Decryptionã€‚ Zero knowledgeï¼ˆproof of knowledge) (é›¶çŸ¥è¯†è¯æ˜Ž)ï¼š Alice çŸ¥é“pã€q(ä¸¤ä¸ª1000ä½çš„è´¨æ•°)ç›¸ä¹˜ç­‰äºŽNã€‚ BobåªçŸ¥é“Nçš„å€¼ï¼Œä¸çŸ¥é“å…·ä½“çš„pã€qå€¼ã€‚ Alice ç»™ Bobè¯´å¥¹èƒ½å¤Ÿåˆ†è§£æ•°Nï¼Œä½†å¥¹ä¸ç”¨å‘Šè¯‰Bob Nçš„å…·ä½“å› å­æ˜¯ä»€ä¹ˆï¼Œåªéœ€è¦è¯æ˜Žæˆ‘èƒ½åˆ†è§£Nï¼Œè¯æ˜Žè¿™æ˜¯æˆ‘çš„çŸ¥è¯†ã€‚ æœ€åŽBobçŸ¥é“Aliceèƒ½å¤Ÿåˆ†è§£Nï¼Œä½†ä»–ä¸çŸ¥é“æ€Žä¹ˆåˆ†è§£ï¼ˆä¸çŸ¥é“Nçš„å› å­åˆ°åº•æ˜¯ä»€ä¹ˆï¼‰ã€‚ A rigorous scienceåœ¨å¯†ç å­¦çš„ç ”ç©¶ä¸­ï¼Œé€šå¸¸æ˜¯è¿™æ ·çš„æ­¥éª¤ï¼š Precisely specify threat model. å‡†ç¡®æè¿°å…¶å¨èƒæ¨¡åž‹æˆ–ä¸ºè¾¾åˆ°çš„ç›®çš„ã€‚æ¯”å¦‚ç­¾åçš„ç›®çš„ï¼šunforgeableï¼ˆä¸å¯ä¼ªé€ ï¼‰ã€‚ Propose a construction. Prove that breaking construction under threat mode will solve an underlying hard problem. è¯æ˜Žæ”»å‡»è€…æ”»å‡»è¿™ä¸ªç³»ç»Ÿå¿…é¡»è§£å†³ä¸€ä¸ªå¾ˆéš¾çš„é—®é¢˜ï¼ˆå¤§æ•´æ•°åˆ†è§£é—®é¢˜ä¹‹ç±»çš„NPé—®é¢˜ï¼‰ã€‚ è¿™æ ·ä¹Ÿå°±è¯æ˜Žäº†è¿™ä¸ªç³»ç»Ÿæ˜¯å®‰å…¨çš„ã€‚ HistorySubstitution cipherï¼ˆæ›¿æ¢ï¼‰what is it æ›¿æ¢å¯†ç å¾ˆå¥½ç†è§£ï¼Œå¦‚ä¸Šå›¾çš„è¿™ç§æ›¿æ¢è¡¨ï¼ˆkeyï¼‰ã€‚ æ¯”è¾ƒhistoricçš„æ›¿æ¢å¯†ç â€”â€”Caesar Cipherï¼ˆå‡¯æ’’å¯†ç ï¼‰ï¼Œå‡¯æ’’å¯†ç æ˜¯ä¸€ç§æ›¿æ¢è§„åˆ™ï¼šå‘åŽç§»ä¸‰ä½ï¼Œå› æ­¤ä¹Ÿå¯ä»¥è¯´å‡¯æ’’å¯†ç æ²¡æœ‰keyã€‚ the size of key spaceç”¨$\\mathcal{K}$ ï¼ˆèŠ±ä½“çš„Kï¼‰æ¥è¡¨ç¤ºå¯†é’¥ç©ºé—´ã€‚ è‹±è¯­å­—æ¯çš„æ›¿æ¢å¯†ç ï¼Œæ˜“å¾—å¯†é’¥ç©ºé—´çš„å¤§å°æ˜¯ $|\\mathcal{K}|=26!\\approx2^{88}$ ï¼ˆå³26ä¸ªå­—æ¯çš„å…¨æŽ’åˆ—ï¼‰ã€‚ è¿™æ˜¯ä¸€ä¸ªå°±çŽ°åœ¨è€Œè¨€ä¹Ÿå°±æ¯”è¾ƒperfectçš„å¯†é’¥ç©ºé—´ã€‚ ä½†æ›¿æ¢å¯†ç ä¹Ÿå¾ˆå®¹æ˜“è¢«ç ´è§£ã€‚ how to break ité—®ï¼šè‹±è¯­æ–‡æœ¬ä¸­æœ€commomçš„å­—æ¯æ˜¯ä»€ä¹ˆï¼Ÿ ç­”ï¼šâ€œEâ€ åœ¨è‹±è¯­æ–‡æœ¬ï¼ˆå¤§é‡ï¼‰ä¸­ï¼Œæ¯ä¸ªå­—æ¯å‡ºçŽ°çš„é¢‘çŽ‡å¹¶ä¸æ˜¯å‡åŒ€åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¸€äº›æœ€commonçš„å­—æ¯å’Œå­—æ¯ç»„åˆæ¥ç ´è§£æ›¿æ¢å¯†ç ã€‚ Use frequency of English letters. Danæ•™æŽˆç»Ÿè®¡äº†æ ‡å‡†æ–‡çŒ®ä¸­å­—æ¯é¢‘çŽ‡ï¼š â€œeâ€: 12.7% , â€œtâ€: 9.1% , â€œaâ€ : 8.1%. ç»Ÿè®¡å¯†æ–‡ä¸­ï¼ˆå¤§é‡ï¼‰å‡ºçŽ°é¢‘çŽ‡æœ€é«˜ã€æ¬¡é«˜ã€ç¬¬ä¸‰é«˜çš„å­—æ¯ï¼Œä»–ä»¬çš„æ˜Žæ–‡ä¹Ÿå°±æ˜¯eã€tã€aã€‚ Use frequency of pairs of letters (diagrams).ï¼ˆäºŒåˆå­—æ¯ï¼‰ é¢‘çŽ‡å‡ºçŽ°è¾ƒé«˜çš„äºŒåˆå­—æ¯ï¼šâ€heâ€, â€œanâ€, â€œinâ€ , â€œthâ€ ä¹Ÿèƒ½å°†h, n,iç­‰ç ´è§£å‡ºã€‚ trigramsï¼ˆç»§ç»­ä½¿ç”¨ä¸‰åˆå­—æ¯ï¼‰ â€¦â€¦ç›´è‡³å…¨éƒ¨ç ´è§£ å› æ­¤substitution cipheræ˜¯CT only attackï¼ï¼ˆå”¯å¯†æ–‡æ”»å‡»ï¼šä»…å‡­å¯†æ–‡å°±å¯ä»¥è¿˜åŽŸå‡ºåŽŸæ–‡ï¼‰ Vigener cipherEncryption åŠ å¯†è¿‡ç¨‹å¦‚ä¸Šå›¾æ‰€ç¤ºï¼š å¯†é’¥æ˜¯ â€œCRYPTOâ€, é•¿åº¦ä¸º6ï¼Œå°†å¯†é’¥é‡å¤ä¹¦å†™ç›´è‡³è¦†ç›–æ•´ä¸ªæ˜Žæ–‡é•¿åº¦ã€‚ å°†å¯†é’¥çš„å­—æ¯å’Œå¯¹åº”çš„æ˜Žæ–‡ç›¸åŠ æ¨¡26ï¼Œå¾—åˆ°å¯†æ–‡ã€‚ Decryptionè§£å¯†åªéœ€è¦å°†å¯†æ–‡å‡åŽ»å¯†é’¥å­—æ¯ï¼Œå†æ¨¡26å³å¯ã€‚ How to break itç ´è§£æ–¹æ³•å’Œæ›¿æ¢å¯†ç ç±»ä¼¼ï¼Œæ€æƒ³ä¹Ÿæ˜¯ä½¿ç”¨å­—æ¯é¢‘çŽ‡æ¥ç ´è§£ã€‚ è¿™é‡Œåˆ†ä¸¤ç§æƒ…å†µè®¨è®ºï¼š ç¬¬ä¸€ç§ï¼šå·²çŸ¥å¯†é’¥é•¿åº¦ ç ´è§£è¿‡ç¨‹ï¼š å°†å¯†æ–‡æŒ‰ç…§å¯†é’¥é•¿åº¦åˆ†ç»„ï¼ŒæŒ‰ç…§å›¾ä¸­çš„è¯ï¼Œ6ä¸ªä¸€ç»„ã€‚ ç»Ÿè®¡æ¯ç»„çš„çš„ç¬¬ä¸€ä¸ªä½ç½®çš„å­—æ¯å‡ºçŽ°é¢‘çŽ‡ã€‚ å‡è®¾å¯†æ–‡ä¸­ç¬¬ä¸€ä¸ªä½ç½®æœ€commonçš„æ˜¯â€Hâ€ å¯†é’¥çš„ç¬¬ä¸€ä¸ªå­—æ¯æ˜¯ï¼šâ€Hâ€-â€œEâ€=â€Câ€ ç»Ÿè®¡å‰©ä¸‹ä½ç½®çš„å­—æ¯é¢‘çŽ‡ï¼Œç›´è‡³å®Œå…¨ç ´è§£å¯†é’¥ã€‚ ç¬¬äºŒç§ï¼šæœªçŸ¥å¯†é’¥é•¿åº¦ æœªçŸ¥å¯†é’¥é•¿åº¦ï¼Œåªéœ€è¦ä¾æ¬¡å‡è®¾å¯†é’¥é•¿åº¦æ˜¯1ã€2ã€3â€¦ï¼Œå†æŒ‰ç…§ç¬¬ä¸€ç§æƒ…å†µç ´è§£ï¼Œç›´è‡³ç ´è§£æƒ…å†µåˆç†ã€‚ Rotor MachinesRotor: è½´è½®ã€‚ æ‰€ä»¥è¿™ç§å¯†ç çš„åŠ å¯†æ ¸å¿ƒæ˜¯ï¼šè¾“å…¥æ˜Žæ–‡å­—æ¯ï¼Œè½´è½®æ—‹è½¬ä¸€å®šè§’åº¦ï¼Œæ˜ å°„ä¸ºå¦ä¸€å­—æ¯ã€‚ single rotor æ—©æœŸçš„æ˜¯å•è½´è½®ï¼Œrotor machineçš„å¯†é’¥å…¶å®žæ˜¯å›¾å³ä¸­é—´é‚£ä¸ªåœ†åœ†çš„å¯ä»¥æ—‹è½¬çš„æŸ±å­ã€‚ å›¾å·¦æ˜¯å˜åŠ¨çš„å¯†é’¥æ˜ å°„è¡¨ã€‚ å˜åŠ¨è¿‡ç¨‹ï¼š ç¬¬ä¸€æ¬¡è¾“å…¥Aï¼Œå¯†æ–‡æ˜¯Kã€‚ è½´è½®æ—‹è½¬ä¸€ä¸ªå­—æ¯ä½ï¼šçœ‹å›¾ä¸­Eï¼Œä»Žæœ€ä¸‹åˆ°æœ€ä¸Šï¼ˆä¸€ä¸ªåœˆï¼Œåªç›¸éš”ä¸€ä½ï¼‰ã€‚ æ‰€ä»¥ç¬¬äºŒæ¬¡å†è¾“å…¥Aï¼Œå¯†æ–‡æ˜¯Eã€‚ â€¦â€¦ Most famous ï¼šthe Enigma Enigma machineæ˜¯äºŒæˆ˜æ—¶æœŸçº³ç²¹å¾·å›½ä½¿ç”¨çš„åŠ å¯†æœºå™¨ï¼Œå› æ­¤å®Œå…¨ç ´è§£äº†Enigmaæ˜¯ç›Ÿå†›æå‰èƒœåˆ©çš„å…³é”®ã€‚ å·¦å›¾ä¸­å¯ä»¥çœ‹å‡ºEnigmaæœºå™¨ä¸­æ˜¯æœ‰4ä¸ªè½´è½®ï¼Œæ¯ä¸ªè½´è½®éƒ½æœ‰è‡ªå·±çš„æ—‹è½¬å­—æ¯ä½å¤§å°ï¼Œå› æ­¤å¯†é’¥ç©ºé—´å¤§å°æ˜¯ $|\\mathcal{K}|=26^4\\approx2^{18}$ (åœ¨plugboardä¸­ï¼Œå®žé™…æ˜¯ $2^{36}$)ã€‚ å¯†é’¥ç©ºé—´å¾ˆå°ï¼Œæ”¾åœ¨çŽ°åœ¨å¾ˆå®¹æ˜“è¢«æš´åŠ›ç ´è§£ã€‚ plugboard å…è®¸æ“ä½œå‘˜é‡æ–°é…ç½®å¯å˜æŽ¥çº¿ï¼Œå®žçŽ°ä¸¤ä¸ªå­—æ¯çš„äº¤æ¢ã€‚plugboardæ¯”é¢å¤–çš„rotoræä¾›äº†æ›´å¤šçš„åŠ å¯†å¼ºåº¦ã€‚ å¯¹äºŽEnigma machineçš„æ›´å¤šçš„å…·ä½“ä»‹ç»å¯ä»¥æˆ³Enigma machine çš„wikié“¾æŽ¥ã€‚ Data Encryption StandardDESï¼š#keys = $2^{56}$ ,block siez = 64bitsï¼Œä¸€æ¬¡å¯ä»¥åŠ å¯†8ä¸ªå­—æ¯ã€‚ Todayï¼šAESï¼ˆ2001ï¼‰ã€Salsa20ï¼ˆ2008ï¼‰â€¦â€¦ è¿™é‡Œåªæ˜¯ç®€å•ä»‹ç»ã€‚ Discrete Probability Background on discrete probability: [html] Randomized algorithmséšæœºç®—æ³•æœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯Deterministic algorithmï¼ˆä¹Ÿå°±æ˜¯ä¼ªéšæœºï¼‰ï¼Œå¦ä¸€ç§æ˜¯Randomized algorithmã€‚ Deterministic algorithm $ y\\longleftarrow A(m)$ ï¼Œè¿™æ˜¯ä¸€ä¸ªç¡®å®šçš„å‡½æ•°ï¼Œè¾“å…¥æ˜ å°„åˆ°å”¯ä¸€è¾“å‡ºã€‚ Randomized algorithm $y\\longleftarrow A(m ; r) \\quad \\text { where } r \\stackrel{R}{\\longleftarrow}{0,1}^{n}$ outputï¼š $y \\stackrel{R}{\\longleftarrow} A(m)$ ï¼Œy is a random variable. $ r \\stackrel{R}\\longleftarrow {0,1}^n $ :æ„æ€æ˜¯ræ˜¯nä½01åºåˆ—ä¸­çš„ä»»æ„ä¸€ä¸ªå–å€¼ã€‚Rï¼Œrandomã€‚å˜é‡ræœä»Žåœ¨ ${0,1}^n$ å–å€¼çš„å‡åŒ€åˆ†å¸ƒã€‚ ç”±äºŽéšæœºå˜é‡rï¼Œå¯¹äºŽç»™å®šmï¼Œ$A(m;r)$ æ˜¯ ${0,1}^n$ ä¸­çš„ä¸€ä¸ªå­é›†ã€‚ æ‰€ä»¥ï¼Œå¯¹mçš„åŠ å¯†ç»“æžœyï¼Œä¹Ÿæ˜¯ä¸€ä¸ªçš„éšæœºå˜é‡ï¼Œè€Œä¸”ï¼Œyåœ¨ $A(m,r)$ ä¹Ÿæ˜¯æœä»Žå‡åŒ€åˆ†å¸ƒã€‚ å› æ­¤ï¼Œç”±äºŽrçš„å½±å“ï¼Œå¯¹äºŽç»™å®šmï¼ŒåŠ å¯†ç»“æžœä¸ä¼šæ˜ å°„åˆ°åŒä¸€ä¸ªå€¼ã€‚ï¼ˆå¦‚ä¸Šå›¾æ‰€ç¤ºï¼‰ XORXORæœ‰ä¸¤ç§ç†è§£ï¼šï¼ˆ $x \\oplus y $ ï¼‰ ä¸€ç§æ˜¯ï¼šx,yçš„bitä½ç›¸æ¯”è¾ƒï¼Œç›¸åŒåˆ™ä¸º0ï¼Œç›¸å¼‚ä¸º1. å¦ä¸€ç§æ˜¯ï¼šx,yçš„bitä½ç›¸åŠ  mod2. å¼‚æˆ–åœ¨å¯†ç å­¦ä¸­è¢«é¢‘ç¹ä½¿ç”¨ï¼Œä¸»è¦æ˜¯å› ä¸ºå¼‚æˆ–æœ‰ä¸€ä¸ªé‡è¦çš„æ€§è´¨ã€‚ å¼‚æˆ–çš„é‡è¦æ€§è´¨ï¼šæœ‰ä¸¤ä¸ªåœ¨ ${0,1}^n$ ï¼ˆnä½01ä¸²ï¼‰å–å€¼çš„éšæœºå˜é‡Xã€Yã€‚Xã€Yç›¸äº’ç‹¬ç«‹ï¼ŒXæœä»Žä»»æ„æŸç§åˆ†å¸ƒï¼Œéšæœºå˜é‡Yæœä»Žå‡åŒ€åˆ†å¸ƒã€‚é‚£ä¹ˆ $Z=Y\\oplus X$ ï¼ŒZåœ¨ ${0,1}^n$ å–å€¼ï¼Œä¸”Zæœä»Žå‡åŒ€åˆ†å¸ƒã€‚ Thm: Y a rand. var. over ${0,1}^n$ , X an index. uniform var. on ${0,1}^n$ â€‹ Then Z := Y $\\oplus$ X is uniform var. on ${0,1}^n$ . Proofï¼š å½“n=1 ç”»å‡ºè”åˆåˆ†å¸ƒ Pr[ Z=0 ]=Pr[ (x,y)=(0,0)] + Pr[(x,y)=(1,1)]=1/2 æ¯ä¸€bitä½éƒ½æœä»Žå‡åŒ€åˆ†å¸ƒï¼Œå¯ä»¥å®¹æ˜“å¾—å‡º Zæ˜¯æœä»Žå‡åŒ€åˆ†å¸ƒã€‚ The birthday paradoxï¼ˆç”Ÿæ—¥æ‚–è®ºï¼‰æ›´å…·ä½“çš„åˆ†æžè§ Birthday problem ã€‚ é—®é¢˜å‰æï¼šä¸€ä¸ªäººçš„ç”Ÿæ—¥åœ¨365å¤©çš„ä»»æ„ä¸€å¤©æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼ˆå®žé™…å½“ç„¶ä¸æ˜¯ï¼Œè²Œä¼¼æ›´å¤šé›†ä¸­åœ¨9æœˆï¼‰ã€‚ æ ¹æ®ä¿¡é¸½ç†è®ºï¼ˆæœ‰Nä¸ªé¸½å­ï¼ŒMä¸ªéš”é—´ï¼Œå¦‚æžœN&gt;Mï¼Œé‚£ä¹ˆä¸€å®šæœ‰ä¸€ä¸ªéš”é—´æœ‰ä¸¤åªé¸½å­ï¼‰ï¼Œæ‰€ä»¥367ä¸ªäººä¸­ï¼Œä»¥100%çš„æ¦‚çŽ‡æœ‰ä¸¤ä¸ªäººçš„ç”Ÿæ—¥ç›¸åŒã€‚ä½†æ˜¯ï¼Œå½“åªæœ‰70ä¸ªäººæ—¶ï¼Œå°±æœ‰99.9%çš„æ¦‚çŽ‡ï¼Œå…¶ä¸­ä¸¤äººç”Ÿæ—¥ç›¸åŒï¼›å½“åªæœ‰23äººï¼Œè¿™ä¸ªæ¦‚çŽ‡å¯ä»¥è¾¾åˆ°50%ã€‚ å…¶å®žè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªæ‚–è®ºï¼Œåªæ˜¯ç›´è§‰è¯¯å¯¼ï¼Œç†æ€§å’Œæ„Ÿæ€§è®¤è¯†çš„çŸ›ç›¾ã€‚å½“åªæœ‰ä¸€ä¸ªäººï¼Œæ¦‚çŽ‡ä¸º0ï¼Œå½“æœ‰367äººæ—¶ï¼Œä¸º100%ï¼Œæ‰€ä»¥æˆ‘ä»¬ç›´è§‰è®¤ä¸ºï¼Œè¿™æ˜¯çº¿æ€§å¢žé•¿çš„ï¼Œå…¶å®žä¸ç„¶ã€‚ æ¦‚çŽ‡è®ºçŸ¥è¯†ï¼š è®¾äº‹ä»¶Aï¼š23ä¸ªäººä¸­ï¼Œæœ‰ä¸¤ä¸ªäººç”Ÿæ—¥ç›¸ç­‰ã€‚ $P\\left(A^{\\prime}\\right)=\\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\frac{362}{365} \\times \\cdots \\times \\frac{343}{365}$ $P\\left(A^{\\prime}\\right)=\\left(\\frac{1}{365}\\right)^{23} \\times(365 \\times 364 \\times 363 \\times \\cdots \\times 343)$ $P\\left(A^{\\prime}\\right) \\approx 0.492703$ $P(A) \\approx 1-0.492703=0.507297 \\quad(50.7297 \\%)$ æŽ¨å¹¿åˆ°ä¸€èˆ¬æƒ…å†µï¼Œnä¸ªäºº(n","link":"/2020/03/03/stanford-crypto-intro/"},{"title":"ã€ŒCryptography-Bonehã€:Stream Cipher 1","text":"Stream Cipherçš„ç¬¬ä¸€éƒ¨åˆ†ï¼šä»‹ç»äº†One Time Padå’ŒStream Cipherä¸­çš„PRGã€‚å…¶ä¸­OTPéƒ¨åˆ†å™è¿°äº†ä»€ä¹ˆæ˜¯Perfect Secrecyï¼Ÿä¸ºä»€ä¹ˆOTPå¾ˆéš¾åœ¨å®žè·µä¸­åº”ç”¨ï¼ŸStream Cipheréƒ¨åˆ†ä¸­ï¼Œæœ¬æ–‡ä¸»è¦é˜è¿°äº†ä»€ä¹ˆæ˜¯PRGï¼ŸStream Cipherçš„å¦ä¸€ç§å®‰å…¨çš„å®šä¹‰ï¼ˆä¾é PRGçš„unpredictable)ã€‚æœ¬æ–‡åŽåŠéƒ¨åˆ†ï¼Œè¯¦ç»†é˜è¿°äº†ä¸€ç§weak PRGâ€”â€”çº¿æ€§åŒä½™ç”Ÿæˆå™¨ï¼Œå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿå®ƒä¸ºä»€ä¹ˆä¸å®‰å…¨ï¼Ÿå¦‚ä½•æ”»å‡»å®ƒ? The One Time PadSymmetric Ciphers: difinitionDef :a cipher difined over $\\mathcal{(K,M,C)}$ is a pair of â€œefiicient â€œ algorithms $(E,D)$ where $$ E :\\mathcal{K \\times M \\longrightarrow \\mathcal{C}} \\quad ,\\quad D:\\mathcal{K\\times\\mathcal{C}\\longrightarrow\\mathcal{M}} \\\\ s.t. \\quad \\forall m\\in \\mathcal{M},k\\in \\mathcal{K}:D(k,E(k,m))=m $$ $\\mathcal{(K,M,C)}$ åˆ†åˆ«æ˜¯å¯†é’¥ç©ºé—´ã€æ˜Žæ–‡ç©ºé—´ã€å¯†æ–‡ç©ºé—´ã€‚ å¯¹ç§°åŠ å¯†å…¶å®žæ˜¯å®šä¹‰åœ¨$\\mathcal{(K,M,C)}$ çš„ä¸¤ä¸ªæœ‰æ•ˆç®—æ³• $(E,D)$ ï¼Œè¿™ä¸¤ä¸ªç®—æ³•æ»¡è¶³consistence equation(ä¸€è‡´æ€§æ–¹ç¨‹)ï¼š$D(k,E(k,m))=m$ ã€‚ ä¸€äº›è¯´æ˜Žï¼š $E$ is ofen randomized. å³åŠ å¯†ç®—æ³•Eæ€»æ˜¯éšæœºç”Ÿæˆä¸€äº›bitsï¼Œç”¨æ¥åŠ å¯†æ˜Žæ–‡ã€‚ $D$ is always deterministic. å³å½“ç¡®å®šå¯†é’¥å’Œæ˜Žæ–‡æ—¶ï¼Œè§£å¯†ç®—æ³•çš„è¾“å‡ºæ€»æ˜¯å”¯ä¸€çš„ã€‚ â€œefficientâ€ çš„å«ä¹‰ å¯¹äºŽç†è®ºæ´¾ï¼šefficientè¡¨ç¤º in polynomial timeï¼ˆå¤šé¡¹å¼æ—¶é—´ï¼‰ å¯¹äºŽå®žè·µæ´¾ï¼šefficientè¡¨ç¤º in a certain time One Time Pad(OTP)Definition of OTPThe one time pad(OTP) åˆå«ä¸€æ¬¡ä¸€å¯†ã€‚ ç”¨å¯¹ç§°åŠ å¯†çš„å®šä¹‰æ¥è¡¨ç¤ºOTPï¼š $\\mathcal{M=C=}{0,1}^n\\quad \\mathcal{K}={0,1}^n$ $Eï¼š\\quad c = E(k,m)=k\\oplus m \\quad$ $D:\\quad m = D(k,c)=k\\oplus c$ æ˜Žæ–‡ç©ºé—´å’Œå¯†æ–‡ç©ºé—´ç›¸åŒï¼Œå¯†é’¥ç©ºé—´ä¹Ÿæ˜¯nä½01ä¸²é›†åˆã€‚ è€Œä¸”ï¼Œåœ¨OTPä¸­ï¼Œå¯†é’¥keyçš„é•¿åº¦å’Œæ˜Žæ–‡messageé•¿åº¦ä¸€æ ·é•¿ã€‚ åŠ å¯†è¿‡ç¨‹å¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ è¯æ˜Žå…¶ä¸€è‡´æ€§æ–¹ç¨‹ Indeed ï¼š $D(k,E(k,m))=D(k,k\\oplus m)=k\\oplus (k\\oplus m)=0\\oplus m=m$ ä½†æ˜¯OTPåŠ å¯†å®‰å…¨å—ï¼Ÿ å¦‚æžœå·²çŸ¥æ˜Žæ–‡(m)å’Œä»–çš„OTPå¯†æ–‡(c)ï¼Œå¯ä»¥ç®—å‡ºç”¨æ¥åŠ å¯†mçš„OTP keyå—ï¼Ÿ ï¼šå½“ç„¶ï¼Œæ ¹æ®å¼‚æˆ–çš„æ€§è´¨ï¼Œkey $k=m\\oplus c$ æ‰€ä»¥ä»€ä¹ˆæ˜¯å®‰å…¨å‘¢ï¼Ÿ Information Theoretic Securityæ ¹æ®Shannon 1949å‘è¡¨çš„è®ºæ–‡ï¼ŒShannonâ€™s basic idea: CT(Ciphertext) should reveal no â€œinfoâ€ about PT(Plaintext)ï¼Œå³å¯†æ–‡ä¸åº”è¯¥æ³„éœ²æ˜Žæ–‡çš„ä»»ä½•ä¿¡æ¯ã€‚ Perfect Security Def:A cipher $(E,D)$ over $\\mathcal{(K,M,C)}$ has perfect security if $\\forall m_0,m_1 \\in \\mathcal{M}\\ (|m_0|=|m_1|) \\quad \\text{and} \\quad \\forall c\\in \\mathcal{C} $$$Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$$ $k \\overset{R}\\longleftarrow \\mathcal{K}$ çš„æ„æ€æ˜¯ $k$ æ˜¯ ä»Ž$\\mathcal{K}$ ä¸­éšæœºå–çš„ï¼Œå³éšæœºå˜é‡ $k$ çš„å–å€¼æ˜¯å‡åŒ€åˆ†å¸ƒã€‚ å¯¹ä»»æ„ $m_0,m_1$ ï¼ˆå¹¶ä¸”messageé•¿åº¦ç›¸åŒï¼‰ï¼Œé‚£ä¹ˆåœ¨å¯†é’¥ç©ºé—´ä»»æ„å– $k$ , $k$ å°† $m_0,m_1$ åŠ å¯†ä¸ºç›¸åŒå¯†æ–‡çš„æ¦‚çŽ‡ç›¸åŒã€‚ å¯¹attackeræ¥è¯´ ï¼šæ”»å‡»è€…æˆªå–ä¸€æ®µå¯†æ–‡cï¼Œé‚£ä¹ˆcæ˜¯ç”± $m_0,m_1$ åŠ å¯†è€Œæ¥çš„æ¦‚çŽ‡æ˜¯ç›¸åŒçš„ï¼Œå³æ”»å‡»è€…ä¹Ÿä¸çŸ¥é“æ˜Žæ–‡åˆ°åº•æ˜¯ $m_0$ è¿˜æ˜¯ $m_1$ ï¼ˆå› ä¸ºæ¦‚çŽ‡ç›¸åŒï¼‰ã€‚ $\\Rightarrow$ Given CT canâ€™t tell if msg is $m_0 \\ \\text{or}\\ m_1 $ (for all $m_i$ ) . ã€æ”»å‡»è€…ä¸èƒ½åŒºåˆ†æ˜Žæ–‡åˆ°åº•æ˜¯ $m_?$ ã€‘ $\\Rightarrow$ most powerful adv.(adversary) learns nothing about PT from CT. ã€ä¸ç®¡æ”»å‡»è€…å¤šèªæ˜Žï¼Œéƒ½ä¸èƒ½ä»Žå¯†æ–‡ä¸­å¾—åˆ°å¯†æ–‡çš„ä¿¡æ¯ã€‘ $\\Rightarrow$ no CT only attack!! (but other attackers possible). ã€å”¯å¯†æ–‡æ”»å‡»å¯¹OTPæ— æ•ˆã€‘ OTP has perfect secrecyLemma : OTP has perfect secrecy. ç”¨ä¸Šä¸€å°èŠ‚çš„perfect securecyçš„å®šä¹‰æ¥è¯æ˜Žè¿™ä¸ªå¼•ç†ã€‚ Proofï¼š è¦è¯æ˜Žï¼š $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$ è¡¨è¾¾å¼ï¼š $\\forall m, c: \\quad \\operatorname{Pr}_{k}[E(k,m)=c]=\\frac{\\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c}{|\\mathcal{K}|}$ å¯¹äºŽä»»æ„m,c, $\\operatorname{Pr}_{k}[E(k,m)=c]$ ç­‰äºŽèƒ½å°†måŠ å¯†ä¸ºcçš„å¯†é’¥ä¸ªæ•°é™¤ä»¥å¯†é’¥ç©ºé—´çš„å¤§å°ã€‚ $\\because |\\mathcal{K}|$ æ˜¯ç›¸åŒçš„ï¼Œæ‰€ä»¥å³è¯ ï¼š $\\{ \\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c \\}=\\text{const}$ å¯¹äºŽä»»æ„ m,cï¼Œèƒ½å°†måŠ å¯†ä¸ºcçš„OTP keyåªæœ‰ä¸€ä¸ªï¼š $k=m\\oplus c$ $\\therefore$ OTP has perfect secrecy. key-len $\\geq$ msg-len Perfect Secrecyçš„æ€§è´¨å¸¦æ¥äº†ä¸€ä¸ªbad newsã€‚ Thm: perfect secrecy $\\Rightarrow$ $|\\mathcal{K}|\\geq|\\mathcal{M}|$ å¦‚æžœä¸€ä¸ªcipheræ»¡è¶³perfect secrecy,é‚£ä¹ˆå…¶å¯†é’¥çš„é•¿åº¦å¿…é¡»å¤§äºŽç­‰äºŽæ˜Žæ–‡é•¿åº¦ã€‚è¿™ä¹Ÿæ˜¯perfect secrecyçš„å¿…è¦æ¡ä»¶ã€‚ æ‰€ä»¥OTPæ˜¯perfect secrecyçš„æœ€ä¼˜æƒ…å†µï¼Œ$|\\mathcal{K}|=|\\mathcal{M}|$ ï¼Œå¯†é’¥é•¿åº¦ç­‰äºŽæ˜Žæ–‡é•¿åº¦ã€‚ ä¸ºä»€ä¹ˆè¯´æ˜¯ä¸€ä¸ªbad newså‘¢ï¼Ÿ å¦‚æžœAliceç”¨OTPç»™Bobå‘ä¸€æ®µmsgï¼Œåœ¨å¥¹å‘ä¹‹å‰ï¼Œå¥¹éœ€è¦å…ˆå‘ä¸€ä¸ªå’Œmsgç­‰é•¿çš„keyï¼Œè¿™ä¸ªkeyåªæœ‰Aliceå’ŒBobçŸ¥é“ã€‚ æ‰€ä»¥å¦‚æžœAliceæœ‰èƒ½ä¿å¯†ä¼ è¾“keyçš„æ–¹æ³•ï¼Œé‚£å¥¹ä½•ä¸ç›´æŽ¥ç”¨è¿™ä¸ªæ–¹æ³•ä¼ è¾“msgå‘¢ï¼Ÿ æ‰€ä»¥OTP : hard to use in practice! (long key-len) å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦key-lençŸ­çš„cipherã€‚ Pseudorandom Generatorsï¼ˆä¼ªéšæœºæ•°ç”Ÿæˆå™¨ï¼‰Stream Ciphers: making OTP practicalStream Ciphersï¼ˆæµå¯†ç ï¼‰çš„æ€æƒ³å°±æ˜¯ï¼šç”¨PRGï¼ˆpseudorandom Generatorsï¼‰ key ä»£æ›¿ â€œrandomâ€ keyã€‚ PRGå…¶å®žå°±æ˜¯ä¸€ä¸ªfunction Gï¼š${ 0,1 }^s\\longrightarrow { 0,1 }^n \\quad, n&gt;&gt;s$ ã€‚ é€šè¿‡å‡½æ•°å°†è¾ƒå°çš„seed spaceæ˜ å°„åˆ°å¤§å¾—å¤šçš„output spaceã€‚ æ³¨æ„ï¼š function G is eff. computable by a deterministic algorithm. å‡½æ•°Gæ˜¯ç¡®å®šçš„ï¼Œéšæœºçš„åªæœ‰sï¼Œsä¹Ÿæ˜¯Gçš„è¾“å…¥ã€‚ PRGçš„è¾“å‡ºåº”è¯¥æ˜¯ â€œlook randomâ€ï¼ˆä¸‹æ–‡ä¼šæåˆ°çš„PRGå¿…é¡»æ˜¯unpredictableï¼‰ Stream Ciphersçš„è¿‡ç¨‹å¦‚ä¸Šå›¾æ‰€ç¤ºï¼šé€šè¿‡PRGï¼Œå°†é•¿åº¦è¾ƒçŸ­çš„kæ˜ å°„ä¸ºè¶³å¤Ÿé•¿çš„G(k)ï¼ŒG(k)å¼‚æˆ–må¾—åˆ°å¯†æ–‡ã€‚ æœ‰ä¸¤ä¸ªé—®é¢˜ï¼Ÿ ç¬¬ä¸€ï¼ŒStream Cipherå®‰å…¨å—ï¼Ÿä¸ºä»€ä¹ˆå®‰å…¨ï¼Ÿ ç¬¬äºŒï¼ŒStream Cipher have perfect secrecy? çŽ°åœ¨ï¼Œåªèƒ½å›žç­”ç¬¬äºŒä¸ªé—®é¢˜ã€‚ ï¼šæµå¯†ç æ²¡æœ‰perfect secrecyã€‚å› ä¸ºå®ƒä¸æ»¡è¶³key-len $\\geq$ msg-lenï¼Œæµå¯†ç çš„å¯†é’¥é•¿åº¦è¿œå°äºŽæ˜Žæ–‡é•¿åº¦ã€‚ æµå¯†ç æ²¡æœ‰perfect secrecyï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜éœ€è¦å¼•å…¥å¦ä¸€ç§å®‰å…¨ï¼Œè¿™ç§å®‰å…¨å’ŒPRGæœ‰å…³ã€‚ PRG must be unpredictablePRGå¦‚æžœpredictableï¼Œæµå¯†ç å®‰å…¨å—ï¼Ÿ Suppose predictableå‡è®¾PRGæ˜¯å¯é¢„æµ‹çš„ï¼Œå³ï¼š $ \\exists:\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1,...,n} $ å·²çŸ¥G(k)è¾“å‡ºçš„å‰i bisï¼Œå­˜åœ¨ä¸€ç§ç®—æ³•ï¼Œèƒ½è®¡ç®—G(k)çš„åŽé¢å‰©ä½™çš„bitsã€‚ æ”»å‡»å¦‚ä¸Šå›¾æ‰€ç¤ºï¼š å¦‚æžœattacker has prior knowledgeï¼šå·²çŸ¥ä¸€æ®µå¯†æ–‡å‰ç¼€çš„å¯¹åº”æ˜Žæ–‡ï¼ˆmæ–œçº¿å­—æ®µï¼‰ï¼ˆæ¯”å¦‚åœ¨SMTPåè®®ä¸­ï¼ŒæŠ¥æ–‡çš„å¼€å¤´æ€»æ˜¯â€fromâ€ï¼‰ attackerå°†è¯¥å¯†æ–‡å­—æ®µä¸Žå·²çŸ¥æ˜Žæ–‡å­—æ®µå¼‚æˆ–ï¼Œå¾—åˆ°G(k)çš„å‰ç¼€ã€‚ å› ä¸ºPRGæ˜¯å¯é¢„æµ‹çš„ï¼Œæ‰€ä»¥å¯ä»¥é€šè¿‡G(k)çš„å‰ç¼€è®¡ç®—å‡ºG(k)çš„å‰©ä¸‹éƒ¨åˆ†ã€‚ å¾—åˆ°çš„G(K)å°±å¯ä»¥æ¢å¤mã€‚ å³ä½¿ï¼ŒG(k)åªèƒ½é¢„æµ‹åŽä¸€ä½ï¼Œå³ $\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1}$ ï¼Œä¹Ÿä¸å®‰å…¨ï¼Œå½“é¢„æµ‹å‡ºä¸‹ä¸€ä½æ—¶ï¼Œåˆå¾—åˆ°äº†æ–°çš„å‰ç¼€ï¼Œæœ€ç»ˆå¾—åˆ°å®Œæ•´çš„G(k)ã€‚ æ‰€ä»¥å½“PRGå¯é¢„æµ‹æ—¶ï¼Œæµå¯†ç å°±ä¸å®‰å…¨äº†ã€‚ æ‰€ä»¥ç”¨Stream Cipheræ—¶ï¼ŒPRGå¿…é¡»unpredictable! Predictable: difinitionPredictable Def : $ \\exists $ \"eff\" alg. A and $\\exists$ $0\\leq i\\leq n-1$ ï¼Œ s.t. $Pr_{k \\overset{R}\\leftarrow \\mathcal{K} } {[A(G(k)|_{1,2,...,i})=G(k)|_{i+1}]}>1/2 +\\epsilon$ for non-negligible $\\epsilon$ (e.g. $\\epsilon=1/2^{30}$) å¯é¢„æµ‹ï¼šå³å­˜åœ¨ç®—æ³•ï¼Œé€šè¿‡G(k)çš„å‰iä½å¯ä»¥è®¡ç®—å‡ºç¬¬i+1ä½çš„æ¦‚çŽ‡å¤§äºŽ1/2 + $\\epsilon$ (ä¸å¯å¿½ç•¥çš„å€¼) Unpredictable Def : å³predictableçš„åé¢ï¼Œ $\\forall i$ : no â€œeff.â€ adv. can predict bit(i+1) for â€œnon-negâ€ $\\epsilon$ . Qï¼šå‡è®¾ $\\mathrm{G}: \\mathrm{K} \\rightarrow{0,1}^{\\mathrm{n}} $ ï¼Œæ»¡è¶³XOR(G(k))=1ï¼ŒGå¯é¢„æµ‹å—ï¼Ÿ Wï¼šGå¯é¢„æµ‹ï¼Œå­˜åœ¨i = n-1,å› ä¸ºå½“å·²çŸ¥å‰n-1ä½,å¯ä»¥é¢„æµ‹ç¬¬nä½ã€‚ Weak PRGsLinear Congruential Generatorsä¸€ç§åº”è¯¥æ°¸è¿œä¸åœ¨å®‰å…¨åº”ç”¨ä¸­ä½¿ç”¨PRGâ€”â€”LCGï¼ˆlinear congruential generatorsï¼‰(çº¿æ€§åŒä½™éšæœºç”Ÿæˆå™¨)ã€‚ è™½ç„¶ä»–ä»¬åœ¨åº”ç”¨ä¸­ä½¿ç”¨å¾ˆå¿«ï¼Œè€Œä¸”å…¶è¾“å‡ºè¿˜æœ‰è‰¯å¥½çš„ç»Ÿè®¡æ€§è´¨ï¼ˆæ¯”å¦‚0çš„ä¸ªæ•°å’Œ1çš„ä¸ªæ•°åŸºæœ¬ç›¸ç­‰ç­‰ï¼‰ï¼Œä½†ä»–ä»¬åº”è¯¥never be used for cryptographicã€‚ å› ä¸ºåœ¨å®žè·µä¸­ï¼Œç»™å‡ºLCGè¾“å‡ºçš„ä¸€äº›è¿žç»­åºåˆ—ï¼Œå¾ˆå®¹æ˜“è®¡ç®—å‡ºè¾“å‡ºçš„å‰©ä½™åºåˆ—ã€‚ Basic LCGDefinitionBasic LCG has four public system parameters: an integer q, two constants a,b $\\in { 0,â€¦,q-1}$ , and a positive integer $w\\leq q$ . The constant a is taken to be relatively prime to q. ã€æœ‰å››ä¸ªå…¬å¼€å‚æ•°ï¼šæ•´æ•°qï¼Œä¸¤ä¸ªqå‰©ä½™ç³»ä¸‹çš„å¸¸æ•°a,bï¼Œï¼ˆaä¸Žqäº’ç´ ï¼‰å’Œä¸€ä¸ªå°äºŽç­‰äºŽqçš„æ­£æ•´æ•°wã€‚ã€‘ We use $\\mathcal{S}_q$ and $\\mathcal{R}$ to denote the sets: $\\mathcal{S}_{q}:=\\{0, \\ldots, q-1\\} ; \\quad \\mathcal{R}:=\\{0, \\ldots,\\lfloor(q-1) / w\\rfloor\\}$ Now, the generators $G_{\\mathrm{lcg}}: \\mathcal{S}_{q} \\rightarrow \\mathcal{R} \\times \\mathcal{S}_{q}$ with seed $s\\in\\mathcal{S}_{q}$ defined as follows: $G_{\\operatorname{lcg}}(s):=(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ ã€LCGçš„è¾“å‡ºæ˜¯ä¸€å¯¹æ•°ï¼Œ$(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ ã€‘ å½“ $w=2^t$ æ—¶ï¼Œ$\\lfloor s / w\\rfloor$ simply erases the t least significant bits of sã€å³ç§»tä½ã€‘ã€‚ Insecureå½“å·²çŸ¥ $s^{\\prime}:=a s+b \\bmod q$ ï¼Œå³å¯ç›´æŽ¥æ±‚å‡ºsï¼Œä¹Ÿå°±æ±‚å‡ºäº†æ‰€è°“çš„éšæœºæ•° $\\lfloor s/w\\rfloor$ . Variant: Blum-Micali constructionDefinition å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå˜ä½“çš„LCGæ˜¯ä¸€ä¸ªè¿­ä»£ï¼Œè¾“å‡ºä¸åŒ…æ‹¬ $s_i$ ï¼ŒæŠŠ $r_1,â€¦,r_n$ ä½œä¸ºä¸€æ¬¡è¿­ä»£çš„è¾“å‡ºã€‚ ä¸åŒçš„åº”ç”¨ç³»ç»Ÿä½¿ç”¨ä¸åŒçš„ $q,a,b,w$ å‚æ•°ï¼Œåœ¨Java 8 Development Kitï¼ˆJDKv8ï¼‰ä¸­ï¼Œ$q=2^{48}$ , $w=2^{22}$ ,constant $a=\\text{0x5DEECE66D}$ , $b=\\text{0x0B}$ ã€‚ æ‰€ä»¥åœ¨JDKv8ä¸­, LCGçš„è¾“å‡ºå…¶å®žæ˜¯ $s_i$ï¼ˆ48bitsï¼‰ çš„å‰48-22=26 bits ã€‚ æ˜¾ç„¶JDKv8ä¸­çš„å‚æ•°å¤§å°åº”ç”¨åœ¨å®‰å…¨ç³»ç»Ÿä¸­ï¼Œè¿˜æ˜¯å¤ªä¸å®‰å…¨äº†ã€‚ how to attack in JDKv8 åœ¨è¿­ä»£çš„ç¬¬ä¸€æ¬¡è¾“å‡ºä¸­ï¼ŒLCGå°± reveal 26bits of the seed sã€‚ å¯¹äºŽså‰©ä¸‹çš„åŽ22ä¸ªbitsï¼Œattacker can easily recover them by exhausitive search(ç©·ä¸¾)ï¼š å¯¹äºŽæ¯ä¸ªå¯èƒ½çš„å–å€¼ï¼Œattackeréƒ½èƒ½å¾—åˆ°ä¸€ä¸ªå€™é€‰seed $\\hat{s}$ ç”¨ $\\hat{s}$ æ¥éªŒè¯æˆ‘ä»¬æ‰€ç›´æŽ¥å¾—åˆ°çš„LCGçš„è¾“å‡ºã€‚ å¦‚æžœ $\\hat{s}$ éªŒè¯å¤±è´¥ï¼Œåˆ™åˆ°ç¬¬ä¸‰æ­¥ç»§ç»­ç©·ä¸¾ã€‚ç›´è‡³éªŒè¯æˆåŠŸã€‚ å½“ç©·ä¸¾è‡³æ­£ç¡®çš„sæ—¶ï¼Œå°±å¯ä»¥ç›´æŽ¥é¢„æµ‹LCGçš„å‰©ä½™è¾“å‡ºã€‚ åœ¨çŽ°ä»£å¤„ç†å™¨ä¸­ï¼Œç©·ä¸¾ $2^{22}$ (4 million) åªéœ€è¦1ç§’ã€‚æ‰€ä»¥LCGçš„å‚æ•°è¾ƒå°æ—¶ï¼Œæ˜¯å¾ˆå®¹æ˜“attackã€‚ å½“ $q=2^{512}$ æ—¶ï¼Œè¿™ç§ç©·ä¸¾çš„æ”»å‡»æ–¹æ³•å°±å¤±æ•ˆäº†ã€‚ä½†æ˜¯æœ‰ä¸€ç§å¯¹äºŽLCGçš„è‘—åæ”»å‡»æ–¹æ³•[1]ï¼Œå³ä½¿æ¯æ¬¡è¿­ä»£ï¼ŒLCGåªè¾“å‡ºè¾ƒå°‘çš„bitsï¼Œä¹Ÿèƒ½ä»Žè¿™äº›è¾ƒå°‘çš„ä½†è¿žç»­çš„è¾“å‡ºåºåˆ—ä¸­é¢„æµ‹å‡ºæ•´ä¸ªLCGè¾“å‡ºåºåˆ—ã€‚ Cryptanalysis ï¼šelegant attackWarning of MathSupposeSuppose : q is large (e.g. $q=2^{512}$ ), and $G_{lcg}^{(n)}$ outputs about half the bits of the state s per iteration. ã€qå¾ˆå¤§ï¼Œ $G_{lcg}^{(n)}$ æ¯æ¬¡è¾“å‡ºsçš„ä¸€åŠå·¦å³çš„bitsã€‘ More precisely, suppose: $w&lt;\\sqrt{q}/c$ for fixed cï¼ˆe.g. $c=32$ ï¼‰ ã€ä¿è¯è¾“å‡ºså‰ä¸€åŠå·¦å³bitsçš„è¿™ä¸ªæ¡ä»¶ã€‘ Suppose the attacker is given two consecutive outputs of the gnerator $r_i,r_{i+1}\\in \\mathcal{R}$ . ã€å·²çŸ¥ä¸¤ä¸ªè¿žç»­è¾“å‡º $r_i,r_{i+1}\\in \\mathcal{R}$ ã€‘ Attacker Knows $r_{i}=\\left\\lfloor s_{i} / w\\right\\rfloor \\quad \\text { and } \\quad r_{i+1}=\\left\\lfloor s_{i+1} / w\\right\\rfloor=\\left\\lfloor\\left(a s_{i}+b \\bmod q\\right) / w\\right\\rfloor$ ã€å·²çŸ¥ï¼š $r_i,r_{i+1},w,a,b,q$ï¼›æœªçŸ¥ï¼š $s_i$ ã€‘ $r_{i} \\cdot w+e_{0}=s \\quad \\text { and } \\quad r_{i+1} \\cdot w+e_{1}=a s+b+q x \\qquad (0\\leq e_0,e_1&lt;w&lt;\\sqrt{q}/c)$ ã€ åŽ»æŽ‰floorç¬¦å·å’Œmodï¼š$e_0,e_1$ æ˜¯ $s_i,s_{i+1}$ é™¤ $w$ çš„ä½™æ•°ã€‘ ã€å·²çŸ¥ï¼š $r_i,r_{i+1},w,a,b,q$ï¼›æœªçŸ¥ï¼š $s_i,e_0,e_1,x$ ã€‘ re-arranging: put $x$ and $s$ on the left $s=r_{i} \\cdot w+e_{0} \\quad \\text { and } \\quad a s+q x=r_{i+1} w-b+e_{1}$ ã€æŠŠæœªçŸ¥å‚æ•°sï¼Œxæ”¾åœ¨ç­‰å¼å·¦è¾¹ï¼Œæ–¹ä¾¿å†™æˆçŸ©é˜µå½¢å¼ã€‘ $s \\cdot\\left(\\begin{array}{l}1 \\ a\\end{array}\\right)+x \\cdot\\left(\\begin{array}{l}0 \\ q\\end{array}\\right)=\\boldsymbol{g}+\\boldsymbol{e} \\quad \\text { where } \\quad \\boldsymbol{g}:=\\left(\\begin{array}{c}r_{i} w \\ r_{i+1} w-b\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{e}:=\\left(\\begin{array}{c}e_{0} \\ e_{1}\\end{array}\\right)$ ã€å·²çŸ¥ï¼š $\\boldsymbol{g},a,q$ ï¼ŒæœªçŸ¥ï¼š$\\boldsymbol{e},s,x$ ã€‘ to break the generator it suffices to find the vector $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}$ . ã€ä»¤ $u\\in {Z}^2$ , $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}=s \\cdot(1, a)^{\\mathrm{T}}+x \\cdot(0, q)^{\\mathrm{T}}$ ã€‘ ã€å¦‚æžœæˆ‘ä»¬æ±‚å‡ºäº† $\\boldsymbol{u}$ ï¼Œé‚£å¯ä»¥ç”¨çº¿æ€§ä»£æ•°çš„çŸ¥è¯†è§£å‡º $s$ å’Œ $x$ ,å†ç”¨ $s$ æ¥é¢„æµ‹PRGçš„å‰©ä¸‹è¾“å‡ºã€‘ konws $\\boldsymbol{g}$ , knows $\\boldsymbol{e}$ is shorter, and $|\\boldsymbol{e} |_{\\infty}$ is at most $\\sqrt{q}/c$ , knows that $\\boldsymbol{u}$ is â€œcloseâ€ to $\\boldsymbol{g}$ . ã€eå‘é‡å¾ˆå°ï¼Œ$|\\boldsymbol{e} |_{\\infty}$ ä¸Šç•Œæ˜¯$\\sqrt{q}/c$ ï¼Œuç¦»gå¾ˆè¿‘ã€‘ Taxicab norm or Manhattan(1-norm) ${\\|}A{\\|}_1=\\max \\{ \\sum|a_{i1}|,\\sum|a_{i2}|,...,\\sum|a_{in}| \\}$ ï¼ˆåˆ—å’ŒèŒƒæ•°ï¼ŒAçŸ©é˜µæ¯ä¸€åˆ—å…ƒç´ ç»å¯¹å€¼ä¹‹å’Œçš„æœ€å¤§å€¼ï¼‰ Euclidean norm(2-norm) $\\|\\mathbf{x}\\|=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}\\right)^{1 / 2}$ $\\infty$-èŒƒæ•° $\\|A\\|_{\\infty}=\\max \\{ \\sum|a_{1j}|,\\sum|a_{2j}|,...,\\sum|a_{mj}| \\}$ ï¼ˆè¡Œå’ŒèŒƒæ•°ï¼ŒAçŸ©é˜µæ¯ä¸€è¡Œå…ƒç´ ç»å¯¹å€¼ä¹‹å’Œçš„æœ€å¤§å€¼ï¼‰ attack can figure the lattice with attacking LCG. the lattice is generated by the vectors $(1,5)^T$ and $(0,29)^T$ , the attacker has a vector $\\boldsymbol{g}=(9,7)^T$ and wishes to find the closest lattice vector $\\boldsymbol{u}$ . ã€ä¸Šå›¾æ˜¯ $(1,5)^T$ å’Œ $(0,29)^T$ ä¸¤ä¸ªå‘é‡ç”Ÿæˆçš„çš„æ ¼ç‚¹ï¼Œå¸Œæœ›èƒ½ä»Žä»¥ä¸Šæ ¼ç‚¹æ‰¾åˆ°ç¦»å·²çŸ¥ $\\boldsymbol{g}$ å‘é‡æœ€è¿‘çš„æ ¼ç‚¹ã€‘ $\\mathcal{L}_a$ :ç”± $(1, a)^{\\mathrm{T}},(0, q)^{\\mathrm{T}}$ ä½œä¸ºåŸºå‘é‡ç”Ÿæˆçš„ç‚¹é›†åˆã€‚ The problem is a special case of a general problem call the closest vector problem: given a lattice $\\mathcal{L}$ and a vector $\\boldsymbol{g}$ ,find a vector in $\\mathcal{L}$ that is closest to $\\mathcal{g}$ . There is an efficient polynomial time algorithm for this problem.[2] ã€é—®é¢˜å½’ç»“äºŽ closest vector problemé—®é¢˜ï¼Œåœ¨å·²çŸ¥æ …æ ¼ç‚¹é›†åˆä¸­æ‰¾ç¦»æŸä¸€å‘é‡æœ€è¿‘çš„ç‚¹ï¼Œæ­¤é—®é¢˜å·²æœ‰å¤šé¡¹å¼æ—¶é—´ç®—æ³•ã€‘ step 8 aboveLemma : * For at least $(1-16/c^2)\\cdot q $ of the a in $\\mathcal{S}_q$ , the lattice $\\mathcal{L}_a\\sub Z_2$ has the following property: for every $\\boldsymbol{g} \\in Z^2$ there is at most one vector $\\boldsymbol{u}\\in \\mathcal{L}_a$ such that $\\|\\boldsymbol{g}-\\boldsymbol{u}\\|_{\\infty}","link":"/2020/03/14/stanford-crypto-streamcipher1/"},{"title":"ã€ŒCryptography-Bonehã€:Stream Cipher 2","text":"ä½œä¸ºStream Cipherçš„ç¬¬äºŒç¯‡æ–‡ç« ã€‚ç¬¬ä¸€éƒ¨åˆ†åˆ†æžäº†åŸºäºŽStream Cipherçš„ä¸¤ç§æ”»å‡»ï¼šç¬¬ä¸€ç§æ˜¯Two time pad,ç¬¬äºŒç§æ˜¯å¯¹ä¸Žå…¶å®Œæ•´æ€§çš„æ”»å‡»ï¼Œå³æµå¯†ç æ˜¯å¯è¢«ç¯¡æ”¹çš„ã€‚ç¬¬äºŒéƒ¨åˆ†å…·ä½“è¯´æ˜Žäº†ä¸€äº›ä½¿ç”¨æµå¯†ç åŠ å¯†çš„ä¾‹å­ã€‚åŒ…æ‹¬åˆ†æžåŸºäºŽè½¯ä»¶çš„RC4æµå¯†ç ã€åŸºäºŽç¡¬ä»¶çš„CSSæµå¯†ç å’ŒçŽ°ä»£çš„å®‰å…¨æµå¯†ç :eStreamä¸­çš„Salsa20ã€‚ Attack on OTP and stream ciphersAttack1: two time pad is insecureNever use strame cipher key more than once!! why insecureä½¿ç”¨ç›¸åŒçš„PRG(k)åŠ å¯†ä¸åŒæ˜Žæ–‡æ—¶ï¼š$$C_1 \\leftarrow m_1 \\oplus \\text{PRG(k)}\\C_2 \\leftarrow m_2 \\oplus \\text{PRG(k)}$$Eavesdropperï¼ˆçªƒå¬è€…ï¼‰æˆªèŽ·è¿™ä¸¤æ®µå¯†æ–‡ $C_1\\ C_2$ ï¼Œå¯¹å¯†æ–‡è¿›è¡Œç–‘æƒ‘æ“ä½œï¼Œå¯å¾—ï¼š $C_1 \\oplus C_2\\rightarrow m_1\\oplus m_2$ ã€‚ åœ¨ä¼ è¾“ä¸­ï¼Œè‹±è¯­å­—æ¯æ˜¯ç”¨ASCIIç¼–ç åŽå†ä¼ è¾“ï¼Œæ‰€ä»¥è¿™æ ·çš„ç¼–ç ä¼šå¸¦æ¥å¾ˆå¤šredundancyï¼ˆå†—ä½™ï¼‰ï¼Œå³æ ¹æ® $m_1\\oplus m_2$ ï¼Œå¯ä»¥å¾—åˆ° $m_1\\ m_2$ ã€‚ å› æ­¤ï¼Œå½“ä¸€ä¸ªå¯†é’¥ä¼šè¢«ä½¿ç”¨å¤šæ¬¡æ—¶ï¼Œå°±ä¸åº”è¯¥ç›´æŽ¥ç”¨stream cipherï¼ŒåŽé¢çš„ç« èŠ‚ä¼šä»‹ç»multi-use ciphersã€‚ Examples: Project Venona(1941-1946)æˆ‘ä»¬å·²ç»çŸ¥é“ï¼šåŠ å¯†åº”è¯¥ç”¨OTPï¼Œå³ä¸€æ¬¡æ€§å¯†é’¥ã€‚ ä½†æ˜¯ï¼Œå½“æ—¶æ˜¯é€šè¿‡äººå·¥æŽ·éª°å­å¹¶è®°å½•å¾—åˆ°å¯†é’¥ï¼Œå·¥ä½œè´¹æ—¶è´¹åŠ›ã€‚å› æ­¤ä¸å¾—ä¸ç”¨ç”Ÿæˆçš„å¯†é’¥åŠ å¯†å¤šæ¡æ¶ˆæ¯ã€‚ æœ€åŽä»…å‡­æˆªèŽ·å¯†æ–‡ï¼Œå°±ç ´è¯‘äº†3000å¤šæ¡æ¶ˆæ¯ã€‚ Examples: MS-PPTP(Windows NT)å¾®è½¯åœ¨Windows NTçš„PPTPåè®®ï¼ˆpoint to point transfer protocolï¼‰ä¸­ä½¿ç”¨çš„æµå¯†ç æ˜¯ï¼šRC4ã€‚ åœ¨è¿™ä¸ªåè®®ä¸­å…è®¸ä¸€ä¸ªç«¯ç³»ç»Ÿå‘å¦ä¸€ä¸ªç«¯ç³»ç»Ÿå‘é€åŠ å¯†åŽçš„ä¿¡æ¯ã€‚ è¿‡ç¨‹å¦‚ä¸‹ï¼š åœ¨ä¸€æ¬¡å¯¹è¯è¿žæŽ¥ä¸­ï¼šä¸»æœºæƒ³å‘é€$m_1\\ m_1\\ m_3$ ä¸‰æ¡æ¶ˆæ¯è¿›è¡ŒæŸ¥è¯¢ï¼ŒæœåŠ¡å™¨æƒ³å‘é€ $s_1\\ s_1\\ s_3$ ä¸‰æ¡æ¶ˆæ¯è¿›è¡Œå“åº”ã€‚ ä¸»æœºå’ŒæœåŠ¡å™¨hava a shared key:kã€‚ çŸ¥é“å¯†é’¥ä¸èƒ½åŠ å¯†å¤šæ¬¡ï¼ŒäºŽæ˜¯ä¸»æœºå°†ä¸‰æ¡æ¶ˆæ¯è¿›è¡Œconcatenationï¼ˆè”ç»“ï¼‰ï¼š $m_1||m_2||m_3$ ã€‚ ä¸»æœºç”¨kä½œä¸ºå¯†é’¥ï¼Œå¾—åˆ°G(k)ï¼Œè¿›è¡ŒåŠ å¯† $[m_1||m_2||m_3]\\oplus\\text{G(k)}$ ã€‚ åŒæ ·ï¼ŒæœåŠ¡å™¨ä¹Ÿå°†å“åº”æ¶ˆæ¯è¿›è¡Œè”ç»“ï¼š $s_1||s_2||s_3$ ã€‚ æœåŠ¡å™¨ä¹Ÿç”¨kä½œä¸ºå¯†é’¥ï¼Œå¾—åˆ°ç›¸åŒçš„G(k)ï¼Œå¯¹å“åº”æ¶ˆæ¯è¿›è¡ŒåŠ å¯† $[s_1||s_2||s_3]\\oplus\\text{G(k)}$ ã€‚ å› æ­¤ï¼Œåœ¨ä¸€æ¬¡å¯¹è¯ä¸­ï¼Œä¸»æœºå’ŒæœåŠ¡å™¨éƒ½ä½¿ç”¨äº†ç›¸åŒçš„ G(k)è¿›è¡ŒåŠ å¯†ï¼Œä¹Ÿå°±æ˜¯ two time padã€‚ å¦‚ä½•æ”¹è¿›ä¸»æœºå’ŒæœåŠ¡å™¨have a shared pair of key, å³ä¸»æœºå’ŒæœåŠ¡å™¨éƒ½ä½¿ç”¨ä¸åŒçš„keyè¿›è¡ŒåŠ å¯†ã€‚ Examples: 802.11b WEPHow it worksWEP(Wired Equivalent Privacy)ï¼Œæœ‰æ•ˆç­‰æ•ˆåŠ å¯†ï¼Œæ˜¯ä¸€ç§ç”¨äºŽIEEE 802.11bçš„ä¸€ç§å®‰å…¨ç®—æ³•ã€‚è¿™ä¸ªç®—æ³•è®¾è®¡çš„å¾ˆç³Ÿç³•ï¼ŒçŽ°å·²è¢«WPAæ‰€æ·˜æ±°ã€‚ WEPç”¨äºŽWi-Fié€šä¿¡ï¼Œæ˜¯ä»–çš„çš„åŠ å¯†å±‚ã€‚ WEPçš„ç®—æ³•è¿‡ç¨‹å¦‚ä¸‹ï¼š ä¸»æœºå’Œè·¯ç”±å™¨è¿›è¡Œé€šä¿¡ï¼š ä¸»æœºå’Œè·¯ç”± have a shared keyã€‚ ä¸»æœºæƒ³è¦å‘é€ä¸€æ®µæ¶ˆæ¯ï¼ŒåŒ…æ‹¬æ˜Žæ–‡må’Œå…¶æ ¡éªŒç CRC(m)ã€‚ PRGâ€™s seedï¼š IV||k, k is a long term keyï¼ŒIV is a counter. Length of IV: 24 bits. IVçš„ä½œç”¨ï¼šæ¯ä¸€æ¬¡ä¼ é€æ•°æ®åŒ…æ—¶ï¼Œç”¨IVæ¥æ”¹å˜æ¯æ¬¡çš„å¯†é’¥ã€‚ ç”¨(IV||kä½œä¸ºå¯†é’¥ï¼Œå¾—åˆ°PRG(IV||k),ä½¿ç”¨æµå¯†ç è¿›è¡ŒåŠ å¯†ä¼ è¾“ã€‚ ä¸»æœºç›´æŽ¥å‘é€IVå’Œå¯†æ–‡ã€‚ è·¯ç”±å™¨ç”¨æ”¶åˆ°çš„IVå’Œkè¿žæŽ¥ï¼Œç”¨PRG(IV||k)ï¼Œå¯¹å¯†æ–‡è§£å¯†ã€‚ Problems of IV IV å¯¼è‡´çš„é—®é¢˜1: two time pad Length of IV: 24 bits Related IV after $2^{24}$ (16M frames) ã€å½“å‘é€16ç™¾ä¸‡çš„å¸§åŽï¼ŒPRGåˆä¼šé‡å¤ã€‘ On some 802.11 cards: IV rests to 0 after power cycle. ã€åœ¨æœ‰äº›å•ç‰‡æœºä¸Šï¼Œé‡å¯åŽIVä¼šå˜æˆ0ï¼šæ¯æ¬¡é‡å¯éƒ½ä¼šä½¿ç”¨PRG(0||k)åŠ å¯†ã€‘ IV å¯¼è‡´çš„é—®é¢˜2: related keys åœ¨PRGä¸­ï¼Œkey for frame is relatedã€‚(IV||k)ï¼Œkæ˜¯104 bits, IV æ˜¯24 bitsï¼Œæ‰€ä»¥keyçš„åŽ104ä½æ€»æ˜¯ç›¸åŒçš„ï¼Œä¸åŒå¯†é’¥ä¹‹é—´çš„å·®å¼‚ä¹Ÿå¾ˆå°ã€‚ å¯¹RC4 PRGçš„æ”»å‡»ï¼š Fluhrer, Mantin, and Shamiråœ¨2001å¹´:åªéœ€è¦ç›‘å¬ $10^6$ å¸§å³å¯ç ´è¯‘å¯†é’¥[1]ã€‚ Recent attacksï¼šåªéœ€è¦ç›‘å¬4000å¸§ï¼Œå³å¯ç ´è¯‘WEPç½‘ç»œä¸­çš„å¯†é’¥ã€‚ æ‰€ä»¥ï¼Œå¯†é’¥å…³è”æ˜¯ç¾éš¾æ€§çš„ã€‚ Avoid related keysï¼ A better constructionå¯¹äºŽWEPï¼Œä¸€ç§æ›´å¥½çš„åšæ³•æ˜¯ï¼šå°†å¤šä¸ªè¦å‘é€çš„å¸§è”ç»“èµ·æ¥ï¼Œå¾—åˆ° $m_1||m_2||â€¦||m_n$ é•¿æµï¼Œå†ç”¨PRGå¯¹è¿™ä¸ªé•¿æµåŠ å¯†ã€‚ å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œkæ‰©å±•åŽï¼Œè¢«åˆ†æˆå¾ˆå¤šæ®µï¼Œç”¨ç¬¬ä¸€æ®µåŠ å¯†ç¬¬ä¸€å¸§ï¼Œç¬¬äºŒæ®µåŠ å¯†ç¬¬äºŒå¸§â€¦â€¦ã€‚ è¿™æ ·ï¼ŒåŠ å¯†æ¯ä¸€å¸§çš„å¯†é’¥éƒ½æ˜¯ä¸€ä¸ªä¼ªéšæœºå¯†é’¥ã€‚(no relationship, and looks like random)ã€‚ å½“ç„¶ï¼Œæ›´å¥½çš„è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨æ›´å¼ºçš„åŠ å¯†ç®—æ³•ï¼ˆWPA2ï¼‰ã€‚ Examples: disk encryptionå¦ä¸€ä¸ªä¾‹å­æ˜¯ç¡¬ç›˜åŠ å¯†ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä½ ä¼šå‘çŽ°ï¼šä½¿ç”¨æµå¯†ç å¯¹ç¡¬ç›˜åŠ å¯†ä¸æ˜¯ä¸€ä¸ªå¥½çš„ä¸¾æŽªã€‚ å¦‚æžœä½¿ç”¨æµå¯†ç ï¼š Alice æƒ³è¦ç»™Bobå†™ä¸€å°é‚®ä»¶ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ é‚®ä»¶ç»è¿‡ç¡¬ç›˜åŠ å¯†ï¼ˆæµå¯†ç ï¼‰åŽï¼Œå­˜å…¥å†…å­˜å—ã€‚ Alice æƒ³è¦å¯¹å­˜åœ¨è¿™ä¸ªç¡¬ç›˜ä¸­çš„é‚®ä»¶è¿›è¡Œä¿®æ”¹ã€‚ AliceåªæŠŠBobæ”¹æˆäº†Eveï¼Œå…¶ä»–éƒ¨åˆ†éƒ½æ²¡æœ‰å˜ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ ä¿å­˜åŽï¼Œé‚®ä»¶å†æ¬¡ç»è¿‡ç¡¬ç›˜åŠ å¯†ï¼ˆæµå¯†ç ï¼‰åŽï¼Œå­˜å…¥å†…å­˜å—ã€‚ Attackerï¼šä»–å¾—åˆ°äº†ç¡¬ç›˜ä¸Šæœ€åˆçš„å¯†æ–‡å’Œä¿®æ”¹åŽçš„å¯†æ–‡ã€‚ é€šè¿‡åˆ†æžï¼Œä»–å‘çŽ°ä¸¤æ®µå¯†æ–‡åªæœ‰å‰å°éƒ¨åˆ†ä¸åŒã€‚ï¼ˆç”¨ç›¸åŒçš„æµå¯†ç å¯†é’¥åŠ å¯†ï¼Œä¿®æ”¹åŽï¼Œå¯†æ–‡å¾ˆå®¹æ˜“çœ‹å‡ºå˜åŒ–ï¼‰ è™½ç„¶Attackerä¸çŸ¥é“Aliceæ˜¯æ€Žä¹ˆä¿®æ”¹çš„ï¼Œä½†æ˜¯ä»–çŸ¥é“äº†Aliceä¿®æ”¹çš„å…·ä½“ä½ç½®ã€‚ $\\Rightarrow$ attackerå¾—åˆ°äº†ä»–ä¸åº”è¯¥çŸ¥é“çš„ä¿¡æ¯ï¼Œå³ä¿®æ”¹çš„å…·ä½“ä½ç½®ã€‚ åœ¨ç¡¬ç›˜åŠ å¯†ä¸­ï¼Œå¯¹äºŽæ–‡æœ¬å†…å®¹çš„ä¿®æ”¹å‰åŽï¼Œä¹Ÿä½¿ç”¨äº†ç›¸åŒçš„å¯†é’¥æ®µåŠ å¯†ä¸åŒçš„æ¶ˆæ¯ï¼Œå³two time padã€‚ å› æ­¤åœ¨ç¡¬ç›˜åŠ å¯†ä¸­ï¼Œä¸æŽ¨èä½¿ç”¨æµå¯†ç ã€‚ Two time pad: SummaryNever use stream cipher key more than once!! Network traffic: negotiate new key for every session. Disk encryption: typically do not use a stream cipher. Attack2: no integrity(OTP is malleable)OPTPå’ŒStream Cipherä¸€æ ·ï¼Œä¸æä¾›å®Œæ•´æ€§çš„ä¿è¯ï¼Œåªæä¾›æœºå¯†æ€§çš„ä¿è¯ã€‚ å¦‚ä¸Šå›¾æ‰€ç¤ºï¼š å¦‚æžœattackeæˆªèŽ·ï¼šå¯†æ–‡ $m\\oplus k$ ã€‚ å¹¶ç”¨sub-permutation keyï¼ˆå­ç½®æ¢å¯†é’¥ï¼‰æ¥å¯¹å¯†æ–‡è¿›è¡Œä¿®æ”¹ï¼Œå¾—åˆ°æ–°çš„å¯†æ–‡ï¼š$(m\\oplus k)\\oplus p$ æ–°çš„å¯†æ–‡æœ€åŽè§£å¯†å¾—åˆ°çš„æ˜Žæ–‡æ˜¯ $m\\oplus p$ ã€‚ æ‰€ä»¥å¯¹äºŽæœ‰ä¿®æ”¹å¯†æ–‡èƒ½åŠ›çš„æ”»å‡»è€…æ¥è¯´ï¼Œæ”»å‡»è€…å¾ˆå®¹æ˜“ä¿®æ”¹å¯†æ–‡ï¼Œå¹¶ä¸”ä¿®æ”¹åŽçš„å¯†æ–‡ï¼Œå¯¹åŽŸæœ¬è§£å¯†åŽçš„æ˜Žæ–‡ä¹Ÿæœ‰å¾ˆå¤§çš„å½±å“ã€‚ å…·ä½“æ”»å‡»å¦‚ä¸‹ï¼š Bobæƒ³è¦å‘é€ä¸€å°é‚®ä»¶ï¼šæ¶ˆæ¯å¼€å¤´æ˜¯From: Bobï¼Œä½¿ç”¨OTPåŠ å¯†åŽï¼Œå‘é€å¯†æ–‡ã€‚ Attackerï¼šæˆªèŽ·äº†è¿™æ®µå¯†æ–‡ å‡è®¾ï¼šattackerçŸ¥é“è¿™å°é‚®ä»¶æ˜¯æ¥è‡ªBobã€‚ attackeræƒ³ä¿®æ”¹è¿™å°å¯†æ–‡é‚®ä»¶ï¼Œä½¿å¾—å®ƒæ¥è‡ªothersã€‚ äºŽæ˜¯å®ƒç”¨ä¸€ä¸ªå­ç½®æ¢å¯†é’¥å¯¹åŽŸå¯†æ–‡çš„ç‰¹å®šä½ç½®ï¼ˆå³Bobå¯†æ–‡ä½ç½®ï¼‰è¿›è¡Œæ“ä½œï¼Œå¾—åˆ°æ–°çš„å¯†æ–‡ï¼šFromï¼š Eveã€‚ è¿™ä¸ªå­ç½®æ¢å¯†é’¥æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒBobçš„ASCIIç æ˜¯ 42 6F 62ï¼ŒEveçš„ASCIIç æ˜¯ 45 76 65ã€‚ é‚£ä¹ˆBob $\\oplus$ Eveçš„ASCIIç æ˜¯ 07 19 07ã€‚ å› æ­¤è¿™ä¸ªå­ç½®æ¢å¯†é’¥æ˜¯07 19 07ã€‚ æœ€åŽæ”¶ä»¶äººè¿›è¡Œè§£å¯†ï¼Œå¾—åˆ°çš„æ˜¯æ˜Žæ–‡ï¼šFromï¼šEveã€‚ å¯¹attackeræ¥è¯´ï¼Œè™½ç„¶ä»–ä¸èƒ½åˆ›å»ºæ¥è‡ªEveçš„å¯†æ–‡ï¼Œä½†æ˜¯ä»–å¯ä»¥é€šè¿‡ä¿®æ”¹åŽŸæœ¬çš„å¯†æ–‡ï¼Œè¾¾åˆ°ç›¸åŒçš„ç›®çš„ã€‚ Conclusion: Modifications to ciphertext are undertected and have predictable impact on plaintext. Real-world Stream CiphersOld example(SW): RC4RC4æµå¯†ç ï¼Œæ˜¯Ron RivestRC4åœ¨1987å¹´è®¾è®¡çš„ã€‚æ›¾ç”¨äºŽsecure Web traffic(in the SSL/TLS protocol) å’Œwireless traffic (in the 802.11b WEP protocol). It is designed to operate on 8-bit processors with little internal memory. At the heart of the RC4 cipher is a PRG, called the RC4 PRG. The PRG maintains an internal state consisting of an array S of 256 bytes plus two additional bytes i,j used as pointers into S. ã€RC4 cipherçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªPRGï¼Œcalled the RC4 PRGã€‚è¿™ä¸ªPRGçš„å†…éƒ¨çŠ¶æ€æ˜¯ä¸€ä¸ª256å­—èŠ‚çš„æ•°ç»„Så’Œä¸¤ä¸ªæŒ‡å‘Sæ•°ç»„çš„æŒ‡é’ˆã€‘ RC4 stream cipher generator setup algorithms: å¯¹Sæ•°ç»„è¿›è¡Œåˆå§‹åŒ–ï¼Œ0-255éƒ½åªå‡ºçŽ°ä¸€æ¬¡å…¥ä¸‹å›¾æ‰€ç¤ºï¼š ä¼ªä»£ç  stream generator: Once tha array S is initialized, the PRG generates pseudo-random output one byte at a time, using the following stream generator: The procedure runs for as long as necessary. Again, the index i runs linearly through the array while the index j jumps around. Security of RC4å…·ä½“å‚è§ã€ŒA Graduate Course in Applied Cryptographyã€çš„p76-78 æŒ–å‘ï¼Œæœ‰ç©ºå¡«å‘ cryptanalysis of RC4[2] Weakness of RC4 Bias in initial output: Pr[2^nd^ byte=0]=2/256. å¦‚æžœPRGæ˜¯éšæœºçš„ï¼Œå…¶æ¦‚çŽ‡åº”è¯¥æ˜¯1/256ã€‚ è€ŒPr[2^nd^ byte=0]=2/256çš„ç»“æžœæ˜¯ï¼šæ¶ˆæ¯çš„ç¬¬äºŒä¸ªå­—èŠ‚ä¸è¢«åŠ å¯†çš„æ¦‚çŽ‡æ¯”æ­£å¸¸æƒ…å†µå¤šä¸€å€ã€‚ï¼ˆ0å¼‚æˆ–ä¸å˜ï¼‰ ç»Ÿè®¡çš„çœŸå®žæƒ…å†µæ˜¯ï¼Œä¸æ­¢ç¬¬äºŒä¸ªå­—èŠ‚ï¼Œå‰é¢å¾ˆå¤šå­—èŠ‚çš„æ¦‚çŽ‡å¾ˆä¸éƒ½å‡åŒ€ã€‚ å› æ­¤ï¼Œå¦‚æžœè¦ä½¿ç”¨RC4 PRGï¼Œä»Žå…¶è¾“å‡ºçš„257ä¸ªå­—èŠ‚å¼€å§‹ä½¿ç”¨ã€‚ Prob. of (0,0) is 1/256^2^ +1/256^3^ . å¦‚æžœPRGæ˜¯éšæœºçš„ï¼Œ00ä¸²å‡ºçŽ°çš„æ¦‚çŽ‡åº”è¯¥æ˜¯(1/256)^2^ . Related key attackes. åœ¨ä¸Šå°èŠ‚ä¸­ã€ŒExamples: 802.11b WEPã€ï¼ŒWEPä½¿ç”¨çš„RC4æµå¯†ç ï¼Œrelated keyå¯¹å®‰å…¨é€šä¿¡ä¹Ÿæ˜¯ç¾éš¾æ€§çš„ã€‚ Old example(HW): CSS (badly broken)The Content Scrambling System (CSS) is a system used for protecting movies on DVD disks. It uses a stream cipher, called the CSS stream cipher, to encrypt movie contents. CSS was designed in the 1980â€™s when exportable encryption was restricted to 40-bits keys. As a result, CSS encrypts movies using a 40-bits key. ã€1980çš„ç¾Žå›½å‡ºå£æ³•ï¼Œé™åˆ¶å‡ºå£çš„åŠ å¯†ç®—æ³•åªèƒ½æ˜¯40ä½ï¼ŒäºŽæ˜¯CSSçš„å¯†é’¥æ˜¯40ä½ã€‘[amazing.jpg] While ciphers using 40-bit keys are woefully insecure, we show that the CSS stream cipher is particularly weak and can be broken in far less time than an exhaustive search over all 2^40^ keys. ã€è™½ç„¶40ä½çš„å¯†é’¥æœ¬æ¥å°±ä¸å¤Ÿå®‰å…¨ï¼Œä½†æ˜¯æˆ‘ä»¬èƒ½ç”¨è¿œå°äºŽç©·ä¸¾æ—¶é—´çš„æ–¹æ³•ç ´è§£CSSã€‘ å› ä¸ºåšä¸»ä¹Ÿæ˜¯ç¬¬ä¸€æ¬¡å­¦ï¼Œå¾ˆå¤šä¸œè¥¿ä¹Ÿä¸äº†è§£ã€‚ æ‰€ä»¥æ¦‚è¿°æ€§è¯­è¨€ï¼Œæˆ‘ç”¨æ•™ç§‘ä¹¦çš„åŽŸæ–‡è®°å½•ï¼Œé™„æ³¨ä¸€äº›ä¸­æ–‡ã€‚æœ›æµ·æ¶µï½ž Linear feedback shift register(LFSR)CSS æµå¯†ç æ˜¯ç”±ä¸¤ä¸ªLFSRï¼ˆçº¿æ€§åé¦ˆç§»ä½å¯„å­˜å™¨ï¼‰ç»„æˆçš„ï¼Œé™¤äº†CSSï¼Œè¿˜æœ‰å¾ˆå¤šç¡¬ä»¶ç³»ç»Ÿæ˜¯åŸºäºŽLFSRè¿›è¡ŒåŠ å¯†æ“ä½œï¼Œä½†æ— ä¸€ä¾‹å¤–ï¼Œéƒ½è¢«ç ´è§£äº†ã€‚ DVD encryption (CSS)ï¼š2 LFSRs GSM encryption (A5/,2): 3 LFSRs ã€å…¨çƒé€šä¿¡ç³»ç»Ÿã€‘ Bluetooth(E0): 4LFSRs LFSR can be implemented in hardware with few transistors. And a result, stream ciphers built from LFSR are attractive for low-cost consumer electronics such as DVD players, cell phones, and Bluetooth devices. ã€LFSRåœ¨ç¡¬ä»¶ä¸Šè¿è¡Œå¾ˆå¿«ï¼Œä¹Ÿå¾ˆçœç”µï¼Œæ‰€ä»¥è™½ç„¶åŸºäºŽLFSRçš„ç®—æ³•éƒ½è¢«ç ´è§£äº†ï¼Œä½†æ˜¯æ”¹ç¡¬ä»¶æœ‰ç‚¹å›°éš¾ï¼Œæ‰€ä»¥è¿˜æ˜¯æœ‰å¾ˆå¤šç³»ç»Ÿåœ¨ä½¿ç”¨ã€‘ ä¸Šå›¾æ˜¯ä¸€ä¸ª8ä½LFSR{4,3,2,0}ã€‚ LFSRæ˜¯ç”±ä¸€ç»„å¯„å­˜å™¨ç»„æˆï¼ŒLFSRæ¯ä¸ªå‘¨æœŸè¾“å‡ºä¸€ä½ï¼ˆå³0ä½ï¼‰ã€‚ æœ‰ä¸€äº›ä½ï¼ˆå¦‚ä¸Šå›¾çš„4ï¼Œ3ï¼Œ2ï¼Œ0ï¼‰ç§°ä¸ºtap positions(å‡ºå¤´)ï¼Œé€šè¿‡è¿™äº›ä½è®¡ç®—å‡ºfeedback bit(åé¦ˆä½)ã€‚ åé¦ˆä½å’Œå¯„å­˜å™¨ç»„çš„æœªè¾“å‡ºä½ç»„æˆæ–°çš„å¯„å­˜å™¨ç»„ã€‚ ä¼ªä»£ç å¦‚ä¸‹ï¼š æ‰€ä»¥åŸºäºŽLFSRçš„PGRçš„seedæ˜¯å¯„å­˜å™¨ç»„çš„åˆå§‹çŠ¶æ€ã€‚ how CSS worksCSSçš„seed=5 bytes=40 bitsã€‚ CSSç”±ä¸¤ä¸ªLFSRç»„æˆï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸€ä¸ª17-bit LFSRï¼Œä¸€ä¸ª25-bit LFSRã€‚ seed of LFSR: 17-bit LFSR: 1||first 2 bytes ï¼Œå³17ä½ã€‚ 25-bit LFSR: 1||last 3 bytesï¼Œå³25ä½ã€‚ CSSè¿‡ç¨‹ï¼š ä¸¤ä¸ªLFSRåˆ†åˆ«è¿è¡Œ8è½®ï¼Œè¾“å‡º8 bitsã€‚ ä¸¤ä¸ª8bits ç›¸åŠ mod 256ï¼ˆè¿˜æœ‰åŠ ä¸Šå‰ä¸€å—çš„è¿›ä½ï¼‰å³æ˜¯CSSä¸€è½®çš„è¾“å‡ºï¼šone byte at a time. Cryptanalysis of CSS (2^16 time attack)å½“å·²çŸ¥CSS PRGçš„è¾“å‡ºæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡ç©·ä¸¾ï¼ˆ2^40^ timeï¼‰ç ´è§£å¾—åˆ°CSSçš„seedã€‚ ä½†è¿˜æœ‰ä¸€ç§æ›´å¿«çš„ç ´è§£ç®—æ³•ï¼Œåªéœ€è¦æœ€å¤š2^16^ çš„å°è¯•ã€‚ ç ´è§£è¿‡ç¨‹å¦‚ä¸‹ï¼š å½±ç‰‡æ–‡ä»¶ä¸€èˆ¬æ˜¯MPEGæ–‡ä»¶ï¼Œå‡è®¾æˆ‘ä»¬å·²çŸ¥æ˜Žæ–‡MPEGæ–‡ä»¶çš„å‰20ä¸ªå­—èŠ‚ã€‚ï¼ˆå·²çŸ¥æ˜Žæ–‡çš„prefixï¼‰ CSSæ˜¯æµå¯†ç ï¼Œå¯ä»¥é€šè¿‡å·²çŸ¥prefixè¿˜åŽŸå‡ºCSSçš„prefixï¼Œå³CSS PRGçš„å‰20ä¸ªå­—èŠ‚çš„è¾“å‡ºã€‚ For all possible initial settings of 17-bit LFSR do: run 17-it LFSR to get 20 bytes of output. ã€å…ˆè®©17-bitè¾“å‡º20ä¸ªå­—èŠ‚ã€‘ subtract from CSS prefix $\\Rightarrow$ candidate 20 bytes output of 25-bit LFSR. ã€é€šè¿‡è¿˜åŽŸçš„CSS PRGçš„è¾“å‡ºï¼Œå¾—åˆ°25-bitè¾“å‡ºçš„å‰20ä¸ªå­—èŠ‚ã€‘ If consistent with 25-bit LFSR, found correct initial settings of both!! ã€å…·ä½“æ˜¯å¦‚ä½•åˆ¤åˆ«è¿™20ä¸ªå­—èŠ‚æ˜¯å¦æ˜¯ä¸€ä¸ª25-bit LFSRçš„è¾“å‡ºå‘¢ï¼Ÿã€‘ å‡è®¾å‰ä¸‰ä¸ªå­—èŠ‚æ˜¯y1, y2, y3. é‚£ä¹ˆ25-bit LFSRçš„initial :{1, y1 , y2, y3},å…¶ä¸­yéƒ½æ˜¯8 bits. ç”¨è¿™ä¸ªåˆå§‹çŠ¶æ€ç”Ÿæˆ20ä¸ªå­—èŠ‚ï¼Œå¦‚æžœå’Œå¾—åˆ°çš„20ä¸ªå­—èŠ‚ç›¸åŒï¼Œåˆ™æ­£ç¡®ï¼Œå¦åˆ™å†æ¬¡æžšä¸¾ã€‚ å½“å¾—åˆ°äº†ä¸¤ä¸ªLFSRçš„seed, å°±å¯ä»¥å¾—åˆ°CSS PRGçš„å…¨éƒ¨è¾“å‡ºï¼Œå³å¯ç ´è§£ã€‚ Modern stream ciphers: eStreammain idea eStream PRG ï¼š $\\{0,1\\}^s\\times R \\rightarrow \\{0,1\\}^n$ (n&gt;&gt;s) seed: ${0,1}^s$ nonce: a non-repeating value for a given key.ã€å°±å¯¹äºŽç¡®å®šçš„seed,nonceç»ä¸é‡å¤ã€‘ Encryption: $\\text{E(k, m; r)}=\\text{m}\\oplus \\text{PRG(k; r)} $ The pair (k,r) is never used more than once. ã€PRGçš„è¾“å…¥æ˜¯(k,r), ç¡®ä¿OTPã€‘ eStram: Salsa 20(SW+HW)Salsa20/12 and Salsa20/20 are fast stream ciphers designed by Dan Bernstein in 2005. Salsa 20/12 is one of four Profile 1 stream cipher selected for the eStream portfolio of stream ciphers. eStream is a project that identifies fast and secure stream ciphers that are appropriate for practicle use. Variants of Salsa20/12 and Salsa20/20, called ChaCha12 and ChaCha20 respectively, were proposed by Bernstein in 2008. These stream ciphers have been incorporated into several widely deployed protocols such as TLS and SSH. Salsa20 PRG: $\\{0,1\\}^{128 \\text { or } 256} \\times\\{0,1\\}^{64} \\longrightarrow\\{0,1\\}^{n}$ (max n = 2^73^ bits) Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ é€šè¿‡è®¡æ•°å™¨ï¼Œä½¿å¾—è¾“å‡ºè”ç»“ï¼Œå¯ä»¥è¾“å‡ºas long as you want pseudorandom segment. ç®—æ³•è¿‡ç¨‹å¦‚ä¸Šå›¾æ‰€ç¤ºï¼šSalsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 32 bytesçš„{k,r,i}é€šè¿‡æ‰©å±•å¾—åˆ°64 bytesçš„{ $\\tau_0,k,\\tau_1,r,i,\\tau_2,k,\\tau_3$ }. k :16 bytesçš„seed. i: 8 bytesçš„indexï¼Œè®¡æ•°å™¨ã€‚ r: 8 bytesçš„nonce. $\\tau_{0,1,2,3}$ éƒ½æ˜¯4 bytesçš„å¸¸æ•°ï¼ŒSalsa20ç®—æ³•è¯´æ˜Žä¹¦ä¸Šå…¬å¼€ç¡®å®šçš„å€¼ã€‚ 64 bytes é€šè¿‡hå‡½æ•°æ˜ å°„ï¼Œåè½®ã€‚ h : invertible function designed to be fast on x86(SEE2). åœ¨x86ä¸Šæœ‰SEE2æŒ‡ä»¤å¯ä»¥ç›´æŽ¥è¿è¡Œhå‡½æ•°ï¼Œæ‰€ä»¥å¾ˆå¿«ã€‚ hæ˜¯é€†å‡½æ•°ï¼ˆä¹Ÿæ˜¯å…¬å¼€çš„å‡½æ•°ï¼‰ï¼Œè¾“å‡ºå¯ä»¥é€šè¿‡é€†è¿ç®—å¾—åˆ°å…¶è¾“å…¥ã€‚ hæ˜¯ä¸€ä¸ªä¸€ä¸€æ˜ å°„çš„mapï¼Œæ¯ä¸€ä¸ª64bytesçš„è¾“å…¥éƒ½æœ‰å”¯ä¸€å¯¹åº”çš„64 bytesçš„è¾“å‡ºã€‚ å°†ç¬¬åè½®Hå‡½æ•°çš„è¾“å‡ºå’Œæœ€å¼€å§‹çš„è¾“å…¥åšåŠ æ³•è¿ç®—ï¼Œword by word(32ä½)ï¼Œå³æ¯4 bytesç›¸åŠ ã€‚ ä¸ºä»€ä¹ˆè¦æœ‰è¿™ä¸€æ­¥ï¼Ÿ hæ˜¯å¯é€†è¿ç®—ï¼Œå¦‚æžœç›´æŽ¥å°†å‡½æ•°çš„è¾“å‡ºä½œä¸ºPRGçš„è¾“å‡ºï¼Œé‚£å¯ä»¥é€šè¿‡hçš„é€†è¿ç®—å¾—åˆ°åŽŸ64 bytesï¼Œä¹Ÿå°±å¾—åˆ°äº†(k; r). Is Salsa20 secure(unpredictable)å‰æ–‡æˆ‘ä»¬é€šè¿‡unpredictableæ¥å®šä¹‰PRGçš„å®‰å…¨ï¼ˆä¸‹ä¸€ç¯‡æ–‡ç« ä¼šç»™å‡ºå®‰å…¨PRGæ›´å¥½çš„å®šä¹‰ï¼‰ï¼Œæ‰€ä»¥Salsa20 å®‰å…¨å—ï¼Ÿæ˜¯å¦æ˜¯ä¸å¯é¢„æµ‹çš„ï¼Ÿ Unknownï¼šno known provably secure PRGs. ã€ä¸èƒ½ä¸¥æ ¼è¯æ˜Žæ˜¯ä¸€ä¸ªå®‰å…¨PRGï¼ˆåŽæ–‡ä¼šç»§ç»­è®²è§£ä»€ä¹ˆæ˜¯å®‰å…¨çš„PRGï¼‰ï¼Œå¦‚æžœä¸¥æ ¼è¯æ˜Žäº†ï¼Œä¹Ÿå°±è¯æ˜Žäº†P=NPã€‘ In realityï¼š no known attacks bertter than exhaustive search. ã€çŽ°å®žä¸­è¿˜æ²¡æœ‰æ¯”ç©·ä¸¾æ›´å¿«çš„ç®—æ³•ã€‘ Performanceé€šè¿‡ä¸‹å›¾çš„æ¯”è¾ƒï¼Œå¦‚æžœåœ¨ç³»ç»Ÿä¸­éœ€è¦ä½¿ç”¨æµå¯†ç ï¼Œå»ºè®®ä½¿ç”¨eStreamã€‚ speed ï¼šè¯¥ç®—æ³•æ¯ç§’åŠ å¯†å¤šå°‘MBçš„æ•°æ®ã€‚ Reference S. Fluhrer, I. Mantin, and A. Shamir. Weaknesses in the key scheduling algorithm of RC4. In proceedings of selected areas of cryptography (SAC), pages 1-24, 2001. ã€ŒA Graduate Course in Applied Cryptographyã€p76-78:æŒ–å‘ å¾…è¡¥å……","link":"/2020/03/18/stanford-crypto-streamcipher2/"},{"title":"ã€ŒCryptography-Bonehã€:Stream Cipher 3","text":"Stream Cipherçš„ç¬¬ä¸‰ç¯‡æ–‡ç« ã€‚ æ–‡ç« ä¸»è¦åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå‰éƒ¨åˆ†é€æ­¥å®šä¹‰Secure PRGçš„å®šä¹‰ï¼Œé€šè¿‡å¼•å…¥statistical testï¼ˆç»Ÿè®¡æµ‹è¯•ï¼‰å’ŒAdvantageï¼ˆä¼˜åŠ¿ï¼‰å¾—å‡ºå½“ä¸”ä»…å½“PRG is unpredictable,PRG is secureçš„ç»“è®ºã€‚ åŽéƒ¨åˆ†ä»‹ç»äº†å¯†ç å­¦ä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µSemantic Securityçš„å®šä¹‰ï¼Œé€šè¿‡å¼•å…¥ computationally indistinguishable(è®¡ç®—ä¸Šä¸å¯åŒºåˆ†)çš„æ¦‚å¿µç»™å‡ºå®šä¹‰ï¼Œå¹¶è¯æ˜Žäº†OTPçš„è¯­æ„å®‰å…¨å’Œåœ¨å®‰å…¨PRGæ¡ä»¶ä¸‹çš„æµå¯†ç çš„è¯­æ„å®‰å…¨ï¼Œå¾—å‡ºå¦‚æžœæµå¯†ç ä¸­ä½¿ç”¨çš„PRG is secure,é‚£ä¹ˆæµå¯†ç å°±å…·å¤‡semantic securityã€‚ æ–‡ç« å¼€å¤´ï¼Œä¹Ÿç®€å•ä»‹ç»äº†å¯†ç å­¦ä¸­negligibleå’Œnon-negligibleçš„å«ä¹‰ã€‚ Negligible and non-negligibleIn practice: $\\epsilon$ is a scalar and $\\epsilon$ non-neg: $\\epsilon \\geq 1/2^{30}$ (likely to happen over 1GB of data) $\\epsilon$ negligible: $\\epsilon \\leq 1/2^{80}$ (wonâ€™t happen over life of key) åœ¨å®žè·µä¸­ï¼Œ$\\epsilon$ æ˜¯ä¸€ä¸ªæ•°å€¼ï¼Œå¦‚æžœæ˜¯non-negä¸å¯å¿½ç•¥çš„è¯ï¼Œå¤§çº¦åœ¨1GBçš„æ•°æ®ä¸‹å°±ä¼šå‘ç”Ÿï¼Œå¦‚æžœæ˜¯å¯å¿½ç•¥çš„å€¼ï¼Œåœ¨å¯†é’¥çš„ç”Ÿå­˜å‘¨æœŸå†…åŸºæœ¬ä¸ä¼šå‘ç”Ÿã€‚ In theory: $\\epsilon$ is a function $\\varepsilon: Z^{\\geq 0} \\rightarrow R^{\\geq 0}$ and $\\epsilon$ non-neg: $\\exists d: \\epsilon(\\lambda)\\geq 1/\\lambda^d$ ($\\epsilon \\geq 1/\\text{poly} $, for many $\\lambda$ ) $\\epsilon$ negligible: $\\forall d, \\lambda \\geq \\lambda_{d}: \\varepsilon(\\lambda) \\leq 1 / \\lambda^{d}$ ( $\\epsilon \\leq 1/\\text{poly}$, for large $\\lambda$ ) [0]ï¼ˆç†è®ºä¸­ï¼Œè¿™ä¸ªè¿˜ä¸å¤ªç†è§£ï¼Œå¾…è¡¥å……ã€‚ï¼‰ PRG Security DefsLet $G:K\\longrightarrow \\{0,1\\}^n$ be a PRG. Goal: define what it means that [ $k\\stackrel{R}{\\longleftarrow} \\mathcal{K}$ , output G(k) ] is â€œindistinguishableâ€ from [ $r\\stackrel{R}{\\longleftarrow} \\{0,1\\}^n$ , output r] . ã€ä½¿å¾—PGRçš„è¾“å‡ºå’ŒçœŸéšæœºæ˜¯ä¸å¯åŒºåˆ†çš„ã€‘ï¼ˆ $\\stackrel{R}{\\longleftarrow}$ çš„æ„æ€æ˜¯åœ¨å‡åŒ€åˆ†å¸ƒä¸­éšæœºå–ï¼‰ ä¸‹å›¾ä¸­ï¼Œçº¢è‰²çš„åœˆæ˜¯å…¨éƒ¨çš„ ${0,1}^n$ ä¸²ï¼ŒæŒ‰ç…§å®šä¹‰æ˜¯å‡åŒ€åˆ†å¸ƒã€‚è€Œç²‰è‰²G()æ˜¯PRGçš„è¾“å‡ºï¼Œç”±äºŽseedå¾ˆå°ï¼Œç›¸å¯¹äºŽå…¨éƒ¨çš„ ${0,1}^n$ ï¼Œæ‰€ä»¥G()çš„è¾“å‡ºèŒƒå›´ä¹Ÿå¾ˆå°ã€‚ å› æ­¤ï¼Œattackerè§‚æµ‹G(k)çš„è¾“å‡ºï¼Œæ˜¯ä¸èƒ½å’Œuniform distributionï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰çš„è¾“å‡ºåŒºåˆ†å¼€ã€‚ è¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€æƒ³æž„é€ çš„å®‰å…¨PGRçš„ç›®æ ‡ã€‚ Statistical TestsStatistical test on ${0,1}^n$ ï¼šæœ‰ä¸€ä¸ªç®—æ³•Aï¼Œ$x\\in{0,1}^n$ ,A(x) æ ¹æ®ç®—æ³•å®šä¹‰è¾“å‡ºâ€0â€æˆ–â€1â€. ç»Ÿè®¡æµ‹è¯•æ˜¯è‡ªå®šä¹‰çš„ã€‚ Exampleï¼š A(x)=1 if $| \\#0(x)-\\#1(x)|\\leq 10\\cdot\\sqrt{n}$ ã€æœŸæœ›ä¸²ä¸­0ã€1çš„æ•°ç›®å·®ä¸å¤šï¼Œè¿™æ ·look randomã€‘ A(x)=1 if $|\\#00(x)-\\frac{n}{4}\\leq10\\cdot\\sqrt{n}$ ã€æœŸæœ›Pr(00)=1/4 ,ä¸²ä¸­00å‡ºçŽ°çš„æ¦‚çŽ‡ä¸º1/4ï¼Œè®¤ä¸ºæ˜¯look randomã€‘ A(x)=1if max-run-of-0(x) &lt; 10Â·log(n) ã€æœŸæœ›ä¸²ä¸­0çš„æœ€å¤§æ¸¸ç¨‹ä¸è¦è¶…è¿‡è§„å®šå€¼ã€‘ ä¸Šé¢çš„ç¬¬ä¸‰ä¸ªä¾‹å­ï¼Œå¦‚æžœè¾“å‡ºä¸ºå…¨1ï¼Œæ»¡è¶³ä¸Šè¿°çš„ç»Ÿè®¡æ¡ä»¶è¾“å‡º1ï¼Œä½†å…¨1ä¸²çœ‹èµ·æ¥å¹¶ä¸randomã€‚ ç»Ÿè®¡æµ‹è¯•ä¹Ÿç”±äºŽæ˜¯è‡ªå®šä¹‰çš„ï¼Œæ‰€ä»¥é€šè¿‡ç»Ÿè®¡æµ‹è¯•çš„ä¹Ÿä¸ä¸€å®šæ˜¯randomï¼Œå…¶PRGä¹Ÿä¸ä¸€å®šæ˜¯å®‰å…¨çš„ã€‚ æ‰€ä»¥ï¼Œå¦‚ä½•è¯„ä¼°ä¸€ä¸ªç»Ÿè®¡æµ‹è¯•çš„å¥½åï¼Ÿ ä¸‹é¢å¼•å…¥ä¸€ä¸ªé‡è¦çš„å®šä¹‰advantageï¼Œä¼˜åŠ¿ã€‚ Advantageå¼•å…¥Advantageï¼ˆä¼˜åŠ¿ï¼‰æ¥è¯„ä¼°ä¸€ä¸ªç»Ÿè®¡æµ‹è¯•çš„å¥½åã€‚ Let G: $k \\rightarrow \\{0,1\\}^n$ be a PRG and A a stat. test on ${0,1}^n$ ã€Gæ˜¯ä¸€ä¸ªPRGï¼ŒAæ˜¯ä¸€ä¸ªå¯¹01ä¸²çš„ç»Ÿè®¡æµ‹è¯•ã€‘ Define: the advantage of statisticaltest A relative to PRG G Adv$_\\text{PRG}[A,G]$ $\\text{Adv}_\\text{PRG}[A,G]=|Pr[A(G(k))=1]-Pr[A(r)=1]|\\in[0,1]$ , $k\\stackrel{R}{\\longleftarrow} \\mathcal{K}, r\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ ã€å®šä¹‰ï¼šAdv$_\\text{PRG}[A,G]$ ä¸ºç»Ÿè®¡æµ‹è¯•Aå¯¹äºŽPRG Gçš„ä¼˜åŠ¿ä¸ºç»Ÿè®¡æµ‹è¯•ä»¥PRGä½œä¸ºè¾“å…¥è¾“å‡ºä¸º1çš„æ¦‚çŽ‡ å‡åŽ» ç»Ÿè®¡æµ‹è¯•ä»¥truly random string ä½œä¸ºè¾“å…¥è¾“å‡ºä¸º1çš„æ¦‚çŽ‡ã€‘ Adv close to 1 : ç»Ÿè®¡æµ‹è¯•èƒ½åŒºåˆ†PRGçš„è¾“å‡ºå’Œtruly random stringï¼Œå³adversaryå¯ä»¥åˆ©ç”¨åŒºåˆ†PRGçš„è¾“å‡ºå’Œrandomçš„è¿™ä¸€ç‚¹ä»Žè€Œç ´è§£ç³»ç»Ÿã€‚ Adv close to 0 : ç»Ÿè®¡æµ‹è¯•ä¸èƒ½åŒºåˆ†PRGçš„è¾“å‡ºå’Œtruly random string, å³adversaryè®¤ä¸ºPRGçš„è¾“å‡ºå’Œrandomåˆ«æ— äºŒè‡´ã€‚ Advantage ä¼˜åŠ¿[1] In cryptography, an adversaryâ€™s advantage is a measure of how successfully it can attack a cryptographic algorithm, by distinguishing it from an idealized version of that type of algorithm.Note that in this context, the â€œadversaryâ€ is itself an algorithm and not a person. A cryptographic algorithm is considered secure if no adversary has a non-negligible advantage, subject to specified bounds on the adversaryâ€™s computational resources (see concrete security). åœ¨å¯†ç å­¦ä¸­ï¼Œadversaryçš„ä¼˜åŠ¿æ˜¯è¯„ä¼°å®ƒé€šè¿‡æŸç§ç†æƒ³ç®—æ³•ç ´è§£ä¸€ä¸ªåŠ å¯†ç®—æ³•çš„æˆåŠŸå°ºåº¦ã€‚ è¿™é‡Œçš„adversaryæ˜¯ä¸€ç§ç ´è§£ç®—æ³•è€Œä¸æ˜¯æŒ‡æ”»å‡»è€…è¿™ä¸ªäººã€‚ å½“æ‰€æœ‰ adversaryå¯¹è¯¥åŠ å¯†ç®—æ³•åªæœ‰å¯å¿½ç•¥çš„ä¼˜åŠ¿æ—¶ï¼Œè¯¥åŠ å¯†ç®—æ³•è¢«è®¤ä¸ºæ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºadversaryåªæœ‰æœ‰é™çš„è®¡ç®—èµ„æºã€‚ e.g.1 : A(x) = 0ï¼Œç»Ÿè®¡æµ‹è¯•Aå¯¹PRGçš„ä»»ä½•è¾“å‡ºéƒ½è¾“å‡º0ï¼Œåˆ™Adv[A,G] = 0. e.g.2 : G: $k \\rightarrow {0,1}^n$ satisfies msb(G(k))=1 for 2/3 of keys in K. Define statistical test A(x) as : if[ msb(x)=1 ] output â€œ1â€ else output â€œ0â€ ã€PRG G(k)çš„2/3çš„è¾“å‡ºçš„æœ€é«˜æœ‰æ•ˆä½æ˜¯1ï¼Œå®šä¹‰ç»Ÿè®¡æµ‹è¯•A(x),è¾“å…¥çš„æœ€é«˜æœ‰æ•ˆä½ä¸º1è¾“å‡º1ï¼Œå¦åˆ™è¾“å‡º0ã€‘ msb: most significant bit,æœ€é«˜æœ‰æ•ˆä½ã€‚ åˆ™ Adv[A,G] = | Pr[A(G(k))] - Pr[A(r)] | = 2/3 - 1/2 = 1/6 å³ A can break the generator G with advantage 1/6, PRG G is not good. Secure PRGs: crypto definitionDef: We say that G: $k \\rightarrow {0,1}^n$ is secure PRG if $\\forall$ â€œeffâ€ stat. tests A : Adv$_\\text{PRG}$ [A,G] is â€œnegligibleâ€. è¿™é‡Œçš„â€effâ€,æŒ‡å¤šé¡¹å¼æ—¶é—´å†…ã€‚ è¿™ä¸ªå®šä¹‰ï¼Œâ€œæ‰€æœ‰çš„ç»Ÿè®¡æµ‹è¯•â€ï¼Œè¿™ä¸€ç‚¹éš¾ä»¥è¾¾åˆ°ï¼Œå› æ­¤æ²¡æœ‰provably secure PRGsã€‚ ä½†æˆ‘ä»¬æœ‰heuristic candidates.ï¼ˆæœ‰é™çš„stat. test èƒ½æ»¡è¶³ï¼‰ Easy fact: a secure PRG is unpredictableè¯æ˜Žå‘½é¢˜ï¼š a secure PRG is unpredictable. å³è¯æ˜Žå…¶é€†å¦å‘½é¢˜ï¼š PRG is predictable is insecureã€‚ suppose A is an efficient algorithm s.t. $\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\\frac{1}{2} + \\epsilon$ for non-negligible $\\epsilon$ . ã€ ç®—æ³•Aæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„é¢„æµ‹ç®—æ³•ï¼Œ $\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\\frac{1}{2} + \\epsilon$ , $\\epsilon$ æ˜¯ä¸å¯å¿½ç•¥çš„å€¼ï¼Œå³Aèƒ½ä»¥å¤§äºŽ1/2çš„æ¦‚çŽ‡æŽ¨æµ‹ä¸‹ä¸€ä½ã€‚ã€‘ Adversaryèƒ½åˆ©ç”¨ç®—æ³•Aæ¥åŒºåˆ†è¿™ä¸ªPRGå’Œrandomä¾æ¬¡æ¥ç ´è§£ç³»ç»Ÿã€‚ Define statistical test B as: B(x)=1 if $A(x|_{1,...,i})=x_{i+1}$ , else B(x)=0. ã€å®šä¹‰ä¸€ä¸ªç»Ÿè®¡æµ‹è¯•Bï¼Œå¦‚æžœé¢„æµ‹ç®—æ³•Aé¢„æµ‹ä¸‹ä¸€ä½æ­£ç¡®è¾“å‡º1ï¼Œå¦åˆ™è¾“å‡º0ã€‘ è®¡ç®—Adv$_\\text{PRG}$ : $r\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ : Pr[B(r) = 1] = 1/2 $k\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ : Pr[B(G(k)) = 1] = 1/2+ $\\epsilon$ Adv$_\\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | &gt; $\\epsilon$ Adversary èƒ½ä»¥ $\\epsilon$ çš„ä¼˜åŠ¿åŒºåˆ†PRGå’Œrandomï¼Œå› æ­¤PRG ä¸å®‰å…¨ã€‚ æ‰€ä»¥ï¼Œå¦‚æžœAæ˜¯ä¸€ä¸ªå¥½çš„é¢„æµ‹ç®—æ³•ï¼Œé‚£ä¹ˆBå°±æ˜¯ä¸€ä¸ªå¥½çš„ç»Ÿè®¡ç®—æ³•ï¼ŒAdversaryå°±èƒ½é€šè¿‡Bä»¥ $\\epsilon$ çš„ä¼˜åŠ¿ç ´è§£ç³»ç»Ÿã€‚ åœ¨æ­¤ï¼Œè¯æ˜Žäº† if A can predict PRG, PRG is insecure $\\Rightarrow$ A secure PRG is unpredictable. Thm(Yaoâ€™82): an unpredictable PRG is secureä¸ŠèŠ‚è¯æ˜Žäº†A secure PRG is unpredictable. åœ¨1982 Yao çš„è®ºæ–‡[2]ä¸­è¯æ˜Žäº†å…¶é€†å‘½é¢˜ï¼š an unpredictable PRG is secure. G: $k \\rightarrow {0,1}^n$ be PRG â€œThmâ€œ : if $\\forall i \\in$ {0,â€¦, n-1} PRG G is unpredictable at pos. i then G is a secure PRG. ã€å®šç†ï¼šå¦‚æžœåœ¨ä»»ä½•ä½ç½®PRGéƒ½æ˜¯ä¸å¯é¢„æµ‹çš„ï¼Œé‚£ä¹ˆPRGå°±æ˜¯å®‰å…¨çš„ã€‘ Proofï¼š A: é¢„æµ‹ç®—æ³•ï¼š $\\forall i \\quad\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]=\\frac{1}{2} $ B:ç»Ÿè®¡æµ‹è¯•ï¼š B(x)=1 if $A(x|_{1,...,i})=x_{i+1}$ , else B(x)=0. Adv$_\\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | = 0 If next-bit predictors cannot distinguish G from random then no statistial test can! ã€next-bit predictoræŒ‡ç”¨é¢„æµ‹ç®—æ³•çš„ç»Ÿè®¡æµ‹è¯•ã€‘ e.g. Let G: $k \\rightarrow {0,1}^n$ be PRG such that from the last n/2 bits of G(k) it is easy to compute the first n/2 bits. Is G predictable for som i $\\in$ {0, â€¦, n-1} ? : Yes.å½“n=2æ—¶ï¼Œå¯ä»¥é¢„æµ‹å‡ºç¬¬ä¸€ä½ã€‚ åœ¨æ­¤ï¼Œå¯ä»¥å¾—å‡ºç»“è®ºï¼š Adversaryä¸å¯åŒºåˆ†PRGçš„è¾“å‡ºå’Œtruly random stringæ—¶è¢«è®¤ä¸ºæ˜¯å®‰å…¨çš„ã€‚ å½“ä¸”ä»…å½“PRGåœ¨ä»»æ„ä½ç½®ä¸å¯é¢„æµ‹æ—¶ï¼ŒPRGæ˜¯å®‰å…¨çš„ã€‚ More generally: computationally indistinguishableLet P1 and P2 be two distributions over ${0,1}^n$ Def : We say that P1 and P2 are computationally indistinguishable (denoted $P_1\\approx_p P_2$) If $\\forall$ â€œeffâ€ stat. tests A $\\left|\\text{Pr}_{x\\leftarrow_{P_1}}[A(x)=1]-\\text{Pr}_{x\\leftarrow_{P_2}}[A(x)=1]\\right|","link":"/2020/06/25/stanford-crypto-streamcipher3/"},{"title":"ã€ŒAlgebraic ECCsã€: Lec1 Basics of ECCs","text":"In this series, I will be learning Algebraic Error Correcting Codes, lectured by Mary Wootters. The lecture videos are available here. Feedback and sugguestions are always welcome! ^ - ^ Topics Covered: Basic problem in coding theory Code and codeword Hamming distance and minimum distance Rate Hamming bound on trade-off of the rate and distance IntroductionThis course is named â€œAlgebraic Error Correcting Codesâ€, containing two parts: Error Correcting Codes (ECC): ECC are a fundamental tool for communication, storage, complexity theory, algorithm design, cryptography, pseudorandomness and etc. Algebraic: algebraic techniques are a fundamental tool for designing ECCs. Basic Problem in Coding TheoryLetâ€™s start with the basic problem in coding theory: We encode a message $x$ of length $k$ into a codeword $c$ of length $n$, which adds some redundancy when $n&gt;k$. When we transmit or store this codeword, something bad might happen, i.e. some bits might be corrupted. Thus, we are left with a corrupted codeword $\\tilde{c}$. The goal is to find (something about) the message $x$ given the corrupted codeword $\\tilde{c}$. This has many applications, including communication and storage: Application1: Communication Suppose Alice has a message $x\\in \\{0,1\\}^{k}$ and wants to sends it to Bob. In order to add some redundancy, she encodes the message into a codeword $c$ and sends the codeword through a noisy channel. After Alice passes $c$ through the noisy channel, Bob receives $\\tilde{c}$ and tries to figure out the original message $x$ that Alice intended to send. Application2: Storage Suppose $x$ is a file we want to store. Instead of storing $x$ directly, we encode it as a codeword $c$, introducing redundancy. If $c$ is stored on a CD or in a RAID array, something bad, like a fire, might happen. However, the owner still wants to recover the original file $x$. Based on the above two applications, we summarize four things we care about in the coding schemes: Things We Care About (Take 1): We should be able to handle SOMETHING BAD. We should be able to recover WHAT WE WANT TO KNOW about $x$. We want to MINIMIZE OVERHEAD. Jumping ahead, the overhead is defined as the quantity $k/n\\in (0, 1]$, which should be as big as possible. We want to do all this EFFICIENTLY. The question about these things are: what is the best trade-off between 1-4? The trade-off depends on how we model things: What is something bad? What exactly do we want to know about $x$?All of $x$ or the first bit of $x$? What counts as efficient? This lecture will gives one way of answering these questions and there are many legit ways. Now, letâ€™s give some formal definitions for a code. CodeLet $\\Sigma$ be any finite set and let $n&gt;0$ be a positive integer. Code: A code $\\mathcal{C}$ of block length $n$ over an alphabet $\\Sigma$ is a subset $\\mathcal{C}\\subseteq\\Sigma^n$. An element $c\\in \\mathcal{C}$ is called a codeword. Sometimes, block length is referred to simply as length. So far, this is not a very interesting definition. A descriptive example for this definition is provided below. Example 1: Another example is slightly more interesting. Example 2: The following is a set of 7 vectors, which forms a binary code of length 4 over $\\Sigma=\\{0, 1\\}$. We can relate this code to the communication between Alice and Bob mentioned earlier. Thus, the code $\\mathcal{C}$ is the image of this map, i.e. $\\mathcal{C}=\\mathrm{Im(ENC)}$. In other words, $\\mathcal{C}$ is the set of all codewords that can be obtained using this encoding map. Example 2 can actually be used to correct one erasure (something bad). An erasure means we know which bit got erased, but we donâ€™t know its original value. Based on the map definition, the missing bit must be 1. Furthermore, it can also be used to detect one error (something bad). An error means we know one bit my have been changed, but we donâ€™t know which one. To sum up, we say that the code in Example 2 can correct one erasure or detect one error. But it cannot correct one error. Letâ€™s see a code that can correct one error. Example 3: Given this encoding map, we can define a code $\\mathcal{C}=\\mathrm{Im(ENC)}$ . Hence, $\\mathcal{C}\\subseteq\\{0,1\\}^7$ is a binary code of length 7. This code has a nice way to visualize as three overlapping circles. We place the messages $x_1, x_2, x_3, x_4$ in the middle and the remaining spaces in the circles correspond to the parity bits, which are the sum of the other bits in each circle. This ensures that the sum of the bits in each circle equals 0. With these parity bits, this code can correct one error. With these circles in mind, we can solve the following puzzle more easily. In a legit codeword, all three circles should sum to 0. Here, both the green and red circles are messed up while the blue circle is correct. Therefore, $\\tilde{c}_3$ must be flipped. But this solution with the help of circles seems pretty ad hoc. In order to make the solution less ad hoc, weâ€™ll introduce more definitions to formalize the solution and flesh out the four things we care about for ECCs mentioned before. Hamming DistanceWe first give the definition of hamming distance, which equals to the number of positions on which two vectors $x, y\\in \\Sigma^n$ differ. Hamming Distance: The Hamming Distance between $x,y\\in \\Sigma^n$ is $$ \\Delta(x,y)=\\sum_{i=1}^n \\mathbb{1}(x_i\\ne y_i) $$ Then we can define the relative hamming distance, which is the hamming distance normalized by $n$. Itâ€™s the fraction of positions on which two vectors differ. Relative Hamming Distance: The Relative Hamming Distance between $x,y\\in \\Sigma^n$ is $$ \\delta(x,y)=\\frac1 n \\sum_{i=1}^n \\mathbb{1}(x_i\\ne y_i)=\\frac{\\Delta(x,y)}{n} $$ Another useful definition is the minimum distance, which is the minimum hamming distance over all distinct pairs. Jumping ahead, we will see that the code with large minimum distance can be used to correct errors. Minimum Distance: The Minimum Distance of a code $\\mathcal{C}\\subseteq \\Sigma^n$ is $$ \\min_{c\\ne c'\\in \\mathcal{C}}\\Delta(c,c') $$ Sometimes, the minimum distance is referred to simply as distance. Aside: The hamming distance is a metric, which obeys the triangle inequality $\\Delta(x, z)\\le \\Delta(x, y) + \\Delta(y, z)$. Claim: The code in Example 3 has minimum distance 3. Now we can convince ourselves the claim is true with the circles in mind. But weâ€™ll see a much less ad hoc way to prove the distance in Lecture 2. If this claim is true, it explains why that code can correct one error using the triangle inequality for hamming distance. The minimum distance equal to 3 means that the hamming distance $\\Delta(c,câ€™)\\ge 3$ for every distinct pair $c, câ€™\\in \\mathcal{C}$. As depicted in the following figure, suppose the corrupted codeword we received is $\\tilde{c}$ and $c$ is the correct code should be $c$. There is another codeword $câ€™\\in \\mathcal{C}$. We know that the codeword $\\tilde{c}$ has exactly one error so the hamming distance $\\Delta(\\tilde{c}, c)=1$. The triangle inequality tells us: $$3\\le \\Delta(c,câ€™)\\le \\Delta(c, \\tilde{c}) + \\Delta(\\tilde{c}, câ€™)$$ that the hamming distance $\\Delta(\\tilde{c}, câ€™)\\ge 2$ for every codeword $câ€™$ other than the correct codeword $c$. Thus, the correct codeword $c\\in \\mathcal{C}$ is uniquely defined by â€œthe one that is closest to $\\tilde{c}$â€. Minimum Distance: Proxy for RobustnessThe point of this discussion is that the minimum distance is a reasonable proxy for robustness. To be more specific, in example 2, the code had minimum distance 2 and could correct 1 erasure and detect 1 error. In example 3, the code had minimum distance 3 and could correct 1 error. More generally, a code with minimum distance $d$ can: correct $\\le d-1$ erasures detect $\\le d-1$ errors correct $\\lfloor \\frac{d-1}2\\rfloor$ errors For point 1 and point 3, the (inefficient) algorithm is â€œif you see $\\tilde{c}$, return $c\\in \\mathcal{C}$ thatâ€™s closest to $\\tilde{c}$. For point 2, the (inefficient) algorithm is â€œif $\\tilde{c}\\notin \\mathcal{C}$, say that something wrong.â€ To understand why the algorithm works, we look at the following picture: Red Points represent the codewords $c, câ€™\\in \\mathcal{C}$. Orange Circle correspond to the hamming balls of radius $\\lfloor \\frac{d-1}{2} \\rfloor$ centered at codewords. These hamming balls are disjoint because the minimum distance is $d$. This hamming ball around a codeword $c$ is indeed a set of points $\\{x\\in \\Sigma^n:\\Delta(c, x)\\le \\lfloor\\frac{d-1}{2} \\rfloor\\}$. Green Circles correspond to the hamming balls of radius $d-1$ centered at codewords. These hamming balls are not disjoint, but they each contain exactly one codeword. The hamming balls help explain the (inefficient) algorithm we mention before. Correct $\\lfloor \\frac{d-1}2\\rfloor$ errors:If $c$ is the â€œcorrectâ€ codeword (the left red point) and $\\le \\lfloor \\frac{d-1}{2}\\rfloor$ errors are introduced, we may end up with $\\tilde{c}_1$ (the purple point) within its orange circle. Since the orange circle are disjoint, the algorithm can correct codeword $c$ from $\\tilde{c}_1$. Detect $d-1$ errors: If $\\le d-1$ errors are introduced, we may end up with $\\tilde{c}_2$ (the pink point) within its green circles. Now itâ€™s possible that $\\tilde{c}_2$ came from $c$ or that it came from $câ€™$; we canâ€™t tell. However, since each green circles only contain exactly one codeword, meaning other codewords has to live outside of this ball, we can tell that something wrong. Correct $d-1$ erasures: If $\\le d-1$ erasures are introduced in $\\tilde{c}$, suppose the candidate codewords are the correct codeword $c$ and some other $câ€™$. We erase the corresponding positions in both candidate codewords $c$ and $câ€™$. Since the hamming distance between $c$ and $câ€™$ is at least $d$,there must be at least one position where $c$ and $câ€™$ differ. This differing position allows us to resolve the ambiguity the identify the correct codeword $c$. Returning to the things we care about, we can now clarify the firs two things: Things We Care About (Take 2): We should be able to handle $\\lfloor \\frac{d-1}{2}\\rfloor$ worst-case errors or $d-1$ worst-case erasures. We should be able to recover all of $x$ (aka correct the errors or erasures) We want to MINIMIZE OVERHEAD. Jumping ahead, the overhead is defined as the quantity $k/n\\in (0, 1]$, which should be as big as possible. We want to do all this EFFICIENTLY. Moreover, the first two things can be combined to the minimum distance $d$ and we want it as large as possible. Aside: In this class, we only focus on the worst-case model (also called the â€œHamming modelâ€ or â€œadversarial modelâ€. We will discuss a little bit about the random-error model (also called the â€œShannon modelâ€ or â€œStochastic modelâ€. RateMoving on to the point 3, what do we mean by â€œoverheadâ€? Dimension: The Message Length (sometimes called Dimension) of a code $\\mathcal{C}$ over an alphabet $\\Sigma$ is defined to be $$ k=\\log_{|\\Sigma|}|\\mathcal{C}| $$ This definition is in line of the the encoding operation which encodes a message $x$ of length $k$ over $\\Sigma$ into a codeword $c\\in \\mathcal{C}$. This map assigns each possible message to a single codeword so we have $|\\Sigma^k|=|\\mathcal{C}|$ aka $k=\\log_{|\\Sigma|}|\\mathcal{C}|$. Rate: The Rate of a code $\\mathcal{C}\\subseteq \\Sigma^n$ with block length $n$ over an alphabet $\\Sigma$ is $$ \\mathcal{R}=\\frac{\\log_{|\\Sigma|}|\\mathcal{C}|}{n}=\\frac{\\text{message length }k}{\\text{block length }n} $$ The rate captures the notion of overhead so minimizing the overhead means to maximize the rate. So if $\\mathcal{R}$ is close to $1$, it means not too much overhead. And if $\\mathcal{R}$ is close to 0, it means lots of overhead. We want the rate to be as close to 1 as possible. Now we can denote a code by these parameters Code: A code with distance $d$, message length $k$, block length $n$, and alphabet $\\Sigma$ is called a $(n, k, d)_{\\Sigma}$ code. After clarifying the things we care about, a question arises: What is the best trade-off between rate and distance? This question is still open for binary codes! Hamming Bound: Rate vs. DistanceThe Hamming bound gives us one bound on the trade of rate and distance. It establishes some limitations on how good this trade-off can be. Letâ€™s return to the picture we had before, with $|C|$ disjoint Hamming balls of radius $\\lfloor \\frac{d-1}{2}\\rfloor$ in the space $\\Sigma^n$. The idea of the Hamming bound is: there canâ€™t be too much of them or they wouldnâ€™t all fit in space $\\Sigma^n$. To be more precise, we define the Hamming ball as follows. Hamming Ball: The Hamming Ball in $\\Sigma^n$ of radius $e$ about some point $x\\in \\Sigma^n$ is $$ B_{\\Sigma^n}(x, e)=\\{y\\in \\Sigma^n: \\Delta(x, y)\\le e\\} $$ The Volume of $B_{\\Sigma^n}(x, e)$ is denoted by $\\mathrm{Vol_{|\\Sigma|}}(e, n)=|B_{\\Sigma^n}(x, e)|$, representing the number of points in the Hamming ball. Notice that the right hand side, $|B_{\\Sigma^n}(x, e)|$, includes $x$ while the volume notation only refers to the radius $e$ and block length $n$. This is because the volume does not depend on $x$. Note: the $\\Sigma$ is sometimes dropped from the notation $B_{\\Sigma^n}(x, e)$. Note: The notation $B_{\\Sigma^n}(x, e/n)$ refers to the Hamming ball with relative distance. Say that $|\\Sigma|=q$, then we can express the volume of a $q$-array Hamming ball as $$\\mathrm{Vol}_q(e, n) = 1 + \\binom{n}{1}\\cdot (q-1) + \\binom{n}{2}\\cdot (q-1)^2 + \\dots + \\binom{n}{e}(q-1)^e$$ where term $1$ refers to the $\\vec{0}$, term $\\binom{n}{1}\\cdot (q-1)$ refers to all the elements of $\\Sigma^n$ of weight 1. Armed with these definitions, we can formalize the bound. If a code $\\mathcal{C}\\subseteq \\Sigma^n$ has distance $d$ and message length $k$, where $|\\Sigma|=q$, we have $$|\\mathcal{C}|\\cdot \\mathrm{Vol}_q\\left(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n\\right) \\le q^n$$ The LHS corresponds to the total volume taken up by all disjoint Hamming balls while the RHS corresponds to the total volume in the whole space $\\Sigma^n$. Taking logs base $q$ of both sides, it can be written as $$\\log_q |\\mathcal{C}|+\\log_q \\left(\\mathrm{Vol}_q\\left(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n\\right)\\right)\\le n$$ It gives the bound of the rate in terms of the distance. Hamming Bound: $$ \\text{Rate}=\\frac{k}{n} \\le 1-\\frac{\\log_q \\left(\\mathrm{Vol}_q\\left(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n\\right)\\right)}{n} $$ Back to Example 3, which was a $(n=7, k=4, d=3)_{q=2}$ code. We have $\\lfloor\\frac{d-1}{2}\\rfloor=1$, $\\mathrm{Vol}_2(1, 7) = 1 + \\binom{7}{1}\\cdot 1 = 8$. So we have rate bounded by $\\frac k n \\le 1 - \\frac{\\log_2(8)}{n} = 1 - \\frac 3 7 = \\frac 4 7$. In fact, the rate $\\frac k n= \\frac 4 7$, so in this case the Hamming bound is tight! It means this code achieves the best trade-off between the minimum distance and rate. When the Hamming bound is tight, we say the code is perfect.Example 3 (which is perfect) is a special case of something called a Hamming Code.","link":"/2024/12/10/stanford-cs250-ecc-lec1/"},{"title":"ã€ŒAlgebraic ECCsã€: Lec2 Linear Codes and Finite Fields","text":"In this series, I will be learning Algebraic Error Correcting Codes, lectured by Mary Wootters. The lecture videos are available here. Feedback and sugguestions are always welcome! ^ - ^ Topics Covered: Linear Algebra over $\\{0, 1\\}$ Generator Matrices Parity-Check Matrices Linear Algebra not Working over $\\{0, 1, 2, 3\\}$ Finite Fields and Linear Codes RecapIn last lecture, we are left with the open question: What is the best trade-off between rate and distance? We called the following example the Hamming Code of length 7 and we saw that it was optimal in that it met the Hamming Bound. We also came up with a decoding algorithm for this code. Viewing the code as overlapping circles, we can identify which circles donâ€™t sum to 0 (mod 2) and flip the unique bit that ameliorates the situation. However, this circle view seems a bit ad hoc. In this Lecture, we are going to generalize this construction and generalize the decoding algorithm as well as the distance argument. Who was Hamming? Richard Hamming (1915-1998) was working at Bell labs starting in the late 1940â€™s, where he was colleagues with Claude Shannan (of the â€œShannon modelâ€ which we also mentioned). Hamming was working on old-school computers (calculating machines), and they would return an error if even on bit was entered in error. This was extremely frustrating, and inspired Hamming to study this rate-vs-distance question, and to come up with Hamming codes. Linear Algebra over {0, 1}The Hamming code in the previous Example 3 has a really nice form of encoding: Generator MatricesWe can see this encoding map as the multiplication by a matrix modulo 2, written as $x\\mapsto Gx\\pmod 2$, where $G$ is some matrix called the Generator Matrix. Viewing codes generated by a generator matrix is a very useful view to look at things. Suppose for now â€œlinear algebra works mod 2â€. For example, the codes in Example 3has a very nice property: it is closed under addition modulo 2, which means that if $c\\in \\mathcal{C}$, $câ€™\\in \\mathcal{C}$, then $c+câ€™\\in \\mathcal{C}$. Writing it as the multiplication by a matrix makes it very clear: Moreover, we can view any codeword $c\\in \\mathcal{C}$ as a linear combination of the column vectors of the generator matrix $G$. In other words, the code $\\mathcal{C}$ is the column span of the generator matrix $G$, aka $\\mathcal{C}=\\text{span}(\\text{cols}(G))$ is a linear subspace of dimension 4. dist (C) = min wt (C)A key observation is that the minimum Hamming distance of the code is the same as the minimum weight of the non-zero codewords. Observation: If $\\mathcal{C}$ is linear, then $\\text{distance}(\\mathcal{C})=\\min \\text{wt}(C)$. Proof: The proof is straightforward: $$\\Delta(Gx, Gxâ€™) = \\Delta(G(x-xâ€™), 0)=\\text{wt}(G(x-xâ€™))$$ where the first equality holds follows from linearity and $G(x-xâ€™)$ is itself a codeword by the definition. $\\blacksquare$ Parity-check MatricesThe other way we looked at Example 3 was placing bits into circles as shown: Each circle constitutes a parity check, meaning the sum of the circle must equal 0 mod 2. This constrain is indeed the linear relation, which we can express as: where each of the three rows corresponds to the linear constrains given by a circle. For example, the first row corresponds to the green circle, indicating that $c_2+c_3+c_4+c_5=0 \\pmod 2$. This implies that multiplying any potential codeword by the matrix $H$ should results in 0. In other words, for any $c\\in \\mathcal{C}$, we have $Hc=0\\pmod2$, which means that $\\mathcal{C}$ is contained in the kernel of the matrix $H$: $$\\mathcal{C}\\subseteq \\text{Ker}(H)$$ A raised question is: Does $\\mathcal{C}=\\text{Ker}(H)?$ The answer is YES! Lemma: $\\mathcal{C}=\\text{Ker}(H)$ We prove it by counting dimension. Proof: $\\text{dim}(\\mathcal{C})=\\text{dim}(\\text{colspan}(G))=4$ Itâ€™s easy to see $\\text{dim}(\\text{colspan}(G))=4$ since the identity matrix is just sitting up there. $\\text{dim}(\\text{Ker}(H))=n - \\text{rank}(H) = 7 - \\text{dim}(\\text{rowspan}(H))=4$Again, itâ€™s easy to see $\\text{dim}(\\text{rowspan}(H))=3$ because of the identity matrix. Having $\\mathcal{C}\\subseteq \\text{Ker}(H)$ with the same dimension, $\\text{dim}(C)=\\text{dim}(\\text{Ker}(H))$, it follows that $\\mathcal{C}=\\text{Ker}(H)$. $\\blacksquare$ Informally, the matrix $H$ here is called a Parity-Check matrix of $\\mathcal{C}$ so that the code $\\mathcal{C}$ is the kernel of this matrix. Easy to Capture DistanceOne benefit of the parity-check matrix is we can read off the distance easily. As mentioned in the previous lecture, we supposed that the distance of the code in Example 3 is 3. Now, we can provide a proof using parity-check matrices. Claim: $\\text{dist}(\\mathcal{C}) = 3$ Proof: As before, it suffices to show $\\min_{c\\in \\mathcal{C}\\backslash\\{0, 1\\} } \\text{wt}(c) = 3$ that the minimum weight of all non-zero codewords is 3. Firstly, we prove $\\text{wt}(c)\\ge 3$ for $\\forall c\\in \\mathcal{C}$, meaning that all non-zero codewords has weight of at least 3. By contradiction, suppose there is some codeword vector $c\\in \\mathcal{C}$ with has weight 1 or 2 so that $Hc = 0 \\pmod 2$. $\\text{wt}(c)=1$It implies one column of $H$ is 0 mod 2. (contradiction) $\\text{wt}(c)=2$ It implies the sum of some two columns of $H$ is 0 mod 2, aka there is a repeated column. (contradiction) Now, the codeword 0101010 has weight exactly 3, so this bound is tight. Hence, the minimum weight is 3. $\\blacksquare$ Easy to DecodeFurthermore, the parity-check matrix gives us a slick decoding algorithm. Recall the puzzle in Example 3: Puzzle: Given $\\tilde{c}=0111010$, which bit has suffered one bit flip, and what is the original codeword $c$? The goal of the solution is to find a codeword $c\\in \\mathcal{C}$ such that the Hamming distance $\\Delta(c, \\tilde{c})\\le 1$. Solution: We write $\\tilde{c}=c+z\\pmod 2$ where $z$ is an error vector with weight 1. Next, we consider what the product of $H\\cdot \\tilde{c}$ actually is? One the one hand, compute $H\\cdot \\tilde{c} \\mod 2$ : On the other hand, write the product as $H\\cdot (c+z) = Hc + Hz =Hz$ since $Hc=0$ for all $c\\in \\mathcal{C}$. This shows that $z$ is just picking one column of $H$. From the computation, we see $H\\cdot \\tilde{c}$ corresponds to the 3rd column of $H$, indicating the error occurred in position 3. This leads to an efficient decoding algorithm for $\\mathcal{C}$: Compute $H\\cdot \\tilde{c}$ and identify which column of $H$ matches the result Determine $z$ recover $\\tilde{c}=c + z\\mod 2$. This view with parity-check matrix gives us a much nicer way of seeing our circle-based algorithm. â€œWhich circles fail to sum 1â€ is the same as â€œwhich bits of $H\\cdot \\tilde{c}$ are 1â€, and it picks out which bit we need to flip. Now, letâ€™s summarize the moral of the story so far: The Hamming code of length 7 $\\mathcal{C}$, is a subspace over $\\{0, 1\\}^7$, which means we can view it in two different liner-algebraic ways: $\\mathcal{C}=\\{G\\cdot x: x\\in \\{0, 1\\}^4\\}=\\text{span(cols}(G))$, as the column-span of the generator matrix. $\\mathcal{C}=\\{x\\in \\{0, 1\\}^7:H\\cdot x=0\\} = \\text{Ker}(H)$ , as the kernel of the parity-check matrix. However, all of these are predicated on the assumption that the linear algebra works over $\\{0, 1\\}$ . Linear Algebra not Working over {0, 1, 2, 3}A natural question arises: Does linear algebra â€œmake senseâ€ over $\\{0,1\\}$? To explore this, letâ€™s consider what happens for $\\{0, 1,2,3\\}$. The statements below contains falsehoods. Non-example: Let $G = \\left[ \\begin{array}{cc} 2 & 0 \\\\ 0 & 2 \\\\ 2 & 2 \\end{array} \\right]$be a generator matrix, mod 4. Using this generator matrix, we can define a code $\\mathcal{C} = \\{G\\cdot x: x\\in \\{0, 1, 2,3\\}^2\\} = \\text{colspan}(G)$, with message length $k = 2$, block length $n = 3$, over the alphabet $\\Sigma = \\{0, 1, 2,3\\}$. Thus, $\\text{dim}(\\mathcal{C})=2$, since the columns of $G$ are not scalar multiples of each other, aka, they are linearly independent. Now, consider the parity-check matrix $H = G^{T} = \\left[\\begin{array}{cc} 2 & 0 & 2 \\\\ 0 & 2 & 2\\\\ \\end{array} \\right]$. We observe $H\\cdot G = 0$, which means $H$ is a legit parity-check matrix for the code $\\mathcal{C}$. Specifically, for any $c\\in \\mathcal{C}$, we have: $H\\cdot c = H\\cdot G\\cdot x = 0 \\mod 4$. However, when we calculate $\\text{dim(Ker}(H))$, we find: $\\text{dim}(\\text{Ker}(H)) = 3 - \\text{rank}(H) = 1$. This leads to a contradiction for the code $\\mathcal{C}$ is a liner subspace such that $\\mathcal{C} = \\text{colspan}(G) = \\text{Ker}(H)$. Additionally, when we look at the distance of code using the parity-check matrix, we find that $\\text{dist}(\\mathcal{C})\\ge 3$ since no two columns of $H$ are linearly dependent. Yet, there exists a codeword $c = G\\cdot \\left(\\begin{array}{c} 1\\\\1 \\end{array} \\right) = \\left(\\begin{array}{c} 2\\\\2 \\\\0 \\end{array} \\right)$, which has weight $2$, leading to another contradiction. The main reason for contradiction is that the linear algebra does not â€œworkâ€ over $\\{0, 1, 2, 3\\} \\mod 4$. In particular, the assertions that two vectors are linearly independent since they are not scalar multiples of each other are not working over $\\{0, 1, 2, 3\\}$. The following definitions are both for the linearly independent. Non-zero vectors $v$ and $w$ are linearly independent iff there is no non-zero $\\lambda$ s.t. $v = \\lambda \\cdot w$. Non-zero vectors $v$ and $w$ are linearly independent iff there is no non-zero $\\lambda_1, \\lambda_2$ s.t. $\\lambda_1 \\cdot v + \\lambda_2 \\cdot w = 0$. They are the same over the real field $\\mathbb{R}$. The proof is straightforward Proof: Suppose $\\exists \\lambda_1, \\lambda_2 \\ne 0$ s.t. $\\lambda_1 \\cdot v + \\lambda_2 \\cdot w = 0$. Then $v = (\\frac{-\\lambda_2}{\\lambda_1})\\cdot w$.Conversely, if $\\exists \\lambda$ s.t. $v = \\lambda w$, then choose $\\lambda_2 = \\lambda, \\lambda_1 = -1$ and $\\lambda_1 v + \\lambda_2 w = 0$. $\\blacksquare$ However, they not the same over $\\{0, 1,2, 3\\}$. There exists $\\lambda_1 =\\lambda_2 = 2$ such that $$ 2\\cdot \\left(\\begin{array}{c} 2 \\\\ 0 \\\\2 \\end{array}\\right ) + 2\\cdot \\left(\\begin{array}{c} 0 \\\\ 2 \\\\2 \\end{array}\\right ) = \\left(\\begin{array}{c} 4 \\\\ 4 \\\\8 \\end{array}\\right ) = \\left(\\begin{array}{c} 0 \\\\ 0 \\\\0 \\end{array}\\right ) \\mod 4 $$ even though the two columns of $G$ are not scalar multiples of each other. If you have the background of the finite field, the reason here is that $\\{0, 1, 2, 3\\}$ is not a finite field and we cannot divide by 2 mod 4. This does not bode well for algebraic coding theory if even linear algebra doesnâ€™t work. Finite Field and Linear Codes The second part of the lecture covers finite fields. I omit it here since we have already covered it in this blog. The key takeaway is all the definitions we know for the linear algebra over $\\mathbb{R}$ also make sense over finite fields. Here, I list some essential points relevant to this lecture. Let $\\mathbb{F}$ be a finite field. Then: $\\mathbb{F}^n = \\{(x_1, \\dots, x_n): x_i \\in \\mathbb{F}\\}$ A subspace $V\\subseteq \\mathbb{F}^n$ is a subset that is closed unde r addition and scalar multiplication. Specifically: $\\forall v, w\\in V, \\forall \\lambda\\in \\mathbb{F}, v+ \\lambda w\\in V$. Vectors $v_1, \\dots, v_t \\in \\mathbb{F}^n$ are linearly independent if $\\forall \\lambda_1, \\dots, \\lambda_t\\in \\mathbb{F}$ that are not all 0, $\\sum_{i} \\lambda_i \\cdot v_i \\ne 0$. For $v_1, \\dots, v_t \\in \\mathbb{F}^n$, their span is defined as $\\text{span}(v_1, \\dots, v_t)=\\{\\sum_i \\lambda_i v_i : \\lambda_i \\in \\mathbb{F}\\}$. A basis for a subspace $V\\subseteq \\mathbb{F}$ is a collection of vectors $v_1, \\dots, v_t\\in V$ s.t. $v_1, \\dots, v_t$ are linearly independent $V = \\text{span}(v_1, \\dots, v_t)$ The dimension of a subspace $V$ is the number of elements in any basis of $V$. Now, we can define a linear code over a finite field. Definition of Linear Codes: A Linear Code $\\mathcal{C}$ of length $n$ and dimension $k$ over a finite field $\\mathbb{F}$ is a $k$-dimension linear subspace of $\\mathbb{F}^n$. (The alphabet of $\\mathcal{C}$ is $\\Sigma = \\mathbb{F}$) This definition implies that a linear code is essentially a linear space. This definition aligns with the one we introduced in the previous lecture. (Recall) Definition of Code : A code with distance $d$, message length $k$, block length $n$, and alphabet $\\Sigma$ is called a $(n, k, d)_{\\Sigma}$ code. Here, we use overloaded $k$ for both message length and dimension. This is intentional, as it makes sense in this context. If $\\mathcal{C}$ is a $k$-dimensional subspace over $\\mathbb{F}$, then $|\\mathcal{C}|=|\\mathbb{F}^k|$, hence $k = \\log _{|\\mathbb{F}|}|\\mathcal{C}| = \\log_{|\\Sigma|}|\\mathcal{C}|=$ message length. Moreover, if $\\mathcal{C}$ is a code of block length $n$, message length $k$, over alphabet $\\mathcal{\\Sigma = \\mathbb{F}}$, then $\\mathcal{C}$ is a linear code and also a $k$-dimensional subspace over $\\mathbb{F}$. Definition of Generator Matrix: Let $\\mathcal{C}$ be a linear code of length $n$ and dimension $k$ over a finite field $\\mathbb{F}$. A matrix $G\\in \\mathbb{F}^{n\\times k}$ is a generator matrix for $\\mathcal{C}$ if $\\mathcal{C} = \\text{colspan}(G)=\\{G\\cdot x: x\\in \\mathbb{F}^k\\}$. Observation: Any linear code has a generator matrix. Proof: Choose the columns of $G$ to be a basis of $\\mathcal{C}$. $\\blacksquare$ Note that the generator matrix for a code is not unique. There can be many generator matrices for the same code. They all describe the same code, but they implicitly describe different encoding maps. For example, the following $G$ and $Gâ€™$ are both generator matrices for the same Hamming code. In particular, we can always permute on rows since it does not change the space of column-span. Some generator matrices may be more useful than others. For example, $G$ above corresponds to a systematic encoding map. This means that that $\\text{Enc}_G(x_1, x_2, x_3, x_4) \\mapsto (x_1, x_2, x_3, x_4, \\text{stuff})$. In particular, for linear codes, there is always a systematic encoding map defined by a generator matrix look like the above $G$, having an identity matrix sit up there and some other stuff down there. Definition of Dual Code: If $\\mathcal{C}\\subseteq \\mathbb{F}^n$ is a linear code over $\\mathbb{F}$, then $\\mathcal{C}^\\perp=\\{v\\in \\mathbb{F}^n: \\langle v, c\\rangle =0 \\text{ for }\\forall c\\in \\mathcal{C}\\}$ Dual code is the same definition as the dual subspace in linear algebra. Note: Let $\\mathcal{C}\\subseteq \\mathbb{F}^n$ be a linear code of length $n$ over $\\mathbb{F}$ with $\\text{dim}(\\mathcal{C})=k$, then $\\text{dim}(\\mathcal{C}^\\perp) = n - k$. (Just like over $\\mathbb{R}$). Definition of Parity Check Matrix: Let $\\mathcal{C}$ be a linear code of length $n$ and dimension $k$ over a finite field $\\mathbb{F}$. A matrix $H\\in \\mathbb{F}^{(n-k)\\times n}$ so that $\\mathcal{C} = \\text{Ker}(H)=\\{c\\in \\mathbb{F}^n: H\\cdot c = 0\\}$ is a parity-check matrix for $\\mathcal{C}$. The rows of $H$ (or any vector $v$ s.t. $\\langle v, c\\rangle =0 \\;\\forall c\\in \\mathcal{C}$ ) are called parity checks. Observation: Any linear code has a parity-check matrix. Proof: Choose the rows of $H$ to be a basis of $\\mathcal{C}^\\perp$. $\\blacksquare$ Similarly, the parity-check matrix for a code is not uniqueâ€”there can be many parity-check matrices for the same code. With these definitions under our belts, we can move to some facts about linear codes. Facts About Linear Codes: If $\\mathcal{C}\\subseteq \\mathbb{F}^n$ is a linear code over $\\mathbb{F}$ of dimension $k$ with the generator matrix $G$ and parity-check matrix $H$, then: $H\\cdot G = 0$ $\\mathcal{C}^\\perp$ is a linear code of dimension $n-k$ with generator matrix $H^T$ and parity-check matrix $G^T$.Note that the parity-check matrix $H$ for code $\\mathcal{C}$ is taken as the generator matrix for $\\mathcal{C}^\\perp$. The distance of $\\mathcal{C}$ is the minimum weight of any nonzero codeword in $\\mathcal{C}$: $\\text{dist}(\\mathcal{C})=\\min_{c\\in \\mathcal{C}\\backslash\\{0\\}}\\sum_{i=0}1\\{c_i \\ne 0\\}$. The distance of $\\mathcal{C}$ is the smallest number $d$ so that $H$ has $d$ linearly dependent columns.Recall how we proved the distance of the code in Example 3: we argue that it suffices to show that no pairs of columns in $H$ are linearly dependent.As shown below, the distance of the code corresponds to the smallest number $d$ so that there exists a codeword $c$ of weight $d$, which is picking up $d$ columns of $H$ that is linearly dependent.","link":"/2024/12/15/stanford-cs250-ecc-lec2/"},{"title":"ã€ŒAlgebraic ECCsã€: Lec3 GV Bound and q-ARY Entropy","text":"In this series, I will be learning Algebraic Error Correcting Codes, lectured by Mary Wootters. The lecture videos are available here. Feedback and sugguestions are always welcome! ^ - ^ Topics Covered: The GV Bound: Efficiency and Maximum-Likelihood Decoding Application: McEliece Cryptosystem Off to Asymptopia Family of Codes q-ary Entropy Trade-off Between Rate and Distance RecapIn the last lecture, we saw linear codes over a finite field in two linear-algebraic ways. As figured below, we can view a code as the column-span of a generator matrix and as the kernel of the parity-check matrix. There can be many different generator matrices and parity-check matrices for the same code. Today, we are going to learn some useful things one can do with linear codes. The GV BoundIn Lecture 2, we explored the Hamming bound: $$R\\le 1 - \\frac{\\log_q\\text{Vol}_q(\\lfloor \\frac{d-1}{2}\\rfloor, n)}{n}$$ which provides an upper bound on the rate of code. This bound highlights an impossible result, meaning that we cannot construct a code with a large rate. In contrast, the Gilbert-Varshamov (GV) bound offers a possible result. It demonstrates that there exist codes with a decent rate. Gilber-Varshamov (GV) Bound: For any prime power $q$, and for any $d\\le n$, there exists a linear code $\\mathcal{C}$ of length $n$, alphabet size $q$, distance $d$, and rate $$ R\\ge 1 - \\frac{\\log_q {(\\text{Vol}_q(d-1, n))-1}}{n} $$ Note: you can remove the words â€œprime powerâ€ and â€œlinearâ€ and the statements is till true. Aside from the fact that the GV bound provides a possibility result, another difference lies in the quantity difference in the $q$-ary volume of the Hamming ball. Weâ€™ll explore the relationship between these two bounds later, but for now, keep in mind that the rate given by the GV bound must be less than that of the Hamming bound: $$R_{\\text{GV}} &lt; R_{\\text{Hamming}}$$ otherwise, the math is broken. Now, weâ€™ll prove the GV bound â€” itâ€™s pretty easy! Proof of the GV Bound: The idea of the proof is to choose a random linear code $\\mathcal{C}$ of specific dimension $k$, and show that it has a distance at least $d$ with non-zero probabilityâ€”that is: $$\\text{Pr}[\\mathcal{C} \\text{ has distance}\\ge d]&gt;0$$ â€‹ This implies that there exists a linear code with a decent rate. This approach is known as the Probabilisitic Method. Let $\\mathcal{C}$ be a random subspace of $\\mathbb{F}_q^n$ with dimension $k = n -\\log_q(\\text{Vol}_q(d-1, n))-1$.If this code achieves a good distance, the $k/n$ basically corresponds to the rate given in the GV bound.This construction works because a linear code is essentially a linear space. Since there are finitely many subspaces of dimension $k$ in $\\mathbb{F}_q^n$, we can uniformly sample a random linear subspace. Let $G\\in \\mathbb{F}_q^{n\\times k}$ be a random generator matrix for $\\mathcal{C}$.As discussed before, each code can have many generator matrices. We can uniformly sample one by selecting a random basis for the subspace and using it as the columns of the generator matrix. Now, since the distance is the minimum weight of all non-zero codewords in the linear code, we have $\\text{dist}(\\mathcal{C})=\\min_{c\\in \\mathcal{C}\\backslash{0}}\\text{wt}(c)=\\min_{x\\in \\mathbb{F}_q^k\\backslash{0}}\\text{wt}(G\\cdot x)$. Useful Fact:For any fixed $x\\ne 0$, $G\\cdot x$ is uniformly random in $\\mathbb{F}_q^n\\backslash{0}$. For any given $x\\ne 0$, using the useful fact, the probability that the weight of $G\\cdot x$ is less than $d$ is equal to the probability of a random non-zero codeword lying within the Hamming ball centered at 0 with radius $d-1$. It is basically the volume of the Hamming ball divided by the volume of the whole space as indicated in the following second equality. $$\\begin{aligned}\\text{Pr}_G{\\text{wt}(G\\cdot x)&lt;d} &amp;= \\text{Pr}_G{G\\cdot x\\in B_q(0, d-1)}\\&amp;= \\frac{\\text{Vol}_q(d-1, n)-1}{q^n-1}\\&amp;\\le \\frac{\\text{Vol}_q(d-1, n)}{q^n}\\end{aligned}$$ By the union bound, we have $$\\text{Pr}{\\exists x\\in \\mathbb{F}_q^k:\\text{wt}(G\\cdot x)&lt;d}\\le q^k\\cdot \\frac{\\text{Vol}_q(d-1, n)}{q^n}$$ The complement of this event is that $\\forall x\\in \\mathbb{F}_q^k$ , $\\text{wt}(G\\cdot x)\\ge d$, which implies that the distance of the code is at least $d$. Thus, we win as long as this probability is strictly less than $1$, which guarantees the existence of a code with a good distance with non-zero probability. Taking logs of both sides, we win if $$k-n+\\log_q(\\text{Vol}_q(d-1, n))&lt;0$$ This is true since we precisely choose $k = n-\\log_q(\\text{Vol}_q(d-1, n))-1$ before. $\\blacksquare$ Efficiency &amp; Maximum-Likelihood DecodingThe GV bound tells us there exists good codes with decent rates. Next, we are going to discuss the extent to which linear codes admit efficient algorithms. We have the following efficient algorithms for linear codes to encode, detect errors and correct erasures: Efficient Encoding:If $\\mathcal{C}$ is linear, we have an efficient encoding map $x\\mapsto G\\cdot x$.The computational cost is one matrix-vector multiplication. Efficient Error Detection:If $\\mathcal{C}$ is linear with distance $d$, we can detect $\\le d - 1$ errors efficiently:If $0&lt; \\text{wt}(e)\\le d-1$ and $c\\in \\mathcal{C}$, then $H(c+e)=H\\cdot e \\ne 0$. Thus, we can just simply check if $H\\tilde{c}=0$. Efficient Erasure Correction:If $\\mathcal{C}$ is linear with distance $d$, we can correct $\\le d-1$ erasures efficiently:Erasing bits in the codeword $c$ corresponds to removing the corresponding rows of the generator matrix $G$. The remaining $n-(d-1)$ rows form a new linear system $Gâ€™\\cdot x = câ€™$. Since we know a code with distance $d$ can handle up to $d-1$ erasures (albeit with a non-efficient algorithm), there must be exactly one $x$ that is consistent with this linear system, and hence $Gâ€™$is full rank. The remaining task is to solve this linear system, which can be done with Gaussian elimination. The above is leaving out one important thingâ€”correcting errors. We know how to correct errors in the $(7, 4, 3)_2$-Hamming code, but what about in general? If $\\mathcal{C}$ is linear with distance $d$, can we correct up to $\\lfloor \\frac{d-1}{2} \\rfloor$ errors efficiently? The bad news is no. Consider the following problem. If we would solve this problem, we can correct up to $\\lfloor \\frac{d-1}{2} \\rfloor$ errors. Maximum-Likelihood Decoding for Linear Codes: Given $\\tilde{c}\\in \\mathbb{F}_q^n$, and $G\\in \\mathbb{F}_q^{n\\times k}$, find $x\\in \\mathbb{F}_q^k$ such that $\\Delta(G\\cdot x, \\tilde{c})$ is minimized. Aka, find the codeword closest to a received word $\\tilde{c}$. This problem (called Maximum-likelihood decoding for linear codes) is NP-hard in general [Berlekamp-McEliece-Van Tilborg 1978], even if the code is known in advance and you have an arbitrary amount of preprocessing time [Bruck-Noar 1990, Lobstein 1990]. It is even NP-hard to approximate (within a constant factor)! [Arora-Babai-Stern-Sweedyk 1993]. Even computing the minimum distance of linear codes is NP-hard given the generator matrix. The take-away here is that we are unlikely to find a polynomial-time algorithm for this task. This may sounds discouraging, but remember that NP-hardness is a worst-case condition. While there exist linear codes that are probably hard to decode, but this does not imply that that all of them are. Going forward, we will focus on designing codes that admit efficiently-decodable algorithms. Before that, letâ€™s look at a cryptography application that leverages this decoding hardness. Application: McEliece CryptosystemMcEliece cryptosystem is a public-key scheme based on the decoding hardness of binary linear codes. Suppose that Alice and Bob want to talk securely. Now there is no noise, just an Eavesdropper Eve. In public key cryptography, everyone has a public key and a private key. To send a message to Bob, Alice encrypts it using Bobâ€™s public key. Bob decodes it with his private key. We hope this process is secure as long as Bobâ€™s private key stays private. The McEliece Cryptosystem consists of three main algorithms: Generate Private and Public Keys Bob chooses $G\\in \\mathbb{F}_2^{n\\times k}$, the generator matrix for an (appropriate) binaray linear code $\\mathcal{C}$ that is efficiently decodable from $t$ errors. Not all codes work for McEliece crytosystem; the chosen code at least must be efficiently decodable. In particular, McEliece cryptosystem uses a binary â€œGoppa Codeâ€. Bob chooses a random invertible $S\\in \\mathbb{F}_2^{k\\times k}$ and a random permutation matrix $P\\in \\mathbb{F}_2^{n\\times n}$. The permutation matrix $P$ has exactly one 1 in each column such that $Px$ permutes the coordinates of the vector $x$. Bobâ€™s private key is $(S, G, P)$. Bobâ€™s public key consists of $\\hat{G}=PGS$ and the parameter $t$. Encrypt with Bobâ€™s Public keyTo send a message $x\\in \\mathbb{F}_2^k$ to Bob: Alice chooses a random vector $e\\in \\mathbb{F}_2^n$ with $\\text{wt}(e)=t$. Alice sends $\\hat{G}x+e$ to Bob. Decrypt with Bobâ€™s Private KeyTo decrypt the message $Gâ€™x+e$: Bob computes $P^{-1}(\\hat{G}x+e)=GSx + P^{-1} e =G(Sx) +eâ€™$, where $\\text{wt}(eâ€™)=t$. At this point, we write it as a corrupted codeword $G(Sx)+eâ€™$ with exactly $t$ errors since the permutation matrix $P^{-1}$ only permutes the coordinates of $e$. Bob can use the fact that $G$ is the generator matrix for a code that is efficiently able to correct up to $t$ errors. Bob uses the efficient decoding algorithm to recover $Sx$. Bob can compute $x = S^{-1}\\cdot Sx$. Why might this be secure? Suppose Eve sees $\\hat{G}x+e$ and she knows $Gâ€™$ and $t$. Hence, this problem is the same as decoding the code $\\hat{C}={\\hat{G}x : x\\in \\mathbb{F}_2^k}$ from $t$ errors. The security of the McEliece crytosystem relies the following assumptions: The public key $\\hat{G}$ looks random: By scrambling $G$ with $S$ and $P$, it is difficult for Eve to distinguish $\\hat{G}$ from a random generator matrix. Decoding a random linear code is computationally hard: While decoding the worst-case code is NP-hard, it is not too much of stretch that decoding a random linear code is also hard on average. If these assumptions hold true, decoding the code $\\hat{C}={\\hat{G}x : x\\in \\mathbb{F}_2^k}$ from $t$ errors is computationally hard for Eve. This assumption that â€œDecoding $\\hat{G}x+e$ is hardâ€ (for an appropriate choice of $G$) is called the McEliece Assumption. Some people believe it and some donâ€™t. â€œDecoding $\\hat{G}x+e$ is hard for Eveâ€ is NOT the same as â€œMaximum likelihood decoding of linear codes is NP-hardâ€. There are two main differences: First, we have some promise that there were $\\le t$ errors in McEliece assumption. Second, NP-harness is a worst-case assumption. For cryptography, we need an average-case assumption. Worst-case vs. Average-case: Worst-case: The problem is considered hard on worst-case if it is difficult to solve for the most difficult case. If a solution is found for the worst-case instance, the problem is solvable for all instances, e.g. $P\\ne NP$. Average-case: The problem is considered hard if it is difficult to solve for a randomly chosen instance. This assumes that solving the problem is computationally hard for the â€œaverageâ€ case rather than just the hardest instance. In cryptography, average-case hardness is more relevant because security relies on the assumption that attackers cannot efficiently solve a typical random instance of the underlying problem (e.g., decoding a random linear code or factoring a randomly chosen large integer). Off to AsymptopiaSo far, weâ€™ve seen the optimal rate for a code with distance $d$ and $|\\Sigma|=q$ is bounded above by the Hamming bound and bounded below by the GV bound. $$1-\\frac{1}{n}\\cdot \\log_q (\\text{Vol}_q(d-1, n))\\le k/n \\le 1 - \\frac{1}{n} \\cdot \\log_q(\\text{Vol}_q(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n)$$ Recall the combinational question we posed in Lecture 1: What is the best trade-off between rate and distance? To address this in the asymptotical setting, we are going to think about the following limiting parameter regime: $n, k, d\\rightarrow \\infty$ so that the rate $k/n$ and the relative distance $\\delta$ approaches constants. The motivations for this parameter regime: It will allow us to better understand whatâ€™s possible and whatâ€™s not. In many applications, $n, k, d$ are pretty large and $R, \\delta$ are the things we want to be thinking about. It will let us talk meaningfully about computational complexity. Family of CodesBefore that, letâ€™s define a family of codes. A Family of Codes A family of codes is a collection $\\mathcal{C}=\\{\\mathcal{C}_i\\}_{i=1}^{\\infty}$, where $\\mathcal{C}_i$ is an $(n_i, k_i, d_i)_{q_i}$ code. Given such a family of codes, we can define the rate and the relative distance: The rate of $\\mathcal{C}$ is $R(\\mathcal{C})=\\lim_{i\\to \\infty} k_i/n_i$. The relative distance of $\\mathcal{C}$ is $\\delta{(\\mathcal{C}})=\\lim_{i\\to \\infty} d_i /n_i$. We will frequently abuse notation and refer to $\\mathcal{C}$ as a â€œcodeâ€, and weâ€™ll drop the subscript $i$ and just think about $n, k, d\\to \\infty$ Now, letâ€™s look at an example of a family of codesâ€”Hamming codes. The $i$-th code $\\mathcal{C}_i$ is a $(2^i-1, 2^i-i-1, 3)_2$ code so $\\mathcal{C}=\\{\\mathcal{C}_i\\}_{i=1}^\\infty$ represents a family of codes. $\\mathcal{C}_i$ is defined by its parity-check matrix, where the columns corresponds to the binary vector representations of all non-zero elements of $\\mathbb{F}_{2^i}$. The rate of this family is: $$\\lim_{i\\to \\infty }\\frac{2^i-i-1}{2^i-1}=1$$ This rate approaches to 1, which is a very good rate. However, the relative distance is: $$\\lim_{i\\to \\infty}\\frac{3}{2^i-1}=0$$ Hence, in the asymptotic setting, our question is: for any family of a code, What is the best trade-off between $R(\\mathcal{C})$ and $\\delta{(\\mathcal{C})}$? As we see in the family of Hamming codes, we cannot achieve good trade-off between rate and distance. While it has a phenomenal rate, its distance is poor. An easier question to ask is: Can we obtain codes with $R(\\mathcal{C})&gt;0$ and $\\delta{(\\mathcal{C})}&gt;0$? We define a family of codes with the rate $R(\\mathcal{C})&gt;0$ and relative distance $\\delta(\\mathcal{C})&gt;0$ (both strictly greater than 0) as asymptotically good. q-ary EntropyNow we have an asymptotic parameter regime, how should we parse the GV and Hamming bounds? In particular, what do these bounds look like in terms of $\\delta$? $$1-\\frac{1}{n}\\cdot \\log_q (\\text{Vol}_q(d-1, n))\\le R(\\mathcal{C}) \\le 1 - \\frac{1}{n} \\cdot \\log_q(\\text{Vol}_q(\\left\\lfloor \\frac{d-1}{2}\\right \\rfloor, n)$$ We know that $\\text{Vol}_q(\\lfloor \\frac{d-1}{2} \\rfloor, n)=\\sum_{j=0}^{\\lfloor {d-1\\over 2}\\rfloor}{n\\choose j}(q-1)^j$ but this expression is not very helpful for analysis. To address this, we use the $q$-ary entropy function, which provides a concise way to capture the volume of Hamming balls. q-ary Entropy Function: The q-ary entropy function $H_q:[0,1]\\to [0, 1]$ is defined as: $$ H_q(x)=x\\log_q(q-1)-x\\log_q(x)-(1-x)\\log_q(1-x) $$ This generalizes the binary entropy function $H_2(x)=-x\\log x - (1-x)\\log (1-x)$. Using q-ary entropy function, we can bound the volume of the Hamming ball with the following propositions, allowing us to replace the pesky volume expression with cleaner approximations. Propositions: Let $q\\ge 2$ be an integer, and let $0\\le p \\le 1 - {1\\over q}$. Then: $\\text{Vol}_q(pn, n)\\le q^{n\\cdot H_q(p)}$ $\\text{Vol}_q(pn, n)\\ge q^{n\\cdot H_q(p) - o(n)}$ Here, the $o(n)$ term is a function $f(n)$ such that $\\lim_{n\\to \\infty}{f(n)\\over n}\\to 0$. We can consider this term as negligible compared to $n\\cdot H_q(p)$. Intuitive Interpretation of q-ary entropy: The binary entropy function $H_2(p)$ is often described in terms of the number of bits needed to describe something. For example, a random string of length $n$, where each bit is 1 with probability $p$, can be described using $n\\cdot H_2(p)$ bits. There is a similar interpretation for $q$-ary entropy. Suppose we choose $x\\in \\mathbb{F}_q^n$ s.t. each $x_i$ is 0 with probability $1-p$ and random in $\\mathbb{F}_q^*$ with probability $p$. $$ x_i=\\begin{cases}0 &\\text{w/ prob. }1-p\\\\\\text{random in }\\mathbb{F}_q^* &\\text{w/ prob. }p\\end{cases} $$ Then, the number of bits needed to describe $x$ is roughly $n\\cdot H_2(p)$. Before proceeding, letâ€™s examine how this q-ary entropy function behaves. $H_2(x)$: This function is 0 at $x=0$ and $x=1$, with a maximum value of $1$ at $x={1\\over 2}$. $H_3(x)$: It resembles $H_2(x)$ but is slightly shoved over to the right. Itâ€™s maximum value occurs at $x={2\\over 3}$. $H_6(x)$: This function is shifted further to the right, with its maximum value occurring at $x=\\frac{5}{6}$. More generally, $H_q(x)$ has the maximum value of $1$ at $x={q-1\\over q}$. As $q$ increases, curve of the function is shoved more and more over to the right. Here are some useful properties of $H_q(x)$: If $p\\in [0, 1]$ is constant and $q\\to \\infty$, then $$ H_q(p)= \\underbrace{p\\cdot \\log_q(q-1)}_{\\text{basically 1}}+\\underbrace{O(\\log_q(\\text{stuff}))}_{\\text{really small}}\\approx p $$ So, eventually the plot looks like a line of $H_q(p)=p$ and a little hicky at the end. If $q$ is constant and $p\\to 0$, then $$ \\begin{align} H_q(p)&=\\underbrace{p\\cdot \\log_q(q-1)}_{O(p)}+\\underbrace{p\\log_q\\left({1\\over p}\\right)}_{\\text{This term is the largest}}+ \\underbrace{(1-p)\\log_q\\left({1\\over 1-p}\\right )}_{\\approx p/\\ln(q) =O(p)}\\\\ &\\approx p\\log_q\\left({1\\over p}\\right) \\end{align} $$ So, near the origin, all those curves look like $x\\ln(1/x)\\over \\ln q$. The following is my own analysis for the last term $(1-p)\\cdot \\log_q\\left ({1\\over 1-p}\\right)$ using Taylor expansion. Analyze the last term using Taylor expansions: The Taylor expansion of $f(x)$ at $x=a$ is $$ \\begin{align} f(x)&amp;=\\sum_{n = 0}^\\infty\\frac{f^{(n)}(a)}{n!}(x-a)^n \\ &amp;=f(a)+fâ€™(a)\\cdot (x-a)+\\frac{fâ€™â€™(a)}{2!}\\cdot (x-a)^2+\\dots \\end{align} $$ Derivatives of $\\ln (x)$ are given as: $\\lnâ€™(x)=x^{-1}$, $\\ln^{â€˜â€™}(x)=(-1)\\cdot x^{-2}$, $\\ln^{(3)}=(-1)\\cdot (-2)\\cdot x^{-3}$. By deduction, we have: $\\ln^{(n)}(x)=(-1)^{n-1}\\cdot (n-1)!\\cdot x^{-n}$. The Taylor expansion of $\\ln(x)$ at $x=1$ is: $$ \\begin{align} \\ln(x) &amp;= \\ln(1)+\\sum_{n=1}^{\\infty} \\frac{\\ln^{n}(1)}{n!}\\cdot (x-1)^n\\&amp;=\\sum_{n=1}^{\\infty}(-1)^{n-1}\\cdot {1\\over n}\\cdot (x-1)^n\\ &amp;=(x-1) -{(x-1)^2\\over 2}+{(x-1)^3\\over 3}-\\dots+{(-1)^{n-1}\\over n}\\cdot (x-1)^n \\end{align} $$ Applying this expansion to the last term: $$ \\begin{align} \\log_q\\left ({1\\over 1-p}\\right)&amp;=-{\\ln (1-p) \\over \\ln q}\\ &amp;=-{1\\over \\ln q}\\cdot [(-p)-\\frac{(-p)^2}{2}+\\dots]\\ &amp;\\approx {1\\over \\ln q}\\cdot p \\end{align} $$ This shows that $\\lim_{p\\to 0}(1-p)\\cdot \\log_q\\left ({1\\over 1-p}\\right)\\approx p/\\ln (q)=O(p)$ Now, we can use them to simplify our expression for GV and Hamming bounds, both involving the the volume of the q-ary Hamming ball. The strategy is to take log base $q$ of the following approximation in terms of the $\\delta$: $$\\text{Vol}_q(\\delta n, n)\\approx q^{n\\cdot H_q(\\delta)}$$ We can replace the pesky term $\\log_q \\text{Vol}_q(\\delta n,n)$ with $n\\cdot H_q(\\delta)$. Hamming Bound: For any family $\\mathcal{C}$ of the q-ary codes, we have $$ R(\\mathcal{C})\\le 1 - H_q(\\delta{\\mathcal{(C)}}/2) $$ GV Bound: GV Bound: Let $q\\ge 2$. For any $0\\le \\delta\\le 1- \\frac{1}{q}$, and for any $0< \\epsilon \\le 1 - H_q(\\delta)$, there exists a q-ary family of codes $\\mathcal{C}$ with $\\delta(\\mathcal{C})\\ge \\delta$ and $$ R(\\mathcal{C})\\ge 1 - H_q(\\delta)-\\epsilon $$ Trade-off Between Rate and DistanceNow, itâ€™s easier to compare these two bounds: The following plot the trade-off for $q=2$ in terms of only the rate $R$ and the relative distance $\\delta$, without considering $n, k, d$. The red line represents the Hamming bound for binary codes.Notably, no point above the Hamming bound is achievable by any binary codes. The blue line represents the GV bound for binary codes.Notably, any point below the GV bound is achievable by some codes. The yellow region is an area of uncertainty. We would like to push the GV bound as much up as possible while at the same time try and push down the Hamming bound as much as possible. Note that the GV bound answers our earlier question: There do exist asymptotic good codes! But, can we find some explicit ones with efficient algorithms? Regarding the yellow uncertain region, there are several other interesting questions: Are there family of codes that beat the GV bound? The answer is both yes and no. Answer 1: Yes. For $q=49$, â€œAlgebraic Geometry Codesâ€ beat the GV bound. Answer 2: For binary codes, we donâ€™t know.This remains an OPEN QUESTION! The GV bound (which is relatively straighforward to prove) is the best-known possibility of result we have for binary codes. Can we find explicit constructions of families of codes that meet the GV bound? Recall that our proof for GV bound is non-constructive; it uses the probabilistic method to show the existence of a random linear codes with decent rates. However, we are looking for explicit descriptions or efficient algorithms to construct such codes. Answer 1: Yes, for large alphabet. (Weâ€™ll see soon) Answer 2: For binary codes, recent work [Ta-Shma 2017] gives something close in a very particular parameter regimeâ€¦but in general, itâ€™s still an OPEN QUESTION!","link":"/2024/12/25/stanford-cs250-ecc-lec3/"},{"title":"ã€ŒCryptography-Bonehã€:Integrity","text":"è¿™ç¯‡æ–‡ç« ä¸»è¦ä»‹ç»æ¶ˆæ¯éªŒè¯ç ï¼Œå³MAC (Message Auth. Code)ã€‚ æ–‡ç« é¦–å…ˆä»‹ç»äº†secure MACçš„æ¨¡åž‹å’Œå®‰å…¨å®šä¹‰ï¼Œå½“æ”»å‡»è€…èƒ½ä¼ªé€ å‡ºæ–°çš„msg/tagå¯¹æ—¶ï¼ŒMACå°±ä¸å†å®‰å…¨ã€‚ æ–‡ç« çš„ç¬¬äºŒéƒ¨åˆ†ä»‹ç»äº†åŸºäºŽPRFçš„MACæž„é€ ï¼Œæ ¹æ®ç›¸å…³å®šç†ï¼Œåªè¦PRFçš„è¾“å‡ºç©ºé—´è¶³å¤Ÿå¤§ï¼Œä¸”è¿™æ˜¯ä¸€ä¸ªå®‰å…¨çš„PRFï¼Œåˆ™åŸºäºŽPRFçš„MACå°±æ˜¯å®‰å…¨çš„ã€‚ ä½†åŸºäºŽPRFçš„MACåªèƒ½è®¡ç®—å›ºå®šæ¶ˆæ¯å¤§å°çš„MACï¼Œå¦‚ä½•åˆ©ç”¨è¿™ä¸ªå·¥å…·æž„é€ å‡ºæ›´å¤§æ¶ˆæ¯ç©ºé—´çš„MACï¼Ÿ æ–‡ç« åŽåŠéƒ¨åˆ†ç»™å‡ºäº†ä¸€äº›ä¸»æµçš„MACæž„é€ ï¼š ä¸²è¡Œæž„é€ ï¼šCBC-MACã€NMACã€CMAC å¹¶è¡Œæž„é€ ï¼šPMACã€HMACï¼ˆä¸‹ä¸€ç¯‡æ–‡ç« ï¼‰ åŸºäºŽone-time MAC: CW MAC æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†MAC PaddingæŠ€æœ¯ã€‚ Message Auth. Codesæœ¬èŠ‚çš„ç›®æ ‡æ˜¯ä¿è¯æ¶ˆæ¯çš„å®Œæ•´æ€§ï¼Œæš‚æ—¶ä¸è€ƒè™‘æ¶ˆæ¯æœºå¯†æ€§ã€‚ æ¯”å¦‚ä¿æŠ¤ç£ç›˜ä¸Šçš„å…¬å…±äºŒè¿›åˆ¶æ–‡ä»¶ä¸ä¼šè¢«ç¯¡æ”¹ï¼Œä¿æŠ¤webé¡µé¢ä¸Šçš„å¹¿å‘Šæ¶ˆæ¯ä¸ä¼šè¢«æ›¿æ¢ç¯¡æ”¹ã€‚ Def: MAC I = (S, V) defined over (K, M, T) is a pair of algs: S(k, m) outputs t in T V(k, m, t) outputs â€˜yesâ€™ or â€˜noâ€™ MACæ˜¯ä¸€ç»„å®šä¹‰åœ¨(K, M, T)ä¸Šçš„ç®—æ³•(S, V)ï¼Œç”Ÿæˆtagçš„ç®—æ³•S(k, m)å’ŒéªŒè¯tagçš„ç®—æ³•V(k, m, tag)ã€‚ æ¶ˆæ¯çš„å®Œæ•´æ€§éœ€è¦å‘é€æ–¹å’ŒæŽ¥æ”¶æ–¹å…±äº«ä¸€ä¸ªå¯†é’¥ã€‚ å¦‚æžœæ˜¯ç”¨CRCæ¥ä¿è¯æ¶ˆæ¯çš„å®Œæ•´æ€§ï¼Œæ”»å‡»è€…å¯ä»¥ä¿®æ”¹æ¶ˆæ¯ï¼Œå¹¶è½»æ˜“çš„è®¡ç®—å‡ºæ–°çš„CRCã€‚ æ‰€ä»¥CRCå¯ä»¥ç”¨äºŽæ£€æµ‹éšæœºé”™è¯¯ï¼ˆä¿¡é“å™ªå£°ï¼‰ï¼Œè€Œä¸èƒ½ç”¨äºŽæ£€æµ‹æ¶æ„ä¿®æ”¹ã€‚ Secure MACs: Modelåœ¨å®šä¹‰å®‰å…¨MACsçš„æ¨¡åž‹ä¸­ï¼š Attackerâ€™s power: chosen message attacké€‰æ‹©æ˜Žæ–‡æ”»å‡»: å¯¹äºŽä»»æ„çš„æ¶ˆæ¯ $\\mathrm{m_1,m_2,â€¦,m_q}$ ï¼Œæ”»å‡»è€…èƒ½å¾—åˆ°å…¶å¯¹åº”çš„æ¶ˆæ¯éªŒè¯ç  $\\mathrm{t}_{\\mathrm{i}} \\leftarrow \\mathrm{S}\\left(\\mathrm{k}, \\mathrm{m}_{\\mathrm{i}}\\right)$ Attackerâ€™s goal: existential forgeryä¼ªé€ : å¸Œæœ›èƒ½ä¼ªé€  æ–°çš„ message/tag (m,t)ã€‚æ–°çš„ï¼Œæ„å‘³ç€ $(\\mathrm{m}, \\mathrm{t}) \\notin\\left\\{\\left(\\mathrm{m}_{1}, \\mathrm{t}_{1}\\right), \\ldots,\\left(\\mathrm{m}_{\\mathrm{q}}, \\mathrm{t}_{\\mathrm{q}}\\right)\\right\\}$ new valid message/tag pair: attacker cannot produce a valid tag for a new message.æ„å‘³ç€ä¸èƒ½ä¸ºæ–°çš„æ¶ˆæ¯ç”Ÿæˆæœ‰æ•ˆçš„tag given (m,t) attacker cannot even produce (m,tâ€™) for $\\mathrm{tâ€™\\ne t}$ä¹Ÿä¸èƒ½ä¸ºåŽŸæ¶ˆæ¯ç”Ÿæˆæ–°çš„tag Secure MACs: Defåœ¨å®šä¹‰çš„MACæ¸¸æˆä¸­: é¦–å…ˆæŒ‘æˆ˜è€…ä¼šéšæœºé€‰æ‹©å¯†é’¥ $\\mathrm{k\\leftarrow K}$ ï¼Œä½œä¸ºMACçš„ç­¾åç®—æ³•çš„å¯†é’¥ã€‚ æ”»å‡»è€…å‘æŒ‘æˆ˜è€…å‘èµ·qä¸ªè¯¢é—®ï¼Œæ¯æ¬¡è¯¢é—®éƒ½å¯ä»¥å¾—åˆ°è¯¥æ¶ˆæ¯å¯¹åº”çš„æ¶ˆæ¯éªŒè¯ç ã€‚ æ”»å‡»è€…ä¼šç”Ÿæˆä¸€å¯¹msg/tag pair: (m, t)ï¼Œå¦‚æžœè¯¥pairé€šè¿‡äº†æŒ‘æˆ˜è€…çš„éªŒè¯ç®—æ³•ï¼Œå¹¶ä¸”æ˜¯ä¸€å¯¹æ–°çš„msg/tag pairï¼ŒæŒ‘æˆ˜è€…è¾“å‡ºï¼š1ï¼Œå¦åˆ™è¾“å‡º0ã€‚ æ‰€ä»¥secure MACså®šä¹‰ä¸ºï¼šæ”»å‡»è€…ç”Ÿæˆnew message/tag pair é€šè¿‡æŒ‘æˆ˜è€…éªŒè¯ç®—æ³•çš„æ¦‚çŽ‡ä¸ºneg. Defï¼šI=(S, V) is a secure MAC if for all â€œefficientâ€ A:$$\\mathrm{Adv_{MAC}[A,I]=Pr[Chal. output 1]\\quad is â€œnegligibleâ€}$$ e.g.1: ä¸¤ä¸ªæ¶ˆæ¯å…·æœ‰åŒæ ·tagçš„æ¦‚çŽ‡ä¸º1/2ï¼Œæ‰€ä»¥Adv[A, I]=1/2. e.g.2: åŒç†ï¼Œtagçš„é•¿åº¦æ˜¯5bitsï¼Œæ‰€ä»¥ä¸¤ä¸ªä¸åŒæ¶ˆæ¯æœ‰åŒæ ·tagçš„æ¦‚çŽ‡ä¸º1/32. MACs Based on PRFsä»»ä½•PRFéƒ½å¯ä»¥è½¬æ¢ä¸ºMACã€‚ å¯¹äºŽä¸€ä¸ªPRF: $\\mathrm{F:K\\times X\\longrightarrow Y}$ ç”±è¯¥PRFå®šä¹‰çš„MAC $\\mathrm{I_F=(S,V)}$ ä¸ºï¼š S(k, m) := F(k, m) V(k, m, t): output â€˜yesâ€™ if t = F(k,m) and â€˜noâ€™ otherwise. ä½†å®‰å…¨çš„PRFsä¸ä¸€å®šå¯ä»¥ç”Ÿæˆå®‰å…¨çš„MACsï¼Œæ¯”å¦‚ï¼š ç”±ä¸Šå¼å­ç”Ÿæˆçš„MACså¹¶ä¸æ˜¯ä¸€ä¸ªå®‰å…¨çš„MACsï¼Œå› ä¸ºtagçš„é•¿åº¦å¤ªçŸ­ï¼Œæ”»å‡»è€…å¯ä»¥ä¸ºä»»ä½•æ¶ˆæ¯çŒœæµ‹å…¶tagã€‚ Securityæ‰€ä»¥ï¼Œè¦ä¿è¯åŸºäºŽPRFsçš„MACsçš„å®‰å…¨æ€§ï¼Œè¿˜éœ€è¦PRFçš„è¾“å‡ºç©ºé—´è¶³å¤Ÿå¤§ï¼Œå¦åˆ™æ”»å‡»è€…å¯ä»¥é€šè¿‡çŒœæµ‹æ”»å‡»æˆåŠŸã€‚ Thm: If $\\mathrm{F:K\\times X\\rightarrow Y}$ is a secure PRF and 1/|Y| is negligible(i.e. |Y| is large) then $\\mathrm{I_F}$ is a secure MAC. In particular, for every eff. MAC adversary A attacking $\\mathrm{I_F}$ there exists an eff. PRF adversary B attacking F s.t.:$$\\mathrm{Adv_{MAC}[A,I_F]\\le Adv_{PRF}[B, F]+1/|Y|}$$$\\mathrm{I_F}$ is secure as long as |Y| is large, say |Y| = $2^{80}$. å¯¹äºŽæ¯ä¸€ä¸ªæ”»å‡»MACçš„æ”»å‡»è€…Aï¼Œéƒ½å­˜åœ¨æ”»å‡»å¯¹åº”PRFçš„æ”»å‡»è€…Bï¼Œæ»¡è¶³ä¸Šè¿°å¼å­ã€‚æ‰€ä»¥å¦‚æžœä¸ç­‰å¼å³è¾¹éƒ½ä¸ºneg.ï¼Œé‚£ä¸ç­‰å¼å·¦è¾¹ä¸€å®šä¸ºneg. Truncating MACs based on PRFsTruncating MACsçš„å®šä¹‰å¦‚ä¸‹ï¼š Easy lemma: suppose $\\mathrm{F:K\\times X\\rightarrow {0,1}}$ is a secure PRF. Then so is $\\mathrm{F_t(k,m)=F(k,m)[1\\dots t]}$ for all $\\mathrm{1\\le t\\le n}$ åŸºäºŽPRFsçš„MACsï¼Œæˆªæ–­åŽçš„MACsï¼Œå¦‚æžœtagé•¿åº¦è¶³å¤Ÿé•¿ï¼Œä¹Ÿæ˜¯å®‰å…¨çš„ã€‚ if (S,V) is a MAC is based on a secure PRF outputing nâ€bit tags the truncated MAC outputing w bits is secure as long as $\\mathrm{1/2^w}$ is still negligible (say wâ‰¥64) ExamplesAESå¯ä»¥çœ‹ä½œä¸ºï¼ša MAC for 16-byte messages. æ‰€ä»¥å¼•å…¥äº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼šhow to convert small-MAC into a Big-MAC ? å®žè·µä¸­æœ‰ä¸¤ç§ä¸»æµæž„é€ æ–¹æ³•ï¼š CBC-MAC (banking â€“ ANSI X9.9, X9.19, FIPS 186â€3) HMAC (Internet protocols: SSL, IPsec, SSH, â€¦) Sequential Constructionsæœ¬èŠ‚çš„ä¸»è¦ç›®æ ‡æ˜¯: given a PRF for short messages (AES) construct a PRF for long messages. From here on let $\\mathrm{X={0,1}^n}$ (e.g. n=128) 1. CBC-MACCBC-MAC ä¹Ÿå« encrypted CBC-MAC (ECBC) PRP: $\\mathrm{F:K\\times X\\rightarrow X}$ . ECBCå®šä¹‰æ–°çš„PRF $\\mathrm{F_{ECBC}:K^2\\times X^{\\le L}\\rightarrow X}$ . æ–°çš„PRFçš„è¾“å…¥ä¸ºï¼šä¸€å¯¹å¯†é’¥ å’Œ long messages (L: blocks) ç»¿è‰²éƒ¨åˆ†å°±æ˜¯åŽŸç”Ÿçš„CBCï¼ˆä½¿ç”¨å¯†é’¥kï¼‰ï¼Œè€Œencryptedè¡¨çŽ°åœ¨å¯¹CBCçš„è¾“å‡ºå†è¿›è¡ŒåŠ å¯†æ“ä½œï¼ˆä½¿ç”¨å¯†é’¥ $\\mathrm{k_1}$ ï¼Œå¾—åˆ°æ¶ˆæ¯çš„MACã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ€åŽå¯¹CBCè¾“å‡ºè¿›è¡ŒåŠ å¯†çš„å¯†é’¥æ˜¯ç‹¬ç«‹äºŽCBCçš„å¯†é’¥ã€‚ 2. NMACNMACå³ nested MACã€‚ PRF: $\\mathrm{F:K\\times X\\rightarrow K}$ NMACå®šä¹‰æ–°çš„PRF $\\mathrm{F_{NMAC}:K^2\\times X^{\\le L}\\rightarrow K}$ . æ–°çš„PRFçš„è¾“å…¥ä¸ºï¼šä¸€å¯¹å¯†é’¥ å’Œ long messages (L: blocks) ç»¿è‰²éƒ¨åˆ†æ˜¯å±‚å (cascade)éƒ¨åˆ†ï¼ŒæŠŠæœ¬æ¬¡æ¶ˆæ¯å—PRFçš„è¾“å‡ºä½œä¸ºä¸‹æ¬¡æ¶ˆæ¯å—PRFè¿ç®—çš„å¯†é’¥ï¼Œcascadeéƒ¨åˆ†çš„è¾“å‡ºä¸ºtã€‚ æœ€åŽå¯¹tåšpaddingæ“ä½œï¼ˆfixed padï¼‰ï¼Œå¯¹äºŽpaddingåŽçš„ç»“æžœï¼Œç”¨æ–°çš„å¯†é’¥ $\\mathrm{k_1}$ å†åšä¸€æ¬¡PRFè¿ç®—ï¼Œå¾—åˆ°æœ€ç»ˆçš„MACã€‚ Last Encryptionåœ¨ECBCå’ŒNMACä¸­ï¼Œæœ€åŽéƒ½æœ‰åŠ å¯†çš„æ­¥éª¤ã€‚ Why the last encryption step in ECBC-MAC ? NMAC: å‡è®¾NMACåªæœ‰cascadeéƒ¨åˆ†ï¼Œå³S(k,m)=cascade(k,m) Then this MAC can be forged with one chosen msg query. Extension Attack: å·²çŸ¥cascade(k, m)ï¼Œå¯¹äºŽä»»æ„æ¶ˆæ¯å—wï¼Œå¯ä»¥è½»æ˜“å¾—åˆ°cascade(k, m||w)çš„å€¼ã€‚ å› ä¸ºcascade(k, m)æ˜¯ä½œä¸ºä¸‹ä¸€ä¸ªæ¶ˆæ¯å—è¿ç®—çš„å¯†é’¥ï¼Œæ‰€ä»¥cascade(k, m||w) = PRF(cascade(k, m), w)ã€‚ æ‰€ä»¥æœ€åŽçš„åŠ å¯†æ­¥éª¤æ˜¯ä¸ºäº†é˜»æ­¢extension attackã€‚ ECBCï¼š åŒæ ·ï¼Œå‡è®¾ECBCåªæœ‰raw CBCéƒ¨åˆ†ï¼Œæ²¡æœ‰æœ€åŽåŠ å¯†çš„æ­¥éª¤ï¼Œå³S(k,m)=rawCBC(k,m). Then $\\mathrm{I_{RAW}=(S,V)}$ is easily broken using a 1-chosen msg attack. 1-chosen msg attack: æ”»å‡»è€…éšæœºé€‰æ‹©one-block message m. è¯·æ±‚å¾—åˆ°må¯¹åº”çš„tag t = F(k,m) æ”»å‡»è€…å¯ä»¥ä¼ªé€ 2-block message $\\mathrm{(m,t\\oplus m)}$ çš„MACï¼Œå…¶å®žä¹Ÿå°±æ˜¯ tã€‚ $\\operatorname{rawCBC}(\\mathrm{k},(\\mathrm{m}, \\mathrm{t} \\oplus \\mathrm{m}))=\\mathrm{F}(\\mathrm{k}, \\mathrm{F}(\\mathrm{k}, \\mathrm{m}) \\oplus(\\mathrm{t} \\oplus \\mathrm{m}))=\\mathrm{F}(\\mathrm{k}, \\mathrm{t} \\oplus(\\mathrm{t} \\oplus \\mathrm{m}))=\\mathrm{t}$ Security AnalysisTheorem: For any L &gt; 0, For every eff. q-query PRF adv. A attacking $\\mathrm{F_{ECBC}}$ or $\\mathrm{F_{NMAC}}$ , there exists an eff. adversary B s.t.: $$ \\begin{array}{l}\\operatorname{Adv}_{\\mathrm{PRF}}\\left[\\mathrm{A}, \\mathrm{F}_{\\mathrm{ECBC}}\\right] \\leq \\mathrm{Adv}_{\\mathrm{PRP}}[\\mathrm{B}, \\mathrm{F}]+2 \\mathrm{q}^{2} /|\\mathrm{X}| \\\\ \\operatorname{Adv}_{\\mathrm{PRF}}\\left[\\mathrm{A}, \\mathrm{F}_{\\mathrm{NMAC}}\\right] \\leq \\mathrm{q} \\cdot \\mathrm{L} \\cdot \\mathrm{Adv}_{\\mathrm{PRF}}[\\mathrm{B}, \\mathrm{F}]+\\mathrm{q}^{2} / 2|\\mathrm{~K}|\\end{array} $$ CBCâ€MAC is secure as long as q &lt;&lt; $|X|^{1/2}$ NMAC is secure as long as q &lt;&lt; $|K|^{1/2}$ (q &lt;&lt; $2^{64}$ for AES-Â­128) ä¸ºäº†ç®€åŒ–è®¨è®ºï¼Œè€ƒè™‘ä¸ç­‰å¼ï¼š $\\operatorname{Adv}_{\\mathrm{PRF}}\\left[\\mathrm{A}, \\mathrm{F}_{\\mathrm{ECBC}}\\right] \\leq \\mathrm{Adv}_{\\mathrm{PRP}}[\\mathrm{B}, \\mathrm{F}]+ \\mathrm{q}^{2} /|\\mathrm{X}|$ q = #messages MAC-ed with k å¦‚æžœæˆ‘ä»¬æƒ³å¾—åˆ° $\\operatorname{Adv}_{\\mathrm{PRF}}\\left[\\mathrm{A}, \\mathrm{F}_{\\mathrm{ECBC}}\\right] \\leq 1 / 2^{32}$ çš„ç»“æžœï¼Œå¿…é¡»æ»¡è¶³ï¼š $\\mathrm{q^{2} /|X|","link":"/2021/11/21/stanford-integrity1/"},{"title":"ã€ŒCryptography-Bonehã€:Collision Resistance","text":"ä¸Šä¸€èŠ‚ä»‹ç»äº†åŸºäºŽPRFsçš„MACæž„é€ å’ŒåŸºäºŽéšæœºçš„MACï¼š æœ¬èŠ‚æˆ‘ä»¬å°†ä»‹ç»æŠ—ç¢°æ’žçš„MAC(MACs from collision resistance)ã€‚ ç¬¬ä¸€éƒ¨åˆ†ä»‹ç»äº†ä»€ä¹ˆæ˜¯æŠ—ç¢°æ’ž (Collision Resistance)ï¼Œä»¥åŠåŸºäºŽC.R.çš„MACçš„å®‰å…¨æ€§ã€‚ ç¬¬äºŒéƒ¨åˆ†ä»‹ç»äº†ç”Ÿæ—¥æ‚–è®ºï¼Œå¦‚ä½•ç”¨ç”Ÿæ—¥æ”»å‡»å¯»æ‰¾2-way collision å’Œ3-way collision. ç¬¬ä¸‰éƒ¨åˆ†ä»‹ç»äº†Merkle-DamgargèŒƒå¼ï¼ˆå¦‚æžœåŽ‹ç¼©å‡½æ•°hæ˜¯C.R.ï¼Œé‚£ä¹ˆæž„é€ å‡ºçš„å“ˆå¸Œå‡½æ•°Hå°±æ˜¯C.R.ï¼‰ä»¥åŠå¦‚ä½•æž„å»ºC.R.çš„åŽ‹ç¼©å‡½æ•°ï¼ˆDavies-MeyeråŽ‹ç¼©å‡½æ•°). æœ€åŽä¸€éƒ¨åˆ†ä»‹ç»äº†HMAC (Hash MAC)å’Œä¸€ç§é’ˆå¯¹MACéªŒè¯çš„timing attack and defense. Collision ResistanceLet H: M â†’T be a hash function (|M|&gt;&gt;|T|) ã€Hæ˜¯ä¸€ä¸ªå“ˆå¸Œå‡½æ•°ã€‘ A collision for H is a pair $\\mathrm{m_0,m_1\\in M}$ such that: $$ \\mathrm{H(m_0)=H(m_1)\\quad and\\quad m_0\\ne m_1} $$ ã€å“ˆå¸Œç¢°æ’žæ˜¯æŒ‡ï¼Œå­˜åœ¨ä¸¤ä¸ªä¸åŒçš„æ¶ˆæ¯ï¼Œå…¶å“ˆå¸Œå‡½æ•°çš„å€¼ç›¸ç­‰ã€‚ã€‘ A function is collision resistant if for all (explicit) â€œeffâ€ algs. A: $$ \\operatorname{Adv}_{\\mathrm{CR}}[\\mathrm{A}, \\mathrm{H}]=\\operatorname{Pr}[\\text { A outputs collision for } \\mathrm{H}] \\quad \\text{is \"neg.\"} $$ ã€å¦‚æžœå¯¹äºŽæ‰€æœ‰å¤šé¡¹å¼æ—¶é—´ç®—æ³•ï¼Œè¾“å‡ºå“ˆå¸Œå‡½æ•°Hçš„ç¢°æ’žçš„æ¦‚çŽ‡æ˜¯å¯å¿½ç•¥çš„ï¼Œå°±è¯´æ˜Žå“ˆå¸Œå‡½æ•°Hæ˜¯æŠ—ç¢°æ’žçš„ï¼ˆcollision resistantï¼‰ï¼Œæ¯”å¦‚SHA-256å°±æ˜¯ä¸€ä¸ªæŠ—ç¢°æ’žçš„å“ˆå¸Œå‡½æ•°ã€‚ã€‘ MACs from C.R.å¦‚ä½•ä»ŽæŠ—ç¢°æ’žçš„å“ˆå¸Œå‡½æ•°æž„å»ºMACsï¼Ÿ Let I = (S,V) be a MAC for short messages over (K,M,T) (e.g. AES) Let $\\mathrm{H}: \\mathrm{M}^{\\mathrm{big}} \\rightarrow \\mathrm{M}$ Def: $\\mathrm{I}^{\\mathrm{big}}=\\left(\\mathrm{S}^{\\mathrm{big}}, \\mathrm{V}^{\\mathrm{big}}\\right)$ over $\\left(\\mathrm{K}, \\mathrm{M}^{\\mathrm{big}}, \\mathrm{T}\\right)$ as:$$\\begin{align}\\mathrm{S}^{\\mathrm{big}(k, m)}=&amp;\\mathrm{S}(k, \\mathrm{H}(m)) \\ \\mathrm{V}^{\\mathrm{big}}(k, m, t)=&amp;\\mathrm{V}(k, \\mathrm{H}(m), t)\\end{align}$$ã€Iæ˜¯ä¸€ä¸ªé’ˆå¯¹çŸ­æ¶ˆæ¯çš„MACç®—æ³•ï¼ŒHæ˜¯ä¸€ä¸ªå“ˆå¸Œå‡½æ•°ï¼Œèƒ½å°†æžå¤§çš„æ¶ˆæ¯æ˜ å°„åˆ°è¾ƒçŸ­çš„æ¶ˆæ¯ç©ºé—´ä¸Šã€‚ã€‘ Thm : If I is a secure MAC and H is collision resistant then $\\mathrm{I^{big}}$ is a secure MAC. ã€å¦‚æžœIæ˜¯ä¸€ä¸ªsecure MACï¼Œå¹¶ä¸”Hæ˜¯æŠ—ç¢°æ’žçš„ï¼Œ$\\mathrm{I^{big}}$ å°±æ˜¯ä¸€ä¸ªsecure MACã€‘ å…¶ä¸­ï¼ŒCollision resistence æ˜¯æ»¡è¶³å®‰å…¨çš„å¿…è¦æ¡ä»¶ï¼š å‡è®¾æ”»å‡»è€…å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªç¢°æ’žï¼š$\\mathrm{m_0\\ne m_1\\quad s.t. H(m_0)=H(m_1)}$ é‚£ä¹ˆï¼Œ$\\mathrm{I^{big}}$ åœ¨1-chosen msg æ”»å‡»ä¸‹å°±æ˜¯ä¸å®‰å…¨çš„ï¼š æ”»å‡»è€…è¯·æ±‚å¾—åˆ°tag: $\\mathrm{t} \\longleftarrow \\mathrm{S}\\left(\\mathrm{k}, \\mathrm{m}_{0}\\right)$ ä¼ªé€ pair $\\mathrm{(m_1,t)}$ ä½œä¸ºè¾“å‡º Generic birthday attackGeneric attack on C.R. functions$\\mathrm{H}: \\mathrm{M} \\rightarrow{0,1}^{\\mathrm{n}}$ æ˜¯ä¸€ä¸ªå“ˆå¸Œå‡½æ•°.$\\left(|M| \\gg 2^{n}\\right)$ ä¸€èˆ¬çš„æ”»å‡»ç®—æ³•å¯ä»¥åœ¨ $\\mathcal{O}(2^{n/2})$ æ—¶é—´å†…æ‰¾åˆ°ä¸€å¯¹å“ˆå¸Œç¢°æ’žï¼š åœ¨Mä¸­éšæœºé€‰æ‹© $2^{n/2}$ ä¸åŒçš„æ¶ˆæ¯ï¼š$\\mathrm{m_1,\\dots,m_{2^{n/2} } }$ for i = 1, â€¦, $2^{n/2}$ ï¼Œè®¡ç®—å‡ºå…¶å¯¹åº”çš„å“ˆå¸Œå€¼ï¼š $\\mathrm{t}_{\\mathrm{i}}=\\mathrm{H}\\left(\\mathrm{m}_{\\mathrm{i}}\\right) \\quad \\in\\{0,1\\}^{\\mathrm{n}}$ æŸ¥çœ‹è¿™å…¶ä¸­æ˜¯å¦æœ‰ç¢°æ’žï¼ˆå³æ»¡è¶³ $\\mathrm{t_i=t_j}$ ï¼‰ï¼Œå¦‚æžœæ²¡æœ‰ï¼Œè¿”å›žç¬¬ä¸€æ­¥ã€‚ ï¼ˆåªéœ€è¦è¿›è¡Œå¾ˆå°‘è¿­ä»£ï¼Œå°±å¯ä»¥æ‰¾åˆ°ç¢°æ’žï¼‰ The birthday paradoxä¸ºä»€ä¹ˆä¸Šè¿°ç®—æ³•æœ‰æ•ˆï¼Ÿ ç”Ÿæ—¥æ‚–è®ºå‘Šè¯‰æˆ‘ä»¬ï¼Œå¦‚æžœ $r_{1}, \\ldots, r_{n} \\in{1, \\ldots, B}$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„å˜é‡ï¼ˆå‡åŒ€åˆ†å¸ƒæ˜¯æœ€åçš„æƒ…å†µï¼‰ï¼Œå½“ $n=1.2 \\times B^{1 / 2}$ æ—¶ï¼Œå­˜åœ¨ç¢°æ’ž $r_i=r_j(i\\ne j)$ çš„æ¦‚çŽ‡å¤§äºŽç­‰äºŽ1/2ã€‚ Let $r_{1}, \\ldots, r_{n} \\in{1, \\ldots, B}$ be indep. identically distributed integers. Thm : when $n=1.2 \\times B^{1 / 2}$ then $\\operatorname{Pr}\\left[\\exists \\mathrm{i} \\neq \\mathrm{j}: \\mathrm{r}_{\\mathrm{i}}=\\mathrm{r}_{\\mathrm{j}}\\right] \\geq 1 / 2$ . Proof: (for uniform indep. $r_{1}, \\ldots, r_{n} $) å–æ ·æ•°nå’Œç¢°æ’žæ¦‚çŽ‡çš„å…³ç³»å¦‚å›¾ï¼š å› æ­¤ä¸Šè¿°çš„æ”»å‡»ç®—æ³•åªéœ€è¦è¿­ä»£ä¸¤æ¬¡ï¼Œå°±èƒ½ä»¥æžå¤§çš„æ¦‚çŽ‡æ‰¾åˆ°ç¢°æ’žã€‚å…¶æ—¶é—´ã€ç©ºé—´å¤æ‚åº¦éƒ½ä¸º $\\mathcal{O}(2^{n/2})$ Sample C.R. hash functionsæŠ—ç¢°æ’žå“ˆå¸Œå‡½æ•°çš„æ¯”è¾ƒï¼š å› æ­¤SHA-1çš„digest sizeå¤ªå°ï¼Œå¯ä»¥å¾ˆå®¹æ˜“çš„æ‰¾åˆ°ç¢°æ’žï¼Œæ‰€ä»¥ä¸æŽ¨èä½¿ç”¨ã€‚ ä¸ç®¡é‡å­è®¡ç®—æœºèƒ½å¤§å¹…é™ä½Žå¯»æ‰¾ç¢°æ’žçš„æ—¶é—´ï¼š Three way collisionç®€åŒ–è€ƒè™‘two-way collision: å“ˆå¸Œå‡½æ•° $\\mathrm{H}: \\mathrm{M} \\rightarrow \\mathrm{T\\quad(|T|=t)} $ ï¼Œåº”è¯¥éšæœºå–æ ·$n$ä¸ªä¸åŒçš„æ•°å­—ï¼Œæ‰èƒ½ä»¥æžå¤§çš„æ¦‚çŽ‡æ‰¾åˆ°ç¢°æ’žæ»¡è¶³ $H(y)=H(x)$ . nä¸ªä¸åŒçš„æ•°å­—ä¸­ï¼Œæœ‰ $n\\times(n-1)$ ä¸ªpair$(x, y)$ . è€Œå¯¹äºŽä»»æ„å€¼$H(x)$ï¼Œ$y$æ»¡è¶³ $H(y)=H(x)$ çš„æ¦‚çŽ‡ä¸º $1/t$ . å› æ­¤åº”è¯¥æ»¡è¶³ï¼š $n^2=t$ å³åªéœ€è¦å–æ · $\\mathcal{O}(|T|^{1/2})$ ä¸ªå€¼ï¼Œå³å¯æ‰¾åˆ°ç¢°æ’žã€‚ åŒç†ï¼Œè€ƒè™‘three-way collision: å“ˆå¸Œå‡½æ•° $\\mathrm{H}: \\mathrm{M} \\rightarrow \\mathrm{T\\quad(|T|=t)} $ ï¼Œåº”è¯¥éšæœºå–æ ·$n$ä¸ªä¸åŒçš„æ•°å­—ï¼Œæ‰èƒ½ä»¥æžå¤§çš„æ¦‚çŽ‡æ‰¾åˆ°ç¢°æ’žæ»¡è¶³ $H(x)=H(y)=H(z)$ . nä¸ªä¸åŒçš„æ•°å­—ä¸­ï¼Œæœ‰ $\\frac{n\\times(n-1)\\times(n-2)}{3!}$ ä¸ªpair $(x,y,z)$ . è€Œå¯¹äºŽä»»æ„å€¼H(x)ï¼Œyå’Œzæ»¡è¶³ $H(y)=H(z)=H(x)$ çš„æ¦‚çŽ‡ä¸º $1/t^2$ å› æ­¤åº”è¯¥æ»¡è¶³ï¼š$n^3/6 = t^2$ æ‰€ä»¥åªéœ€è¦å–æ · $\\mathcal{O}(|T|^{2/3})$ ä¸ªå€¼ï¼Œå³å¯æ‰¾åˆ°ç¢°æ’žã€‚ The Merkle-Damgard ParadigmThe MD iterated constructioné€šè¿‡åŽ‹ç¼©å‡½æ•°(compression function) $\\mathrm{h: T \\times X \\rightarrow T}$ ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°å“ˆå¸Œå‡½æ•° $\\mathrm{H}: X^{\\leq L} \\longrightarrow T$ . $\\mathrm{H_i}:$ chaining variables $\\mathrm{PB}:$ padding block ï¼ˆIf no space for PB add another blockï¼‰. MD collision resistanceä¸Šè¿°æž„é€ æ˜¯æŠ—å“ˆå¸Œçš„ã€‚ Thm: If h is collision resistant then so is H. ã€å¦‚æžœåŽ‹ç¼©å‡½æ•°hæ˜¯æŠ—å“ˆå¸Œçš„ï¼Œé‚£ä¹ˆæž„é€ å‡ºçš„å“ˆå¸Œå‡½æ•°ä¹Ÿæ˜¯æŠ—å“ˆå¸Œçš„ã€‘ Proof: collision on H $\\Rightarrow$ collision on h ã€é€šè¿‡åè¯æ³•è¯æ˜Žï¼Œå¦‚æžœèƒ½æ‰¾åˆ°Hçš„ç¢°æ’žï¼Œé‚£ä¹ˆä¹Ÿèƒ½æ‰¾åˆ°håŽ‹ç¼©å‡½æ•°çš„ç¢°æ’žã€‘ å‡è®¾ $\\mathrm{H(M)=H(Mâ€™)}$ ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•æž„é€ å‡ºhçš„ç¢°æ’žã€‚ ã€1ã€‘åªéœ€è¦æœ€åŽä¸€å—æ»¡è¶³ï¼š$\\mathrm{h\\left(H_{t}, M_{t} | P B\\right)=H_{t+1}=H_{r+1}^{\\prime}=h\\left(H_{r}^{\\prime}, M_{r}^{\\prime} | P B^{\\prime}\\right)}$ . ã€1ã€‘å¦‚æžœhå‡½æ•°çš„å‚æ•°ä¸å®Œå…¨ç›¸åŒï¼Œé‚£ä¹ˆå°±æ‰¾åˆ°äº†hçš„ç¢°æ’žå³ $\\mathrm{H_t\\ne H_râ€™}$ or $ \\mathrm{M_t\\ne M_râ€™}$ or $\\mathrm{PB\\ne PBâ€™}$ . ã€2ã€‘å¦åˆ™ï¼šå¦‚æžœ $\\mathrm{H_t= H_râ€™}$ and $ \\mathrm{M_t= M_râ€™}$ and $\\mathrm{PB= PBâ€™}$ ï¼ˆå¯ä»¥å¾—å‡º $t=r$ ï¼‰æ»¡è¶³å¼å­ï¼š$\\mathrm{h\\left(H_{t-1}, M_{t-1} \\right)=H_{t}=H_{t}^{\\prime}=h\\left(H_{t-1}^{\\prime}, M_{t-1}^{\\prime} \\right)}$ . ã€2ã€‘åŒæ ·çš„ï¼Œå¦‚æžœhå‡½æ•°çš„å‚æ•°ä¸å®Œå…¨ç›¸åŒï¼Œå°±æ‰¾åˆ°äº†ä¸€ä¸ªhçš„ç¢°æ’žï¼š å³ $\\mathrm{H_{t-1}\\ne H_{t-1}â€™}$ or $ \\mathrm{M_{t-1}\\ne M_{t-1}â€™}$ . ã€*ã€‘å› æ­¤é€šè¿‡è¿­ä»£çš„æ–¹å¼ï¼Œåˆ°æœ€åŽï¼š è¦ä¹ˆæ‰¾åˆ°håŽ‹ç¼©å‡½æ•°çš„ç¢°æ’ž è¦ä¹ˆ $\\forall i: M_i=M_iâ€™ \\Rightarrow M=Mâ€™$ ï¼Œä½†è¿™å°±ä¸æ»¡è¶³æ˜¯Hå“ˆå¸Œå‡½æ•°çš„ç¢°æ’žäº†ã€‚ å› æ­¤ï¼Œå¦‚æžœèƒ½æ‰¾åˆ°Hå“ˆå¸Œå‡½æ•°çš„ç¢°æ’žï¼Œå°±ä¸€å®šèƒ½æ‰¾åˆ°håŽ‹ç¼©å‡½æ•°çš„ç¢°æ’žã€‚ æ‰€ä»¥ï¼Œåœ¨MDç»“æž„ä¸­ï¼Œå¦‚æžœæƒ³è¦æž„å»ºæŠ—ç¢°æ’žçš„å“ˆå¸Œå‡½æ•°ï¼Œå°±éœ€è¦å…ˆä¿è¯åŽ‹ç¼©å‡½æ•°æ˜¯æŠ—ç¢°æ’žçš„ã€‚ Constructing Compression Functionsæ ¹æ®ä¸Šæ–‡MDçš„å®šç†ï¼šå¦‚æžœåŽ‹ç¼©å‡½æ•°hæ˜¯æŠ—ç¢°æ’žçš„ï¼Œé‚£ä¹ˆHå“ˆå¸Œå‡½æ•°ä¹Ÿæ˜¯æŠ—ç¢°æ’žçš„ã€‚ å› æ­¤ï¼Œæœ¬èŠ‚çš„ç›®æ ‡å°±æ˜¯å¦‚ä½•æž„å»ºåŽ‹ç¼©å‡½æ•° $\\mathrm{h: T \\times X \\rightarrow T}$ Comp. func. from a block cipheræˆ‘ä»¬å¯ä»¥ä»Žå—å¯†ç ä¸­æž„å»ºåŽ‹ç¼©å‡½æ•°ã€‚ block cipher: $\\mathrm{E}: \\mathrm{K} \\times{0,1}^{\\mathrm{n}} \\longrightarrow{0,1}^{\\mathrm{n}}$ Davies-Meyer compression function: $\\mathrm{h(H, m)=E(m, H) \\oplus H}$ ã€æŠŠæ¶ˆæ¯å—mä½œä¸ºå¯†é’¥ã€‘ Thm: Suppose E is an ideal cipher (collection of |K| random perms.)Finding a collision $\\mathrm{h(H,m)=h(Hâ€™,mâ€™)}$ takes $\\mathcal{O}(2^{n/2})$ evaluations of (E, D). ã€å¦‚æžœEæ˜¯ä¸€ä¸ªç†æƒ³çš„å¯†ç ï¼Œå³æ¯ä¸€ä¸ªå¯†é’¥éƒ½å¯¹åº”ä¸€ä¸ªéšæœºç½®æ¢ï¼Œé‚£ä¹ˆé€šè¿‡ç”Ÿæ—¥æ”»å‡»ï¼Œæ‰¾åˆ°ä¸€ä¸ªç¢°æ’žéœ€è¦ $\\mathcal{O}(2^{n/2})$ æ¬¡(E, D)çš„è®¡ç®—ã€‚ã€‘ å¦‚æžœDMåŽ‹ç¼©å‡½æ•°æ˜¯ $\\mathrm{h(H, m)=E(m, H)}$ ï¼Œå‡½æ•°hå°±ä¸æŠ—ç¢°æ’žã€‚ å…¶ä»–é€šè¿‡å—å¯†ç æž„å»ºçš„åŽ‹ç¼©å‡½æ•°ï¼š ç®€åŒ– $\\mathrm{E}:{0,1}^{\\mathrm{n}} \\times{0,1}^{\\mathrm{n}} \\rightarrow{0,1}^{\\mathrm{n}}$ Miyaguchi-Preneel: $\\mathrm{h(H, m)=E(m, H) \\oplus H \\oplus m}$ (Whirlpool) $\\mathrm{h(H, m)=E(H \\oplus m, m) \\oplus m}$ and so on. åŒæ ·çš„ï¼Œå¯¹äºŽè¿™æ ·çš„å˜ä½“ $\\mathrm{h}(\\mathrm{H}, \\mathrm{m})=\\mathrm{E}(\\mathrm{m}, \\mathrm{H}) \\oplus \\mathrm{m}$ æ˜¯ä¸å®‰å…¨çš„ã€‚ Case study: SHA-256SHA-256å“ˆå¸Œå‡½æ•°çš„ç»„æˆè¦ä»¶ï¼š Merkle-Damgard function Davies-Meyer compression function Block cipher: SHACAL-2 å…¶ä¸­512-bitçš„keyæ˜¯ä»Žmsg blockä¸­æˆªå–çš„ï¼Œ256-bit blockæ˜¯chaining variable. Provable compression functionsè¿˜æœ‰ä¸€ç§å¯è¯æ˜Žçš„åŽ‹ç¼©å‡½æ•°ï¼Œå³å¦‚æžœä½ èƒ½æ‰¾åˆ°è¯¥åŽ‹ç¼©å‡½æ•°çš„ç¢°æ’žï¼Œå¿…é¡»è§£å†³å›°éš¾é—®é¢˜ã€‚ å¯¹äºŽä»»æ„ $\\mathrm{m}, \\mathrm{h} \\in{0, \\ldots, \\mathrm{p}-1}$ ï¼Œå®šä¹‰åŽ‹ç¼©å‡½æ•° $\\mathrm{h(H, m)=u^{H} \\cdot v^{m} \\quad(\\bmod p)}$ . Fact: ï¬nding collision for h(.,.) is as hard as solving â€œdiscreteâ€logâ€ modulo p. Problem: slow. HMACé€šè¿‡ä¸Šè¿°Merkle-Damgardçš„æž„é€ ï¼Œå¦‚æžœåŽ‹ç¼©å‡½æ•°hæ˜¯æŠ—ç¢°æ’žçš„ï¼ŒHå“ˆå¸Œå‡½æ•°ä¹Ÿæ˜¯æŠ—ç¢°æ’žçš„ã€‚ é‚£ä¹ˆï¼Œæˆ‘ä»¬èƒ½å¦ç›´æŽ¥ä½¿ç”¨è¯¥å“ˆå¸Œå‡½æ•°H(Â·)æž„é€ MAC? $\\mathrm{\\mathrm{H}: X^{\\leq L} \\longrightarrow T}$ a C.R. Merkle-Damgard Hash Function Attemptï¼š $\\mathrm{s(k, m)=H(k | m)}$ è¿™æ ·æ˜¯ä¸å®‰å…¨çš„ï¼Œç±»ä¼¼ä¸Šç¯‡æ–‡ç« ä¸­æåˆ°çš„NMACï¼Œå¦‚æžœæ²¡æœ‰æœ€åŽä¸€æ­¥åŠ å¯†ï¼Œå°±æ˜¯ä¸å®‰å…¨çš„ã€‚ é€šè¿‡Extension Attack ï¼Œå¯¹ç»™å®šå“ˆå¸Œå€¼ $\\mathrm{H(k| m)}$ ï¼Œå¯ä»¥è®¡ç®—å‡ºä»»æ„ $\\mathrm{H(k| m|w)}$ . HMAC: Hash MACå› æ­¤ï¼Œé€šè¿‡å“ˆå¸Œå‡½æ•°æž„é€ MACçš„æ ‡å‡†æ–¹æ³•æ˜¯HMAC(Hash MAC)ï¼ŒHæ˜¯å“ˆå¸Œå‡½æ•°ï¼Œé€šè¿‡ä¸åŒçš„å“ˆå¸Œå‡½æ•°ï¼Œè¯¥MACå€¼ä¹Ÿæœ‰ä¸åŒçš„è¾“å‡ºé•¿åº¦ã€‚æ¯”å¦‚ç”¨SHA-256ä½œä¸ºå“ˆå¸Œå‡½æ•°ï¼ŒMACçš„è¾“å‡ºå°±æ˜¯256æ¯”ç‰¹ã€‚ HMACå¯ä»¥å°†ä»»æ„å“ˆå¸Œå‡½æ•°ä½œä¸ºé»‘ç›’ï¼Œå› æ­¤HMACå¹¿æ³›åº”ç”¨äºŽäº’è”ç½‘ä¸­çš„åè®®ã€‚ HMAC:$$\\mathrm{S}(\\mathrm{k}, \\mathrm{m})=\\mathrm{H}(\\mathrm{k} \\oplus \\mathrm{opad} | \\mathrm{H}(\\mathrm{k} \\oplus \\mathrm{ipad} | \\mathrm{m}))$$ï¼ˆopad: outer pad, 512-bit) ï¼ˆipad: inner pad, 512-bitï¼‰ HMACçš„ç»“æž„å’ŒåŸºäºŽPRFçš„NMACç»“æž„å¾ˆåƒï¼š $\\mathrm{h(IV,k\\oplus ipad)}$ : æ˜¯NMACçš„ $\\mathrm{k_1}$ . $\\mathrm{h(IV,k\\oplus opad)}$ : æ˜¯NMACçš„ $\\mathrm{k_2}$ . ä¸»è¦çš„ä¸åŒç‚¹åœ¨äºŽï¼ŒHMACä¸­çš„ä¸¤ä¸ªå¯†é’¥ $\\mathrm{k_1,k_2}$ æ˜¯ç›¸å…³çš„ã€‚ Timing Attacks on MAC Verificationæœ‰ä¸€ç§é’ˆå¯¹MACéªŒè¯çš„æ—¶é—´æ”»å‡»ã€‚ æ¯”å¦‚åœ¨Keyczar crypto libraryä¸­ï¼ŒéªŒè¯MACçš„å‡½æ•°ç®€åŒ–ä¸ºï¼š Problem: â€˜==â€™åœ¨pythonä¸­çš„å®žçŽ°æ˜¯æŒ‰å­—èŠ‚å¯¹æ¯” (byte-by-byte comparison)ã€‚ å› æ­¤ï¼Œå¦‚æžœå¯¹æ¯”å‡ºäº†ç¬¬ä¸€ä¸ªä¸ç›¸ç­‰çš„å­—èŠ‚ï¼Œå°±ä¼šè¿”å›žfalse. Timing AttackTiming attack: å¯¹ç›®æ ‡æ¶ˆæ¯mè®¡ç®—tag å‘æœåŠ¡å™¨éšæœºè¯·æ±‚ä¸€ä¸ªéšæœºtagï¼Œè®°å½•ä¸‹éªŒè¯æ—¶é—´ã€‚ éåŽ†éšæœºtagçš„ç¬¬ä¸€ä¸ªå­—èŠ‚ï¼Œå‘é€ç»™æœåŠ¡å™¨ã€‚å½“éªŒè¯æ—¶é—´æ¯”ç¬¬ä¸€æ­¥ä¸­çš„æ—¶é—´ç•¥é•¿æ—¶ï¼Œåœæ­¢ï¼Œå³æ‰¾åˆ°äº†ç¬¬ä¸€ä¸ªå­—èŠ‚çš„å€¼ã€‚ é‡å¤tagçš„æ‰€æœ‰å­—èŠ‚ï¼Œç›´åˆ°æ‰¾åˆ°æœ€ç»ˆæœ‰æ•ˆçš„tagã€‚ ä¿æŠ¤æ–¹æ³•çš„æ ¸å¿ƒå°±æ˜¯ï¼šè®©æ‰€æœ‰å­—ç¬¦ä¸²çš„æ¯”è¾ƒæ—¶é—´éƒ½ç›¸åŒã€‚ Defense 1ç¬¬ä¸€ç§æ˜¯è¦æ¯”è¾ƒå®Œæ‰€æœ‰å­—èŠ‚ã€‚ ä½†å›¿äºŽç¼–è¯‘å™¨çš„ä¼˜åŒ–ç­‰å› ç´ ï¼Œè¿™å¾ˆéš¾å®žçŽ°ã€‚ Defense 2ç¬¬äºŒç§åˆ™æ˜¯é€šè¿‡å†æ¬¡MACæ¥æ¯”è¾ƒä¸¤ä¸ªå­—ç¬¦æ˜¯å¦ç›¸åŒã€‚ ç”±æ­¤å¯è§ï¼Œå¯†ç ç®—æ³•çš„å®žçŽ°ä¹Ÿä¼šè®©ç®—æ³•å˜å¾—ä¸å®‰å…¨ã€‚ Boneh: Donâ€™t implement crypto yourself!","link":"/2021/12/27/stanford-integrity2/"},{"title":"ã€ŒAlgebraic ECCsã€: Lec4 Singleton + Plotkin Bounds and RS Code","text":"In this series, I will be learning Algebraic Error Correcting Codes, lectured by Mary Wootters. The lecture videos are available here. Feedback and sugguestions are always welcome! ^ - ^ Topics Covered: Singleton Bound Plotkin Bound Reed-Solomon Codes Dual View of RS Codes Generalized RS Code In the previous lecture, we explored the GV bound and the Hamming bound in the asymptotic setting where $n,k,d$ gets big. We would like to push the GV bound as much up as possible while at the same time push down the Hamming bound as much as possible. Today, we are going to learn two additional bounds, the Singleton and Plotkin bounds, which narrow down the yellow region a little bit. Additionally, we will learn Reed Solomon codes, which meet the Singleton bound. Singleton Bound Singleton Bound: If $\\mathcal{C}$ is an $(n,k,d)_q$ code, then $$ k\\le n-d+1 $$ Before proving the Singleton bound, letâ€™s examine what it looks like. The Singleton bound states that $k\\le n-d+1$ for an $(n,k,d)_q$ code. In the asymptotic setting, this translates to$$R\\le 1-\\delta$$ as illustrated below: The purple line represents the Singleton bound for binary codes ($q=2$). This implies that all points above the Singleton bound are unachievable, which we already know from the Hamming bound. Hence, for $q=2$, the Singleton bound is strictly weaker than the Hamming bound, despite its simplicity. However, when $q\\to \\infty$, the Singleton bound can be better than the Hamming bound. For $q&gt;2$, the Hamming bound shifts up while the Singleton bound stays the same as illustrated below: This shows that the region to the bottom right (below the Hamming bound but above the Singleton bound) becomes unachievable. This is information we cannot infer from the Hamming bound alone. Now, letâ€™s prove the Singleton bound. Proof of the Singleton Bound: For a codeword $c=(x_1,\\dots, x_n)\\in \\mathcal{C}$, consider throwing out the last $d-1$ coordinates. $$ c=(\\underbrace{x_1, x_2, \\dots, x_{n-d+1}}_{\\text{call this }\\varphi(c)\\in \\Sigma^{n-d+1}}, \\underbrace{x_{n-d+2}, \\dots, x_n}_{\\text{get rid of these}}) $$ Define the first $n-d+1$ coordinates as $\\varphi(c)\\in \\Sigma^{n-d+1}$. Now, we define a new code: $$ \\tilde{\\mathcal{C}}={\\varphi(c):c\\in \\mathcal{C}} $$ which is the set of all $\\varphi(c)$ for every codeword $c\\in \\mathcal{C}$. Thus, $\\tilde{\\mathcal{C}}\\subseteq \\Sigma^{n-d+1}$. We derive two key claims: Claim 1: $|\\mathcal{C}|=|\\tilde{\\mathcal{C}}|$.If not, then there must be a collision: $\\exists c, câ€™\\in \\mathcal{C}, c\\ne câ€™$ such that $\\varphi(c)=\\varphi(câ€™)$. This implies that $c$ and $câ€™$ differ only in their last $d-1$ coordinates, meaning $\\Delta(c, câ€™)\\le d-1$, which contradicts the distance $d$ of the original code. Claim 2: $|\\tilde{\\mathcal{C}}|\\le q^{n-d+1}$.This follows because $\\tilde{\\mathcal{C}}\\subseteq \\Sigma^{n-d+1}$. Combining these two claims, we have: $|\\mathcal{C}|=q^k\\le q^{n-d+1}$. Taking log base $q$ gives us $k\\le n-d+1$. $\\blacksquare$ Plotkin BoundRecall that the GV bound only works up to the relative distance $\\delta=d/n\\le 1-1/q$. Hence, as depicted below, there is a gap between $\\delta\\in (1-1/q, 1)$. We are wondering if it is possible to have codes with relative distance greater than $1-1/q$ and rate greater than $0$. Unfortunately, the answer is no. This is what the Plotkin bound tells us: Plotkin Bound: Let $\\mathcal{C}$ be a $(n, k, d)_q$ code. 1. If $d=(1-{1/q})\\cdot n$, then $|\\mathcal{C}|\\le 2\\cdot q\\cdot n$. 2. If $d>(1-1/q)\\cdot n$, then $|\\mathcal{C}|\\le \\frac{d}{d-(1-1/q)\\cdot n}$. This implies that the asymptotic rate $R=\\frac{\\log_q|\\mathcal{C}|}{n}$ approaches to $0$ as $n\\to \\infty$. Thus, in order to have a constant-rate code, we must have $d&lt;(1-1/q)\\cdot n$. The proof of the Plotkin bound is omitted in class. For details, refer to â€œEssential Coding Theoryâ€, Section 4.4. Instead, we prove a corollary, which extends the Plotkin bound and achieves a trade-off when distance $\\delta &lt; 1 - 1/q$. Corollary of the Plotkin Bound: Let $\\mathcal{C}$ be a family of codes of rate $R$ and relative distance $\\delta< 1-1/q$. Then: $$ R\\le 1-\\left({q\\over q-1}\\right )\\cdot \\delta + o(1) $$ where $o(1)$ can be dropped since we are considering a family of codes with $n,k,d\\to \\infty$. Before proving this corollary, letâ€™s look at what the Plotkin bound looks like when $q=2$. The green line represents the Plotkin bound for binary codes ($q=2$). When $\\delta$ is small, the Plotkin bound is a little worse than the Hamming bound. But when $\\delta$ gets larger, it is better than the Hamming bound. When $q&gt;2$, the Plotkin bound is also a straight line ending at $\\delta=1-1/q$, which is also the endpoint of the GV bound. Proof of Corollary of Plotkin Bound: Assuming the Plotkin Bound holds. Choose $nâ€™=\\lfloor{dq \\over q-1}\\rfloor-1$ such that $d&gt;(1-1/q)\\cdot nâ€™$.Notice that $nâ€™&lt;{dq\\over q-1}$, so $d&gt;(1-1/q)\\cdot nâ€™$. This will be useful when applying the Plotkin bound. For all $x\\in \\Sigma^{n-nâ€™}$, define a new code $\\mathcal{C}_x$: $$ \\mathcal{C}_x=\\{(\\underbrace{c_{n-n'+1}, \\dots, c_n}_{n'}): c\\in \\mathcal{C} \\text{ with }(\\underbrace{c_1, \\dots, c_{n-n'}}_{n-n'})=x\\} $$ which is the set of ENDs (the last $nâ€™$ symbols) of codewords that BEGIN with $x$. Now $\\mathcal{C}_x$ has distance $\\ge d$ with block length $nâ€™&lt;{d\\over 1-1/q}$. Why is this true? For any two different codeword $c, câ€™\\in \\mathcal{C}$, they must have distance from each other at least $d$. If they have the same first $n-nâ€™$ symbols denoted by $x$, they corresponds to the codewords in $\\mathcal{C}_x$. Thus, their distance must come from the end part, meaning that $\\mathcal{C}_x$ also has the distance $\\ge d$. Applying the Plotkin bound for $\\mathcal{C}_x$ with $d&gt;(1-1/q)\\cdot nâ€™$, we have $$ |\\mathcal{C}_x|\\le {d\\over d-(1-1/q)\\cdot nâ€™}= {qd\\over qd - (q-1)\\cdot nâ€™}\\le qd $$ The second inequality follows by the fact that the denominator $qd-(q-1)\\cdot nâ€™$ is an integer $&gt;0$. Thus, in particular, it is $\\ge 1$. We can plug this bound into the original code $\\mathcal{C}$ since each codeword in $\\mathcal{C}$ shows in only a certain $\\mathcal{C}_x$. $$ \\begin{align} |\\mathcal{C}| =\\sum_{x\\in \\Sigma^{n-n'}}|\\mathcal{C}_x| &\\le q^{n-n'} \\cdot qd \\\\ &=q^{(n-\\lfloor {qd\\over q-1}\\rfloor +1)}\\cdot qd\\\\ &=\\exp_q(n-{qd\\over q-1} + o(n))\\\\ &=\\exp_q(n (1-\\delta\\cdot ({q\\over q-1})+o(1)) \\end{align} $$ where the third equality captures the floor operation and constant by $o(n)$. Taking log base $q$ to the both sides, we have $R\\le 1-\\delta\\cdot ({q\\over q-1}+o(1))$ as desired. $\\blacksquare$ Discussion of Two BoundsBoth the Singleton and Plotkin bounds indicate the impossible results. They demonstrate that what trade-off between the distance and rate is impossible while the GV bound shows the possible trade-off. As depicted below, the Plotkin bounds seems strictly better than the Singleton bound. Why would we bother to discuss the Singleton bound? On one hand, it is true. But one the other hand, weâ€™ll see a family of codes that indeed achieve the Singleton bound. Before we get there, you might think it impossible given what weâ€™ve just stated that the Singleton bound shows the impossible results. We table the discussion in the next section. The trick here is the alphabet size $q$ will be growing with $n$. Reed-Solomon CodesReed-Solomon code is a family of codes that achieve the Singleton bound while admitting efficient error-correcting algorithm. This code is widely used in practice. Before getting into Reed-Solomon codes, letâ€™s first discuss the polynomials over finite fields and the Vandermonde matrix. Polynomial: A (univariate) polynomial in variable $X$ over the finite field $\\mathbb{F}_q$ of degree $d$ is of the form: $$f(X)=a_0+a_1\\cdot X + \\dots a_d\\cdot X^d$$ where each coefficient $a_i\\in \\mathbb{F}_q$ and the top coefficient $a_d\\ne 0$. The set of all univariate polynomials with coefficients in $\\mathbb{F}_q$ is denoted by $\\mathbb{F}_q[X]$. Polynomials over finite fields behave similarly to those over $\\mathbb{R}$. There is a simple but super useful fact about polynomials that low-degree polynomial do not have too many roots. Fact: A non-zero polynomial of $f$ of degree $d$ over $\\mathbb{F}_q$ has at most $d$ roots. Proof Sketch: If $f(\\beta)=0$, then $(x-\\beta)\\mid f$. If $f$ has $d+1$ (distinct) $\\beta_1,\\dots, \\beta_{d+1}$ roots, then $(x-\\beta_1)(x-\\beta_2)\\dots(x-\\beta_{d+1})\\mid f$ . This leads to a contradiction because the grand product on the left has degree $d+1$, while $f(X)$ has degree of $d$. Example: Consider the field $\\mathbb{F}_3=\\{0, 1, 2\\}$: $f(X)=X^2-1$ has two roots $[f(2)=f(1)=0]$ $f(X)=X^2+2X+1$ has one root $[f(2)=0]$ $f(X)=X^2+1$ has zero root Note: The polynomial $X^2+1$ DOES have a root over $\\mathbb{F}_2$, showing that the choice of the field matters. Vandermonde MatrixNext, we will explore some useful facts about Vandermonde matrix. Vandermonde matrix: A Vandermonde matrix has the form $$ V=\\left[\\begin{array}{c} 1 & \\alpha_1 & \\alpha_1^2 & \\dots & \\alpha_1^m \\\\ 1 & \\alpha_2 & \\alpha_2^2 & \\dots & \\alpha_2^m \\\\ 1 & \\alpha_3 \\\\ \\vdots \\\\ 1 & \\alpha_n & \\alpha_n^2 & \\dots & \\alpha_n^m \\end{array}\\right] $$ where $\\alpha_1, \\dots, \\alpha_n\\in \\mathbb{F}_q$ are distinct. Aka, $V_{ij}=\\alpha_i^{j-1}$. Theorem: A square Vandermonde matrix is invertible. Proof 1: Consider a square Vandermonde matrix $V\\in \\mathbb{F}^{n\\times n}$. Suppose $\\vec{a}=(a_0, \\dots, a_n)$ is a vector. Then $V\\cdot \\vec{a}$ can be expressed as $$ V\\cdot \\vec{a}=\\left ( \\begin{array}{c} \\sum_{i=0}^{n-1} a_i\\cdot \\alpha_1^i \\\\ \\sum_{i=0}^{n-1} a_i\\cdot \\alpha_2^i \\\\ \\vdots \\\\ \\sum_{i=0}^{n-1} a_i\\cdot \\alpha_n^i \\end{array} \\right )=\\left ( \\begin{array}{c} f(\\alpha_1)\\\\ f(\\alpha_2)\\\\ \\vdots \\\\ f(\\alpha_n) \\end{array} \\right ) $$ where $f(X)=a_0+a_1X+\\dots a_{n-1}X^{n-1}$. To prove that the Vandermonde matrix is invertible, weâ€™d like to show:if $V\\cdot \\vec{a}=0$, then $\\vec{a}$ itself must be $0$. This is true because Case 1: If $f$ is a zero polynomial (i.e., $\\vec{a}$ itself is the zero vector), it is clear that $V\\cdot \\vec{a}=0$. Case 2: If $f$ is a non-zero polynomial (i.e., $\\vec{a}$ is a non-zero vector), then $f(X)$ has degree at most $n-1$ and cannot have $n$ roots. So, we have $V\\cdot \\vec{a}\\ne 0$. Hence, $\\text{Ker}(V)=\\emptyset$, so $V$ is invertible. $\\blacksquare$ Proof 2: It can be proven by its determinate. The determinate of a Vandermonde matrix is $$\\det(V)=\\prod_{1\\le i&lt;j\\le n}(a_i-a_j)\\ne 0$$ since $a_i\\ne a_j$ for any $i\\ne j$. $\\blacksquare$ Corollary: Any square contiguous submatrix of a Vandermonde matrix is invertible. Caveat: If one of the evaluation points is 0, then the submatrix must include part of the all-ones column for it to be invertible. Otherwise, it is not inveritble. Proof: A $(r+1)\\times (r+1)$ square submatrix takes the following form: $$ \\left[ \\begin{array}{c} \\alpha_i^j & \\alpha_i^{j+1} & \\dots &\\alpha_i^{j+r-1} \\\\ \\alpha_{i+1}^j & \\alpha_{i+1}^{j+1} & \\dots &\\alpha_{i+1}^{j+r-1} \\\\ \\vdots \\\\ \\alpha_{i+r}^j & \\alpha_{i+r}^{j+1} & \\dots & \\alpha_{i+r}^{j+r} \\end{array} \\right]=D\\cdot V $$ where: $D$ is a diagonal matrix with non-zero diagonal entries $\\alpha_i^j, \\dots, a_{i+r}^j$, $V$ is a $(r+1)\\times (r+1)$ Vandermonde matrix. The invertibility of $D\\cdot V$ depends on the invertibility of both $D$ and $V$: $D$ is invertible if all diagonal entries are non-zero. This is guaranteed if either $j=0$ or $a_i\\ne 0$ for all $i$, as required in the caveat. $V$ is invertible by the aforementioned theorem. $\\blacksquare$ These facts about Vandermonde matrices will be useful. First, they imply that â€œpolynomial interpolation works over $\\mathbb{F}_q$â€. Theorem: Given $(\\alpha_i, y_i)\\in \\mathbb{F}_q\\times \\mathbb{F}_q$ for $i=1,\\dots, d+1$, there is a unique degree-$d$ polynomial $f$ so that $f(\\alpha_i)=y_i$ for $\\forall i$. Proof: If $f(X)=a_0+a_1X+\\dots+a_dX^d$, then the requirement that $f(\\alpha_i)=y_i$ for all $i$ can be written as $V\\cdot \\vec{a}=\\vec{y}$, where $V$ is a square Vandermonde matrix. Hence, $\\vec{a}=V^{-1}y$ is the unique solution because linear algebra â€œworksâ€ over $\\mathbb{F}_q$. $\\blacksquare$ Moreover, the proof implies that we can find $f$ efficiently. Actually, we can compute $f$ very efficiently by using NTT (Number Theoretic Transform), which allows for fast multiplication by certain special Vandermonde matrix . Fact: All functions $f:\\mathbb{F}_q\\mapsto \\mathbb{F}_q$ are polynomials of degree $\\le q-1$. Proof: There are only $q$ points in $\\mathbb{F}_q$, so we can interpolate a (unique) degree $\\le q-1$ polynomial through any function. Example: $f(X)=X^q$ must have some representation as a degree $\\le q-1$ polynomial over $\\mathbb{F}_q$. It is $X^q\\equiv X$ because $\\alpha^q=\\alpha$ for all $\\alpha\\in \\mathbb{F}_q$ (by Fermatâ€™s little theorem). RS codeNow, we are finally ready to define the Reed-Solomon codes. Reed-Solomon Codes: Let $q\\ge n \\ge k$. The Reed-Solomon code of dimension $k$ over $\\mathbb{F}_q$, with (distinct) evaluation points $\\vec{\\alpha}=(\\alpha_1, \\dots, \\alpha_n)$ is $$ \\text{RS}_q(\\vec{\\alpha}, n, k)=\\{(f(\\alpha_1), f(\\alpha_2), \\dots, f(\\alpha_n)):f\\in \\mathbb{F}_q[X], \\deg(f)\\le k-1\\} $$ The basic idea is that low-degree polynomial does not have too many roots so that the RS code can achieve a fairly good trade-off between the rate and the distance. The codeword of Reed-Solomon code consists of evaluations of low-degree polynomials. This implies that these codewords donâ€™t have too many zeros so that the distance of the code is good. Additionally, this definition implies a natural encoding map for RS code: $$\\vec{x}=(x_0, \\dots, x_{k-1})\\mapsto (f_{\\vec{x}}(\\alpha_0),\\dots,f_{\\vec{x}}(\\alpha_{n}))$$ where $f_{\\vec{x}}(X)=x_0+x_1\\cdot X+\\dots x_{k-1}\\cdot X^{k-1}$. Note that this isn't the only encoding map, but itâ€™s the commonest one. This also implies the Reed-Solomon code is a linear code with Vandermonde matrix as the generator matrix. Property: $\\text{RS}_q(\\vec{\\alpha}, n, k)$ is a linear code, and the generator matrix is the $n\\times k$ Vandermonde matrix with rows corresponding to $\\alpha_1, \\alpha_2, \\dots, \\alpha_n$. Proof: The proof is clear when we write down the generator matrix. In the view of generator matrix, we have $\\dim(\\text{RS}(n, k))=k$ since the Vandermonde matrix has rank $k$. When $\\vec{\\alpha }$ is clear from context, $\\vec{\\alpha}$ can be omitted. Then we can easily compute distance of a linear code. Property: The distance of $\\text{RS}_q(n,k)$ is $d=n-k+1$. Proof: Since RS code is a linear code, $\\text{dist}(\\text{RS}_q(n, k)=\\min_{c\\in \\text{RS}}\\text{wt}(c)$. It suffices to show that $\\max$ #non-zeros of any non-zero codewords is $k-1$. The #zeros of a non-zero codeword corresponds to the #roots of a non-zero polynomial of degree at most $k-1$. $\\blacksquare$ The distance of the RS codes achieves the Singleton bound, and it is optimal for any $n$ and $k$ we choose. Corollary: RS codes exactly meet the Singleton bound. Maximum Distance Separable (MDS): A linear $(n, k, d)_q$ code with $d=n-k+1$ (aka, meeting the Singleton bound) is called Maximum Distance Separable. Notice that MDS-ness is equivalent to the property: â€œevery $k\\times k$ submatrix of the generator matrix is full rankâ€. This property implies that if $\\mathcal{C}$ is MDS, then any $k$ positions of the codeword $c\\in \\mathcal{C}$ determine all of $c$. Proof Sketch: To prove the distance is $n-k+1$, it suffices to show the code can correct any $n-k$ erasures. For a codeword, any $n-k$ erasures leave us $k$ remaining non-erased positions, which corresponds to $k$ rows of the generator matrix, forming a $k\\times k$ submatrix. We can recover the message $x$ if and only if the submatrix is invertible/full rank. $\\blacksquare$ We have seen that the RS code is MSD and it has the Vandermonde matrix as the generator matrix, so the property also holds for RS code. Discussion of Two Bound (cont.)We previously showed that the distance-rate trade-off in the Plotkin Bound and the Singleton Bound. Both bounds indicate the impossible results, and the Plotkin bound is strictly better than the Singleton bound for any $q\\ge 2$. This suggests that the Singleton bound should never be achievable. However, the MDS codes are defined as codes that achieves the Singleton bound. Why donâ€™t MDS codes violate the Singleton bound? The key lies in the fact that the above figure only applies to any fixed $q$. However, in RS codes, the alphabet size $q$ is NOT fixedâ€”itâ€™s growing with $n$. Thus, to get a MDS code, $q$ must be growing with $n$. How big does $q$ have to be? It is an open question in general! It was settled for prime field in 2012 by Ball. Dual View of RS CodesWhat is the parity-check matrix of an RS code? Before getting this, we need to recall some preliminary algebra. Multiplicative Group $\\mathbb{F}_q^*$: The set $\\mathbb{F}_q^*$ is the multiplicative group of non-zero elements in $\\mathbb{F}_q$, i.e. $\\mathbb{F}_q^*=\\mathbb{F}_q\\backslash\\{0\\}$. $\\mathbb{F}_q^*$ is CYCLIC, which means thereâ€™s some $\\gamma\\in \\mathbb{F}_q^*$ so that $$ \\mathbb{F}_q^*=\\{\\gamma, \\gamma^2, \\dots, \\gamma^{q-1}\\} $$ where $\\gamma$ is called a Primitive Element of $\\mathbb{F}_q$. Note that the multiplicative group $\\mathbb{F}_q^*$ is equipped with multiplication while the field $\\mathbb{F}_q$ is equipped with both multiplication and addition. For example, $\\mathbb{F}_5^*=\\{1, 2,3,4\\}$ and $2+3=0$ is not in the set. Example: 2 is a primitive element of $\\mathbb{F}_5$, and $\\mathbb{F}_5^*=\\{2, 2^2=4, 2^3=3, 2^4=1\\}$. 4 is NOT a primitive element, since $4^2=1,4^3=-1, 4^4=1, 4^5=-1, \\dots$, and we will never generate 2 or 3 as a power of 4. Lemma: For any integer $d$ with $0&lt;d&lt;q-1$, the sum of all $d$-th powers in $\\mathbb{F}_q^*$ is zero, i.e. $$ \\sum_{\\alpha\\in \\mathbb{F}_q}\\alpha^d =0$$ Proof: $\\sum_{\\alpha\\in \\mathbb{F}_q}\\alpha^d=\\sum_{\\alpha\\in \\mathbb{F}_q^*}\\alpha^d$ We are leaving out $\\alpha=0$ since $0^d$ contributes 0 to the sum. $=\\sum_{j=0}^{q-2}(\\gamma^j)^d$ for a primitive element $\\gamma$. The index start from 0 since $\\gamma^0=\\gamma^{q-1}=1$.) $=\\sum_{j=0}^{q-2}(\\gamma^d)^j$ for a primitive element $\\gamma$. We are switching the order of the exponents. $={1-(\\gamma^d)^{q-1}\\over 1-\\gamma^d}$ Fact: For any $x\\ne 1$, it follows that $(1-x)\\cdot (\\sum_{j=0}^{n-1}x^j)=1-x^n$, and so $\\sum_{j=0}^{n-1}x^j={1-x^n\\over 1-x}$ for any $n$. $={1-1\\over 1-\\gamma^d}=0$ $(\\gamma^d)^{q-1}\\cdot \\gamma^d = (\\gamma^d)^q=\\gamma^d$. (Fermatâ€™s little theorem) $\\gamma^d\\ne0$ since $0&lt;d&lt;q-1$. So $(\\gamma^d)^{q-1}=1$. Now, we can view the RS code in a new perspective, which allows us to capture what the parity-check matrix of the RS code looks like. Proposition: Let $n=q-1$, and let $\\gamma$ be a primitive element of $\\mathbb{F}_q$. $$\\text{RS}_q((\\gamma^0, \\dots, \\gamma^{n-1}),n, k)\\\\ =\\{(c_0, c_1, \\dots, c_{n-1})\\in \\mathbb{F}_q^n:c(\\gamma^j)=0 \\text{ for }j=1, 2, \\dots, n-k\\}$$ where $c(X)=\\sum_{i=0}^{n-1}c_i\\cdot X^i$. This proposition kinds of flipping things around. In the original definition, the messages are coefficients of some polynomial and the codewords are evaluations of that polynomial. In this view, the codewords are coefficients of some polynomial. Proof: First, letâ€™s prove one direction of this proposition. Let $f(X)=\\sum_{i=0}^{k-1}f_i\\cdot X^i$ be a message, so the RS codeword is $(f(\\gamma^0), f(\\gamma),\\dots, f(\\gamma^{n-1}))$. $c(\\gamma^j)=\\sum_{\\ell=0}^{n-1}c_{\\ell}\\cdot \\gamma^{j\\cdot \\ell}$ $=\\sum_{\\ell=0}^{n-1}(\\sum_{i=0}^{k-1}f_i\\cdot \\gamma^{i\\cdot \\ell})\\cdot \\gamma^{j\\cdot \\ell}$ $=\\sum_{\\ell=0}^{n-1}\\sum_{i=0}^{k-1}f_i\\cdot \\gamma^{(i+j)\\cdot \\ell}$ $=\\sum_{i=0}^{k-1}\\sum_{\\ell=0}^{n-1}f_i\\cdot \\gamma^{(i+j)\\cdot \\ell}$ (switching the order of summation) $=\\sum_{i=0}^{k-1}f_i\\cdot \\sum_{\\ell=0}^{n-1}\\gamma^{(i+j)\\cdot \\ell}$ $=\\sum_{i=0}^{k-1}f_i\\cdot \\sum_{\\ell=0}^{n-1}(\\gamma^{\\ell})^{i+j}$ $0\\le i\\le k-1$ and $1\\le j\\le n-k$ Thus, $1\\le i+j\\le n-1&lt;q-1$. (strictly less than $q-1$) Apply the aforementioned lemma to have $\\sum_{\\ell=0}^{n-1}(\\gamma^{\\ell})^{i+j}=0$ $=0$. $\\text{RS}_q((\\gamma^0, \\dots, \\gamma^{n-1}),n, k)\\\\ \\subseteq\\{(c_0, c_1, \\dots, c_{n-1})\\in \\mathbb{F}_q^n:c(\\gamma^j)=0 \\text{ for }j=1, 2, \\dots, n-k\\}$ This shows one direction of the propositionâ€”RS code is contained in the above set. To prove the equality, we need to count dimension. We will show the set also has dimension $k$. Let $H$ be a matrix of this form: $$ H=\\left[ \\begin{array}{c} 1 & \\gamma & \\gamma^2 & \\dots & \\gamma^{n-1} \\\\ 1 & \\gamma^2 & \\gamma^4 & \\dots & \\gamma^{2(n-1)} \\\\ \\vdots \\\\ 1 & \\gamma^{n-k} & \\gamma^{2(n-k)} & \\dots & \\gamma^{(n-k)(n-1)} \\\\ \\end{array} \\right]\\in \\mathbb{F}_q^{(n-k)\\times n} $$ Consider $H\\cdot c$ where $c$ is a vector in that set: The $j$â€™th entry is $\\sum_{i=0}^{n-1}c_i\\cdot \\gamma^{ji}=c(\\gamma^j)=0$. This implies that the set $\\{(c_0, c_1, \\dots, c_{n-1})\\in \\mathbb{F}_q^n:c(\\gamma^j)=0 \\text{ for }j=1, 2, \\dots, n-k\\}$ is exactly $\\text{Ker}(H)$. This shows that the set has dimension $k$ since $H$ is a Vandermonde matrix of dimension $n-k$. The proposition also answer our question about the parity-check matrix of the RS-codes. Corollary: The parity-check matrix of $\\text{RS}_q((\\gamma^0, \\dots, \\gamma^{n-1}),n,k)$ is $$ H=\\left[ \\begin{array}{c} 1 & \\gamma & \\gamma^2 & \\dots & \\gamma^{n-1} \\\\ 1 & \\gamma^2 & \\gamma^4 & \\dots & \\gamma^{2(n-1)} \\\\ \\vdots \\\\ 1 & \\gamma^{n-k} & \\gamma^{2(n-k)} & \\dots & \\gamma^{(n-k)(n-1)} \\\\ \\end{array} \\right]\\in \\mathbb{F}_q^{(n-k)\\times n} $$ Notice that the dual code $\\text{RS}(n, k)^\\perp$ has a generator matrix $H^T$, which again looks a lot like a Vandermonde matrix. It suggests that the dual of the RS code is basically an RS code. This particular derivation of the proposition used the choice of evaluation points heavily. However, a statement like this is true in general. More precisely, we define a generalized RS codes with an additional parameter vector $\\vec\\lambda$. Generalized Reed-Solomon Code (GRS): A generalized RS code $\\text{GRS}_q(\\vec{\\alpha},n,k;\\vec{\\lambda})$ is $\\text{GRS}_q(\\vec{\\alpha},n,k;\\vec{\\lambda})\\\\=\\{\\lambda_0f(\\alpha_0), \\lambda_1f(\\alpha_1),\\dots, \\lambda_nf(\\alpha_n):f\\in \\mathbb{F}_q[X],\\deg(f)\\le k\\}$ where $\\vec{\\lambda}=(\\lambda_0, \\dots, \\lambda_n)\\in (\\mathbb{F}_q^*)^n$. The GRS code is almost identical to the RS code, except for the scaling factors $\\lambda_i$ applied to each coordinate. The introduction of $\\vec{\\lambda}$ enables the following property weâ€™ve seen before: if we take the transpose of the parity-check matrix of a GRS code, it yields a generator matrix for another GRS code. Theorem: Theorem: For any distinct evaluation points $\\vec{\\alpha}$ and any $\\vec{\\lambda}\\in (\\mathbb{F}_q^*)^n$, there exists some $\\vec{\\sigma}\\in (\\mathbb{F}_q^*)^n$ s.t. $$ \\text{GRS}_q(\\vec{\\alpha},n,k;\\vec{\\lambda})^\\perp=\\text{GRS}_q(\\vec{\\alpha},n,n-k;\\vec{\\sigma}) $$","link":"/2025/01/12/stanford-cs250-ecc-lec4/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€ï¼šTips for Deep Learning","text":"è¿™ç¯‡æ–‡ç« ä¸­ï¼Œè¯¦å°½é˜è¿°äº†åœ¨è®­ç»ƒDeep Neural Networkæ—¶ï¼Œæ”¹å–„performanceçš„ä¸€äº›tipsã€‚tipsä»ŽTrainingå’ŒTestingä¸¤ä¸ªæ–¹é¢å±•å¼€ã€‚åœ¨Trainingä¸­ç»“æžœä¸å°½äººæ„æ—¶ï¼Œå¯ä»¥é‡‡å–æ›´æ¢æ–°çš„activation functionï¼ˆå¦‚ReLu,Maxoutç­‰ï¼‰å’Œé‡‡ç”¨Adaptive Learning Rateçš„GradientDescentç®—æ³•ï¼ˆé™¤äº†Adagrad,è¿˜æœ‰RMSpropã€Momentumã€Adamç­‰ï¼‰ã€‚å½“åœ¨Trainingä¸­å¾—åˆ°å¥½çš„performanceï¼Œä½†åœ¨testingä¸­perform badæ—¶ï¼Œå³é‡åˆ°äº†overfittingï¼Œåˆè¯¥æ€Žä¹ˆå¤„ç†å‘¢ï¼Ÿæ–‡ç« åŽåŠéƒ¨åˆ†è¯¦å°½ä»‹ç»äº†EarlyStoppingã€Regularizationå’ŒDropoutä¸‰ä¸ªsolutionã€‚ Recipe of Deep LearningDeep Learning çš„ä¸‰ä¸ªæ­¥éª¤ï¼š å¦‚æžœåœ¨Training Dataä¸­æ²¡æœ‰å¾—åˆ°å¥½çš„ç»“æžœï¼Œéœ€è¦é‡æ–°è®­ç»ƒNeural Networkã€‚ å¦‚æžœåœ¨Training Dataä¸­å¾—åˆ°å¥½çš„ç»“æžœï¼Œåœ¨Testing Dataï¼ˆè¿™é‡Œçš„Testing Dataæ˜¯æŒ‡æœ‰Labelçš„Dataï¼Œæ¯”å¦‚Kaggleçš„Public Dataæˆ–è€…æ˜¯ä»ŽTraining Dataä¸­åˆ’åˆ†å‡ºçš„Development Dataï¼‰æ²¡æœ‰å¾—åˆ°çš„å¥½çš„ç»“æžœï¼Œè¯´æ˜ŽOverfittingäº†ï¼Œéœ€è¦é‡æ–°è®¾è®¡Neural Networkçš„ç»“æž„ã€‚ Do not always blame Overfitting å¦‚æžœåœ¨Testing Dataä¸­ï¼Œçœ‹åˆ°ä¸Šå›¾ï¼Œ20-layerçš„errorå°ï¼Œ56-layerçš„errorå¤§ï¼Œ56-layerä¸€å®šoverfittingäº†ã€‚ No!!!ä¸è¦æ€»æŠŠåŽŸå› å½’å’ŽäºŽOverfittingã€‚ å†çœ‹Testing Data errorä¹‹å‰ï¼Œå…ˆçœ‹çœ‹Training Dataçš„errorã€‚ä¸Šå›¾ä¸­ï¼Œ56-layerçš„DNNåœ¨Training Dataçš„erroræœ¬æ¥å°±æ¯”20-layerçš„å¤§ï¼Œè¯´æ˜Ž56-layerçš„DNNæ ¹æœ¬æ²¡æœ‰trainå¥½ã€‚ æ‰€ä»¥56-layerçš„DNNåœ¨Testing Dataä¸Šçš„errorå¤§ï¼ŒåŽŸå› ä¸æ˜¯overfittingï¼Œè€Œæ˜¯æ¨¡åž‹æ ¹æœ¬æ²¡æœ‰trainå¥½ã€‚ æ³¨ï¼š Overfittingæ˜¯åœ¨Training Dataä¸Šerrorå°ï¼Œä½†åœ¨Testing Dataä¸Šçš„errorå¤§ã€‚ å› æ­¤ï¼Œå¯¹äºŽåœ¨Training Dataä¸Šå¾—åˆ°ä¸å¥½çš„ç»“æžœå’Œåœ¨Training Dataä¸Šå¾—åˆ°å¥½çš„ç»“æžœä½†åœ¨Testing Dataä¸Šå¾—åˆ°ä¸å¥½çš„ç»“æžœè¿™ä¸¤ç§æƒ…å†µï¼Œéœ€è¦ä¸åŒçš„è§£å†³æ–¹æ³•ã€‚ Bad Results on Training Dataåœ¨ä¸é‡æ–°è®¾è®¡DNNç»“æž„æ—¶ï¼Œå¦‚æžœåœ¨Training Dataä¸­å¾—åˆ°Bad Resultsï¼Œä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ³•æ¥æ”¹è¿›ç»“æžœï¼š New activation functionã€neuronæ¢æ–°çš„æ¿€æ´»å‡½æ•°ã€‘ Adaptive Learning Rate New activation functionVanishing Gradient Problem ä¸Šå›¾è¡¨ç¤ºï¼Œåœ¨æ‰‹å†™æ•°å­—è¾¨è¯†ä¸­ï¼ŒDeeper layerså¹¶ä¸èƒ½æœ‰å¥½çš„performanceã€‚ ä¸ºä»€ä¹ˆä¼šè¿™æ ·å‘¢ï¼Ÿ å› ä¸ºå‡ºçŽ°äº†Vanishing Gradient Problemï¼Œå³gradientéšç€deeper layeré€æ¸æ¶ˆå¤±çš„é—®é¢˜ã€‚ ä¸Šå›¾ä¸­ï¼Œå‡è®¾neuronçš„activation functionæ˜¯sigmodå‡½æ•°ã€‚ é è¿‘Input layerå±‚çš„å‚æ•°çš„å˜åŒ–å¯¹Lossçš„å½±å“å¾ˆå°ï¼Œæ‰€ä»¥å¯¹Loss functionåšå¾®åˆ†ï¼Œgradientå¾ˆå°ï¼Œå‚æ•°æ›´æ–°æ…¢ã€‚ è€Œé è¿‘Output layerå±‚çš„å‚æ•°çš„ç¼–å·å¯¹Lossçš„å½±å“æ›´å¤§ï¼Œæ‰€ä»¥å¯¹Loss functionåšå¾®åˆ†ï¼Œgradientå¾ˆå¤§ï¼Œå‚æ•°æ›´æ–°å¿«ã€‚ å› ä¸ºé è¿‘Output Layerå±‚çš„å‚æ•°æ›´æ–°å¿«ï¼Œæ‰€ä»¥å¾ˆå¿«convergeï¼ˆæ”¶æ•›ã€è¶‹äºŽç¨³å®šï¼‰ï¼›ä½†é è¿‘Input Layerå±‚çš„å‚æ•°æ›´æ–°æ…¢ï¼Œå‡ ä¹Žè¿˜å¤„åœ¨randomï¼ˆéšæœºï¼‰çš„çŠ¶æ€ã€‚ å½“é è¿‘Output Layerå±‚çš„å‚æ•°è¶‹äºŽç¨³å®šæ—¶ï¼Œç”±äºŽé è¿‘Output Layerå±‚çš„å‚æ•°å¯¹Losså½±å“å¤§ï¼Œæ‰€ä»¥è§‚å¯Ÿåˆ°çš„Lossçš„å€¼ä¹Ÿè¶‹äºŽç¨³å®šï¼ŒäºŽæ˜¯ï¼Œä½ å°±æŠŠtrainingåœæŽ‰äº†ã€‚ ä½†æ˜¯ï¼Œé è¿‘Inputå±‚çš„å‚æ•°å‡ ä¹Žå¤„åœ¨randomçŠ¶æ€ï¼Œæ‰€ä»¥æ‹¿æ¨¡åž‹ç”¨åœ¨Testing Dataä¸Šï¼Œå‘çŽ°ç»“æžœå‡ ä¹Žæ˜¯éšæœºçš„ã€‚ æ€Žä¹ˆç›´è§‚ç†è§£é è¿‘Input Layerçš„å‚æ•°çš„gradientå°å‘¢ï¼Ÿ ç”¨å¾®åˆ†çš„ç›´è§‚å«ä¹‰æ¥è¡¨ç¤ºgradient $\\partial{l}/\\partial{w}$ : å½“ $w$ å¢žåŠ  $\\Delta{w}$ æ—¶ï¼Œå¦‚æžœ $l$ çš„å˜åŒ– $\\Delta{l}$ å˜åŒ–å¤§ï¼Œè¯´æ˜Ž $\\partial{l}/\\partial{w}$ å¤§ï¼Œå¦åˆ™ $\\partial{l}/\\partial{w}$ å°ã€‚ æˆ‘ä»¬åœ¨DNNä¸­ä½¿ç”¨çš„activation functionæ˜¯sigmodå‡½æ•°ï¼Œsigmodå‡½æ•°ä¼šæŠŠå€¼åŽ‹åˆ°0å’Œ1ä¹‹é—´ã€‚ å› æ­¤ï¼Œä¸Šå›¾ä¸­ï¼Œå…¶ä»–å€¼ä¸å˜ï¼Œåªæœ‰è¿žæŽ¥ $x_N$ çš„å‚æ•° $w$ å¢žåŠ  $\\Delta w$ æ—¶ï¼Œè¾“å…¥é€šè¿‡neuronçš„sigmodå‡½æ•°ï¼Œå‡½æ•°çš„è¾“å‡ºå¢žåŠ çš„ $\\Delta$ ä¼šå˜å°ï¼Œéšç€Deeper Layerï¼Œneuronçš„è¾“å‡ºçš„ $\\Delta$ ä¼šè¶Šå˜è¶Šå°ï¼Œè¶‹è‡³0ã€‚ æœ€åŽDNNè¾“å‡ºçš„å˜åŒ–å¯¹ lossçš„å½±å“å°ï¼Œå³ $\\Delta{l}$ è¶‹è‡³0ï¼Œå³å‚æ•°çš„gradient $\\partial{l}/\\partial{w}$ è¶‹è‡³0ã€‚ï¼ˆå³ Vanishing Gradientï¼‰ ReLu ï¼šRectified Linear Unitä¸ºäº†é˜²æ­¢å‘ç”ŸVanishing Gradient Problemï¼Œåœ¨DNNä¸­é€‰æ‹©ä½¿ç”¨æ–°çš„activation functionã€‚ ReLué•¿ä¸‹é¢è¿™ä¸ªæ ·å­ï¼š z: input a: output å½“ $z\\leq0$ æ—¶ï¼Œ $a=0$ ï¼›å½“ $z &gt;0$ æ—¶ï¼Œ $a=z$ ã€‚ Reason : Fast to compute Biological reasonã€æœ‰ç”Ÿç‰©ä¸Šçš„åŽŸå› ã€‘ Infinite sigmod with different biases. ã€æ˜¯æ— ç©·ä¸ª æœ‰ä¸åŒbiasçš„sigmodå‡½æ•° çš„å åŠ ã€‘ Vanishing gradient problem ã€æœ€é‡è¦çš„æ˜¯æ²¡æœ‰vanishing gradient problemã€‘ ä¸ºä»€ä¹ˆReLuæ²¡æœ‰vanishing gradient problem ä¸Šå›¾DNNä¸­ï¼ŒReLuåœ¨è¾“å…¥æ˜¯è´Ÿæ•°æ—¶ï¼Œè¾“å‡ºæ˜¯0ã€‚å› æ­¤è¿™äº›è¾“å‡ºæ˜¯0çš„neuronå¯ä»¥åŽ»æŽ‰ã€‚ å°±å˜æˆäº†ä¸‹å›¾è¿™ä¸ªA Thinner linear networkã€‚ç”±äºŽReLuå‡½æ•°çš„æ€§è´¨ï¼Œé è¿‘Input Layerçš„å‚æ•°ä¸ä¼šæœ‰smaller gradientã€‚ è¿™é‡Œæœ‰ä¸€ä¸ªQ&amp;A: Q1: functionå˜æˆlinearçš„ï¼Œä¼šä¸ä¼šDNNå°±å˜å¼±äº†ï¼Ÿ ï¼š å½“neuronçš„operation regionä¸å˜çš„è¯ï¼ŒDNNçš„ç¡®æ˜¯linearçš„ï¼Œä½†æ˜¯å½“neuronçš„operation regionæ”¹å˜åŽï¼Œå°±æ˜¯unlinearçš„ã€‚ ï¼šå³ï¼Œå½“inputçš„å˜åŒ–å°ï¼Œoperation regionä¸å˜ï¼ˆå³è¾“å…¥ä¸ä¼šä»Žå¤§äºŽ0å˜æˆå°äºŽ0ï¼Œå°äºŽ0å˜æˆå¤§äºŽ0è¿™ç§ï¼‰ï¼Œmodelè¿˜æ˜¯linearçš„ï¼›ä½†å½“inputçš„å˜åŒ–å¤§æ—¶ï¼Œå¾ˆå¤šneuronçš„operation regionéƒ½å˜åŒ–äº†ï¼Œmodelå…¶å®žå°±æ˜¯unlinearçš„ã€‚ Q2: ReLu æ€Žä¹ˆå¾®åˆ†ï¼Ÿ ï¼šReLuåœ¨0ç‚¹ä¸å¯å¾®ï¼Œé‚£å°±éšä¾¿æŒ‡å®šä¸º0è¿™æ ·ï¼ˆå°æ¹¾è…”QAQï¼‰ã€‚ ReLu - variantå½“ $z\\leq 0$ æ—¶ï¼Œè¾“å‡ºä¸º0ï¼Œå°±ä¸èƒ½æ›´æ–°å‚æ•°äº†ã€‚äºŽæ˜¯å°±æœ‰ä¸‹å›¾å˜ä½“ï¼š å½“ $z\\leq0$ æ—¶ï¼Œgradientéƒ½ä¸º0.01ï¼Œä¸ºä»€ä¹ˆä¸èƒ½æ˜¯å…¶ä»–å€¼ã€‚äºŽæ˜¯å°±æœ‰ä¸‹å›¾å˜ä½“ï¼šå…¶ä¸­ $\\alpha$ ä¹Ÿæ˜¯ä¸€ä¸ªéœ€è¦å­¦ä¹ çš„å‚æ•° MaxoutMaxoutï¼Œå¦‚ä¸‹å›¾ï¼Œåœ¨è®¾è®¡neural networkæ—¶ï¼Œä¼šç»™æ¯ä¸€å±‚çš„neuronåˆ†ç»„ï¼Œæˆä¸ºä¸€ä¸ªæ–°çš„neuronã€‚ Maxoutä¹Ÿæ˜¯ä¸€ä¸ªLearnable activation functionã€‚ ReLuæ˜¯Maxoutå­¦å‡ºæ¥çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚ ä¸Šå›¾ä¸­ï¼Œå·¦å›¾æ˜¯ReLuã€‚ ReLuçš„è¾“å…¥ $z = wx+b$ ï¼Œè¾“å‡º $a$ å¦‚ä¸Šå›¾çš„ç»¿è‰²çš„çº¿ã€‚ å³å›¾æ˜¯Maxoutã€‚Maxoutçš„è¾“å…¥ $z_1 =wx+b,z_2=0$ ï¼Œé‚£ä¹ˆè¾“å‡ºå–maxï¼Œè¾“å‡º $a$ å¦‚ä¸Šå›¾ä¸­ç»¿è‰²çš„çº¿ï¼Œå’Œå·¦å›¾çš„ReLuç›¸åŒã€‚ Maxout is more than ReLuã€‚ å½“å‚æ•°æ›´æ–°æ—¶ï¼ŒMaxoutçš„å‡½æ•°å›¾åƒå¦‚ä¸‹å›¾ï¼š DNNä¸­çš„å‚æ•°æ˜¯learnableçš„ï¼Œæ‰€ä»¥Maxoutä¹Ÿæ˜¯ä¸€ä¸ªlearnableçš„activation functionã€‚ Reason ï¼š Learnable activation function [Ian J. Goodfellow, ICMLâ€™13] Activation function in maxout network can be any piecewise linear convex function. åœ¨maxoutç¥žç»ç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•°å¯ä»¥æ˜¯ä»»æ„çš„åˆ†æ®µå‡¸å‡½æ•°ã€‚ How many pieces depending on how many elements in a group. åˆ†æ®µå‡½æ•°åˆ†å‡ æ®µå–å†³äºŽä¸€ç»„ä¸­æœ‰å¤šå°‘ä¸ªå…ƒç´ ã€‚ Maxout : how to trainGiven a training data x, we know which z would be the max. ã€å½“ç»™å‡ºæ¯ç¬”training dataæ—¶ï¼Œæˆ‘ä»¬èƒ½çŸ¥é“Maxout neuronä¸­å“ªä¸€ä¸ªæœ€å¤§ã€‘ å¦‚ä¸Šå›¾ï¼Œåœ¨è¿™ç¬”training data xä¸­ï¼Œæˆ‘ä»¬åªtrain this thin and linear network çš„å‚æ•°ï¼Œå³max zç›¸è¿žçš„å‚æ•°ã€‚ æ¯ç¬”ä¸åŒçš„training data xï¼Œä¼šå¾—åˆ°ä¸åŒçš„thin and linear networkï¼Œæœ€åŽï¼Œä¼štrainåˆ°æ¯ä¸€ä¸ªå‚æ•°ã€‚ Adaptive Learning RateReview Adagradåœ¨è¿™ç¯‡æ–‡ç« ï¼š Gradient ç¬¬ä¸€å°èŠ‚è®²åˆ°ä¸€ç§adaptive learning rateçš„gradient ç®—æ³•ï¼šAdagrad ç®—æ³•ã€‚åœ¨é‚£ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å¾—å‡ºçš„ç»“è®ºæ˜¯ the best step $\\propto$ |First dertivative| / Second derivative. åœ¨ä¸Šå›¾ä¸­ï¼Œä¸¤ä¸ªæ–¹å‘ï¼Œå› ä¸ºè“è‰²æ–¹å‘çš„äºŒé˜¶å¾®åˆ†æ›´å°ï¼Œæ‰€ä»¥è“è‰²æ–¹å‘åº”è¯¥æœ‰æ›´å¤§çš„learning rateã€‚ å› æ­¤ï¼Œåœ¨Adagradä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€é˜¶å¾®åˆ†æ¥ä¼°é‡äºŒé˜¶å¾®åˆ†çš„å¤§å°ï¼š $$ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t} $$ RMSPropä½†æ˜¯ï¼Œåœ¨è®­ç»ƒNNæ—¶ï¼ŒError Surfaceï¼ˆTotal Losså¯¹å‚æ•°çš„å˜åŒ–ï¼‰çš„å›¾åƒå¯èƒ½ä¼šæ›´å¤æ‚ï¼Œå¦‚ä¸‹å›¾ï¼š å› ä¸ºå‡½æ•°å›¾åƒè¿‡äºŽå¤æ‚ï¼Œå¯èƒ½åœ¨åŒä¸€æ–¹å‘çš„ä¸åŒä½ç½®ï¼Œä¹Ÿéœ€è¦æœ‰ä¸åŒçš„learning rateã€‚ RMSPropæ˜¯Adagradçš„è¿›é˜¶ç‰ˆã€‚ RMSPropè¿‡ç¨‹ï¼š $w^{1} \\leftarrow w^{0}-\\frac{\\eta}{\\sigma^{0}} g^{0} \\quad \\sigma^{0}=g^{0}$ $w^{2} \\leftarrow w^{1}-\\frac{\\eta}{\\sigma^{1}} g^{1} \\quad \\sigma^{1}=\\sqrt{\\alpha (\\sigma^{0})^2+(1-\\alpha)(g^1)^2}$ $w^{3} \\leftarrow w^{2}-\\frac{\\eta}{\\sigma^{2}} g^{2} \\quad \\sigma^{2}=\\sqrt{\\alpha (\\sigma^{1})^2+(1-\\alpha)(g^2)^2}$ â€¦ $w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sigma^{t}} g^{t} \\quad \\sigma^{t}=\\sqrt{\\alpha (\\sigma^{t-1})^2+(1-\\alpha)(g^t)^2}$ $\\sigma^t$ ä¹Ÿæ˜¯åœ¨ç®—gradientsçš„ root mean squarã€‚ ä½†æ˜¯åœ¨RMSPropä¸­ï¼ŒåŠ å…¥äº†å‚æ•° $\\alpha$ (éœ€è¦æ‰‹åŠ¨è°ƒèŠ‚å¤§å°çš„å‚æ•°)ï¼Œå¯ä»¥ç»™å½“å‰ç®—å‡ºæ¥çš„gradient $g^t$ æ›´å¤§çš„æƒé‡ï¼Œå³æ›´ç›¸ä¿¡çŽ°åœ¨gradientçš„æ–¹å‘ï¼Œä¸é‚£ä¹ˆç›¸ä¿¡ä»¥å‰gradientçš„æ–¹å‘ã€‚ MomentumMomentumï¼Œåˆ™æ˜¯å¼•ç”¨ç‰©ç†ä¸­çš„æƒ¯æ€§ã€‚ ä¸Šå›¾ä¸­ï¼Œå½“å°çƒåˆ°è¾¾local minimaæ—¶ï¼Œä¼šå› ä¸ºæƒ¯æ€§ç»§ç»­å¾€å‰æ›´æ–°ï¼Œåˆ™æœ‰å¯èƒ½åˆ°è¾¾minimaçš„ä½ç½®ã€‚ è¿™é‡Œçš„Momentumï¼Œå°±ä»£æŒ‡ä¸Šä¸€æ¬¡å‰è¿›ï¼ˆå‚æ•°æ›´æ–°ï¼‰çš„æ–¹å‘ã€‚ Vanilla Gradient Descent å¦‚æžœå°†Gradientçš„æ­¥éª¤ç”»å‡ºå›¾æ¥ï¼Œå°±æ˜¯ä¸‹å›¾è¿™æ ·ï¼š è¿‡ç¨‹ï¼š Start at position $\\theta^0$ Compute gradietn at $\\theta^0$ Move to $\\theta^1=\\theta^0-\\eta\\nabla{L(\\theta^0)}$ Compute gradietn at $\\theta^1$ Move to $\\theta^2=\\theta^1-\\eta\\nabla{L(\\theta^1)}$ â€¦ â€¦Stop until $\\nabla{L(\\theta^t)}\\approx0$ Momentum åœ¨Momentumä¸­ï¼Œå‚æ•°æ›´æ–°æ–¹å‘æ˜¯å½“å‰Gradientæ–¹å‘å’ŒMomentumæ–¹å‘ï¼ˆä¸Šä¸€æ¬¡æ›´æ–°æ–¹å‘ï¼‰çš„å åŠ ã€‚ Movementæ–¹å‘ï¼šä¸Šä¸€æ¬¡æ›´æ–°æ–¹å‘ - å½“å‰gradientæ–¹å‘ã€‚ è¿‡ç¨‹ï¼š Start at position $\\theta^0$ Movement: $v^0=0$ Compute gradient at $\\theta^0$ Movement $v^1=\\lambda v^0-\\eta\\nabla{L(\\theta^0)}$ Move to $\\theta^1=\\theta^0+v^1$ Compute gradient at $\\theta^1$ Movement $v^2=\\lambda v^1-\\eta\\nabla{L(\\theta^1)}$ Move to $\\theta^2=\\theta^1+v^2$ â€¦ â€¦Stop until $\\nabla{L(\\theta^t)}\\approx0$ å’ŒVanilla Gradient Descentæ¯”è¾ƒï¼Œ$v^i$ å…¶å®žæ˜¯è¿‡åŽ»gradient( $\\nabla{L(\\theta^0)}$ ã€$\\nabla{L(\\theta^1)}$ ã€â€¦ ã€ $\\nabla{L(\\theta^{i-1})}$ )çš„åŠ æƒå’Œã€‚ è¿­ä»£è¿‡ç¨‹ï¼š $v^0=0$ $v^1=-\\eta\\nabla{L(\\theta^0)}$ $v^2=-\\lambda\\eta\\nabla{L(\\theta^0)}-\\eta\\nabla{L(\\theta^1)}$ â€¦ å†ç”¨é‚£ä¸ªå°çƒçš„ä¾‹å­æ¥ç›´è§‰çš„è§£é‡ŠMomentumï¼š å½“å°çƒåœ¨local minimaæ—¶ï¼Œgradientä¸º0ï¼Œä½†æ˜¯Momentumï¼ˆå³ä¸Šæ¬¡ç§»åŠ¨æ–¹å‘ï¼‰æ˜¯ç»§ç»­å¾€å‰ï¼ŒäºŽæ˜¯å°çƒå¯ä»¥ç»§ç»­å‘å‰æ›´æ–°ã€‚ Adam = RMSProp + Momentum Algorithmï¼šAdam, our proposed algorithm for stochastic optimization. ã€Adamï¼Œæ˜¯ä¸ºäº†ä¼˜åŒ–stochastic gradientã€‘ï¼ˆè‡³äºŽä»€ä¹ˆæ˜¯stochastic gradientï¼Œå»ºè®®æˆ³Post not found: Gradietn è¿™ç¯‡) $g_t^2$ indicates the elementwise square $g_t\\odot g_t$ . ã€$g_t^2$ æ˜¯gradient $g_t$ å‘é‡å’Œ $g_t$ çš„å…ƒç´ ä¹˜ã€‘ Good default settings for the tested machine learning problems are $\\alpha=0.001$ , $\\beta_1=0.9$ , $\\beta_2=0.999$ and $\\epsilon=10^{-8}$ . All operations on vectors are element-wise. With $\\beta_1^t$ and $\\beta_2^t$ we denote $\\beta_1$ and $\\beta_2$ to the power t. ã€å‚æ•°è¯´æ˜Žï¼šç®—æ³•é»˜è®¤çš„å‚æ•°è®¾ç½®æ˜¯ $\\alpha=0.001$ , $\\beta_1=0.9$ , $\\beta_2=0.999$ ï¼Œ $\\epsilon=10^{-8}$ ã€‚ç®—æ³•ä¸­æ‰€æœ‰vectorä¹‹é—´çš„æ“ä½œéƒ½æ˜¯å¯¹å…ƒç´ æ“ä½œã€‚ $\\beta_1^t$ å’Œ $\\beta_2^t$ æ˜¯ $\\beta_1$ å’Œ $\\beta_2$ çš„ $t$ æ¬¡å¹‚ã€‘ Adam Pseudo Codeï¼š Requireï¼š$\\alpha$ : Stepsize ã€æ­¥é•¿/learning rate $\\eta$ ã€‘ Requireï¼š$\\beta_1,\\beta_2\\in\\left[0,1\\right)$ : Exponential decay rates for the moment estimates. Requireï¼š$f(\\theta)$ : Stochastic objective function with parameters $\\theta$ .ã€å‚æ•° $\\theta$ çš„æŸå¤±å‡½æ•°ã€‘ Require: $\\theta_0$ ï¼šInitial parameter vector ã€åˆå€¼ã€‘ $m_0\\longleftarrow 0$ (Initial 1st moment vector) ã€ $m$ æ˜¯Momentumç®—æ³•ä¸­çš„æ›´æ–°å‚æ•°åŽçš„æ–¹å‘ $v$ ã€‘ $v_0\\longleftarrow 0$ (Initial 2nd moment vector) ã€ $v$ æ˜¯RMSpropç®—æ³•ä¸­gradientçš„root mean square $\\sigma$ ã€‘ $t\\longleftarrow 0$ (Initial timestep) ã€æ›´æ–°æ¬¡æ•°ã€‘ while $\\theta_t$ not concerged do ã€å½“ $\\theta$ è¶‹äºŽç¨³å®šï¼Œå³ $\\nabla{f(\\theta)}\\approx0$ æ—¶ã€‘ $t\\longleftarrow t+1$ $g_t\\longleftarrow \\nabla{f_t(\\theta_{t-1})}$ (Get gradients w.r.t. stochastic objective at timestep t) ã€ç®—ç¬¬tæ¬¡æ—¶ $\\theta$ çš„gradientã€‘ $m_{t} \\leftarrow \\beta_{1} \\cdot m_{t-1}+\\left(1-\\beta_{1}\\right) \\cdot g_{t}$ (Update biased first momen t estimate) ã€ç”¨Momentumç®—æ›´æ–°æ–¹å‘ã€‘ $v_{t} \\leftarrow \\beta_{2} \\cdot v_{t-1}+\\left(1-\\beta_{2}\\right) \\cdot g_{t}^{2}$ (Update biased second raw moment estimate) ã€RMSpropä¼°æµ‹æœ€ä½³æ­¥é•¿ï¼ˆ å’Œ$v$ è´Ÿç›¸å…³ï¼‰ ã€‘ $\\widehat{m}_{t} \\leftarrow m_{t} /\\left(1-\\beta_{1}^{t}\\right)$ ï¼ˆComppute bbi. as-corrected first momen t estima te) ã€ç®—å‡ºæ¥çš„å€¼æœ‰biasï¼Œè®ºæ–‡ä¸­æœ‰å…·ä½“è§£é‡Šä¸ºä»€ä¹ˆæœ‰ã€‚å½“æ›´æ–°æ¬¡æ•°å¢žåŠ æ—¶ï¼Œ $1-\\beta_1^t$ ä¹Ÿè¶‹è¿‘äºŽ1ã€‘ $\\widehat{v}_{t} \\leftarrow v_{t} /\\left(1-\\beta_{2}^{t}\\right)$ (Compute bias-corrected second raw momen t estimate) ã€å’Œä¸ŠåŒç†ã€‘ $\\theta_{t} \\leftarrow \\theta_{t-1}-\\alpha \\cdot \\widehat{m}_{t} /(\\sqrt{\\widehat{v}_{t}}+\\epsilon)$ ï¼ˆUpdate parametersï¼‰ ã€ $\\widehat{m}t$ ç›¸å½“äºŽæ˜¯æ›´å‡†ç¡®çš„gradientçš„æ–¹å‘ï¼Œ$\\sqrt{\\widehat{v}{t}}+\\epsilon$ æ˜¯ä¸ºäº†ä¼°æµ‹æœ€å¥½çš„æ­¥é•¿ï¼Œè°ƒèŠ‚learning rateã€‘ Gradient Descent Limitationï¼Ÿåœ¨Gradientè¿™ç¯‡æ–‡ç« ä¸­ï¼Œè®²åˆ°è¿‡Gradientæœ‰ä¸€äº›é—®é¢˜ä¸èƒ½å¤„ç†ï¼š Stuck at local minima Stuck at saddle point Very slow at the plateau ï¼ˆæŽè€å¸ˆè¯´çš„ï¼Œä¸æ˜¯æˆ‘è¯´çš„QAQï¼‰ï¼šä½†æ˜¯Andrewï¼ˆå´æ©è¾¾ï¼‰åœ¨2017å¹´è¯´è¿‡ï¼Œä¸ç”¨å¤ªæ‹…å¿ƒè¿™ä¸ªé—®é¢˜ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ å¦‚æžœè¦stuck at local minimaï¼Œå‰ææ˜¯æ¯ä¸€ç»´åº¦éƒ½æ˜¯local minimaã€‚ å¦‚æžœåœ¨ä¸€ä¸ªç»´åº¦é‡åˆ°local minimaçš„æ¦‚çŽ‡æ˜¯pï¼Œå½“NNå¾ˆå¤æ‚æ—¶ï¼Œæœ‰å¾ˆå¤šå‚æ•°æ—¶ï¼Œæ¯”å¦‚1000ï¼Œé‚£ä¹ˆé‡åˆ°local minimaçš„æ¦‚çŽ‡æ˜¯ $p^{1000}$ ï¼Œè¶‹è¿‘äºŽ0äº†ï¼Œå‡ ä¹Žä¸ä¼šå‘ç”Ÿã€‚ ï¼šæ‰€ä»¥ä¸ç”¨å¤ªæ‹…å¿ƒGradient Descentçš„å±€é™æ€§ã€‚ Bad Results on Testing DataEarly Stoppingåœ¨æ›´æ–°å‚æ•°æ—¶ï¼Œå¯èƒ½ä¼šå‡ºçŽ°è¿™æ ·æ›²çº¿å›¾ï¼š å›¾ä¸­ï¼ŒTotal Lossåœ¨training setä¸­é€æ¸å‡å°ï¼Œä½†åœ¨validation setä¸­é€æ¸å¢žå¤§ã€‚ è€Œæˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„å…¶å®žæ˜¯validation setçš„Lossã€‚ æ‰€ä»¥æƒ³è®©å‚æ•°åœåœ¨validation setä¸­lossæœ€ä½Žæ—¶ã€‚ Kerasèƒ½å¤Ÿå®žçŽ°EarlyStoppingåŠŸèƒ½[1]ï¼šclick here 123from keras.callbacks import EarlyStoppingearly_stopping = EarlyStopping(monitor='val_loss', patience=2)model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) RegularizationRegularizationï¼šFind a set of weight not only minimizing original cost but also close to zero. æž„é€ ä¸€ä¸ªæ–°çš„loss functionï¼Œé™¤äº†æœ€å°åŒ–åŽŸæ¥çš„loss functionï¼Œè¿˜èƒ½ä½¿å¾—å‚æ•°è¶‹ç´§0ï¼Œä½¿å¾—functionæ›´å¹³æ»‘ã€‚ functionçš„æ›²çº¿æ›´å¹³æ»‘ï¼Œå½“è¾“å…¥æœ‰è½»å¾®æ‰°åŠ¨ï¼Œä¸ä¼šå¤ªå½±å“è¾“å‡ºçš„ç»“æžœã€‚ L2 norm regularizationNew loss function: $$ \\begin{equation} \\begin{aligned} \\mathrm{L}^{\\prime}(\\theta)&=L(\\theta)+\\lambda \\frac{1}{2}\\|\\theta\\|_{2} \\\\ \\theta &={w_1,w_2,...} \\\\ \\|\\theta\\|_2&=(w1)^2+(w_2)^2+... \\end{aligned} \\end{equation} $$ å…¶ä¸­ç”¨ç¬¬äºŒèŒƒå¼ $\\lambda\\frac{1}{2}|\\theta|_2$ ä½œä¸ºregularization termã€‚åšregularizationæ˜¯ä¸ºäº†ä½¿å‡½æ•°æ›´å¹³æ»‘ï¼Œæ‰€ä»¥ä¸€èˆ¬ä¸è€ƒè™‘bias) New gradient: $$ \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w}=\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda w $$ New update: $$ \\begin{equation} \\begin{aligned} w^{t+1} &\\longrightarrow w^{t}-\\eta \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w} \\\\ &=w^{t}-\\eta\\left(\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda w^{t}\\right) \\\\ &=(1-\\eta \\lambda) w^{t}-\\eta \\frac{\\partial \\mathrm{L}}{\\partial w} \\end{aligned} \\end{equation} $$ åœ¨æ›´æ–°å‚æ•°æ—¶ï¼Œå…ˆä¹˜ä¸€ä¸ª $(1-\\eta\\lambda)$ ï¼Œå†æ›´æ–°ã€‚ weight decayï¼ˆæƒå€¼è¡°å‡ï¼‰ï¼šç”±äºŽ $\\eta,\\lambda$ éƒ½æ˜¯å¾ˆå°çš„å€¼ï¼Œæ‰€ä»¥ $w^t$ æ¯æ¬¡éƒ½ä¼šå…ˆä¹˜ä¸€ä¸ªå°äºŽ1çš„æ•°ï¼Œå³é€æ¸è¶‹äºŽ0ï¼Œå®žçŽ°regularizationã€‚ä½†æ˜¯ï¼Œå› ä¸ºæ›´æ–°ä¸­è¿˜æœ‰gradientéƒ¨åˆ†ï¼Œæ‰€ä»¥ä¸ä¼šç­‰äºŽ0ã€‚ L1 norm regularizationRegularizationé™¤äº†ç”¨ç¬¬äºŒèŒƒå¼ï¼Œè¿˜å¯ä»¥ç”¨å…¶ä»–çš„ï¼Œæ¯”å¦‚ç¬¬ä¸€èŒƒå¼ $|\\theta|_1=|w_1|+|w_2|+â€¦$ New loss function: $$ \\begin{equation}\\begin{aligned}\\mathrm{L}^{\\prime}(\\theta)&=L(\\theta)+\\lambda \\frac{1}{2}\\|\\theta\\|_1\\\\ \\theta &={w_1,w_2,...} \\\\ \\|\\theta\\|_1&=|w_1|+|w_2|+...\\end{aligned}\\end{equation} $$ ç”¨sgn()ç¬¦å·å‡½æ•°æ¥è¡¨ç¤ºç»å¯¹å€¼çš„æ±‚å¯¼ã€‚ ç¬¦å·å‡½æ•°ï¼šSgn(number) å¦‚æžœnumber å¤§äºŽ0ï¼Œè¿”å›ž1ï¼›ç­‰äºŽ0ï¼Œè¿”å›ž0ï¼›å°äºŽ0ï¼Œè¿”å›ž-1ã€‚ New gradient: $$ \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w}=\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda \\text{sgn}(w) $$ New update: $$ \\begin{equation} \\begin{aligned} w^{t+1} &\\longrightarrow w^{t}-\\eta \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w} \\\\ &=w^{t}-\\eta\\left(\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda \\text{sgn}(w^t)\\right) \\\\ &=w^{t}-\\eta \\frac{\\partial \\mathrm{L}}{\\partial w}-\\eta \\lambda \\operatorname{sgn}\\left(w^{t}\\right) \\end{aligned} \\end{equation} $$ åœ¨ç”¨ç¬¬ä¸€èŒƒå¼åšregularizationæ—¶ï¼Œæ¯æ¬¡ $w^t$ éƒ½è¦å‡ä¸€ä¸ªå€¼ $\\eta\\lambda\\text{sgn}(w^t)$ ï¼Œå’Œç”¨ç¬¬äºŒèŒƒå¼åšregularizationæ¯”è¾ƒï¼ŒåŽè€…æ¯æ¬¡éƒ½è¦ä¹˜ä¸€ä¸ªå°äºŽ1çš„å€¼ï¼Œå³ä½¿æ˜¯ä¹˜0.99ï¼Œwä¸‹é™ä¹Ÿå¾ˆå¿«ã€‚ Weight decayï¼ˆæƒå€¼è¡°å‡ï¼‰çš„ç”Ÿç‰©æ„ä¹‰ï¼š Our brain prunesï¼ˆä¿®å‰ªï¼‰ out the useless link between neurons. DropoutWiki: Dropoutæ˜¯Googleæå‡ºçš„ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç”¨ä»¥åœ¨äººå·¥ç¥žç»ç½‘ç»œä¸­å¯¹æŠ—è¿‡æ‹Ÿåˆã€‚Dropoutæœ‰æ•ˆçš„åŽŸå› ï¼Œæ˜¯å®ƒèƒ½å¤Ÿé¿å…åœ¨è®­ç»ƒæ•°æ®ä¸Šäº§ç”Ÿå¤æ‚çš„ç›¸äº’é€‚åº”ã€‚Dropoutè¿™ä¸ªæœ¯è¯­ä»£æŒ‡åœ¨ç¥žç»ç½‘ç»œä¸­ä¸¢å¼ƒéƒ¨åˆ†ç¥žç»å…ƒï¼ˆåŒ…æ‹¬éšè—ç¥žç»å…ƒå’Œå¯è§ç¥žç»å…ƒï¼‰ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œdropoutä½¿å¾—æ¯æ¬¡åªæœ‰éƒ¨åˆ†ç½‘ç»œç»“æž„å¾—åˆ°æ›´æ–°ï¼Œå› è€Œæ˜¯ä¸€ç§é«˜æ•ˆçš„ç¥žç»ç½‘ç»œæ¨¡åž‹å¹³å‡åŒ–çš„æ–¹æ³•ã€‚[2] è¿™é‡Œè®²Dropoutæ€Žä¹ˆåšã€‚ Training Each time before updating the parameters: Each neuron has p% to dropout. Using the new thin network for training. ã€å¦‚ä¸Šå›¾ï¼Œæ¯ä¸ªneuronæœ‰pçš„æ¦‚çŽ‡è¢«dropoutã€‚äºŽæ˜¯NNå°±å˜æˆäº†ä¸‹å›¾thinnerçš„NNã€‘ For each mini-batch, we resample the dropout neurons. ã€æ¯æ¬¡mini-batchï¼Œéƒ½è¦é‡æ–°dropoutï¼Œæ›´æ–°NNçš„ç»“æž„ã€‘ TestingTestingä¸­ä¸åšdropout If the dropout rate at training is p%, all the weights times 1-p%. ã€å¦‚æžœåœ¨trainingä¸­ dropout rateæ˜¯ p%ï¼Œåœ¨testingæ˜¯ï¼Œæ¯ä¸ªå‚æ•°éƒ½ä¹˜ ï¼ˆ1-p%)ã€‘ ã€æ¯”å¦‚dropout rate æ˜¯0.5ã€‚å¦‚æžœtrainå‡ºæ¥çš„wæ˜¯ 1ï¼Œé‚£ä¹ˆtestingä¸­ w=0.5ã€‘ Why dropout in trainingï¼šIntuitive Reason è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒæœ‰è¶£çš„æ¯”å–»ï¼š è¿™ä¹Ÿæ˜¯ä¸€ä¸ªæœ‰è¶£çš„æ¯”å–»hhh: å³ï¼Œå›¢é˜Ÿåˆä½œçš„æ—¶å€™ï¼Œå¦‚æžœæ¯ä¸ªäººéƒ½è®¤ä¸ºé˜Ÿå‹åœ¨å¸¦æˆ‘ï¼Œé‚£æ¯ä¸ªäººéƒ½å¯èƒ½åˆ’æ°´ã€‚ ä½†æ˜¯ï¼Œï¼ˆtrainingä¸­ï¼‰å¦‚æžœä½ çŸ¥é“ä½ çš„é˜Ÿå‹åœ¨åˆ’æ°´ï¼Œé‚£ä½ å¯èƒ½ä¼šåšçš„æ›´å¥½ã€‚ ä½†æ˜¯ï¼Œï¼ˆtestingä¸­ï¼‰å‘çŽ°æ¯ä¸ªäººéƒ½æœ‰æ›´å¥½åœ°åšï¼Œéƒ½æ²¡æœ‰åˆ’æ°´ï¼Œé‚£ä¹ˆç»“æžœå°±ä¼šå¾ˆå¥½ã€‚ ï¼ˆhhhhï¼ŒæŽè€å¸ˆæ¯æ¬¡è®²Intuitive Reasonçš„æ—¶å€™ï¼Œéƒ½è§‰å¾—å¥½æœ‰é“ç†hhhï¼Œç§‘å­¦çš„ç›´è§‰orzç»™æˆ‘ä¹Ÿæ•´ä¸€ä¸ªï¼‰ Why multiply (1-p%) in testing: Intuitive reasonä¸ºä»€ä¹ˆåœ¨testingä¸­ weightsè¦ä¹˜ï¼ˆ1-p%)? ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥ç›´è§‚è¯´æ˜Žï¼š ä¸Šå›¾ä¸­ï¼Œå¦‚æžœdropout rate=0.5ï¼Œå‡è®¾åªè®­ç»ƒä¸€æ¬¡ï¼Œ $w_2,w_4$ ç›¸è¿žçš„neuronéƒ½è¢«dropoutã€‚ åœ¨testingä¸­ï¼Œå› ä¸ºä¸å¯¹neurondropoutï¼Œæ‰€ä»¥å¦‚æžœä¸æ”¹å˜weightï¼Œè®¡ç®—å‡ºçš„ç»“æžœ $zâ€™\\approx 2z$ ã€‚ å› æ­¤å°†æ‰€æœ‰weightç®€å•åœ°å’Œ(1-p%) ç›¸ä¹˜ï¼Œèƒ½å°½é‡ä¿è¯è®¡ç®—å‡ºçš„ç»“æžœ $zâ€™\\approx z$ ã€‚ Dropout is a kind of ensembleEnsemble(åˆå¥)ï¼Œå¦‚ä¸‹å›¾ï¼Œå°†testing dataä¸¢ç»™trainå¥½çš„NNæ¥ä¼°è®¡ï¼Œæœ€åŽçš„ä¼°è®¡å€¼å–æ‰€æœ‰NNè¾“å‡ºçš„å¹³å‡ï¼Œå¦‚ä¸‹å›¾ï¼š ä¸ºä»€ä¹ˆè¯´dropout is a kind of ensemble? Using one mini-batch to train one network ã€dropoutç›¸å½“äºŽæ¯æ¬¡ç”¨ä¸€ä¸ªmini-batchæ¥è®­ç»ƒä¸€ä¸ªnetworkã€‘ Some parameters in the network are shared ã€æœ‰äº›å‚æ•°å¯èƒ½ä¼šåœ¨å¾ˆå¤šä¸ªmini-batchéƒ½è¢«trainåˆ°ã€‘ ç”±äºŽæ¯ä¸ªç¥žç»å…ƒæœ‰ p%çš„æ¦‚çŽ‡è¢«dropoutï¼Œå› æ­¤ç†è®ºä¸Šï¼Œå¦‚æžœæœ‰Mä¸ªneuronï¼Œå¯èƒ½ä¼šè®­ç»ƒ $2^M$ ä¸ªnetworkã€‚ ä½†æ˜¯åœ¨Ensembleä¸­ï¼Œå°†æ¯ä¸ªnetworkå­˜ä¸‹æ¥ï¼Œtestingçš„æ—¶å€™è¾“å‡ºå–å¹³å‡ï¼Œè¿™æ ·çš„è¿‡ç¨‹å¤ªå¤æ‚äº†ï¼Œç»“æžœä¹Ÿä¸ä¸€å®šä¼šå¾ˆå¥½ã€‚ æ‰€ä»¥åœ¨testingä¸­ï¼Œno dropoutï¼Œå¯¹åŽŸå§‹networkä¸­çš„æ¯ä¸ªå‚æ•°ä¹˜ (1-p%)ï¼Œç”¨è¿™æ ·ç®€å•çš„æ“ä½œæ¥è¾¾åˆ°ensembleçš„ç›®çš„ã€‚ Reference Keras: how can i interrupt training when the validation loss isnâ€™t decresing anymore. Dropout-wikiï¼šhttps://zh.wikipedia.org/wiki/Dropout","link":"/2020/04/20/tips-for-DL/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€:Unsupervised-PCA","text":"è¿™ç¯‡æ–‡ç« è¯¦ç»†è®²è§£äº†æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised learningï¼‰çš„PCAï¼ˆä¸»æˆåˆ†åˆ†æžæ³•ï¼‰ã€‚ æ–‡ç« å¼€ç¯‡ä»Žèšç±»ï¼ˆClusteringï¼‰å¼•å‡ºDistributed Representionï¼Œå…¶ä¸­ç²—ç•¥é˜è¿°äº†èšç±»ä¸­K-meanså’ŒHACï¼ˆå±‚æ¬¡èšç±»ï¼‰çš„æ€æƒ³ã€‚ æ–‡ç« çš„åŽåŠéƒ¨åˆ†å…·ä½“é˜è¿°äº†PCAçš„æ•°å­¦ç»†èŠ‚ï¼ŒPCAçš„åŽ»ç›¸å…³æ€§æ€§è´¨ï¼ŒPCAçš„å¦ä¸€ç§è§£é‡Šè§’åº¦ï¼ˆcomponentçš„è§’åº¦ï¼‰ï¼ŒPCAçš„ä¸è¶³ç­‰ã€‚ Unsupervised Learningæ— ç›‘ç£å­¦ä¹ åˆ†ä¸ºä¸¤ç§ï¼š Dimension Reductionï¼šåŒ–ç¹ä¸ºç®€ã€‚ function åªæœ‰inputï¼Œèƒ½å°†é«˜ç»´ã€å¤æ‚çš„è¾“å…¥ï¼ŒæŠ½è±¡ä¸ºä½Žç»´çš„è¾“å‡ºã€‚ å¦‚ä¸‹å›¾ï¼Œèƒ½å°†3Dçš„æŠ˜å å›¾åƒï¼ŒæŠ½è±¡ä¸ºä¸€ä¸ª2Dçš„è¡¨ç¤ºï¼ˆæŠŠä»–æ‘Šå¼€ï¼‰ã€‚ Generationï¼šæ— ä¸­ç”Ÿæœ‰ã€‚ function åªæœ‰outputã€‚ ï¼ˆåŽé¢çš„åšå®¢ä¼šæåŠï¼‰ Dimension Reductionæ­¤å‰ï¼Œåœ¨semi-supervised learningçš„æœ€åŽï¼ŒæåŠè¿‡better presentationçš„æ€æƒ³ï¼ŒDimension Reduction å…¶å®žå°±æ˜¯è¿™æ ·çš„æ€æƒ³ï¼šåŽ»èŠœå­˜èï¼ŒåŒ–ç¹ä¸ºç®€ã€‚ æ¯”å¦‚ï¼Œåœ¨MNISTä¸­ï¼Œä¸€ä¸ªæ•°å­—çš„è¡¨ç¤ºæ˜¯28*28ç»´åº¦çš„å‘é‡ï¼ˆå›¾å¦‚å·¦ï¼‰ï¼Œä½†å¤§å¤š28 *28ç»´åº¦çš„å‘é‡ï¼ˆå›¾ä¸ºå³ï¼‰éƒ½ä¸æ˜¯æ•°å­—ã€‚ å› æ­¤ï¼Œåœ¨è¡¨è¾¾ä¸‹å›¾ä¸€ä¼—â€œ3â€çš„å›¾åƒä¸­ï¼Œæ ¹æœ¬ä¸éœ€è¦28*28ç»´çš„å‘é‡è¡¨ç¤ºï¼Œ1-Då³å¯è¡¨ç¤ºä¸€å¼ å›¾ï¼ˆå›¾ç‰‡çš„æ—‹è½¬è§’åº¦ï¼‰ã€‚28 * 28çš„å›¾åƒè¡¨ç¤ºå°±åƒå·¦è¾¹ä¸­è€è€…çš„å¤´å‘ï¼Œ1-Dçš„è¡¨ç¤ºå°±åƒè€è€…çš„å¤´ï¼Œæ˜¯å¯¹å¤´å‘è¿åŠ¨è½¨è¿¹ä¸€ç§æ›´ç®€å•çš„è¡¨è¾¾ã€‚ Clusteringåœ¨å°†Dimension Reductionä¹‹å‰ï¼Œå…ˆå°†ä¸€ç§ç»å…¸çš„æ— ç›‘ç£å­¦ä¹ â€”â€”clustering. clusteringä¹Ÿæ˜¯ä¸€ç§é™ç»´çš„è¡¨è¾¾ï¼Œå°†å¤æ‚çš„å‘é‡ç©ºé—´æŠ½è±¡ä¸ºç®€å•çš„ç±»åˆ«ï¼Œç”¨æŸä¸€ä¸ªç±»åˆ«æ¥è¡¨ç¤ºè¯¥æ•°æ®ç‚¹ã€‚ è¿™é‡Œä¸»è¦è®²è¿°clusterçš„ä¸»è¦æ€æƒ³ï¼Œç®—æ³•ç»†èŠ‚å¯å‚è€ƒå…¶ä»–èµ„æ–™ ã€‚ï¼ˆå¾…è¡¥å……ï¼‰ K-meansK-meansçš„åšæ³•æ˜¯ï¼š Clustering $X=\\left\\{x^{1}, \\cdots, x^{n}, \\cdots, x^{N}\\right\\}$ into K clusters. æŠŠæ‰€æœ‰dataåˆ†ä¸ºKä¸ªç±»ï¼ŒKçš„ç¡®å®šæ˜¯empiricalçš„ï¼Œéœ€è¦è‡ªå·±ç¡®å®š Initialize cluster center $c^i, i=1,2,â€¦,K$ .(K random $x^n$ from $X$) åˆå§‹åŒ–Kä¸ªç±»çš„ä¸­å¿ƒæ•°æ®ç‚¹ï¼Œå»ºè®®ä»Žtraining set $X$ ä¸­éšæœºé€‰K ä¸ªç‚¹ä½œä¸ºåˆå§‹ç‚¹ã€‚ ä¸å»ºè®®ç›´æŽ¥åœ¨å‘é‡ç©ºé—´ä¸­éšæœºåˆå§‹åŒ–Kä¸ªä¸­å¿ƒç‚¹ï¼Œå› ä¸ºå¾ˆå¯èƒ½éšæœºçš„ä¸­å¿ƒç‚¹ä¸å±žäºŽä»»ä½•ä¸€ä¸ªclusterã€‚ Repeatï¼šæ ¹æ®ä¸­å¿ƒç‚¹æ ‡è®°æ‰€å±žç±»ï¼Œå†æ›´æ–°æ–°çš„ä¸­å¿ƒç‚¹ï¼Œé‡å¤ç›´æ”¶æ•›ã€‚ For all $x^n$ in $X$ : æ ‡è®°æ‰€å±žç±»ã€‚ $$ b_{i}^{n}\\left\\{\\begin{array}{ll}1 & x^{n} \\text { is most \"close\" to } c^{i} \\\\ 0 & \\text { Otherwise }\\end{array}\\right. $$ Updating all $c^i$ : $c^{i}=\\sum_{x^{n}} b_{i}^{n} x^{n} / \\sum_{x^{n}} b_{i}^{n}$ (è®¡ç®—è¯¥ç±»ä¸­å¿ƒç‚¹) HACï¼šHierarchical Agglomerative Clustering(HAC)å¦ä¸€ç§clusteringçš„æ–¹æ³•æ˜¯å±‚æ¬¡èšç±»ï¼ˆHierarchical Clusteringï¼‰ï¼Œè¿™é‡Œä»‹ç»Agglomerativeï¼ˆè‡ªä¸‹è€Œä¸Šï¼‰çš„ç­–ç•¥ã€‚ Build a tree. å¦‚ä¸Šå›¾ä¸­ï¼Œè®¡ç®—å½“å‰ä¸¤ä¸¤æ•°æ®ç‚¹ï¼ˆç‚¹æˆ–ç»„åˆï¼‰çš„ç›¸ä¼¼åº¦ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»æˆ–å…¶ä»–ï¼‰ã€‚ é€‰å‡ºæœ€ç›¸è¿‘çš„ä¸¤ä¸ªåˆä¸ºä¸€ç»„ï¼ˆå³è¿žæŽ¥åœ¨åŒä¸€çˆ¶å­ç»“ç‚¹ä¸Šï¼Œå¦‚æœ€å·¦è¾¹çš„ä¸¤ä¸ªï¼‰ é‡å¤1-2ç›´è‡³æœ€åŽåˆä¸ºrootã€‚ è¯¥æ ‘ä¸­ï¼Œè¶Šæ—©åˆ†æ”¯çš„ç‚¹é›†åˆï¼Œè¯´æ˜Žè¶Šä¸åƒã€‚ Pick a threshold. é€‰ä¸€ä¸ªé˜ˆå€¼ï¼Œå³ä»Žå“ªä¸ªåœ°æ–¹å¼€å§‹åˆ’å¼€ï¼Œæ¯”å¦‚é€‰ä¸Šå›¾ä¸­çº¢è‰²çš„çº¿ä½œä¸ºé˜ˆå€¼ï¼Œé‚£ä¹ˆç‚¹é›†åˆ†ä¸ºä¸¤ä¸ªcluseterï¼Œè“è‰²ã€ç»¿è‰²åŒç†ã€‚ HACå’ŒK-meansç›¸æ¯”ï¼ŒHACä¸ç›´æŽ¥å†³å®šclusterçš„æ•°ç›®ï¼Œè€Œæ˜¯é€šè¿‡å†³å®šthresholdçš„å€¼é—´æŽ¥å†³å®šclusterçš„æ•°ç›®ã€‚ Distributed RepresentationClusterï¼šan object must belong to one cluster. åœ¨åšèšç±»æ—¶ï¼Œä¸€ä¸ªæ•°æ®ç‚¹å¿…é¡»æ ‡æ³¨ä¸ºæŸä¸€å…·ä½“ç±»åˆ«ã€‚è¿™å¾€å¾€ä¼šä¸¢å¤±å¾ˆå¤šä¿¡æ¯ï¼Œæ¯”å¦‚ä¸€ä¸ªäººå¯èƒ½æ˜¯70%çš„å¤–å‘ï¼Œ30%çš„å†…æ•›ï¼Œå¦‚æžœåšclusteringï¼Œå°±å°†è¿™ä¸ªäººç›´æŽ¥å½’ä¸ºå¤–å‘ï¼Œè¿™æ ·çš„è¡¨ç¤ºè¿‡äºŽç²—ç³™ã€‚ å› æ­¤ä»ç”¨vectoræ¥è¡¨ç¤ºè¿™ä¸ªäººï¼Œå¦‚ä¸‹å›¾ã€‚ Distributed Representationï¼ˆä¹Ÿå«Dimension Reductionï¼‰å°±æ˜¯ï¼šä¸€ä¸ªé«˜ç»´çš„vectoré€šè¿‡functionï¼Œå¾—åˆ°ä¸€ä¸ªä½Žç»´çš„vectorã€‚ Distributedçš„æ–¹æ³•æœ‰å¸¸è§çš„ä¸¤ç§ï¼š Feature selectionï¼š å¦‚ä¸‹å›¾æ•°æ®ç‚¹çš„åˆ†å¸ƒï¼Œå¯ä»¥ç›´æŽ¥é€‰æ‹©feature $x_2$ . ä½†è¿™ç§æ–¹æ³•å¾€å¾€åªèƒ½å¤„ç†2-Dçš„æƒ…å†µï¼Œå¯¹äºŽä¸‹å›¾è¿™ç§3-Dæƒ…å†µå¾€å¾€ä¸å¥½åšç‰¹å¾é€‰æ‹©ã€‚ Principle component analysisï¼ˆPCAï¼‰ å¦ä¸€ç§æ–¹æ³•å°±æ˜¯è‘—åçš„PCAï¼Œä¸»æˆåˆ†åˆ†æžæ³•ã€‚ PCAä¸­ï¼Œè¿™ä¸ªfunctionå°±æ˜¯ä¸€ä¸ªç®€å•çš„linear functionï¼ˆ$W$ï¼‰ï¼Œé€šè¿‡ $z=Wx$ ï¼Œå°†é«˜ç»´çš„ $x$ è½¬åŒ–ä¸ºä½Žç»´çš„ $z$ . PCAï¼šPrinciple Component AnalysisPCAçš„å‚è€ƒèµ„æ–™è§Bishop, Chapter12. PCAå°±æ˜¯è¦æ‰¾ $z=Wx$ ä¸­çš„ $W$ . Main IdeaReduce 1-Då¦‚æžœå°†dimension reduce to 1-Dï¼Œé‚£ä¹ˆå¯ä»¥å¾—å‡º $z_1 = w^1\\cdot x$ . $w^1$ æ˜¯vectorï¼Œ$x$ æ˜¯vectorï¼Œåšå†…ç§¯ã€‚ å¦‚ä¸‹å›¾ï¼Œå†…ç§¯å³æŠ•å½±ï¼Œå°†æ‰€æœ‰çš„ç‚¹ $x$ æŠ•å½±åˆ° $w^1$ æ–¹å‘ä¸Šï¼Œç„¶åŽå¾—åˆ°å¯¹åº”çš„ $z_1$ å€¼ã€‚ è€Œå¯¹äºŽå¾—åˆ°çš„ä¸€ç³»åˆ— $z_1$ å€¼ï¼Œæˆ‘ä»¬å¸Œæœ› $z_1$ çš„varianceè¶Šå¤§è¶Šå¥½ã€‚ å› ä¸º $z_1$ çš„åˆ†å¸ƒè¶Šå¤§ï¼Œç”¨ $z_1$ æ¥åˆ»ç”»æ•°æ®ï¼Œæ‰èƒ½æ›´å¥½çš„åŒºåˆ†æ•°æ®ç‚¹ã€‚ å¦‚ä¸‹å›¾ï¼Œå¦‚æžœ $w^1$ çš„æ–¹å‘æ˜¯small varianceçš„æ–¹å‘ï¼Œé‚£ä¹ˆè¿™äº›ç‚¹ä¼šé›†ä¸­åœ¨ä¸€èµ·ï¼Œè€Œlarge varianceæ–¹å‘ï¼Œ$z_1$ èƒ½æ›´å¥½çš„åˆ»ç”»æ•°æ®ã€‚ $z_1$ çš„æ•°å­¦è¡¨è¾¾æ˜¯ï¼š $ \\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2} \\quad \\left\\|w^{1}\\right\\|_{2}=1$ (åŽæ–‡è§£é‡Šä¸ºä»€ä¹ˆè¦ $w^1$ çš„é•¿åº¦ä¸º1) Reduce 2-DåŒç†ï¼Œå¦‚æžœå°†dimension reduce to 2-D . $z=Wx$ å³ $$ \\left\\{ \\begin{array}{11}z_1=w^1\\cdot x \\\\ z_2=w^2 \\cdot x \\end{array} \\right. ,\\quad W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\end{bmatrix} $$ å°†æ‰€æœ‰ç‚¹ $x$ æŠ•å½±åˆ° $w^1$ æ–¹å‘ï¼Œå¾—åˆ°å¯¹åº”çš„ $z_1$ ï¼Œä¸”è®© $z_1$ çš„åˆ†å¸ƒå°½å¯èƒ½çš„å¤§ï¼š $$ \\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2} ,\\quad \\left\\|w^{1}\\right\\|_{2}=1 $$ å°†æ‰€æœ‰ç‚¹æŠ•å½±åˆ° $w^2$ æ–¹å‘ï¼Œå¾—åˆ°å¯¹åº”çš„ $z_2$ ï¼ŒåŒæ ·è®© $z_2$ çš„åˆ†å¸ƒä¹Ÿå°½å¯èƒ½å¤§ï¼Œå†åŠ ä¸€ä¸ªçº¦æŸæ¡ä»¶ï¼Œè®© $w^2$ å’Œ $w^1$ æ­£äº¤ï¼ˆåŽæ–‡ä¼šå…·ä½“è§£é‡Šä¸ºä»€ä¹ˆï¼‰ $$ \\operatorname{Var}\\left(z_{2}\\right)=\\frac{1}{N} \\sum_{z_{2}}\\left(z_{2}-\\overline{z_{2}}\\right)^{2} ,\\quad \\left\\|w^{2}\\right\\|_{2}=1 ,\\quad w^1\\cdot w^2=0 $$ å› æ­¤çŸ©é˜µ $W$ æ˜¯Orthogonal matrix (æ­£äº¤çŸ©é˜µ)ã€‚ Detail[Warning of Mathæƒ³è·³è¿‡mathéƒ¨åˆ†çš„ï¼Œå¯ä»¥ç›´æŽ¥çœ‹Conclusionã€‚ 1-Dä¸­ï¼š Goalï¼šfind $w^1$ to maximum $(w^1)^T S w^1$ s.t.$(w^1)^Tw^1=1$ ç»“è®ºï¼š$w^1$ å°±æ˜¯åæ–¹å·®çŸ©é˜µ $S$ æœ€å¤§ç‰¹å¾å€¼ $\\lambda_1 $ å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ s.t.$(w^1)^Tw^1=1$ 2-Dä¸­ï¼š Goalï¼šfind $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ ç»“è®ºï¼š$w^2$ å°±æ˜¯åæ–¹å·®çŸ©é˜µ$S$ ç¬¬äºŒå¤§ç‰¹å¾å€¼ $\\lambda_2 $ å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ s.t.$(w^2)^Tw^2=1$ k-Dä¸­ï¼š ç»“è®ºï¼š$w$ å°±æ˜¯åæ–¹å·®çŸ©é˜µ $S$ å‰ $k$ å¤§çš„ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚s.t. $W$ æ˜¯æ­£äº¤çŸ©é˜µã€‚ 1-DGoalï¼šFind $w^1$ to maximum the variance of $z_1$ . $\\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2}$ $z_1=w^1\\cdot x ,\\quad \\overline{z_{1}}=\\frac{1}{N} \\sum_{z_{1}}=\\frac{1}{N} \\sum w^{1} \\cdot x=w^{1} \\cdot \\frac{1}{N} \\sum x=w^{1} \\cdot \\bar{x}$ $\\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2}=(w^1)^T\\operatorname{Cov}(x)w^1$ $=\\frac{1}{N} \\sum_{x}\\left(w^{1} \\cdot x-w^{1} \\cdot \\bar{x}\\right)^{2} $ $=\\frac{1}{N} \\sum\\left(w^{1} \\cdot(x-\\bar{x})\\right)^{2}$ $a,b$ æ˜¯vectorï¼š $(a\\cdot b)^2=(a^Tb)^2=a^Tba^Tb$ $a^Tb$ æ˜¯scalar: $(a\\cdot b)^2 = (a^Tb)^2=a^Tba^Tb =a^Tb(a^Tb)^T=a^Tbb^Ta$ $=\\frac{1}{N} \\sum\\left(w^{1}\\right)^{T}(x-\\bar{x})(x-\\bar{x})^{T} w^{1}$ $ = \\left(w^{1}\\right)^{T}\\sum\\frac{1}{N}(x-\\bar{x})(x-\\bar{x})^{T} \\ w^{1}$ $=(w^1)^T\\operatorname{Cov}(x)w^1$ ä»¤ $S=\\operatorname{Cov}(x)$ ä¹‹å‰é—ç•™çš„ä¸¤ä¸ªé—®é¢˜ï¼š $\\left|w^1\\right|_2=1$ ? $w^1\\cdot w^2=1$ ? çŽ°åœ¨æ¥çœ‹ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œä¸ºä»€ä¹ˆè¦ $\\left|w^1\\right|_2=1$ ï¼Ÿ çŽ°åœ¨çš„ç›®æ ‡ï¼Œå˜æˆäº† maximum $(w^1)^T S w^1$ ï¼Œå¦‚æžœä¸é™åˆ¶ $\\left|w^1\\right|_2$ ï¼Œè®© $\\left|w^1\\right|_2$ æ— ç©·å¤§ï¼Œé‚£ä¹ˆ $(w^1)^T S w^1$ çš„å€¼ä¹Ÿä¼šæ— ç©·å¤§ï¼Œé—®é¢˜æ— è§£äº†ã€‚ Goalï¼šmaximum $(w^1)^T S w^1$ s.t. $(w^1)^Tw^1=1$ Lagrange multiplier[æŒ–å‘] æ±‚è§£å¤šå…ƒå˜é‡åœ¨æœ‰é™åˆ¶æ¡ä»¶ä¸‹çš„é©»ç‚¹ã€‚ æž„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š $g\\left(w^{1}\\right)=\\left(w^{1}\\right)^{T} S w^{1}-\\alpha\\left(\\left(w^{1}\\right)^{T} w^{1}-1\\right)$ ï¼Œ$\\alpha\\neq 0$ ä¸ºæ‹‰æ ¼æœ—æ—¥ä¹˜æ•° $\\nabla_{w^1}g=0$ çš„å€¼ä¸ºé©»ç‚¹ï¼ˆä¼šå•ç‹¬å†™ä¸€ç¯‡åšå®¢æ¥è®²æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼‰ $\\frac{\\partial g}{\\partial \\alpha}=0$ ä¸ºé™åˆ¶å‡½æ•° å¯¹çŸ©é˜µå¾®åˆ†ï¼šè¯¦æƒ…è§wiki scalar-by-vector(scalarå¯¹vectorå¾®åˆ†) $S$ æ˜¯å¯¹ç§°çŸ©é˜µï¼Œä¸æ˜¯ $w^1$ çš„å‡½æ•°ï¼Œç»“æžœç”¨ $w^1$ è¡¨è¾¾ï¼š$2Sw^1-2\\alpha w^1=0$ maximum: $(w^1)^T S w^1=\\alpha (w^1)^Tw^1=\\alpha$ *Goalï¼šfind $w^1$to maximum $\\alpha$ * $\\alpha$ æ»¡è¶³ç­‰å¼ï¼š$Sw^1=\\alpha w^1$ $\\alpha$ æ˜¯ $S$ çš„ç‰¹å¾å‘é‡ï¼Œ$w^1$ æ˜¯ $S$ å¯¹åº”äºŽç‰¹å¾å€¼ $\\alpha$ çš„ç‰¹å¾å‘é‡ã€‚ å…³äºŽç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„çŸ¥è¯†å‚è€ƒï¼šå‚è€ƒä¸‹é¢çº¿ä»£çŸ¥è¯† $w^1$ is the eigenvector(ç‰¹å¾å‘é‡) of the covarivance matrix S corresponding to the largest eigenvalue $\\lambda_1$ . ç»“è®ºï¼š$w^1$ å°±æ˜¯åæ–¹å·®çŸ©é˜µæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ æ¢¦å›žçº¿ä»£QWQï¼ˆè‡ªå·±çº¿ä»£å­¦çš„å¤ªå·®å•¦ å•Šè¿™ï¼ ç‰¹å¾å‘é‡ï¼Œç‰¹å¾å€¼å®šä¹‰ï¼š $A$ æ˜¯né˜¶æ–¹é˜µï¼Œå¦‚æžœå­˜åœ¨æ•° $\\lambda$ å’Œnç»´éžé›¶å‘é‡ $\\alpha$ ï¼Œæ»¡è¶³ $A\\alpha=\\lambda \\alpha$ , åˆ™ç§° $\\lambda$ ä¸ºæ–¹é˜µ $A$ çš„ä¸€ä¸ªç‰¹å¾å€¼ï¼Œ$\\alpha$ ä¸ºæ–¹é˜µ $A$ å¯¹åº”äºŽç‰¹å¾å€¼ $\\lambda$ çš„ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚ æ±‚è§£ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ï¼š $A\\alpha -\\lambda \\alpha=(A-\\lambda I)\\alpha=0$ é½æ¬¡æ–¹ç¨‹æœ‰éžé›¶è§£çš„å……è¦æ¡ä»¶æ˜¯ç‰¹å¾æ–¹ç¨‹ $det(A-\\lambda I)=0$ ï¼ˆè¡Œåˆ—å¼ä¸º0ï¼‰ æ ¹æ®ç‰¹å¾æ–¹ç¨‹å…ˆæ±‚è§£å‡º $\\lambda$ çš„æ‰€æœ‰å€¼ã€‚ å†æ ¹å°† $\\lambda$ ä»£å…¥é½æ¬¡æ–¹ç¨‹ï¼Œæ±‚è§£é½æ¬¡æ–¹ç¨‹çš„è§£ $\\alpha$ ï¼Œå³ä¸ºå¯¹åº” $\\lambda$ çš„ç‰¹å¾å‘é‡ã€‚ 2-DGoalï¼šfind $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ æž„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š $g\\left(w^{2}\\right)=\\left(w^{2}\\right)^{T} S w^{2}-\\alpha\\left(\\left(w^{2}\\right)^{T} w^{2}-1\\right)-\\beta\\left(\\left(w^{2}\\right)^{T} w^{1}-0\\right)$ å¯¹ $w^2$ æ±‚å¾®åˆ†ï¼Œæ‰€æ±‚ç‚¹æ»¡è¶³ç­‰å¼ï¼š $S w^{2}-\\alpha w^{2}-\\beta w^{1}=0$ å·¦ä¹˜ $(w^1) ^T$ï¼š $(w^1)^TSw^2-\\alpha (w^1)^Tw^2-\\beta(w^1)^Tw^1=0$ å·²æœ‰ï¼š $(w^1)^Tw^2=0, (w^1)^Tw^1=1$ è¯æ˜Žï¼š$ (w^1)^TSw^2=0$ $\\because (w^1)^TSw^2$ æ˜¯scalar $\\therefore (w^1)^TSw^2=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1$ $\\because S^T=S$ (åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çŸ©é˜µ) $\\because Sw^1=\\lambda_1 w^1$ $\\therefore (w^1)^TSw^2=(w^2)^TSw^1=\\lambda_1(w^2)^Tw^1=0$ $(w^1)^TSw^2-\\alpha (w^1)^Tw^2-\\beta(w^1)^Tw^1=0-\\alpha\\cdot 0-\\beta \\cdot 1=0$ $\\therefore \\beta=0$ $w^2$ æ»¡è¶³ç­‰å¼ï¼š$S w^{2}-\\alpha w^{2}=0$ å’Œ1-Dçš„æƒ…å†µç›¸åŒï¼šfind $w^2$ maximum $(w^2)^TSw^2$ $(w^2)^TSw^2=\\alpha$ $w^2$ is the eigenvector(ç‰¹å¾å‘é‡) of the covarivance matrix S corresponding to the largest eigenvalue $\\lambda_2$ . OVER! Conclusionæœ€åŽè§£å†³ä¹‹å‰çš„Q2ï¼š$(w^1)^Tw^2=0$ ? å…ˆè¯´æ˜Žä¸€ä¸‹$S$ çš„æ€§è´¨ï¼š æ˜¯å¯¹ç§°çŸ©é˜µï¼Œå¯¹åº”ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡éƒ½æ˜¯æ­£äº¤çš„ã€‚ ï¼ˆå‚è€ƒ1ï¼Œ2ï¼‰ ä¹Ÿæ˜¯åŠæ­£å®šçŸ©é˜µï¼Œå…¶ç‰¹å¾å€¼éƒ½æ˜¯éžè´Ÿçš„ã€‚ ï¼ˆå‚è€ƒ4ï¼Œ5ï¼Œ6ï¼‰ å…¶æ¬¡å…³äºŽ $W$ çš„æ€§è´¨ $ W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\\\ ...\\end{bmatrix}$ ,æ˜“å¾— $W$ æ˜¯orthogonal matrix(æ­£äº¤çŸ©é˜µ)ã€‚ æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªçº¦æŸæ¡ä»¶ï¼Œèƒ½è®©PCAçš„æœ€ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºæ±‚å…¶ç‰¹å¾å€¼çš„é—®é¢˜ã€‚ ï¼ˆå…·ä½“è§ä¸‹ä¸€å°èŠ‚ï¼šPCA-decorrelationï¼‰ å…¶æ¬¡ $z=Wx$ ï¼Œä¹Ÿå› ä¸º $W$ çš„æ­£äº¤æ€§è´¨ï¼Œè®© $z$ çš„å„ç»´åº¦ï¼ˆç‰¹å¾ï¼‰decorrelationï¼ŒåŽ»æŽ‰ç›¸å…³æ€§ï¼Œé™ç»´åŽçš„ç‰¹å¾ç›¸äº’ç‹¬ç«‹ï¼Œæ–¹ä¾¿åŽé¢generative modelçš„å‡è®¾ã€‚ $S=Cov(x)$ ä¸ºå®žå¯¹ç§°çŸ©é˜µã€‚ å®žå¯¹ç§°çŸ©é˜µçš„æ€§è´¨ï¼š$A$ æ˜¯ä¸€ä¸ªå®žå¯¹ç§°çŸ©é˜µï¼Œå¯¹äºŽäºŽ $A$ çš„ä¸åŒç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡å½¼æ­¤æ­£äº¤ã€‚ æ­£äº¤çŸ©é˜µçš„æ€§è´¨ï¼š$W^TW=WW^T=I$ $Var(z)=(w^1)^T S w^1\\geq 0$ ï¼Œæ–¹å·®ä¸€å®šå¤§äºŽç­‰äºŽ0 ã€‚ åŠæ­£å®šçŸ©é˜µçš„å®šä¹‰ï¼š å®žå¯¹ç§°çŸ©é˜µ $A$ ï¼Œå¯¹ä»»æ„éžé›¶å®žå‘é‡ $X$ ï¼Œå¦‚æžœäºŒæ¬¡åž‹ $f(X)=X^TAX\\geq0$ ï¼Œ åˆ™æœ‰å®žå¯¹ç§°çŸ©é˜µ $A$ æ˜¯åŠæ­£å®šçŸ©é˜µã€‚ åŠæ­£å®šçŸ©é˜µçš„æ€§è´¨ï¼šåŠæ­£å®šçŸ©é˜µçš„ç‰¹å¾å€¼éƒ½æ˜¯éžè´Ÿçš„ã€‚ 1-Dä¸­ï¼š Goalï¼šfind $w^1$ to maximum $(w^1)^T S w^1$ s.t.$(w^1)^Tw^1=1$ ç»“è®ºï¼š$w^1$ å°±æ˜¯åæ–¹å·®çŸ©é˜µ $S$ æœ€å¤§ç‰¹å¾å€¼ $\\lambda_1 $ å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ s.t.$(w^1)^Tw^1=1$ 2-Dä¸­ï¼š Goalï¼šfind $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ ç»“è®ºï¼š$w^2$ å°±æ˜¯åæ–¹å·®çŸ©é˜µ$S$ ç¬¬äºŒå¤§ç‰¹å¾å€¼ $\\lambda_2 $ å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ s.t.$(w^2)^Tw^2=1$ k-Dä¸­ï¼š ç»“è®ºï¼š$w$ å°±æ˜¯åæ–¹å·®çŸ©é˜µ $S$ å‰ $k$ å¤§çš„ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚s.t. $W$ æ˜¯æ­£äº¤çŸ©é˜µã€‚ PCA-decorrelation$z=Wx$ é€šè¿‡PCAæ‰¾åˆ°çš„ $W$ ï¼Œ$x$ å¾—åˆ°æ–°çš„presentation $z$ ï¼Œå¦‚ä¸‹å›¾ã€‚ å¯è§ï¼Œç»è¿‡PCAåŽï¼Œoriginal dataå˜ä¸ºdecorrelated dataï¼Œå„ç»´åº¦ï¼ˆfeatureï¼‰æ˜¯åŽ»ç›¸å…³æ€§çš„ï¼Œå³å„ç»´åº¦æ˜¯ç‹¬ç«‹çš„ï¼Œæ–¹ä¾¿generative modelçš„å‡è®¾ï¼ˆæ¯”å¦‚Gaussian distribution). $z$ æ˜¯docorrelatedï¼Œå³ $Cov(z)=D$ æ˜¯diagonal matrix(å¯¹è§’çŸ©é˜µ) è¯æ˜Žï¼š$Cov(z)=D$ is diagonal matrix $W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\\\ ...\\end{bmatrix}$ ï¼Œ$S=\\operatorname{Cov}(x)$ $\\operatorname{Cov}(z)=\\frac{1}{N} \\sum(z-\\bar{z})(z-\\bar{z})^{T}=W S W^{T}$ $=W S\\left[\\begin{array}{lll}w^{1} & \\cdots & w^{K}\\end{array}\\right]=W\\left[\\begin{array}{lll}S{w}^{1} & \\cdots & S w^{K}\\end{array}\\right]$ $=W\\left[\\lambda_{1} w^{1} \\quad \\cdots \\quad \\lambda_{K} w^{K}\\right]=\\left[\\lambda_{1} W w^{1} \\quad \\cdots \\quad \\lambda_{K} W w^{K}\\right]$ ($\\lambda$ is scalar) $=\\left[\\begin{array}{lll}\\lambda_{1} e_{1} & \\cdots & \\lambda_{K} e_{K}\\end{array}\\right]=D$ ($W$ is orthogonal matrix) PCA-Another Point of ViewMain Idea: ComponentPCAçœ‹ä½œæ˜¯ä¸€äº›basic componentçš„ç»„æˆï¼Œå¦‚ä¸‹å›¾ï¼Œæ‰‹å†™æ•°å­—éƒ½æ˜¯ä¸€äº›åŸºæœ¬ç¬”ç”»ç»„æˆçš„ï¼Œè®°åš $\\{u^1,u^2,u^3,...\\}$ å› æ­¤ï¼Œä¸‹å›¾çš„â€7â€çš„ç»„æˆä¸º $\\{u^1,u^3,u^5\\}$ æ‰€ä»¥åŽŸ28*28 vector $x$ è¡¨ç¤ºçš„å›¾åƒèƒ½è¿‘ä¼¼è¡¨ç¤ºä¸ºï¼š $$ x \\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}+\\bar{x} $$ å…¶ä¸­ $\\{u^1,u^2,u^3,...\\}$ æ˜¯compomentçš„vectorè¡¨ç¤ºï¼Œ $\\{c^1,c^2,c^3,...\\}$ æ˜¯componentçš„ç³»æ•°ï¼Œ$\\bar{x}$ æ˜¯æ‰€æœ‰imagesçš„å¹³å‡å€¼ã€‚ å› æ­¤ $\\begin{bmatrix}c_1 \\\\c_2 \\\\... \\\\ c_k \\end{bmatrix}$ ä¹Ÿèƒ½è¡¨ç¤ºä¸€ä¸ªæ•°å­—å›¾åƒã€‚ çŽ°åœ¨é—®é¢˜æ˜¯æ‰¾åˆ°è¿™äº›component $\\{u^1,u^2,u^3,...\\}$ , å†å¾—åˆ° ä»–çš„çº¿å½¢è¡¨å‡º $\\begin{bmatrix}c_1 \\\\c_2 \\\\... \\\\ c_k \\end{bmatrix}$ å°±æ˜¯æˆ‘ä»¬æƒ³å¾—åˆ°çš„better presentation. Detailè¦æ»¡è¶³ï¼š$x \\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}+\\bar{x}$ å³ï¼Œ$x -\\bar{x}\\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}$ ï¼Œç­‰å¼ä¸¤è¾¹çš„è¯¯å·®è¦å°½é‡å°ã€‚ é—®é¢˜å˜æˆï¼šæ‰¾ $\\{u^1,u^2,u^3,...\\}$ minimize the reconstruction error = $\\|(x-\\bar{x})-\\hat{x}\\|_2$ . æŸå¤±å‡½æ•°ï¼š $L=\\min _{\\left\\{u^{1}, \\ldots, u^{K}\\right\\}} \\sum\\left\\|(x-\\bar{x})-\\left(\\sum_{k=1}^{K} c_{k} u^{k}\\right)\\right\\|_{2}$ è€Œæ±‚è§£PCAçš„è¿‡ç¨‹å°±æ˜¯åœ¨minimizeæŸå¤±å‡½æ•° $L$ ï¼ŒPCAä¸­æ±‚è§£å‡ºçš„ $\\{w^1,w^2,...,w^K\\}$ å°±æ˜¯è¿™é‡Œçš„component $\\{u^1,u^2,...,u^K\\}$ .(Proof è§Bisho, Chapter 12.1.2) *Goal: minimize the reconstruction error = $\\|(x-\\bar{x})-\\hat{x}\\|_2$ * $x -\\bar{x}\\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}$ æ¯ä¸ªsample: $\\left\\{ \\begin{matrix} x^{1}-\\bar{x} \\approx c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\\cdots \\\\ x^{2}-\\bar{x} \\approx c_{1}^{2} u^{1}+c_{2}^{2} u^{2}+\\cdots \\\\x^{3}-\\bar{x} \\approx c_{1}^{3} u^{1}+c_{2}^{3} u^{2}+\\cdots \\\\ ...\\end{matrix} \\right.$ ä¸‹å›¾ä¸­ $X=x-\\bar{x}$ çŸ©é˜µçš„ç¬¬ä¸€åˆ—éƒ½å’Œä¸Šé¢çš„ $x^1-\\bar{x}$ å¯¹åº”ï¼š è€Œä¸Šé¢çš„ $c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\\cdots$ å’Œä¸‹å›¾çš„componentçŸ©é˜µä¹˜ç³»æ•°çŸ©é˜µçš„ç¬¬ä¸€åˆ—å¯¹åº”ï¼š å› æ­¤ï¼Œæ˜¯è¦è®©ä¸‹å›¾çŸ©é˜µçš„ç»“æžœ minimize errorï¼š å¦‚ä½•æ±‚è§£: SVDçŸ©é˜µåˆ†è§£-å…¶å®žå°±æ˜¯æœ€å¤§è¿‘ä¼¼åˆ†è§£ï¼ˆæŒ–å‘ï¼‰ SVDèƒ½å°†ä¸€ä¸ªä»»æ„çš„çŸ©é˜µï¼Œåˆ†è§£ä¸ºä¸‹é¢ä¸‰ä¸ªçŸ©é˜µçš„ä¹˜ç§¯ã€‚ $X = U\\Sigma V$ $U,V$ éƒ½æ˜¯orthogonal matrixï¼Œ$\\Sigma$ æ˜¯diagonal matrixã€‚ ç»„æˆ$U$ (M*K) çš„Kä¸ªåˆ—å‘é‡æ˜¯ $XX^T$ çŸ©é˜µçš„å‰Kå¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ ç»„æˆ $V$ (K*N)çš„Kä¸ªè¡Œå‘é‡æ˜¯ $X^TX$ çŸ©é˜µçš„å‰Kå¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ $XX^T$ å’Œ $X^TX$ çš„ç‰¹å¾å€¼ç›¸åŒ $\\Sigma$ çš„å¯¹è§’å€¼ $\\sigma_i=\\sqrt{\\lambda_i}$ è§£ï¼š$U$ çŸ©é˜µä½œä¸º componentçŸ©é˜µï¼Œ $\\Sigma V$ ä¹˜åœ¨ä¸€èµ·ä½œä¸ºç³»æ•°çŸ©é˜µã€‚ $U=\\{u^1,u^2,u^3,...\\}$ çŸ©é˜µæ˜¯$XX^T$ çš„ç‰¹å¾å‘é‡ç»„æˆæ­£äº¤çŸ©é˜µã€‚ è€ŒPCAçš„è§£ $W^T=\\{w^1,w^2,...,w^K\\}$ ä¹Ÿæ˜¯ç‰¹å¾å‘é‡ç»„æˆçš„æ­£äº¤çŸ©é˜µã€‚ æ‰€ä»¥å’ŒPCAçš„å…³ç³»ï¼š$U$ çŸ©é˜µæ˜¯ $XX^T=Cov(x)$ çš„ç‰¹å¾å‘é‡ï¼Œæ‰€ä»¥$U$ çŸ©é˜µå°±æ˜¯PCAçš„è§£ã€‚ PCA-NNï¼šAutoencoderä¸Šæ–‡è¯´åˆ°æ±‚è§£PCAçš„è§£ $\\{w^1,w^2,...,w^K\\}$ å°±æ˜¯åœ¨æœ€å°åŒ–restruction error $x -\\bar{x}\\approx \\sum_{k=1}^K c_kw^k$ . ä¸¤è€…çš„è”ç³»å°±æ˜¯PCAçš„è§£ $\\{w^1,w^2,...,w^K\\}$ å°±æ˜¯component $\\{u^1,u^2,u^3,...\\}$ ,ä¸”PCAçš„è¡¨ç¤ºæ˜¯ $z$ å¯¹åº”è¿™é‡Œçš„ $c_k$ (ç¬¬kä¸ªimageçš„è¡¨ç¤ºï¼‰. PCAè§†è§’ï¼š $z=c_k=(x-\\bar{x})\\cdot w^k$ PCA looks like a neural network with one hidden layer(linear activation function)ã€‚ æŠŠPCAè§†è§’çœ‹ä½œä¸€ä¸ªNNï¼Œå¦‚ä¸‹å›¾ï¼Œå…¶hidden layerçš„æ¿€æ´»å‡½æ•°æ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚ å†çœ‹componentè§†è§’ï¼š $\\hat{x}=\\sum_{k=1}^K c_kw^k\\approx x-\\bar{x}$ PCAå°±æž„æˆäº†ä¸‹é¢çš„NNï¼Œhidden layerå¯ä»¥æ˜¯deepï¼Œè¿™å°±æ˜¯autoencoder(åŽé¢çš„åšå®¢ä¼šå†è¯¦ç»†è®²)ã€‚ ç”¨Gradient Descentå¯¹è¾“å…¥è¾“å‡ºåšminimize errorï¼Œhidden layerçš„è¾“å‡º $c$ å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ç¼–ç ï¼ˆé™ç»´åŽçš„ç¼–ç ï¼‰ã€‚ Qï¼šç”¨PCAæ±‚å‡ºçš„ç»“æžœå’Œç”¨Gradient Descentè®­ç»ƒNNçš„ç»“æžœä¸€æ ·å—ï¼Ÿ Aï¼šå½“ç„¶ä¸ä¸€æ ·ï¼ŒPCAçš„ $w$ éƒ½æ˜¯æ­£äº¤çš„ï¼Œè€ŒNNçš„ç»“æžœæ˜¯gradient descentè¿­ä»£å‡ºæ¥çš„ï¼Œå¹¶ä¸”è¯¥ç»“æžœè¿˜ä¼šäºŽåˆå€¼æœ‰å…³ã€‚ Qï¼šæœ‰äº†PCAï¼Œä¸ºä»€ä¹ˆè¿˜è¦ç”¨NNå‘¢ï¼Ÿ Aï¼šå› ä¸ºPCAåªèƒ½å¤„ç†linearçš„æƒ…å†µï¼Œå¯¹å‰æ–‡é‚£ç§é«˜ç»´çš„éžçº¿å½¢çš„æ— æ³•å¤„ç†ï¼Œè€ŒNNå¯ä»¥æ˜¯deepçš„ï¼Œèƒ½è¾ƒå¥½å¤„ç†éžçº¿å½¢çš„æƒ…å†µã€‚ tips: how many components?æ¯”å¦‚åœ¨å¯¹Pokemonè¿›è¡ŒPCAæ—¶ï¼Œæœ‰å…­ä¸ªfeaturesï¼Œå¦‚ä½•ç¡®å®šprinciple componentçš„æ•°ç›®ï¼Ÿ å¾€å¾€åœ¨å®žé™…æ“ä½œä¸­ï¼Œä¼šå¯¹æ¯ä¸ªcomponentè®¡ç®—ä¸€ä¸ªratioï¼Œå¦‚å›¾ä¸­çš„å…¬å¼ï¼š å› ä¸ºæ¯ä¸€ä¸ªcomponentå¯¹åº”ä¸€ä¸ªeigenvectorï¼Œæ¯ä¸ªeigenvectorå¯¹åº”ä¸€ä¸ªeigenvalueï¼Œè€Œè¿™ä¸ªeigenvalueçš„å€¼ä»£è¡¨äº†åœ¨è¿™ä¸ªcomponentçš„ç»´åº¦çš„varianceæœ‰å¤šå¤§ï¼Œè¶Šå¤§å½“ç„¶èƒ½æ›´å¥½çš„è¡¨ç¤ºã€‚ å› æ­¤è®¡ç®—eigenvalueçš„ratioï¼Œæ¥æ‰¾å‡ºåˆ†å¸ƒè¾ƒå¤§çš„componentä½œä¸ºä¸»æˆåˆ†ã€‚ More About PCAå¦‚æžœå¯¹MNISTåšPCAåˆ†æžï¼Œç»“æžœå¦‚ä¸‹å›¾ï¼Œä¼šå‘çŽ°ä¸‹é¢eigen-digitsè¿™äº›å¹¶ä¸åƒæ•°å­—çš„æŸä¸ªç»„æˆéƒ¨åˆ†ï¼š åŒæ ·ï¼Œå¯¹faceåšPCAåˆ†æžï¼Œç»“æžœä¸‹å›¾ï¼š ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ åœ¨MNISTä¸­ï¼Œä¸€å¼ imageçš„è¡¨ç¤ºå¦‚ä¸‹å›¾ï¼š å…¶ä¸­ï¼Œ$\\alpha$ å¯ä»¥æ˜¯ä»»æ„å®žæ•°ï¼Œé‚£ä¹ˆå°±æœ‰æ­£æœ‰è´Ÿï¼Œæ‰€ä»¥PCAçš„è§£åŒ…å«äº†ä¸€äº›çœŸæ­£componentçš„adding up and subtractingï¼Œæ‰€ä»¥MNISTçš„è§£ä¸åƒè¿™äº›æ•°å­—çš„ä¸€éƒ¨åˆ†ã€‚ å¦‚æžœæƒ³å¾—åˆ°çš„è§£çœ‹èµ·æ¥åƒçœŸæ­£çš„componentï¼Œå¯ä»¥è§„å®šå›¾åƒåªèƒ½æ˜¯åŠ ï¼Œå³ $\\alpha$ éƒ½æ˜¯éžè´Ÿçš„ã€‚ Non-negative matrix factorization(NMF) Forcing $\\alpha$ be non-negative: additive combination Forcing $w$ be non-negative: components more like â€œparts of digitsâ€ Weakness of PCA PCAæ˜¯unsupervisedï¼Œå› æ­¤å¯èƒ½ä¸èƒ½åŒºåˆ†æœ¬æ¥æ˜¯ä¸¤ä¸ªç±»åˆ«çš„ä¸œè¥¿ã€‚ å¦‚å›¾ï¼ŒPCAçš„ç»“æžœå¯èƒ½æ˜¯ä¸Šå›¾çš„ç»´åº¦æ–¹å‘ï¼Œä½†å¦‚æžœå¼•å…¥labeled dataï¼Œæ›´å¥½çš„è¡¨è¾¾åº”è¯¥æŒ‰ç…§ä¸‹å›¾LDAçš„ç»´åº¦æ–¹å‘ã€‚ LDA (Linear Discriminant Analysis) æ˜¯ä¸€ç§supervisedçš„åˆ†æžæ–¹æ³•ã€‚ PCAæ˜¯Linearçš„ï¼Œå‰æ–‡å·²ç»æåŠè¿‡ï¼Œé™¤äº†å¯ä»¥ç”¨NNçš„æ–¹å¼ä¹Ÿæœ‰å¾ˆå¤šå…¶ä»–çš„non-linearçš„è§£æ³•ã€‚ Reference HACçš„ç®—æ³•ç»†èŠ‚å¾…è¡¥å……å®Œå–„ï¼šhttps://zhuanlan.zhihu.com/p/34168766 PCA: Bishop, Chapter12. çº¿ä»£çŸ¥è¯†ï¼šç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡ã€å®žå¯¹ç§°çŸ©é˜µç­‰ï¼š æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼šBishop, Appendix E çŸ©é˜µå¾®åˆ†ï¼šhttps://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors Proof-PCAçš„è¿‡ç¨‹å°±æ˜¯åœ¨minimizeæŸå¤±å‡½æ•° $L$ :Bisho, Chapter 12.1.2 SVDï¼š https://www.cnblogs.com/pinard/p/6251584.html https://www.youtube.com/watch?v=rYz83XPxiZo NMFï¼šNon-negative matrix factorization LDAï¼šLinear Discriminant Analysis","link":"/2020/10/30/unsupervised-learning-pca/"},{"title":"ã€Œæœºå™¨å­¦ä¹ -æŽå®æ¯…ã€:Unsupervised Learningï¼šWord Embedding","text":"è¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯ä»‹ç»ä¸€ç§æ— ç›‘ç£å­¦ä¹ â€”â€”Word Embeddingï¼ˆè¯åµŒå…¥ï¼‰ã€‚ æ–‡ç« å¼€ç¯‡ä»‹ç»äº†wordç¼–ç çš„1-of-N encodingæ–¹å¼å’Œword classæ–¹å¼ï¼Œä½†è¿™ä¸¤ç§æ–¹å¼å¾—åˆ°çš„å•è¯å‘é‡è¡¨ç¤ºéƒ½ä¸èƒ½å¾ˆå¥½è¡¨è¾¾å•è¯çš„è¯­ä¹‰å’Œå•è¯ä¹‹é—´çš„è¯­ä¹‰è”ç³»ã€‚ Word Embeddingå¯ä»¥å¾ˆå¥½çš„è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ Word Embeddingæœ‰count basedå’Œprediction basedä¸¤ç§æ–¹æ³•ã€‚æ–‡ç« ä¸»è¦ä»‹ç»äº†prediction basedçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¦‚ä½•predict the word vector? ä¸ºä»€ä¹ˆè¿™æ ·çš„æ¨¡åž‹worksï¼Ÿä»‹ç»äº†prediction basedçš„å˜ä½“ï¼›è¯¦ç»†é˜è¿°äº†è¯¥æ¨¡åž‹ä¸­sharing parametersçš„åšæ³•å’Œå…¶å¿…è¦æ€§ã€‚ æ–‡ç« æœ€åŽç®€å•åˆ—ä¸¾äº†word embeddingçš„ç›¸å…³åº”ç”¨ï¼ŒåŒ…æ‹¬multi-lingual embedding, multi-domain embedding, document embedding ç­‰ã€‚ Word to Vectorå¦‚ä½•æŠŠwordè½¬æ¢ä¸ºvector? 1-of-N Encodingç¬¬ä¸€ç§æ–¹æ³•æ˜¯1-of-N Encodingï¼š Vectorçš„ç»´åº¦æ˜¯å•è¯æ€»æ•°ï¼Œæ¯ä¸€ç»´åº¦éƒ½ä»£è¡¨ä¸€ä¸ªå•è¯ã€‚ 1-of-N Encodingçš„æ–¹æ³•ç®€å•ï¼Œä½†è¿™ç§å‘é‡çš„è¡¨ç¤ºæ–¹å¼not imformativeï¼Œå³å‘é‡è¡¨ç¤ºä¸èƒ½ä½“çŽ°å•è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚ Word Classå¯¹1-of-N Encodingæ–¹å¼æ”¹è¿›ï¼ŒWord Classé‡‡ç”¨èšç±»clusterçš„æ–¹å¼ï¼Œæ ¹æ®ç±»åˆ«è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ã€‚ ä½†è¿™ç§äººä¸ºåˆ†ç±»çš„æ–¹å¼ï¼Œä¿¡æ¯æ˜¯ä¼šéƒ¨åˆ†ä¸¢å¤±çš„ï¼Œå³å…‰åšclusteringæ˜¯ä¸å¤Ÿçš„ï¼Œä¼šä¸¢å¤±å•è¯çš„éƒ¨åˆ†ä¿¡æ¯ã€‚ Word Embeddingç¬¬ä¸‰ç§æ–¹å¼æ˜¯Word Embeddingã€‚ï¼ˆè¯åµŒå…¥ï¼‰ Word Embedding: Machine learns the meaning of words from reading a lot of documents without supervision. Word Embeddingï¼Œæœºå™¨é€šè¿‡é˜…è¯»å¤§é‡æ–‡ç« å­¦ä¹ å•è¯çš„å«ä¹‰ï¼Œç”¨vectorçš„å½¢å¼è¡¨ç¤ºå•è¯çš„è¯­ä¹‰ã€‚è®­ç»ƒæ—¶åªéœ€è¦ç»™æœºå™¨å¤§é‡æ–‡ç« ï¼Œä¸éœ€è¦labelï¼Œå› æ­¤æ˜¯æ— ç›‘ç£å­¦ä¹ ã€‚ Word Embeddingå¦‚ä½•åšWord Embeddingå‘¢ï¼Ÿ auto-encoderï¼Ÿèƒ½å¦ç”¨auto-encoderçš„æ–¹å¼æ¥åšè¯åµŒå…¥å‘¢ï¼Ÿ å³ç”¨1-of-N encodingçš„æ–¹å¼å¯¹å•è¯ç¼–ç ï¼Œä½œä¸ºè®­ç»ƒçš„è¾“å…¥å’Œè¾“å‡ºã€‚ word2vecæ—¶ï¼ŒæŠŠmodelä¸­çš„æŸä¸€hidden layerçš„è¾“å‡ºä½œä¸ºè¯¥å•è¯çš„å‘é‡è¡¨ç¤ºã€‚ è¿™ç§æ–¹å¼æ˜¯ä¸å¯ä»¥çš„ï¼Œä¸å¯ä»¥ç”¨auto-encoderã€‚å› ä¸ºauto-encoderä¸èƒ½å­¦åˆ°informativeçš„ä¿¡æ¯ï¼Œå³ç”¨auto-encoderè¡¨ç¤ºçš„å‘é‡ä¸èƒ½è¡¨è¾¾wordçš„è¯­ä¹‰ã€‚ Exploit the ContextA word can be understood by its context. æ‰€ä»¥Word Embeddingå¯ä»¥åˆ©ç”¨ä¸Šä¸‹æ–‡æ¥å­¦ä¹ wordçš„è¯­ä¹‰ã€‚ å¦‚ä½•åˆ©ç”¨å•è¯çš„ä¸Šä¸‹æ–‡æ¥å­¦ä¹ å‘¢ï¼Ÿ Count based å¦‚æžœä¸¤ä¸ªå•è¯ $w_i$ å’Œ $w_j$ åœ¨æ–‡ç« ä¸­ç»å¸¸åŒæ—¶å‡ºçŽ°ï¼Œé‚£ä¹ˆ $V(w_i)$ ( $w_i$ çš„å‘é‡è¡¨ç¤º)å’Œ $V(w_j)$ çš„å‘é‡è¡¨ç¤ºä¼šå¾ˆclose. E.g. Glove Vector: https://nlp.stanford.edu/projects/glove/ GloVeçš„è¡¨ç¤ºæ³•æœ‰ä¸¤ä¸ªäº®ç‚¹ï¼š Nearest neighborsï¼švectorsä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆæˆ–è€…ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰èƒ½è¾ƒå¥½è¡¨ç¤ºwordsä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚ Linear substructuresï¼šç”¨GloVeæ–¹æ³•è¡¨ç¤ºçš„vectorsæœ‰æœ‰è¶£çš„çº¿æ€§å­ç»“æž„ã€‚ Prediction based ä½¿ç”¨é¢„æµ‹çš„æ–¹å¼æ¥è¡¨ç¤ºã€‚ Prediction basedHow to predictï¼Ÿprediction basedçš„æ–¹æ³•æ˜¯ç”¨å‰ä¸€ä¸ªå•è¯æ¥é¢„æµ‹å½“å‰å•è¯ã€‚ è®­ç»ƒæ—¶ï¼š $w_{i-1}$ çš„1-of-N encodingç¼–ç ä½œä¸ºè¾“å…¥ï¼Œ$w_i$ çš„1-of-N encodingçš„ç¼–ç ä½œä¸ºè¾“å‡ºã€‚ NNå¦‚ä¸Šå›¾ï¼Œ$w_{i-1}$ çš„1-of-N encodingç¼–ç ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºçš„vectorè¡¨ç¤ºä¸‹ä¸€ä¸ªå•è¯æ˜¯ $w_i$ çš„æ¦‚çŽ‡ã€‚ word2vec : $w_{i-1}$ çš„1-of-N encodingç¼–ç ä½œä¸ºNNçš„è¾“å…¥ï¼Œ$w_i$ çš„å‘é‡è¡¨ç¤ºä¸ºç¬¬ä¸€ä¸ªhidden layerçš„neuronsçš„è¾“å…¥ $z$ ã€‚ Why it works?ç›´è§‰çš„è§£é‡Šä»–ä¸ºä»€ä¹ˆèƒ½workã€‚ å¦‚ä¸Šå›¾ï¼Œè”¡è‹±æ–‡ å®£èª“å°±èŒ å’Œ é©¬è‹±ä¹ å®£èª“å°±èŒï¼Œè™½ç„¶ $w_{i-1}$ ä¸åŒï¼Œä½†NNçš„è¾“å‡ºä¸­ï¼Œâ€œå®£èª“å°±èŒâ€çš„æ¦‚çŽ‡åº”è¯¥æœ€å¤§ã€‚ å³hidden layerså¿…é¡»æŠŠä¸åŒçš„ $w_{i-1}$ projectåˆ°ç›¸åŒçš„spaceï¼Œè¦æ±‚hidden layerçš„inputæ˜¯ç›¸è¿‘çš„ï¼ŒNNçš„è¾“å‡ºæ‰æ˜¯ç›¸è¿‘çš„ã€‚ Prediction-based ï¼šVarious Architectureå› ä¸ºä¸€ä¸ªå•è¯çš„ä¸‹ä¸€ä¸ªå•è¯èŒƒå›´éžå¸¸å¤§ï¼Œæ‰€ä»¥ä½¿ç”¨å‰ä¸€ä¸ªå•è¯é¢„æµ‹å½“å‰å•è¯çš„æ–¹æ³•ï¼Œperformanceæ˜¯è¾ƒå·®çš„ã€‚ å› æ­¤å¸¸å¸¸ä¼šä½¿ç”¨å¤šä¸ªå•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼ŒNNçš„è¾“å…¥æ˜¯å¤šä¸ªå•è¯è¿žæŽ¥åœ¨ä¸€èµ·ç»„æˆçš„å‘é‡ï¼Œä¸€èˆ¬NNçš„è¾“å…¥è‡³å°‘ä¸º10ä¸ªå•è¯ï¼Œword embeddingçš„performanceè¾ƒå¥½ã€‚ é™¤äº†ä½¿ç”¨å¤šä¸ªå•è¯çš„æ–¹æ³•ï¼Œprediction-basedçš„æ–¹æ³•è¿˜ç”¨ä¸¤ç§å˜ä½“ç»“æž„ã€‚ Continuous bag of word (CBOW) model: predicting the word given its context. ä½¿ç”¨å•è¯çš„å‰åŽæ–‡ï¼ˆå‰ä¸€ä¸ªå•è¯å’ŒåŽä¸€ä¸ªå•è¯ï¼‰æ¥é¢„æµ‹å½“å‰å•è¯ã€‚ Skip-gram: predicting the context given a word. ä½¿ç”¨ä¸­é—´å•è¯æ¥é¢„æµ‹å•è¯çš„å‰ä¸€ä¸ªå•è¯å’ŒåŽä¸€ä¸ªå•è¯ã€‚ Sharing Parametersä½¿ç”¨å¤šä¸ªå•è¯ä½œä¸ºNNçš„è¾“å…¥ï¼Œæé«˜äº†word embeddingçš„performanceï¼Œä½†ä¹Ÿå¤§å¹…å¢žåŠ äº†æ¨¡åž‹è®­ç»ƒçš„å‚æ•°æ•°é‡ã€‚ ä½¿ç”¨sharing parametersï¼ˆå…±äº«å‚æ•°ï¼‰èƒ½å¤§é‡å‡å°‘æ¨¡åž‹çš„å‚æ•°æ•°é‡ã€‚ å¦‚ä¸Šå›¾ï¼Œè¾“å…¥å•è¯è¿žæŽ¥åˆ°neuronsçš„æƒé‡åº”è¯¥æ˜¯ç›¸åŒçš„ã€‚ é™¤äº†èƒ½å‡å°‘å‚æ•°ï¼Œsharing parametersä¹Ÿæ˜¯å¿…è¦çš„ã€‚å¦åˆ™ï¼Œå¦‚æžœNNçš„è¾“å…¥çš„å•è¯é¡ºåºäº¤æ¢ï¼Œé‚£ä¹ˆå¾—åˆ°çš„å•è¯å‘é‡æ˜¯ä¸åŒçš„ã€‚ How to train sharing parameters? å‡è®¾ä¸¤ä¸ªå•è¯ç›¸åŒç»´åº¦è¿žæŽ¥åˆ°neuronçš„weightæ˜¯ $w_i,w_j$ ï¼Œåœ¨è®­ç»ƒä¸­ï¼Œå¦‚ä½•è®© $w_i=w_j$ ? Given the same initialization.(ç›¸åŒçš„åˆå§‹åŒ–) åŽŸæ¥çš„å‚æ•°æ›´æ–°ï¼š$$w_i \\longleftarrow w_i - \\frac{\\partial C}{\\partial w_i} \\w_j \\longleftarrow w_j - \\frac{\\partial C}{\\partial w_j}$$è™½ç„¶æœ‰ç›¸åŒçš„åˆå§‹åŒ–ï¼Œä½†åœ¨Backpropagationæ±‚åå¾®åˆ†æ—¶ï¼Œ$\\frac{\\partial C}{\\partial w_i}$ å’Œ $\\frac{\\partial C}{\\partial w_j}$ ä¸ä¸€æ ·ï¼Œé‚£ä¹ˆå‚æ•° $w_i$ å’Œ $w_j$ æ›´æ–°ä¸€æ¬¡åŽå°±ä¸åŒäº†ã€‚ åœ¨è®­ç»ƒsharing parametersçš„å‚æ•°æ›´æ–°ï¼š$$w_i \\longleftarrow w_i - \\frac{\\partial C}{\\partial w_i} -\\frac{\\partial C}{\\partial w_j}\\w_j \\longleftarrow w_j - \\frac{\\partial C}{\\partial w_j}-\\frac{\\partial C}{\\partial w_i}$$è¿™æ ·æ›´æ–°åŽï¼Œ$w_i$ å’Œ $w_j$ ä»ä¿æŒä¸€è‡´ã€‚å¦‚æžœæœ‰å¤šä¸ªå•è¯ï¼Œäº¦ç„¶ã€‚ Word2Vec åœ¨word2vecæ—¶ï¼Œæ ¹æ®sharing parametersçš„æ€§è´¨ï¼Œè®¡ç®—å•è¯çš„å‘é‡è¡¨ç¤ºæ—¶ï¼Œå¯ä»¥ç®€åŒ–è¿ç®—ã€‚ å¦‚ä¸Šå›¾ï¼Œç”¨å‰æ–‡å•è¯ $x_{i-1},x_{i-2}$ è¡¨ç¤ºå•è¯ $x_i$ çš„å‘é‡è¡¨ç¤º $z=W_1x_{i-2}+W_2x_{i-1}=W(x_{i-2}+x_{i-1})$ . å…¶ä¸­ $x_{i-1},x_{i-2}$ çš„ç»´åº¦æ˜¯|V|ï¼Œ$x_i$ çš„å‘é‡è¡¨ç¤º $z$ çš„ç»´åº¦æ˜¯ |Z|ï¼Œ$W_1=W_2=W$ çš„ç»´åº¦ä¸º|Z|*|V|ã€‚ Advantages of Word EmbeddingWord Embeddingèƒ½å¾—åˆ°ä¸€äº›æœ‰è¶£çš„ç‰¹æ€§ã€‚ å‘é‡ä¹‹é—´æœ‰è¶£çš„çº¿æ€§å­ç»“æž„ ç›¸è¿‘çš„å‘é‡æœ‰ç›¸è¿‘çš„è¯­ä¹‰ å‘é‡ä¹‹é—´è¡¨ç¤ºçš„è¯­ä¹‰ç‰¹æ€§ å…¶ä»–åº”ç”¨Multi-lingual Embeddingï¼šå®žçŽ°ç¿»è¯‘ ä¸åŒè¯­è¨€ä¹‹é—´åˆ†å¼€è®­ç»ƒï¼Œè®­ç»ƒå‡ºçš„ä¸åŒè¯­è¨€æ‰€å¯¹åº”è¯æ±‡çš„å‘é‡è¡¨ç¤ºè‚¯å®šä¸åŒï¼Œå†å°†å¯¹åº”è¯æ±‡çš„å‘é‡projectåˆ°åŒä¸€ç‚¹ï¼Œå³å®žçŽ°äº†ç¿»è¯‘ã€‚ Multi-domain Embeddingè¿˜å¯ä»¥åšå½±åƒåµŒå…¥ã€‚ Document Embeddingï¼šå°†æ–‡ä»¶è¡¨ç¤ºä¸ºä¸€ä¸ªå‘é‡ Bag of Word: ç”¨Bag-of-wordçš„æ–¹å¼ç¼–ç æ–‡ä»¶ï¼Œå†å®žçŽ°semantic embeddingã€‚å¾—åˆ°çš„æ–‡ä»¶è¡¨ç¤ºå‘é‡å¯ä»¥è¡¨ç¤ºæ–‡ä»¶çš„è¯­ä¹‰ä¸»é¢˜ã€‚ Beyond Bag of Word: å¥å­ä¸­å•è¯çš„é¡ºåºä¹Ÿå¾ˆå¤§ç¨‹åº¦å½±å“å¥å­çš„è¯­ä¹‰ã€‚ å› æ­¤ï¼Œä¸‹å›¾çš„ä¸¤å¥è¯æœ‰ç›¸åŒçš„bag-of-wordï¼Œä½†è¡¨è¾¾çš„å«ä¹‰å®Œå…¨ç›¸åã€‚ å…³äºŽbeyond bag of wordçš„ç›¸å…³å·¥ä½œå‚è€ƒreference 2. Reference GloVe: Global Vectors for Word Representation beyond bag of word:","link":"/2020/10/10/unsupervised-learning-word-embedding/"},{"title":"ã€ŒTools-VSCodeã€:Remote SSH-è·³æ¿æœºè®¾ç½®","text":"VSCodeå°±æ˜¯æœ€æ£’çš„IDEï¼ æœ€è¿‘é‡åˆ°ä¸€ä¸ªRemote SSHçš„é—®é¢˜ï¼šæƒ³è¦è¿žæŽ¥æ ¡å†…çš„æœåŠ¡å™¨ï¼Œå¿…é¡»ç»è¿‡ä¸¤ä¸ªè·³æ¿æœºã€‚ å³éœ€è¦ä¸‰æ¬¡sshï¼Œæ‰èƒ½è¿žæŽ¥åˆ°ç›®æ ‡æœåŠ¡å™¨Dï¼šAâž¡ï¸Bâž¡ï¸Câž¡ï¸D Terminalï¼šå¦‚ä½•æ— ç—›å…å¯†ç™»å½•æ ¡å†…æœåŠ¡å™¨ ï¼ˆéœ€è¦è¾“å…¥ä¸‰æ¬¡sshå‘½ä»¤ï¼‰ VSCodeï¼šå¦‚ä½•æ— ç—›ç”¨VSCodeè¿žæŽ¥è¿œç¨‹æœåŠ¡å™¨ä»¥è¿›è¡Œå¼€å‘ï¼ˆä¸€é”®å¼ï¼‰ Terminalå¦‚æžœåªéœ€è¦åœ¨terminalä¸­å…å¯†è¿žæŽ¥è¿œç¨‹æœåŠ¡å™¨ï¼Œæ­¥éª¤æ¯”è¾ƒç›´è§‚ï¼Œä½†ä»éœ€è¦ä¸‰æ¬¡è¾“å…¥sshå‘½ä»¤ï¼š Aã€Bã€Cä¸»æœºé€šè¿‡ssh-keygen ç”Ÿæˆå…¬ç§é’¥ å…¬é’¥è®¤è¯é“¾ï¼š Açš„å…¬é’¥å†…å®¹åŠ å…¥Bä¸»æœºä¸‹çš„~/.ssh/authorized_keys ä¸­ Bçš„å…¬é’¥å†…å®¹åŠ å…¥Cä¸»æœºä¸‹çš„~/.ssh/authorized_keys ä¸­ Cçš„å…¬é’¥å†…å®¹åŠ å…¥Dä¸»æœºä¸‹çš„~/.ssh/authorized_keys ä¸­ Done VSCodeåœ¨vscodeå®˜æ–¹åšå®¢: Remote SSH: Tips and Tricks æœ‰æåˆ°ï¼Œé…ç½®æ–‡ä»¶çš„ProxyCommandè®¾ç½®ï¼Œå¯ä»¥è®©remote sshé€šè¿‡ä¸€ä¸ªè·³æ¿æœºå†è¿žæŽ¥åˆ°å—ä¿æŠ¤çš„ç›®æ ‡ä¸»æœºã€‚ To use a jump-box setup with the Remote - SSH extension, you can use the ProxyCommand config option. é€šè¿‡ä¸€ä¸ªè·³æ¿æœºAâž¡ï¸Bâž¡ï¸Cï¼šAæ˜¯ç”¨æˆ·ä¸»æœºï¼ŒBæ˜¯è·³æ¿æœºï¼ŒCæ˜¯ç›®æ ‡ä¸»æœº æŠŠAçš„å…¬é’¥å†…å®¹åŠ å…¥åˆ°Bä¸»æœºä¸‹çš„~/.ssh/authorized_keys å’ŒCä¸»æœºä¸‹çš„~/.ssh/authorized_keys æ³¨æ„ï¼šè¿™é‡Œå’Œä¸Šé¢terminalçš„æ–¹å¼ä¸åŒã€‚ä¸Šé¢æ˜¯é“¾å¼éªŒè¯ï¼Œè¿™é‡Œæ˜¯ä¸­å¿ƒå¼çš„éªŒè¯ã€‚ï¼ˆæ„ä¼šå°±å¥½ï¼Œä¹±å–çš„åå­—ï¼‰å¦‚æžœä¸åŠ å…¥ï¼ŒVSCodeä¸­æ¯æ¬¡è·³è½¬éƒ½è¦è¾“å¯†ç ã€‚ï¼ˆæ‡’zzzï¼Œå¹¶ä¸æƒ³ï¼‰ Qï¼šä¸ºä»€ä¹ˆè¦æŠŠAçš„å…¬é’¥åŠ å…¥åˆ°Bä¸»æœºå’ŒCä¸»æœºçš„å·²è®¤è¯å¯†é’¥ä¸­ï¼ŸåŽŸå› å¦‚ä¸‹ã€‚ This configuration will open a background SSH connection to the jump box, and then connect via a private IP address to the target. Aä¸»æœºä¸‹ç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ï¼šb_private å’Œc_private Aä¸»æœºï¼š ~/.ssh/b_private : Bè·³æ¿æœºçš„ç§é’¥å†…å®¹ã€‚ï¼ˆæ³¨æ„æ˜¯ç§é’¥ï¼‰ ~/.ssh/c_private : Cè·³æ¿æœºçš„ç§é’¥å†…å®¹ã€‚ ç¼–è¾‘VSCode sshçš„é…ç½®æ–‡ä»¶~/.ssh/config 12345678910111213# B is Jump Box with public IP addressHost B HostName &lt;IP address of B&gt; User fred IdentityFile ~/.ssh/b_private# C is Target Machine with private IP addressHost C HostName &lt;IP address of C&gt; User fred Port 6000 IdentityFile ~/.ssh/c_private ProxyCommand ssh -q -W %h:%p B IdentityFile: ç§é’¥æ–‡ä»¶çš„è·¯å¾„ Doneï¼šåœ¨VSCodeä¸­ä¸€é”®è¿žæŽ¥è¿œç¨‹æœåŠ¡å™¨Cã€‚ é€šè¿‡å¤šä¸ªè·³æ¿æœºç”±æ­¤å¯ä»¥å®žçŽ°å¤šæ¬¡è·³è½¬ã€‚ æ¯”å¦‚ç»è¿‡ä¸¤æ¬¡è·³è½¬Aâž¡ï¸Bâž¡ï¸Câž¡ï¸Dï¼šAæ˜¯ç”¨æˆ·ä¸»æœºï¼ŒBã€Cæ˜¯è·³æ¿æœºï¼ŒDæ˜¯ç›®æ ‡ä¸»æœºã€‚ æŠŠAçš„å…¬é’¥å†…å®¹åŠ å…¥åˆ°Bã€Cã€Dä¸»æœºä¸‹çš„~/.ssh/authorized_keys Aä¸»æœºä¸‹ç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ï¼šb_private ã€c_private å’Œd_private Aä¸»æœºï¼š ~/.ssh/b_private : Bè·³æ¿æœºçš„ç§é’¥å†…å®¹ã€‚ï¼ˆæ³¨æ„æ˜¯ç§é’¥ï¼‰ ~/.ssh/c_private : Cè·³æ¿æœºçš„ç§é’¥å†…å®¹ã€‚ ~/.ssh/d_private : Dè·³æ¿æœºçš„ç§é’¥å†…å®¹ã€‚ ç¼–è¾‘VSCode sshçš„é…ç½®æ–‡ä»¶~/.ssh/config 1234567891011121314151617181920# B is Jump Box with public IP addressHost B HostName &lt;IP address of B&gt; User fred IdentityFile ~/.ssh/b_private# C is Jump Box with private IP addressHost C HostName &lt;IP address of C&gt; User fred Port 6000 IdentityFile ~/.ssh/c_private ProxyCommand ssh -q -W %h:%p B# D is Target Machine with private IP addressHost D HostName &lt;IP address of D&gt; User abc IdentityFile ~/.ssh/d_private ProxyCommand ssh -q -W %h:%p C Doneï¼šåœ¨VSCodeä¸­ä¸€é”®è¿žæŽ¥è¿œç¨‹æœåŠ¡å™¨Dã€‚","link":"/2021/10/18/vscode-remote-ssh/"},{"title":"ã€ŒCryptography-ZKPã€: Lec4-SNARKs via IP","text":"In this series, I will learn Zero Knowledge Proofs (ZKP) on this MOOC, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. Any corrections and advice are welcome. ^ - ^ Topics: Differences between Interactive Proofs and SNARKs Outline of SNARKs from IP Brief intro to Functional Commitments SZDL Lemma Multilinear Extensions Sum-check Protocol and its application. Counting Triangles SNARK for Circuit-satisfiability Before proceeding to todayâ€™s topic, letâ€™s briefly recall what is a SNARK? SNARK is a succinct proof that a certain statement is true.For example, such a statement could be â€œI know an m such that SHA256(m)=0â€. SNARK indicates that the proof is â€œshortâ€ and â€œfastâ€ to verify.Note that if m is 1GB then the trivial proof, i.e. the message m, is neither short nor fast to verify. Interactive Proofs: Motivation and ModelIn traditional outsourcing, the Cloud Provider stores the userâ€™s data and the user can ask the Cloud Provider to run some program on its data. The user just blindly trusts the answer returned by the Cloud Provider. The motivation for Interactive Proofs (IP) is that the above procedure can be turned into the following Challenge-Response procedure. The user is allowed to send a challenge to the Cloud Provider and the Cloud Provider is required to respond for such a challenge. The user has to accept if the response is valid or reject as invalid. Hence, the Challenge-Response procedure or IP can be modeled as follows. P solves a problem and tells V the answer. Then they have a conversation. Pâ€™s goal is to convince V that the answer is correct. Requirements: Completeness: an honest P can convince V to accept the answer. (Statistical) Soundness: V will catch a lying P with high probability. Note that statistical soundness is information-theoretically soundness so IPs are not based on cryptographic assumptions. Hence, the soundness must hold even if P is computationally unbounded and trying to trick V into accepting the incorrect answer. If soundness holds only against polynomial-time provers, then the protocol is called an interactive argument. It is worth noting that SNARKs are arguments so it is not statistically sound. IPs v.s. SNARKsThere are three main differences between Interactive Proofs and SNARKs. Weâ€™ll list them first and elaborate in the section. SNARKs are not statistically sound. SNARKs have knowledge soundness. SNARKs are non-interactive. Not Statistically SoundThe first one is mentioned above that SNARKs are arguments so the soundness is only against polynomial-time provers. Knowledge Soundness v.s. SoundnessThe second one is that SNARKs has knowledge soundness. SNARKs that donâ€™t have knowledge soundness are called SNARGs, they are studied too. Considering a public arithmetic circuit such that $C(x,w)=0$ where $x$ is the public statement and $w$ is the secret witness. Compare soundness to knowledge soundness for such a circuit-satisfiability. Sound: V accepts â†’ There exist $w$ s.t. $C(x,w)=0$. Knowledge sound: V accepts â†’ P actually â€œknowsâ€ $w$ s.t. $C(x,w)=0$.The prover is establishing that he necessarily knows the witness. As for the soundness, the prover is only establishing the existence of such a witness. The knowledge soundness is establishing that the prover necessarily knows the witness. Hence, knowledge soundness is stronger. But sometimes standard soundness is meaningful even in contexts where knowledge soundness isnâ€™t, and vice versa. Standard soundness is meaningful. Because thereâ€™s no natural â€œwitnessâ€. E.g., P claims the output of Vâ€™s program on $x$ is 42. Knowledge soundness is meaningful. E.g., P claims to know the secret key that controls a certain Bitcoin wallet. It is actually claimed that the prover knows a pre-image such that the hash is 0. The hash function is surjective so a witness for this claim always exists. In fact, there are many and many witnesses for this claim. It turns to a trivial sound protocol. Hence, it needs to establish that the prover necessarily knows the witness. Non-interactive and Public VerifiabilityThe final difference is that SNARKs are non-interactive. Interactive proof and arguments only convince the party that is choosing or sending the random challenges. This is bad if there are many verifiers as in most blockchain applications. P would have to convince each verifier separately. For public coin protocols, we have a solution, Fiat-Shamir, which renders the protocol non-interactive and publicly verifiable. In quiz 4, it is a false statement that non-interactive implies publicly verifiable. In my perspective, it only holds for non-interactive protocols rendered from the public coin protocols where the verifier only sample random coins and send the sampled coins to the provers. SNARKs from Interactive Proofs: OutlineWeâ€™ll describe the outline to build SNARKs from interactive proofs in this section. Trivial SNARKsThe first thing to point out is that the trivial SNARK is not a SNARK. The trivial SNARK is as follows: Prover sends $w$ to verifier. Verifier checks if $C(x,w)=0$ and accepts if so.The verifier is required to rerun the circuit. The above trivial SNARK has two problems: The witness $w$ might be long. We want a â€œshortâ€ proof $\\pi$ â†’ $\\text{len}(\\pi)=\\text{sublinear}(|w|)$ Computing $C(x,w)$ may be hard. We want a â€œfastâ€ verifier â†’ $\\text{time}(V)=O_\\lambda(|x|, \\text{sublinear(|C|)})$. As described in Lecture 2, succinctness means the proof length is sublinear in the length of the witness and the verification time is linear to the length of public statement $x$ and sublinear to the size of the circuit $C$. Note that the verification time linear to $|x|$ means that the verifier at least read the statement $x$. Less TrivialWe can make it less trivial as follows: Prover sends $w$ to verifier. Prover uses an IP to prove that $w$ satisfies the claimed property. It gives a fast verifier, but the proof is still too long. Actual SNARKsIn actual SNARKs, instead of sending $w$, the prover commits cryptographically to $w$. Consequently, the actual SNARKs is described as follows: Prover commits cryptographically to $w$. Prover uses an IP to prove that $w$ satisfies the claimed property. The IP procedure reveals just enough information about the committed witness $w$ to allow the verifier to run its checks. Moreover, the IP procedure can be rendered non-interactive via Fiat-Shamir. Functional CommitmentsThere are several important functional families introduced in Lecture 2 we want to build commitment schemes. Polynomial commitments: commit to a univariate $f(X)$ in $\\mathbb{F}_p^{(\\le d)}[X]$. $f(X)$ is a univariate polynomial in the variable $X$ that has a degree at most $d$. The prover commits to a univariate polynomial of degree $\\le d$ . Later the verifier requests to know the evaluation of this polynomial at a specific point $r$. The prover can reveal $f(r)$ and provide proof that the revealed evaluation is consistent with the committed polynomial. Note that the proof size and verifier time should be $O_\\lambda(\\log d)$ in SNARKs. Multilinear commitments: commit to multilinear $f$ in $\\mathbb{F}_p^{(\\le 1)}[X_1,\\dots, X_k]$. $f$ is a multilinear polynomial in variables $X_1,\\dots, X_k$ where each variable has a degree at most 1. E.g., $f(x_1,\\dots, x_k)=x_1x_3 + x_1 x_4 x_5+x_7$. Vector commitments (e.g. Merkle trees): commit to a vector $\\vec{u}=(u_1,\\dots, u_d)\\in \\mathbb{F}_p^d$. The prover commits to a vector of $d$ entries. Later the verifier requests the prover to open a specific entry of the vector, e.g. the $i$th entry $f_{\\vec{u}}(i)$. The prover can open the $i$th entry $f_{\\vec{u}}(i)=u_i$ and provide a short proof that the revealed entry is consistent with the committed vector. Inner product commitments (inner product arguments - IPA): commit to a vector $\\vec{u}=(u_1,\\dots, u_d)\\in \\mathbb{F}_p^d$. The prover commits to a vector $\\vec{u}$. Later the verifier requests the prover to open an inner product $f_{\\vec{u}}(\\vec{v})$ that takes a vector $\\vec{v}$ as input. The prover can open the inner product $f_{\\vec{u}}(\\vec{v})=(\\vec{u},\\vec{v})$ and provide a short proof that the revealed inner product is consistent with the committed vector. Vector Commitments: Merkle TreesIn vector commitments, the prover wants to commit a vector. We can pair up the values in the vector and hash them to form a binary tree. A Merkle tree is a binary tree where the leaf node stores the values of the vector that we want to commit and the other internal nodes calculate the hash value of its two children nodes. The root hash is the commitment of the vector so the prover just sends the root hash to the verifier as the commitment. Then the verifier wants to know the 6th entry in the vector. The prover provides the 6th entry (T) and proof that the revealed entry is consistent with the committed vector. The proof is also called the authentication information. The authentication information is the sibling hashes of all nodes on the root-to-leaf path that includes $C, m_4, h_1$. Hence, the proof size is $O(\\log n)$ hash values. The verifier can check these hashes are consistent with the root hash. Under the assumption that $H$ is a collision-resistant hash family, the vector commitment has the binding property that once the root hash is sent, the committer is bound to a fixed vector. Because opening any leaf to two different values requires finding a hash collision along the root-to-leaf path. Poly Commitments via Merkle TreesA natural way of constructing polynomial commitments is to use the Merkle trees. For example, we can commit to a univariate $f(X)$ in $\\mathbb{F}_7^{(\\le d)}[X]$ with the following Merkle tree. When the verifier requests to reveal $f(4)$, the prover can provide $f(4)$ and the following sibling hashes as proof. In summary, if we want to commit a univariate $f(X)$ in $\\mathbb{F}^{(\\le d)}[X]$, the prover needs to Mekle-commit to all evaluations of the polynomial $f$. When the verifier requests $f(r)$, the prover reveals the associated leaf along with opening information. However, it has two problems. The number of leaves is $|\\mathbb{F}|$ which means the time to compute the commitment is at least $|\\mathbb{F}|$. It is a big problem when working over large fields, e.g., $|\\mathbb{F}|\\approx 2^{64}$ or $|\\mathbb{F}|\\approx 2^{128}$.â†’ We want the time proportional to the degree bound $d$. The verifier does not know if $f$ has a degree at most $d$ !. In lecture 5, we will introduce KZG polynomial commitment scheme using bilinear groups, which addresses both issues. Tech PreliminariesSZDL LemmaThe heart of IP design is based on a simple observation. For a non-zero $f\\in \\mathbb{F}_p^{(\\le d)}[X]$, if we sample a random $r$ from the field $\\mathbb{F}_p$, the probability of $f(r)=0$ is at most $d/p$. Suppose $p\\approx 2^{256}$ and $d\\le 2^{40}$, then $d/p$ is negligible. If $f(r)=0$ for a random $r\\in \\mathbb{F}_p$, then $f$ is identically zero w.h.p. It gives us a simple zero test for a committed polynomial. Moreover, we can achieve a simple equality test for two committed polynomials. Let $p,q$ be univariate polynomials of degree at most $d$. Then $\\operatorname{Pr}_{r\\overset{\\$}\\leftarrow \\mathbb{F}}[p(r)=q(r)]\\le d/p$. If $f(r)=g(r)$ for a random $\\overset{\\$}\\leftarrow \\mathbb{F}_p$, then $f=g$ w.h.p. The Schwartz-Zippel-Demillo-Lipton lemma is a multivariate generalization of the above facts. Schwartz-Zippel-Demillo-Lipton Lemma (SZDL Lemma): Let $p,q$ be $\\ell$-variate polynomials of total degree at most $d$. Then $\\operatorname{Pr}_{r\\in \\mathbb{F}^{\\ell}}[p(r)=q(r)]\\le d/{|\\mathbb{F}|}$. â€Total degreeâ€ refers to the maximum sum of degree of all variables in any term. Low-Degree and Multilinear ExtensionsUsing many variables, we are able to keep the total degree of polynomials quite low, which ensures the proof is short and fast to verify. Definition of Polynomial Extensions: Given a function $f:\\{0,1\\}^{\\ell}\\rightarrow \\mathbb{F}$, a $\\ell$-variate polynomial $g$ over $\\mathbb{F}$ is said to extend $f$ if $f(x)=g(x)$ for all $x\\in \\{0,1\\}^{\\ell}$. Note that the original domain of $f$ is $\\{0,1\\}^{\\ell}$ and the domain of extension $g$ is much bigger, thatâ€™s $\\mathbb{F}^{\\ell}$. Definition of Multilinear Extensions: Any function $f:\\{0,1\\}^{\\ell}\\rightarrow \\mathbb{F}$ has a unique multilinear extension (MLE) denoted by $\\tilde{f}$. The total degree of the multilinear extension can be vastly smaller than the degree of the original univariate polynomial. Consider a univariate polynomial $f:\\{0,1\\}^2\\rightarrow \\mathbb{F}$ as follows. It maps $00$ to $1$, maps $01$ to 2, and so on. The multilinear extension $\\tilde{f}:\\mathbb{F}^2\\rightarrow \\mathbb{F}$ is defined as $\\tilde{f}(x_1,x_2)=(1-x_1)(1-x_2)+2(1-x_1)x_2+8x_1(1-x_2)+10x_1x_2$. Its domain is field by field and itâ€™s easy to check that $\\tilde{f}(0,0)=1,\\tilde{f}(0,1)=2,\\tilde{f}(1,0)=8$ and $\\tilde{f}(1,1)=10$. Another non-multilinear extension of $f$ could be defined as $g(x_1,x_2)=-x_1^2+x_1x_2+8x_1+x_2+1$. Evaluating multilinear extensions quicklyThe sketch of evaluating the multilinear extension is Lagrange interpolation. Fact: Given as input all $2^{\\ell}$ evaluations of a function $f:\\{0,1\\}^\\ell \\rightarrow \\mathbb{F}$, for any point $r\\in \\mathbb{F}^{\\ell}$, there is an $O(2^{\\ell})$-time algorithm for evaluating $\\tilde{f}(r)$. Algorithm: Define $\\tilde{\\delta}_w(r)=\\prod_{i=1}^\\ell (r_iw_i+(1-r_i)(1-w_i)).$ This is called the multilinear Lagrange basis polynomial corresponding to $w$. For any input $r$, $\\tilde{\\delta}_w(r)=1$ if $r=w$, and $0$ otherwise. Hence, we can evaluate the multilinear extension of any input $r$ as follows. $$ \\tilde{f}(r)=\\sum_{w\\in \\{0,1\\}^\\ell}f(w)\\cdot \\tilde{\\delta}_w(r) $$ Complexity: For each $w\\in \\{0,1\\}^{\\ell}$, $\\tilde{\\delta}_w(r)$ can be computed with $O(\\ell)$ field operations, which yields an $O(\\ell 2^\\ell)$-time algorithm. It can be reduced to time $O(2^\\ell)$ via dynamic programming. If we feed this algorithm with the description of $f$ whose domain is $\\{0,1\\}^\\ell$ as inputs and the description consists of all $2^\\ell$ evaluations of $f$, then it is possible to evaluate the multilinear extension of $f$ at any desired point. This fact means that evaluating multilinear extension is essentially as fast as $O(2^\\ell)$, which is constantly slower than reading the whole description of $f$. The Sum-Check ProtocolIn this part, weâ€™ll introduce the sum-check protocol [Lund-Fortnow-Karloff-Nissanâ€™90]. We have a verifier with an oracle access to a $\\ell$-variate polynomial $g$ over field $\\mathbb{F}$. The verifierâ€™s goal is to compute the following quantity: $$ \\sum_{b_1\\in\\{0,1\\}}\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(b_1,\\dots, b_\\ell) $$ As described above (functional commitments), the prover commit a multilinear polynomial, later the verifier can request the prover to evaluate at some specific points. Then the prover provide the evaluation and the proof that the revealed evaluation is consistent with the committed polynomial. In this part, we consider this process as a black box or an oracle. The verifier can go to the oracle and requests the evaluation of $g$ at some points. Note that this sum is the sum of all $g$â€™s evaluations over inputs $\\{0,1\\}^\\ell$ so the verifier can compute it on his own by just asking the oracle for the evaluations. But it costs the verifier $2^\\ell$ oracle queries. ProtocolInstead, we can offload the work of the verifier to the prover where the prover computes the sum and convince the verifier that the sum is correct. It turns out that the verifier only have to run $\\ell$-rounds to check the proverâ€™s answer with only $1$ oracle query. Denote $P$ as prover and $V$ as verifier. Letâ€™s dive into the start phase and the first round. Start: $P$ sends claimed answer $C_1$. The protocol must check that: $C_1=\\sum_{b_1\\in\\{0,1\\}}\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(b_1,\\dots, b_\\ell)$ Round 1: $P$ sends a univariate polynomial $s_1(X_1)$ claimed to equal: $H_1(X_1):=\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(X_1,\\dots, b_\\ell)$ $V$ checks that $C_1=s_1(0)+s_1(1)$. If this check passes, it is safe for $V$ to believe that $C_1$ is the correct answer as long as $V$ believes that $s_1=H_1$. It can be checked that $s_1$ and $H_1$ agree at a random point $r_1\\in \\mathbb{F}_p$ by SZDL lemma. $V$ picks $r_1$ at random from $\\mathbb{F}$ and sends $r_1$ to $P$. In round 1, $s_1(X_1)$ is the univariate polynomial that prover actually sends while $H_1(X_1)$ is what the prover claim to send if the prover is honest. Note that $H_1(X_1)$ is the true answer except that we cut off the first sum, which leave the first variable free. It reduce $2^\\ell$ terms to $2^{\\ell-1}$ terms. $g$ is supposed to have low degree (2 or 3) in each variable so the univariate polynomial $H_1$ derived from $g$ has low degree. Note: We can benefit from the low degree of the univariate polynomial. One is that specifying $H_1$ can be done by just sending 2 or 3 coefficients. Moreover, the low degree gives us acceptable or negligible sound error. After receiving the $s_1$, $V$ can compute $s_1(r_1)$ directly, but not $H_1(r_1)$. It turns out that $P$ can compute $H_1(r_1)$ and sends claimed $H_1(r_1)$ where $H_1(r_1)$ is the sum of $2^{\\ell-1}$ terms where $r_1$ is fixed so the first variable in $g$ is bound to $r_1\\in \\mathbb{F}$. $$ H_1(r_1):=\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,b_2,\\dots, b_\\ell) $$ Hence, the round 2 is indeed a recursive sub-protocol that checks $s_1(r_1)=H_1(r_1)$ where $s_1(r_1)$ is computed on $V$â€™s own. Round 2: They recursively check that $s_1(r_1)=H_1(r_1)$, i.e. that $$ s_1(r_1)=\\sum_{b_2\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,b_2,\\dots, b_\\ell) $$ $P$ sends univariate polynomial $s_2(X_2)$ claimed to equal: $$ H_2(X_1):=\\sum_{b_3\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,X_2,\\dots, b_\\ell) $$ $V$ checks that $s_1(r_1)=s_2(0)+s_2(1)$. If this check passes, it is safe for $V$ to believe that $s_1(r_1)$ is the correct answer as long as $V$ believes that $s_2=H_2$. It can be checked that $s_2$ and $H_2$ agree at a random point $r_2\\in \\mathbb{F}_p$ by SZDL lemma. $V$ picks $r_2$ at random from $\\mathbb{F}$ and sends $r_2$ to $P$. Round $i$: They recursively check that $$ s_{i-1}(r_{i-1})=\\sum_{b_i\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,\\dots,r_{i-1},b_i,\\dots b_\\ell) $$ $P$ sends univariate polynomial $s_i(X_i)$ claimed to equal: $$ H_i(X_i):=\\sum_{b_{i+1}\\in\\{0,1\\}}\\dots \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,\\dots,r_{i-1},X_i,\\dots b_\\ell) $$ $V$ checks that $s_{i-1}(r_{i-1})=s_i(0)+s_i(1)$. $V$ picks $r_i$ at random from $\\mathbb{F}$ and sends $r_i$ to $P$. Round $\\ell$: (Final round): They recursively check that $$ s_{\\ell-1}(r_{\\ell-1})= \\sum_{b_\\ell\\in\\{0,1\\}} g(r_1,\\dots,r_{\\ell-1},b_\\ell) $$ $P$ sends univariate polynomial $s_{\\ell}(X_\\ell)$ claimed to equal : $$ H_\\ell(X_\\ell):= g(r_1,\\dots,r_{\\ell-1},X_\\ell) $$ $V$ checks that $s_{\\ell-1}(r_{\\ell-1})=s_\\ell(0)+s_\\ell(1)$. $V$ picks $r_\\ell$ at random, and needs to check that $s_\\ell(r_\\ell)=g(r_1,\\dots,r_\\ell)$. No need for more rounds. $V$ can perform this check with one oracle query. Consequently, the final claim that the verifier is left to check $s_\\ell(r_\\ell)=g(r_1,\\dots,r_\\ell)$ where $s_\\ell(r_\\ell)$ can be computed on its own and $g(r_1,\\dots, r_\\ell)$ can be computed with just a single query to the oracle. If the final checks passes, then the verifier is convinced that $s_\\ell(r_\\ell)=g(r_1,\\dots,r_\\ell)$ and recursively convinced the claims left in the previous rounds, i.e. $s_i=H_i$. Finally, the verifier accepts the first claim that $C_1$ is the correct sum. AnalysisCompleteness: Completeness holds by design. If $P$ sends the prescribed message, then all of $V$â€™s checks will pass. Soundness: If $P$ dose not send the prescribed messages, then $V$ rejects with probability at least $1-\\frac{\\ell\\cdot d}{|\\mathbb{F}|}$, where $d$ is the maximum degree of $g$ in any variable. Proof of Soundness (Non-Inductive): It is conducted by the union bound. Specifically, if $C_1\\ne \\sum_{(b_1,\\dots, b_\\ell)\\in \\{0,1\\}^{\\ell}}g(b_1,\\dots, b_\\ell)$, then the only way the prover convince the verifier to accept is if there at least one round $i$ such that the prover sends a univariate polynomial $s_i(X_i)$ that dose not equal the prescribed polynomial $$ H_i(X_i)=\\sum_{(b_{i+1},\\dots, b_\\ell)}g(r_1, r_2,\\dots, X_i,b_{i+1},\\dots, b_\\ell) $$ yet $s_i(r_i)=H_i(r_i)$. For every round $i$, $s_i$ and $H_i$ both have degree at most $d$, and hence if $s_i\\ne H_i$, then probability that $s_i(r_i)=H_i(r_i)$ is at most $d/|\\mathbb{F}|$. By a union bound over all $\\ell$ rounds, the probability that there (is a bad event) is any round $i$ such that the prover send a polynomial $s_i\\ne H_i$ yet $s_i(r_i)=H_i(r_i)$ is at most $\\frac{\\ell\\cdot d}{|\\mathbb{F}|}$ Proof of Soundness by Induction: Base case: $\\ell=1$. In this case, $P$ sends a single message $s_1(X_1)$ claimed to equal $g(X_1)$. $V$ picks $r_1$ at random, and checks $s_1(r_1)=g(r_1)$. If $s_1\\ne g$, then the probability that $s_1(r_1)=g(r_1)$ is at most $d/|\\mathbb{F}|$. Inductive case: $\\ell &gt;1$. Recall that $P$â€™s first message $s_1(X_1)$ is claimed to equal $H_1(X_1)$. Then $V$ picks a random $r_1$ and sends $r_1$ to $P$. They recursively invoke sum-check to confirm $s_1(r_1)=H_1(r_1)$. If $s_1\\ne H_1$, then then probability that $s_1(r_1)=H_1(r_1)$ is at most $d/|\\mathbb{F}|$. Conditioned on $s_1(r_1)=H_1(r_1)$, $P$ is left to prove a false claim in the recursive call. The recursive call applies sum-check to $g(r_1, X_2, \\dots, X_\\ell)$, which is $\\ell-1$ variate. By induction hypothesis, $P$ convinces $V$ in the recursive call with probability at most $\\frac{d(\\ell-1)}{|\\mathbb{F}|}$. In summary, if $s_1\\ne H_1$, the probability $V$ accepts is at most $$ \\begin{aligned} \\le &\\operatorname{Pr}_{r_1\\in \\mathbb{F}}[s_1(r_1)=H_1(r_1)]+\\operatorname{Pr}_{r_2,\\dots, r_\\ell\\in \\mathbb{F}}[V \\text{ accepts}\\mid s_1(r_1)\\ne H_1(r_1)] \\\\ \\le & \\frac{d}{|\\mathbb{F}|}+ \\frac{d(\\ell-1)}{|\\mathbb{F}|}\\le \\frac{d\\ell}{|\\mathbb{F}|}\\end{aligned} $$ CostsLet $\\mathrm{deg}_i(g)$ denote the degree of variable $X_i$ in $g$ and each variable has degree at most $d$. $T$ denotes the time required to evaluate $g$ at one point. Total communication is $O(d\\cdot \\ell)$ field elements. The total prover-to-verifier communication is $\\sum_{i=1}^\\ell(\\mathrm{deg}_i(g)+1)=\\ell+\\sum_{i=1}^\\ell \\mathrm{deg}_i(g)=O(d\\cdot \\ell)$ field elements. The total verifier-to-prover communication is $\\ell-1$ field elements. Verifierâ€™s runtime is $O(d\\ell+T)$. The running time of the verifier over the entire execution of the protocol is proportional to the total communication, plus the cost of a single oracle query to $g$ to compute $g(r_1,r_2, \\dots, r_\\ell)$. Proverâ€™s runtime is $O(d\\cdot 2^\\ell\\cdot T)$.Counting the number of evaluations over $g$ required by the prover is less straightforward. In round $i$, $P$ is required to send a univariate polynomial $s_i$, which can be specified by $\\mathrm{deg}_i(g)+1$ points. Hence, $P$ can specify $s_i$ by sending for each $j\\in {0, \\dots, \\mathrm{deg}_i(g)}$ the value: $$ s_i(j)=\\sum_{(b_{i+1},\\dots, b_\\ell)}g(r_1,\\dots,r_{i-1},j,b_{i+1},\\dots, b_\\ell) $$ An important insight is that the number of the terms defining $s_i(j)$ falls geometrically with $i$: in the $i$th sum, there are only $(1+\\mathrm{deg}_i(g))\\cdot 2^{\\ell-i}\\approx d\\cdot 2^{\\ell-i}$ terms, with the $2^{\\ell-i}$ factor due to the number of vectors in $\\{0,1\\}^{\\ell-i}$. Thus, the total number of terms that must be evaluated is $\\sum_{i=1}^\\ell d\\cdot 2^{\\ell-i}=O(d\\cdot 2^{\\ell})$. Application of Sum-check ProtocolAn IP for counting triangles with linear-time verifierThe sum-check protocol can be applied to design an IP for counting triangles in a graph with linear-time verifier. The input is an adjacent matrix of a graph $A\\in \\{0,1\\}^{n\\times n}$. The desired output is $\\sum_{(i,j,k)\\in [n]^3}A_{ij}A_{jk}A_{ik}$, which counts the number of triangles in the graph. The fastest known algorithm runs in matrix-multiplication time, currently about $n^{2.37}$, which is super linear time in the size of the matrix. Likewise, we can offload the work to the prover to have a linear-time verifier. To design an IP derived from sum-check protocol, we need to view the matrix $A$ to a function mapping $\\{0,1\\}^{\\log n}\\times \\{0,1\\}^{\\log n}$ to $\\mathbb{F}$. It can be done easily by Lagrange interpolation. As for the following matrix $A\\in \\mathbb{F}^{4\\times 4}$, we can interpret the entry location $(i,j)\\in \\{0,1\\}^{\\log n}\\times \\{0,1\\}^{\\log n}$ as input and maps to the corresponding value $A_{i,j}\\in \\mathbb{F}$. E.g., $A(0,0,0,0)=1,A(0,0,0,1)=3$ and so on. Note that the domain of function $A$ is $\\{0,1\\}^{2\\log n}$, which has $2\\log n$ variables as inputs. It make sense to extend function $A$ to its multilinear polynomial $\\tilde{A}$ with domain $\\mathbb{F}^{2\\log n}$, each variable having degree at most 1. Hence, we can define a polynomial $g(X,Y,Z)=\\tilde{A}(X,Y)\\tilde{A}(Y,Z),\\tilde{A}(X,Z)$ that has $3\\log n$ variables, each variable having degree at most 2. Having defined the function $g$ with domain $\\{0,1\\}^{3\\log n}$, the prover and the verifier simply apply the sum-check protocol to $g$ to compute: $$ \\sum_{(a,b,c)\\in \\{0,1\\}^{3\\log n}}g(a,b,c) $$ In summary, the design of the protocol is as follows. Protocol: View $A$ as a function mapping $\\{0,1\\}^{\\log n}\\times \\{0,1\\}^{\\log n}$ to $\\mathbb{F}$. Extend $A$ to obtain its multilinear extension denoted by $\\tilde{A}$. Define the polynomial $g(X,Y,Z)=\\tilde{A}(X,Y)\\tilde{A}(Y,Z),\\tilde{A}(X,Z)$. Apply the sum-check protocol to $g$ to compute $\\sum_{(a,b,c)\\in \\{0,1\\}^{3\\log n}}g(a,b,c)$. Costs: Note that $g$ has $3\\log n$ variables and it has degree at most 2 in each variable. Total communication is $O(\\log n)$. Verifier runtime is $O(n^2)$. The total communication is logarithmic. Hence, the verifier runtime is dominated by evaluating $g$ at one point $g(r_1,r_2,r_3)=\\tilde{A}(r_1,r_2)\\tilde{A}(r_2,r_3)\\tilde{A}(r_1,r_3)$, which amounts to evaluating $\\tilde{A}$ at three points. The matrix $A$ gives the lists of all $n^2$ evaluations of the multilinear extension $\\tilde{A}:\\{0,1\\}^{2\\log n}\\rightarrow \\mathbb{F}$. As described above, the **verifier** can in linear time evaluate the multilinear extension function at any desired point in $\\mathbb{F}^{2\\log n}$. Note that the verifier runtime is linear to the size of the input/matrix, thatâ€™s $O(n^2)$. Prover runtime is $O(n^3)$. The proverâ€™s runtime is clearly at most $O(n^5)$ since there are $3\\log n$ rounds and $g$ can be evaluated at any point in $O(n^2)$ time. But more sophisticated algorithm insights can bring the prover runtime down to $O(n^3)$. We recommend reader to refer to Chapter 4 and Chapter 5 in Thaler A SNARK for circuit-satisfiabilityWe can apply the sum-check protocol to design SNARKs for circuit satisfiability. Given an arithmetic circuit $C$ over $\\mathbb{F}$ of size $S$ and output $y$. $P$ claims to know a $w$ such that $C(x,w)=y$. For simplicity, letâ€™s take $x$ to be the empty input. A transcript $T$ for $C$ is an assignment of a value to every gate as follows. $T$ is a correct transcript if it assigns the gate values obtained by evaluating $C$ on a valid witness $w$. The main idea is to assign each gate in $C$ a $(\\log S)$-bit label and view such a transcript as a function with domain $\\{0,1\\}^{\\log S}$ mapping to $\\mathbb{F}$. Hence, $P$â€™s first message is a $(\\log S)$-variate polynomial $h$ claimed to extend a correct transcript $T$, which means $$ h(x)=T(x) \\text{ }\\forall x\\in \\{0,1 \\}^{\\log S} $$ As usual, $T$ is defined over the hypercube $\\{0,1\\}^{\\log S}$ and $h$ is multilinear extension of $T$ with domain $\\mathbb{F}^{\\log S}$. $V$ can check this claim by evaluating all $S$ evaluations on $h$. Like the sum-check protocol, suppose the verifier is only able to learn a few evaluations of $h$ rather than $S$ points. Intuition of extension functionBefore describing the design details, letâ€™s dig the intuition for why we use the extension polynomial $h$ of the transcript $T$ for $P$ to send. Intuitively, we think $h$ as a distance-amplified encoding of the transcript $T$. The domain of $T$ is $\\{0,1\\}^{\\log S}$. The domain of $h$ is $\\mathbb{F}^{\\log S}$, which is vastly bigger. By Schwart-Zippel lemma, if two transcripts disagree at even a single gate value, their extension polynomial $h,hâ€™$ disagree at almost all points in $\\mathbb{F}^{\\log S}$. Specifically, a $1-\\log (S)/|\\mathbb{F}|$ fraction. The distance-amplifying nature of the encoding will enable $V$ to detect even a single â€œinconsistencyâ€ in the entire transcript. As a result, it kind of blows up the tiny difference in transcripts by the extension polynomials into easily detectable difference so that it can be detectable even by the verifier that is only allowed to evaluate the extension polynomials at a single point or a handful points. Two-step plan of attackThe original claim the prover makes is that the $(\\log S)$-variate polynomial $h$ extends the correct transcript. In order to offload work of the verifier and apply the sum-check protocol, the prover instead claims a related $(3\\log S)$-variate polynomial $g_h=0$ at every single boolean input, i.e. $h$ extends a correct transcript $T$ â†” $g_h(a,b,c)=0$ $\\forall (a,b,c)\\in \\{0,1\\}^{3\\log S}$. Moreover, to evaluate $g_h(r)$ at any input $r$, suffices to evaluate $h$ at only 3 inputs. Specifically, the first step is as follows. Step 1: Given any $(\\log S)$-variate polynomial $h$, identify a related $(3\\log S)$-variate polynomial $g_h(a,b,c)$ via $$ \\widetilde{add}(a,b,c)\\cdot (h(a)-(h(b)+h(c))+\\widetilde{mult}(a,b,c)\\cdot (h(a)-h(b)\\cdot h(c)) $$ $\\widetilde{add},\\widetilde{mult}$ are multilinear extension called wiring predicates of the circuit. $\\widetilde{add}(a,b,c)$ splits out 1 iff $a$ is assigned to an addition gate and its two input neighbors are $b$ and $c$. Likewise, $\\widetilde{mult}(a,b,c)$ splits out 1 iff $a$ is assigned to the product of values assigned to $b$ and $c$. $g_h(a,b,c)=h(a)-(h(b)+h(c))$ if $a$ is the label of a gate that computes the sum of gates $b$ and $c$. $g_h(a,b,c)=h(a)-(h(b)\\cdot h(c))$ if $a$ is the label of a gate that computes the product of gates $b$ and $c$. $g_h(a,b,c)=0$ otherwise. Then we need to design an interactive proof to check that $g_h(a,b,c)=0 \\text{ } \\forall (a,b,c)\\in \\{0,1\\}^{3\\log S}$ in which $V$ only needs to evaluate $g_h(r)$ at one random point $r\\in \\mathbb{F}^{3\\log S}$. It is very different from the zero test. Using zero test, we are able to check $g_h=0$ for any input in $\\mathbb{F}^{3\\log S}$ by evaluating a random point $r$, but now we need to check $g_h=0$ over a hypercube $\\{0,1\\}^{3\\log S}$. Imagine for a moment that $g_h$ were a univariate polynomial $g_h(X)$. And rather than needing to check that $g_h$ vanishes over input set $\\{0,1\\}^{3\\log S}$, we need to check that $g_h$ vanishes over some set $H\\subseteq \\mathbb{F}$. We can design the polynomial IOP based on following fact. Fact: $g_h(x)=0$ for all $x\\in H$ â†” $g_h$ is divisible by $Z_H(x)=\\prod_{a\\in H}(x-a)$. We call $Z_H$ the vanishing polynomial for $H$. The polynomial IOP works as follows. More details can be referred to the next Lecture. Polynomial IOP: $P$ sends a polynomial $q$ such that $g_h(X)=q(X)\\cdot Z_H(X)$. $V$ checks this by picking a random $r\\in \\mathbb{F}$ and checking that $g_h(r)=q(r)\\cdot Z_H(r)$. However, it dosenâ€™t work when $g_h$ is not a univariate polynomial. Moreover, having $P$ find and send the quotient polynomial is expensive for high-degree polynomial. In the final SNARK, this would mean applying polynomial commitment to additional polynomials. This is what Marlin, Plonk and Groth16 do. In the next lecture, we will elaborate on the Plonk. Instead, the solution is to use the sum-check protocol. Concretely speaking, the sum-check protocol is able to handle multivariate polynomials and dosenâ€™s require $P$ to send additional large polynomials. For simplicity, imagine working over the integers instead of $\\mathbb{F}$. The general idea is as follows. (Note that it is not a full version of solution.) Step2: General Idea of IP $V$ checks this by running sum-check protocol with $P$ to compute: $$ \\sum_{a,b,c\\in \\{0,1\\}^{\\log S}}g_h(a,b,c)^2 $$ If all terms in the sum are 0, the sum is 0. If working over the integers, any non-zero term in the sum will cause the sum to be strictly positive. At end of sum-check protocol, $V$ needs to evaluate $g_h(r_1, r_2, r_3)$. Suffices to evaluate $h(r_1),h(r_2),h(r_3)$. Outside of these evaluations, $V$ runs in time $O(\\log S)$ with $3\\log S$ rounds. $P$ performs $O(S)$ field operations given a witness $w$.","link":"/2023/07/17/zkp-lec4/"},{"title":"ã€ŒCryptography-ZKPã€: Lec7 Poly-commit based on ECC","text":"In this series, I will learn Zero Knowledge Proofs (ZKP) on this MOOC, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. Any corrections and advice are welcome. ^ - ^ Topics: Poly-commit based on Error-correcting Codes Argument for Vector-Matrix Product Proximity Test Consistency Test Linear-time encodable code based on expanders Lossless Expander Recursive Encoding with constant relative distance An important component in the common paradigm for efficient SNARK is the polynomial commitment scheme. In Lecture 6 we introduced the KZG polynomial commitment based on bilinear pairing and other polynomial commitments based on discrete-log. It is worth noting that prover time is dependent on $O(d)$ exponentiations, which is not strictly linear-time. Today we are going to present a new class of polynomial commitments based on error-correcting codes. Here are the motivations and drawbacks. Motivations: Plausibly post-quantum secure No group exponentiationsProver only uses hashes, additions and multiplications. Small global parameters Drawbacks: Large proof size Not homomorphic and hard to aggregate BackgroundError-correcting CodeLetâ€™s briefly introduce the error-correcting code, which is allowed to correct errors. A $[n,k,\\Delta]$ code is defined below: $Enc(m)$: Encode a message of size $k$ to a codeword of size $n$. Minimum distance (Hamming distance) between any two codewords is $\\Delta$. Note that we omit the alphabet $\\Sigma$ (binary or field) here, which is another important parameter in code. A simple example is the repetition code, which just repeat each symbol three times. Consider the binary alphabet with $k=2$ and $n=6$. The codewords are Enc(00)=000000, Enc(01)=000111, Enc(10)=111000 and Enc(11)=111111 with minimum distance $\\Delta=3$. This repetition code with minimum distance $\\Delta=3$ can correct 1 error duting the transmission. E.g., suppose the transmission can induce at most 1 error, then the message 010111 received from the sender can be decoded to 01. It is worth mentioning that we are going to build poly-commit using error-correcting code without efficient decoding algorithm. The truth is that we donâ€™t use the decoding algorithm at all. We define rate and relative distance over a $[n,k,\\Delta]$ code. The rate $\\frac{k}{n}$ represents the ratio of the minimal message in the codeword of size $n$. We want the rate close to 1. The relative distance $\\frac{\\Delta}{n}$ represents the ratio of the different locations between any two codewords. E.g., repetition code with rate $\\frac 1 a$ repeats each symbol $a$ times with $n=ak$, and has $\\Delta=a$ and relative distance $\\frac 1 k$. We want rate and relative distance as big as possible, but increasing rate could decrease the relative distance. The trade-off between the rate and the distance of a code is well studied in code theory. Linear CodeThe most common type of code is linear code. An important property is any linear combination of codewords is also a codeword. It is equivalent to say that encoding can always be represented as vector-matrix multiplication between $m$ (of size $k$) and the generator matrix (of size $k\\times n$). Moreover, the minimum (Hamming) distance is the same as the codeword with the least number of non-zeros (weight). (The weight of a codeword indicats the number of non-zeros.) The subtraction of any two codewords is also a codeword so the number of the different locations directly implies the weight of another non-zero codeword. Reed-Solomon CodeA classical construction of linear code is Reed-Solomon Code. It encodes messages in $\\mathbb{F}_p^k$ to codewords in $\\mathbb{F}_p^n$. The idea of encoding is veiwing the message of size $k$ as a unique degree $k-1$ univariate polynomial and the codeword is the evaluations at $n$ points. It treats each symbol of the message as an evaluation at a pre-defined point so the polynomial can be uniquely defined by interpolation on the fixed set of public points. Then we can evaluate $n$ pre-defined public points as the codeword. E.g., $(\\omega,\\omega^2,\\dots, \\omega^n)$ for $n$-th root-of-unity $\\omega^n=1 \\mod p$. The minimal distance is $\\Delta=n-k+1$ (indicating the least number of non-zeros) since a degree $k-1$ polynomial has at most $k-1$ roots (indicating the most number of zero evaluations). E.g., when $n=2k$, rate is $\\frac 1 2$ and relative distance is $\\frac 1 2$. It is pretty good in practice and is almost the best we can achieve. RS code is a linear code that the encoding algorithm can be represented as vector-matrix multiplication where the vector is the message and the generator matrix can be derived from Fourier matrix. The encoding time is $O(n\\log n)$ using the fast Fourier transform (FFT). Poly-commit based on error-correcting codesRecall the polynomial commitment scheme we discussed in previous lectures. keygen generates global parameters for $\\mathbb{F}_p^{(\\le d)}$. Prover commits to a univariate polynomial of degree $\\le d$ . Later verifier requests to evaluate at point $u$. Prover opens $v$ with proof that $v=f(u)$ and $f\\in \\mathbb{F}_p^{(\\le d)}$. Reduce PCS to Vec-Max ProductIn the poly-commit based on error-correcting codes, we write the polynomial coefficients in a matrix of size $\\sqrt{d}$ by $\\sqrt{d}$. For simplicity, we assume $d$ is an exact power. Note that the vectorization of the above matrix forms the original vector of polynomial coefficients, that is: $$ [f_{1,1},f_{2,1},\\dots, f_{\\sqrt{d},1},\\dots,f_{1,\\sqrt{d}},\\dots,f_{\\sqrt{d},\\sqrt{d}}]^{T} $$ Hence, the polynomial behind the matrix can be written with two indices: $$ f(u)=\\sum_{i=1}^{\\sqrt{d}}\\sum_{j=1}^{\\sqrt{d}} f_{i,j}u^{i-1+(j-1)\\sqrt{d}} $$ Then the evaluation of $f(u)$ can be viewed as two steps as follows. Two steps of evaluation: (Vecor-Matrix Product) Multiply a vector defined by point $u$ with the matrix of coefficients to get a vector of size $\\sqrt{d}$. (Inner Product) Multiply the vector of size $\\sqrt{d}$ with another vector defined by point $u$ to obtain the final evaluation. With this nice observation, we actually reduce the poly-commit to an argument for vector-matrix product. Roughly speaking, prover can only evaluate the first step and sends a vector of size $\\sqrt{d}$ as proof. Verifier checks whether the Vec-Mat product is correct using proof system and evaluates the second step locally, which is an inner product of the Vec-Mat product and the vector defined by point $u$. As a result, the argument for Vec-Mat product gives us a polynomial commitment with $\\sqrt{d}$ proof size. Argument for Vec-Mat ProductNow our goal is to design a scheme to test the Vec-Mat product without sending the matrix directly. CommitAs usual, we need to commit to the polynomial. Here we instead commit to an encoded matrix defined by the polynomial. We first use a linear code to encode the original matrix defined by the coefficients of polynomial. Concretely speaking, we encode each row with a linear code to compute an encoded matrix of size $\\sqrt{d}\\times n$ where $n$ is the size of the codeword. We will use a linear code with constant rate so that the size of the encoded matrix is asymptotical to $d$. Then we can commit to each column of the encoded matrix using Merkle tree. Recall the Merkle tree commitment introduced in [Lecture 4]. The root hash is served as the commitment to the encoded matrix. Then verifier can request to open each column individually and checks whether the opened column is altered. It is worth noting that the key generation for this Merkle tree commitment is only sampling a hash function, resulting in a constant size global parameters with no trusted setup. Eval and VerifyWe actually perform the evaluation together with verification. It consists of two tests, proximity test and consistency check. We fisrt consider how a malicious prover could cheat in the commitment. A malicious prover can commit to a matrix of inappropriate size but it can be recognized easily by the Merkle tree proof. A malicious prover can commit to an abitrary matrix of specified size in which each row is not a valid codeword. E.g., a valid RS code is a vector of evaluations of a polynomial specified by the message. Hence, verifier uses the proxomity test to test if the committed matrix indeed consists of $\\sqrt{d}$ codewords. Having checked this proximity test, verifier is convinced that the committed matrix is nearly close to the encoded matrix defined by the original matrix of coefficients. Then verifier can move to the consistency check to compute (and verifiy) the actual evaluation. Step1: Proximity Test Ligero [AHIVâ€™2017] and [BCGGHJâ€™2017] are two independent works to introduce the proximity test. Ligero proposed the so-called interleaved test using Reed-Solomon code with quasi-linear prover time. [BCGGHJâ€™2017] instead used a linear-time encodable code to build the ideal linear commitment model, which is the first work to build SNARK with strictly linear prover time. Note that the proximity test in these two works are proposed to build general-purpose SNARKs. Here we use it to build poly-commit as a specified protocol. Ligero [AHIVâ€™2017] and [BCGGHJâ€™2017] are two independent works to introduce the proximity test. Ligero proposed the so-called interleaved test using Reed-Solomon code with quasi-linear prover time. [BCGGHJâ€™2017] instead used a linear-time encodable code to build the ideal linear commitment model, which is the first work to build SNARK with strictly linear prover time. Note that the proximity test in these two works are proposed to build general-purpose SNARKs. Here we use it to build poly-commit as a specified protocol. We first present the description of the proximity test as below. Verifier samples a random vector $r$ of size $\\sqrt{d}$ and sends it to prover. Prover returns the vector-matrix product of the random vector $r$ and the encoded matrix. Verifier requests to open several random columns and prover reveals them with Merkle tree proof. Verifier performs 3 checks The returned vector is a codeword The opened columns are consistent with the committed Merkle tree. The inner product between $r$ and the opened column is consistent with the corresponding element of the returned vector. The completeness is evident. The vec-mat product computed by the honest prover is indeed the linear combination of rows (codewords) specified by the random vector chosen by the verifier. Recall the propery of the linear codes that a linear combination of codewords is a codeword. So these 3 checks will be passed by verifier. Letâ€™s intuitively give the proof of soundness. Assume for the contradiction that the malicious prover commits to a fake matrix, and computes the vec-mat product by this fake matrix. Soundness (Intuition): If the vector is correctly computed, under our assumption, the product is not a codeword. â†’ check 1 will be failed. If the vector is false meaning that the prover just returns an arbitrary codeword, there are many different locations from the correct answer. By check 2, columns are as committed. Probability of passing check 3 is extreamly small. Letâ€™s discuss the second case where the vector sent by the prover is false and $w=r^TC$ denotes the correct answer. In the formal proof for soundness: [AHIVâ€™2017], it defines a parameter $e&lt;\\frac{\\Delta}{4}$, which is related to the minimal distance $\\Delta$, to measure the distance between the committed matrix and the codeword space. Concretely speaking, $e$ measures the minimal distance between any row (of the committed matrix) and any codeword (in the codeword space). If the committed (fake) matrix is $e$-far from any codeword for $e&lt;\\frac{\\Delta}{4}$, then the probability that the vec-mat product $w=r^T C$ is $e$-close to any codeword is $\\le \\frac{e+1}{\\mathbb{F}}$, which is extreamly small. $$ \\operatorname{Pr}[w=r^TC\\text{ is }e\\text{-close to any codeword}]\\le \\frac{e+1}{\\mathbb{F}} $$ Then we can rule out this case, and the remaining case is that the correct answer $w=r^TC$ is $e$-far from any codeword. Under this condition, we know there are at least $e$ different positions between the codeword sent by prover and the correct answer $w$. Then the probability that check 3 is true for $t$ random columns is bounded by $(1-\\frac e n)^t$ where $\\frac e n$ is constant for the linear code with constant relative distance, e.g. RS code. $$ \\operatorname{Pr}[\\text{check 3 is true for }t \\text{ random columns}] \\le (1-\\frac{e}{n})^t $$ Hence, soundness probability can be reduced to negligible probability. Thatâ€™s why we want linear codes with constant relative distance. Optimization for Proximity TestThere is one optimization for the proximity test. Instead of sending the codeword, prover can send the message behind the codeword to verifier. Note that the message is computed by the random vector and the original matrix defined by the polynomial coefficients. Then verifier can encode the message to obtain the corresponding codeword that is supposed to be sent by prover. This nice optimization reduces the proof size from $n$ to $k$. Moreover, there is no need for verifier to perform the first check explicitly that the vector is a codeword since the encoding is done by the verifier. We depict the optimized proximity test as below. Verifier samples a random vector $r$ of size $\\sqrt{d}$ and sends it to prover. Prover returns the vector-matrix product of the random vector $r$ and the original matrix of coefficients. Verifier encodes the message to compute the codeword. Verifier requests to open several random columns and prover reveals them with Merkle tree proof. Verifier performs 2 checks The returned vector is a codeword The opened columns are consistent with the committed Merkle tree. The inner product between $r$ and the opened column is consistent with the corresponding element of the returned vector. Step2: Consistency CheckWith the proximity test, the verifier knows the committed matrix is close to an encoded matrix with overwhelming probability. Next we can perform the consistency check to really test the evaluation of vec-mat product between the vector defined by point $u$ and the original matrix of size $\\sqrt{d}\\times \\sqrt{d}$. The consistency check is almost the same as the proximity test with the optimization mentioned above excetp that the vector is defined by point $u$ rather than a random vector $r$. Likewise, the verifier encodes the message to compute the codeword so the first check can be removed. Futhermore, the verifier can use the same opened columns in the proximity test to perform the third check. The cosistency check is depicted as below where the first two checks can be removed. Knowledge Soundness (Intuition): In the consistency test, we actually need to prove the knowledge soundness. By the proximity test, the committed matrix $C$ is close to an encoded matrix that can be uniquely decoded to a matrix $F$ defined by polynomial coefficients. Intuitively speaking, there exists an extractor that extracts $F$ by Merkle tree commitment and decoding $C$, s.t. $\\vec{u}\\times F=m$ with probability $1-\\epsilon$. SummaryTo put everything together, the poly-commit scheme based on linear code is described as below. PCS based on Linear Code: Keygen: sample a hash function Commit: encode the coefficient matrix $F$ of $f$ row-wise with a linear code compute the Merkle tree commitment col-wise Eval and Verify: Proximity test: random linear combination of all rows, check its consistency with $t$ random columns Consistency test: compute $\\vec{u}\\times F=m$, encode $m$ and check its consistency with $t$ random columns Evaluate $f(u)=&lt;m,uâ€™&gt;$ by verifier where $uâ€™$ is another vector defined by point $u$. An important thing to point out is that the proximity test is necessary for evaluation and verification although it is almost the same as the consistency test. Suppose we only perform the consistency test, then the verifier checks consistency of the inner-product of vector $\\vec{u}$ and the random columns. But it dose not work since vector $\\vec{u}$ is defined in a very structured way. Intuitively speaking, it has to use random challenges chosen by the verifier to guarantee the consistency. Properties: Keygen: $O(1)$, transparent setup with constant size $gp$. Commit: Encoding: $O(d\\log d)$ field multiplications using RS code, $O(d)$ using linear-time encodable codes. Merkle tree: $O(d)$ hashes, $O(1)$ commitment size. Eval: $O(d)$ field multiplications Proof size: $O(\\sqrt{d})$ (several vectors of size $\\sqrt{d}$ ) Verifier time: $O(\\sqrt{d})$ Look at the concrete performance in [GLSTWâ€™21] with degree $d=2^{25}$ and linear-time encodable code. Commit: 36s Eval: 3.2s Proof size: 49MB Verifier time: 0.7s It is excellent in practice and significantly faster than PCS based on pairing or discrete-log (such as KZG, Bulletproofs) because it only uses linear operations without any exponentiations. Related WorksLetâ€™s disscuss the following up-to-date works based on error-correcting codes. [Bootle-Chiesa-Grothâ€™20] It proposed the tensor query IOP $&lt;f,(\\vec{u}\\otimes \\vec{u}â€™)&gt;$, which evaluates inner-product of vector $f$ of size $\\sqrt{d}$ and another vector generated by tensor product between two sub-vectors of size $\\sqrt{d}$. (dimentsion 2) Note that this IOP only works for the product of specific form. Moreover, it generalizes to multiple dimentsions and performs the proximity test and consistency test dimension by dimension with smaller proof size $O(n^\\epsilon)$ for constant $\\epsilon&lt;1$. Brakedown [GLSTWâ€™21] This work proposed the polynomial commitment based on tensor query. As described above, we donâ€™t use decoding algorithm at all in the poly-commit. The prover just sends the message and the verifier encodes the message to get the corresponding codeword. It gives relaxation on the design of the poly-commit which allows to use linear codes without efficient decoding algorithm. Unfortunately, when we prove the knowledge soundness, it has to extract the matrix of polynomial coefficients from the committed encoded matrix in which the efficient decoding is required. If the decoding algorithm is not efficient, the extractor is not polynomial as well. Back to this work, the another contribution is showing an alternative way to prove knowledge soundness without efficient decoding algorithm. As a result, it enables us to build poly-commit using any linear codes without efficient decoding algorithm. [Bootle-Chiesa-Liuâ€™21] It improves proof size to $\\text{poly}\\log (n)$ with a proof composition of tensor IOP and PCP of proximity. [Mieâ€™09] Orion [Xie-Zhang-Songâ€™22] It improves the proof size to $O(\\log^2 n)$ with a proof composition of the code-switching technique [Ron-Zewi-Rothblumâ€™20] Concretely, the proof size is 5.7MB for $d=2^{25}$, which is quite large in practice. Linear-time encodable code based on expandersIt is noteworthy that the following line of works all build SNARKs with linear prover using the linear-time encodable code with constant relative distance. In the last segment, we are going to present the construction of linear-time encodable code based on expanders. Linear-time encodable code is proposed by [Spielmanâ€™96] and generalized from binary to field by [Druk-Ishaiâ€™14], which relies on the expander graph. Expander GraphLook at the following bipartite graph where each node on the left set has 3 outgoing edges connecting to the nodes on the right edges and every two nodes on the left set connect at least 5 nodes on the right set. Bipartite Graph (from wiki) A bipartite graph is a graph whose vertices can be divided into two disjoint and independent sets U and V, that is, every edge connects a vertex in U to one in V. Vertex sets U and V are usually called the parts of the graph. It is a good expander since every two nodes on the left set have at most 6 outgoing edges. In terms of encoding, consider the left nodes as message where each symbol is put on a node and the right nodes is the codeword. The encoding of the message is to compute for each right-side node the sum of nodes connected from the left set, which can be represented as the vector-matrix multiplication between the message and the adjacent matrix of the graph so it is a linear code. A nature question is why we need such an expander graph to achieve linear codes with a good relative distance. Intuitively speaking, with such a good expander, even a single non-zero node on the left set can be expanded to many non-zero nodes on the right set, that is, it amplies the number of non-zeros (weight) from the message to the codeword, enabling us to achieve good relative distance. Lossless Expander Note that the relative distance in the above simple example is not constant. We are going to describe the lossless expander in a formal way. Let $|L|$ denote the number of left nodes and the number of the right nodes is $\\alpha |L|$ for a constant $\\alpha\\in (0,1)$. The degree of a left node is denoted by $g$. Consider a good (almost perfect) expander that for every subset $S$ of nodes on the left, the number of neighers $|\\Tau(S)|=g|S|$ for $|S|\\le \\frac{\\alpha |L|}{g}$, which is bounded by the number of right nodes. But it is too good to achieve. We need to relax the equality and the boundary. Lossless Expander: For every subset $S$ of nodes on the left, the number of neighbors $|\\Tau(S)|\\ge (1-\\beta)g|S|$ for $|S|\\le \\frac{\\delta |L|}{g}$.( $\\beta\\rightarrow 0$, $\\delta \\rightarrow \\alpha$ ) Likewise, the encoding on the lossless expander is to sum up the connected nodes from the left nodes for each right node to compute the codeword. Recursive EncodingThen we move to the construction of linear-time encodable codes, which uses the recursive encoding with the lossless expander. The encoding algorithm is depicted as below. Letâ€™s elaborate on the detailed procedure of encoding a message $m$ of size $k$ to a codeword of size $4k$ with rate $1/4$. Recursive Encoding: Copy the message as the first part of the final codeword. Apply the lossless expander with $\\alpha=\\frac 1 2$ to compute the codeword $m_1$ of size $k/2$. Assume we already had an encoding algorithm for message $m_1$ of size $k/2$ with rate $1/4$ and good relative distance $\\Delta$, then we apply it to compute the codeword $c_1$ of size $2k$ as the second part of the final codeword. Apply another lossless expander with $\\alpha =\\frac 1 2$ for messages of size $2k$ to compute the codeword $c_2$ of size $k$ as the third part. The final codeword is the concatenation $c=m|| c_1||c_2$ The remaining thing is how we get the encoding algorithm for messages of size $k/2$ with rate $1/4$ and good relative distance. Thatâ€™s exactly the recursiving encoding that we just use the entire encoding algorithm for the message of size $k/2$. Hence, we repeate the entire encoding algorithm in recursion from $k/2,k/4,\\dots$ until a constant size. Note that the lossless expanders used in each iteration are different since the size of message are different. Finally we can use any code with good distance for a constant-size message. E.g., Reed-Solomon code. Distance of the CodeThe recursive way of encoding enables to achieve a constant relative distance: $$ \\Delta'=\\min \\{\\Delta,\\frac{\\delta}{4g}\\} $$ where $\\Delta$ is the relative distance of the code used in the middle from $k/2$ to $2k$ and $\\frac{\\delta}{4g}$ depends on the expander graph. Proof of relative distance (case by case): [Druk-Ishaiâ€™14] If weight of $m$ is larger than $4k\\Deltaâ€™$, then the relative distance is larger than $\\frac{4k\\Deltaâ€™}{4k}=\\Deltaâ€™$.â†’ Done. It means that for all messages with large weight, we automatically get codewords with large weight. If weight of $m\\le 4k\\Delta'\\le \\frac{\\delta k}{g}$, the condition of the first lossless expander holds. (since $\\Delta'\\le \\frac{\\delta}{4g}$ ) Let $S$ be the set of non-zero nodes in $m$, then we have $|\\Tau(S)|\\ge (1-\\beta)g|S|$. We can set $g\\ge 10$ and $\\beta &lt; 0.1$, then at least $(1-2\\beta)g|S| &gt; 8|S|&gt;0$ vertices in Hamming ball have a unique neighbor in $S$. Hence, $m_1$ (the output of this lossless expander) is non-zero. After applying the encoding for $m_1$ of size $k/2$ with relative distance $\\Delta$, the wight of $c_1$ $\\ge 2k\\Delta\\ge 2k\\Deltaâ€™$.(The second inequality holds by the definition of min). If the weight of $c_1$ is larger than $4k\\Deltaâ€™$, then the relative distance is larger than $\\Deltaâ€™$.â†’Done Else, weight of $c_1$ is $\\le 4k\\Deltaâ€™&lt;\\frac{\\delta2k}{g}$, the condition of the second lossless expander holds. Let $Sâ€™$ be the set of non-zero nodes in $c_1$, then we can show the weight of $c_2$ is at least $|\\Tau(Sâ€™)|\\ge (1-\\beta)g|Sâ€™|&gt;8|Sâ€™|&gt;16k\\Deltaâ€™ &gt;(4k)\\Deltaâ€™$. Sampling of the Lossless ExpanderWith lossless expander, we can build the linear-time encodable codes with constant relative distance. The last piece of the puzzle is how to construct the lossless expander. [Capalbo-Reingold-Vadhan-Widgersonâ€™2002] proposed an explicit construction of lossless expander. Note that being explicit is deterministic. Unfortunately, it has large hidden constant so the concrete efficiency is not good. An alternative way is random sampling since a random graph is supposed to have good expansion. Since the sample space is polynomial, there is a $1/\\text{poly}(n)$ failure probability instead of negligible probability. Improvements of the Code Brakedown [GLSTWâ€™21] Instead of the plain summations when encoding, it uses random summations, which assign a random weight for each edge and performs the weighted summation, to significantly boost the distance. Orion [Xie-Zhang-Songâ€™22] It proposes an expander testing with a negligible failure probability via maximum density of the graph. Letâ€™s sum up the pros and cons of the polynomial commitment (and SNARK) based on linear code. Pros Transparent setup: $O(1)$ Commit and Prover time: $O(d)$ field additions and multiplications Plausibly post-quantum secure Field agnosticIt means that we can use any field. Cons Proof size: $O(\\sqrt{d})$, MBs","link":"/2023/08/01/zkp-lec7/"},{"title":"ã€ŒCryptography-ZKPã€: Lec5-The Plonk SNARK","text":"In this series, I will learn Zero Knowledge Proofs (ZKP) on this MOOC, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. Any corrections and advice are welcome. ^ - ^ Topics: Preprocessing SNARK KZG Poly-Commit Scheme Proving Properties of committed polys ZeroTest Product Check Permutation Check Prescribed Permutation Check Plonk IOP for General Circuits What is SNARK?Before proceeding to todayâ€™s topic, letâ€™s review what is SNARK. Note that this part is well explained in Lecture 2. preprocessing NARKNARK is Non-interactive ARgument of Knowledge. Given a public arithmetic circuit $C(x,w)\\rightarrow \\mathbb{F}$ where $x$ is the public statement in $\\mathbb{F}^n$ and $w$ is the secret witness in $\\mathbb{F}^m$, a preprocessing (setup) algorithm $$S(C)\\rightarrow \\text{ public parameters } (pp,vp)$$ takes a description of the circuit as inputs, and outputs public parameters $(pp,vp)$ for prover and verifier. NARK works as follows. A preprocessing NARK is a triple of algorithms $(S,P,V)$. $S(C)\\rightarrow$ public parameters $(pp,vp)$ for prover and verifier. $P(pp,x,w)\\rightarrow$ proof $\\pi$. $V(vp,x,\\pi)\\rightarrow$ accept or reject. Note that all algorithms and the adversary have access to a random oracle. The informal requirements of NARK are completeness and adaptively knowledge soundness. Completeness means that an honest prover always convinces the verifier to accept the answer, i.e. $$ \\forall x,w: C(x,w)=0 \\text{ then } \\operatorname{Pr}[V(vp,x,P(pp,x,w))=\\text{accept}]=1 $$ Adaptively knowledge soundness means that if the verifier accepts the proof, then the prover must know a witness such that $C(x,w)=0$. In other words, there exists an extractor $E$ that can extract a valid $w$ from $P$. Zero-knowledge suggests that $(C,pp,vp,x,\\pi)$ reveals nothing new about $w$. Note that the privacy requirement, i.e. zero knowledge, in NARK is optional. So there is a trivial NARK in which the prover just sends $w$ as proof and the verifier just rerun the circuit and check if $C(x,w)=0$. SNARK: a Succinct ARgument of KnowledgeSNARK is a NARK (complete and knowledge sound) that is succinct. zk-SNARK is a SNARK that is also zero knowledge, meaning that the SNARK proof reveals nothing about the witness. A preprocessing SNARK is a triple of algorithms $(S,P,V)$. $S(C)\\rightarrow$ public parameters $(pp,vp)$ for prover and verifier. $P(pp,x,w)\\rightarrow$ short proof $\\pi$; $\\text{len}(\\pi)=\\text{sublinear}(|w|)$. $V(vp,x,\\pi)\\rightarrow$ fast to verify; $\\text{time}(V)=O_\\lambda(|x|,\\text{sublinear}(|C|))$. SNARK requires the length of proof to be sublinear in the length of the witness, and the verify runtime to be linear in the statement and sublinear in the size of the circuit. Being linear in the statement $x$ means that the verifier at least read the statements. Furthermore, a strongly succinct preprocessing NARK is not only sublinear but logarithmic in the size of the circuit. $S(C)\\rightarrow$ public parameters $(pp,vp)$ for prover and verifier. $P(pp,x,w)\\rightarrow$ short proof $\\pi$; $\\text{len}(\\pi)=O_\\lambda(\\log(|C|))$. $V(vp,x,\\pi)\\rightarrow$ fast to verify; $\\text{time}(V)=O_\\lambda(|x|,\\log (|C|))$. Note that the verifier runtime is logarithmic in the size of the circuit, which implies that the verifier even does not know what the underlying circuit is. An insight is that the verifier parameter $vp$ is the short â€œsummaryâ€ of the circuit so the verifier is able to verify the evaluations of the circuit at an arbitrary input $x$. Thatâ€™s the reason why we need the preprocessing or setup. It is worth noting that the trivial SNARK is not a SNARK. Types of preprocessing SetupThe setup for a circuit $C$ is an algorithm $S(C;r)\\rightarrow \\text{ public parameters}(pp,vp)$, which takes the circuit and random bits as inputs and generates public parameters for the prover and the verifier. The types of setup are more detailed. trusted setup per circuit: random $r$ must be kept secret from the prover. $S(C;r)$ trusted but universal (updatable) setup: secret $r$ is independent of $C$. $S=(S_{init},S_{index})$ $S_{init}(\\lambda;r)\\rightarrow gp$: onetime setup, secret $r$ $S_{index}(gp,C)\\rightarrow (pp,vp)$: deterministic algorithm transparent setup: does not use secret data $S(C)$ In the trusted setup, random $r$ must be kept secret from the prover, meaning that it has to be run for every circuit. Once the prover learns $r$, it allows the prover to prove false statements. In practice, $r$ will be destroyed so that nobody can ever know $r$. Sometimes, it is called the trusted setup ceremony. In the trusted but universal setup, it splits the setup into two parts. $S_{init}$ is a one-time setup that does not take the circuit as input and generates global parameters $gp$. Note that $r$ is required to be secret as well. Then $S_{index}$ is a deterministic algorithm executed for every circuit so it takes the circuit and $gp$ as inputs and generates public parameters for the prover and the verifier. In the transparent setup, $S(C)$ does not use secret data. General Paradigm for Building SNARKThe general paradigm for building SNARK is made up of two steps. One is the functional commitment scheme, which is a cryptographic object that depends on some assumptions. The other is the compatible interactive oracle proof (IOP). IOP is an information-theoretic object, which provides unconditional security without any assumptions. To be precise, they are not separate steps. For example, using poly-IOP, we can boost polynomial functional commitment for $\\mathbb{F}_p^{(\\le d)}[X]$ to build SNARK for any circuit $C$ where $|C|&lt;d$. Polynomial CommitmentsReview the polynomial commitments introduced in the last lecture. Prover commits to a univariate polynomial $f(X)$ in $\\mathbb{F}_p^{(\\le d)}[X]$ where the variable $X$ has degree at most $d$. Later the verifier can request to know the evaluation of this polynomial at a specific point. In other words, for public $u,v\\in \\mathbb{F}_p$, the prover can convince the verifier that the committed polynomial satisfies $$f(u)=v \\text{ and deg}(f)\\le d$$ Note that the verifier has the upper bound of the degree $d$, the commitment received from the prover, and $u,v$. In SNARK, the evaluation proof size and verifier time should be logarithmic in the degree, i.e. $O_\\lambda(\\log d)$. Equality Test ProtocolRecall the observation that if $p,q$ are univariate polynomials of degree at most $d$, then $\\operatorname{Pr}_{r\\overset{\\$}\\leftarrow \\mathbb{F}}[p(r)=q(r)]\\le d/p$. If $f(r)=g(r)$ for a random $r \\overset{\\$}\\leftarrow \\mathbb{F}_p$, then $f=g$ w.h.p. Having the polynomial commitments, we can construct the equality test protocol as follows. Prover has committed to two polynomial $f,g$, so verifier has two commitments depicted in two boxes. Verifier samples a random coin $r$ in $\\mathbb{F}_p$ and sends to prover.Hence, it is a public coin. Prover sends the evaluations $y,yâ€™$ and proof that $y=f(r)$ and $yâ€™=g(r)$. Verifier accepts if $y=yâ€™$ and the proof is valid. Fiat-Shamir TransformNote that the equality teat protocol is interactive but the verifier just sends coins to prover. A solution to making it a SNARK (non-interactive) is the Fiat-Shamir transform, which can render a public-coin interactive protocol non-interactive. Let $H:M\\rightarrow R$ be a cryptographic hash function. The main idea is having prover generates verifierâ€™s random bits on its own using $H$, i.e. $r\\leftarrow H(x)$. Prover and verifier can compute $r\\leftarrow H(x)$, making it non-interactive. The completeness is evident and one has to prove knowledge soundness. Thm via Fiat-Shamir Transform: This is a SNARK if ( i ) $d/p$ is negligible where $f,g\\in \\mathbb{F}_p^{(\\le d)}[X]$, and ( ii ) $H$ is modeled as a random oracle. In practice, we use SHA256 as $H$. The succinctness holds by a succinct poly commitment scheme. Note that it is not zk since verifier learns the value of $f(r),g(r)$ that he didnâ€™t know before. In this blog, weâ€™ll introduce a specific polynomial commitment scheme â€” KZGâ€™10 with a trusted setup. KZG requires a trusted but universal setup that generates global parameters once, then it can commit to an arbitrary polynomial of bounded degree $d$. $\\mathcal{F}$-IOPHaving the functional commitments, we can build SNARKs for some circuits, e.g. zero test, equality test. But with $\\mathcal{F}$-IOP, we can boost functional commitment to build SNARK for any circuit $C(x,w)$. Hence, $\\mathcal{F}$-IOP is a proof system that proves $\\exist w:C(x,w)=0$ as follows. Setup: The setup algorithm outputs public parameters for prover and verifier where $vp$ contains a number of oracles for functions in $\\mathcal{F}$.Verifier can ask the oracle for evaluations at some points.The oracles will be replaced to functional commitment schemes in SNARKs. IP proving $C(x,w)=0$: In each round, prover sends an oracle for a function $f_i$ which later on the verifier can evaluate at any point of his choice.Likewise, the oracles will be compiled to the commitment scheme when instantiating actual SNARKs. Properties: Complete: if $\\exist w:C(x,w)=0$ then $\\operatorname{Pr}[\\text{verifier accepts}]=1$. (Unconditional) knowledge sound (as an IOP): extractor is given $(x,f_1, r_1, \\dots, r_{t-1},f_t)$ and outputs $w$.Note that the given functions are in clear since the functional commitments are SNARKs so the extractor can extract the functions from the commitments. Optional: zero knowledge for a zk-SNARK KZG Poly-Commit SchemeLetâ€™s introduce todayâ€™s highlight, KZG polynomial commitment scheme [Kate-Zaverucha-Goldbergâ€™2010]. KZG: A Binding PSCFixed a group $\\mathbb{G}={0,G,2\\cdot G, 3\\cdot G,\\dots, (p-1)\\cdot G}$ of order $p$. CommitmentIt requires a trusted but universal setup. $\\text{setup}(1^\\lambda)\\rightarrow gp$ Sample random $\\tau\\in \\mathbb{F}_p$ $gp=(H_0=G,H_1=\\tau\\cdot G, H_2=\\tau^2\\cdot G, \\dots, H_d=\\tau^d \\cdot G)\\in \\mathbb{G}^{d+1}$. delete $\\tau$!! $\\text{commit}(gp,f)\\rightarrow \\text{com}_f$ where $\\text{com}_f=f(\\tau)\\cdot G \\in \\mathbb{G}$ $f$ as prover parameter $\\text{com}_f$ as verifier parameter The setup generates global parameters $gp$ in which the random $\\tau$ used must be destroyed. Then prover can use $gp$ to generate the commitment for any specific polynomial $f\\in \\mathbb{F}_p^{(\\le d)}[X]$. It is worth noting the prover can compute $f(\\tau)\\cdot G$ without knowing $\\tau$. It can be evaluated by $gp$: $$\\begin{aligned}f(X)&amp; =f_0+f_1X+\\dots+f_d X^d \\ \\text{com}_f &amp;= f_0 \\cdot H_0 +\\dots f_d\\cdot H_d \\ &amp;=f_0\\cdot G + f_1\\tau \\cdot G +f_2 \\tau^2\\cdot G +\\cdots \\ &amp;= f(\\tau)\\cdot G\\end{aligned}$$ where $f_0,\\dots, f_d$ are coefficients of the polynomial. This is a binding commitment but not hiding since it reveals $f(\\tau)\\cdot G$. EvaluationAfter committing to $f$, verifier can request for evaluations at a specific point. For public $u,v\\in \\mathbb{F}_p$, the prover can convince the verifier that the committed polynomial satisfies $f(u)=v$. The proof hinges on some cute algebraic properties: $f(u)=v$ iff $u$ is a root of $\\hat{f}=f-v$ iff $(X-u)$ divides $\\hat{f}$ iff exists $q\\in \\mathbb{F}_p[X]$ s.t. $q(X)\\cdot (X-u)=f(X)-v$ As a result, we can construct an equality test on two polynomials to verify the original claim $f(u)=v$. The whole idea is depicted as follows. Eval: Prover compute the quotient polynomial $q(X)$ and $\\text{com}_q=q(\\tau)\\cdot G$ as the commitment. send the proof $\\pi:=\\text{com}_q\\in \\mathbb{G}$ Note that the proof is just one group element, which is const size better than logarithmic in $d$. Verifier accept if $(\\tau-u)\\cdot \\text{com}_q=\\text{com}_f -v\\cdot G$ The equality test for $(X-u)q(X)\\cdot G=(f(X)-v)\\cdot G$ can be checked by the random $\\tau$. It is worth noticing that $\\tau$ is secret. But verifier can use a â€œpairingâ€ to do the above computation with only $H_0,H_1$ from $gp$, which is a fast computation for verifier. As for the prover, computing the quotient is indeed an expensive computation for large $d$. GeneralizationsThere are two ways to generalize KZG. [PSTâ€™13] Can use KZG to commit to $k$-variate polynomials. Batch proofs: Can commit to $n$ polynomials and provide a batch proof for multiple evaluations. suppose verifier has commitments $\\text{com}_{f_1},\\dots, \\text{com}_{f_n}$ prover wants to prove $f_i(u_{i,j})=v_{i,j}$ for $i\\in [n],j\\in[m]$ â†’ batch proof $\\pi$ is only one group element ! Properties: linear time commitmentOne wonderful property of KZG is the proverâ€™s runtime for commitment is linear in the degree $d$. There are two ways to represent a polynomial $f(X)$ in $\\mathbb{F}_p^{(\\le d)}[X]$: Coefficient representation:$f(X)=f_0+f_1X+\\dots +f_d X^d$. computing $\\text{com}_f=f_0\\cdot H_0 +\\dots +f_d\\cdot H_d$ takes linear time in $d$. Point-value representation:$((a_0,f(a_0)),\\dots,(a_d,f(a_d))$ computing $\\text{com}_f$ naively: construct coefficients $(f_0,f_1,\\dots, f_d)$ takes time $O(d\\log d)$ using Number Theoretic Transform (NTT). Using the point-value representation, the naive way of constructing coefficients takes time $O(d\\log d)$ yet we want it linear in $d$. A better way to compute the commitment with point-value representation is the Lagrange interpolation. $$ f(\\tau)=\\sum_{i=0}^d \\lambda_i (\\tau)\\cdot f(a_i) \\\\ \\text{where } \\lambda_i(\\tau)=\\frac{\\prod_{j=0,j\\ne i}^d (\\tau -a_j)}{\\prod _{j=0,j\\ne i}^{d}(a_i-a_j)}\\in \\mathbb{F}_p $$ One can transform $gp$ into Lagrange form $\\hat{gp}$ by a linear map, not involving any secrets so that anyone can fulfill this transformation. $$ \\hat{gp}=(\\hat{H}_0=\\lambda_0(\\tau)\\cdot G,\\hat{H}_1=\\lambda_1(\\tau)\\cdot G,\\dots, \\hat{H_d}=\\lambda_d(\\tau)\\cdot G) $$ Now, prover can in linear time computes the commitment from the point-value representation as follows. $$ \\text{com}_f=f(\\tau)\\cdot G=f(a_0)\\cdot \\hat{H}_0+\\dots +f(a_d)\\cdot \\hat{H}_d $$ To sum up, prover can compute the commitment in linear time from the coefficient representation or the point-value representation. Multi-point Proof GenerationLet $\\Omega\\subseteq \\mathbb{F}_p$ and $|\\Omega|=d$. Consider such a case in which prover has some $f(X)$ in $\\mathbb{F}_p^{(\\le d)}[X]$ and needs evaluation proofs $\\pi_a\\in G$ for all $a\\in \\Omega$. When it comes to generate evaluation proofs for multi-points, prover naively takes time $O(d^2)$ for $d$ proofs, each takes time $O(d)$. Feist-Khovratovich (FK2020) algorithm optimize to time $O(d\\log d)$ if $\\Omega$ is a multiplicative subgroup time $O(d\\log^2 d)$ otherwise. The Dory polynomial commitmentThe difficulties with KZG lies in two parts One has to require trusted setup for $gp$, and $gp$ size is linear in $d$. Dory (eprint/2020/1274) is proposed to get over the trusted setup. transparent setup: no secret randomness in the setup $\\text{com}_f$ is a single group element (independent of degree $d$ ) eval proof size for $f\\in \\mathbb{F}_p^{(\\le d)}[X]$ is $O(\\log d)$ group elements.Note the eval proof size is constant size in original KZG. eval verify time is $O(\\log d)$Note the eval verify time is constant. prover time is $O(d)$ Applications: vector commitmentPolynomial Commitment Scheme (PCS) have many applications. One is to perform a drop-in replacement for Merkle trees. The idea is to view the vector $(u_1,\\dots, u_k)\\in \\mathbb{F}_p^{(\\le d)}$ as a function $f$ such that $f(i)=u_i$ for $i=1,\\dots, k$, then prover can commits to this polynomial. Instead of proving the revealed entry is consistent with the committed vector, prover can generate evaluation proof that $f(2)=a,f(4)=b$ as depicted follows. The proof $\\pi$ is a single group element (using batch proof) that is shorter than a Merkle proof. Proving properties of committed polynomialsHaving PCS, not only verifier can query evaluations of a committed polynomial, but prover can convince verifier that the committed polynomials $f,g$ satisfy some properties, e.g. equality test. It can be summed up in the following process. Start: Prover has functions $f,g$ in clear and verifier has the corresponding commitments via PCS. Verifier samples a random $r\\in \\mathbb{F}_p$. Prover computes a related polynomial $q$ and commits to it. Verifier can query $f,g,q$ at point $x$ and accept if valid.Note that when we say verifier query a committed polynomial $f(x)$, it means verifier sends $x$ to prover who responds with $f(x)$ and eval proof $\\pi$. (described in here[link]) Polynomial Equality Testing with KZGAs described above, we can construct equality test for two committed polynomials. But for KZG, $f=g$ if and only if $\\text{com}_f=\\text{com}_g$, resulting that verifier can tell if $f=g$ on its own. But prover is needed to test equality of computed polynomials. For example, verifier has four individual commitments to $f,g_1,g_2,g_3$ where all four are in $\\mathbb{F}_p^{(\\le d)}[X]$ to test $f=g_1g_2g_3$. Then verifier queries all four polynomials at a random point $r\\overset{$}\\leftarrow \\mathbb{F}_p$ and tests equality. It is complete and sound assuming $3d/p$ is negligible since $\\text{deg}(g_1g_2g_3)\\le 3d$. Summary of Proof Gadgets for UnivariatesIn order to construct Poly-IOPs for an arbitrary circuit. In this section, weâ€™ll introduce some important proof gadgets for univariates. Let $\\Omega$ be some subset of $\\mathbb{F}_p$ of size $k$. Let $f\\in \\mathbb{F}_p^{(\\le d)}[X]$ where $d\\ge k$ and verifier has the commitment to $f$. We can construct efficient Poly-IOPs for the following tasks. Zero Test: prove that $f$ is identically zero on $\\Omega$. Sum Check: prove that $\\sum_{a\\in \\Omega}f(a)=0$. Prod Check: prove that $\\prod_{a\\in \\Omega}f(a)=1$. â†’ prove for rational functions that $\\prod_{a\\in \\Omega}f(a)/g(a)=1$ Permutation Check: prove that $g(\\Omega)$ is the same as $f(\\Omega)$, just permuted. Prescribed Permutation Check: prove that $g(\\Omega)$ is the same as $f(\\Omega)$, permuted by the prescribed $W$. Vanishing PolynomialBefore staring, letâ€™s introduce the vanishing polynomial. Def: Vanishing Polynomial of $\\Omega$: The vanishing polynomial of $\\Omega$ is $$Z_\\Omega(X):=\\prod_{a\\in \\Omega}(X-a)$$ such that $\\text{deg}(Z_\\Omega)=k$. By definition, the vanishing polynomial is a univariate polynomial to be $0$ everywhere on subset $\\Omega$. We can construct a cute vanishing polynomial by constructing a special subset $\\Omega$. Let $\\omega\\in \\mathbb{F}_p$ be a primitive $k$-th root of unity so that $\\omega ^k=1$. If $\\Omega=\\{1,\\omega, \\omega^2, \\dots, \\omega^{k-1}\\}\\subseteq \\mathbb{F}_p$ then $Z_\\Omega(X)=X^k-1$. Then for $r\\in \\mathbb{F}_p$, evaluating $Z_\\Omega(r)$ takes $\\le 2\\log_2{k}$ field operations by repeated squaring algorithm. Itâ€™s super fast. In the following tasks, we fix $\\Omega=\\{1,\\omega, \\omega^2, \\dots, \\omega^{k-1}\\}$. ZeroTest on $\\Omega$In zero test, prove wants to convince verifier that $f$ is identically zero on $\\Omega$. We build zero test by the following lemma. Lemma: $f$ is zero on $\\Omega$ if and only if $f(X)$ is divisible by $Z_{\\Omega}(X)$. The IOP of zero test is depicted as follows. Prover computes the quotient polynomial $q(X)=f(X)/Z_{\\Omega}(X)$ and commits to this polynomial. Note that with KZG prover can only commits to a polynomial in $\\mathbb{F}_p^{(\\le d)}$ rather than a rational functions. Verifier samples a random $r\\in \\mathbb{F}_p$. Verifier query $q(X)$ and $f(X)$ at point $r$ to learn $q(r)$ and $f(r)$. And verifier evaluates $Z_\\Omega(r)$ by itself. Verifier accepts if $f(r)=q(r)\\cdot Z_\\Omega(r)$ since it implies $f(X)=q(X)\\cdot Z_\\Omega(X)$ w.h.p. Theorem: This protocol is complete and sound assuming $d/p$ is negligible. Costs: Verifier time: $O(\\log k)$ for evaluating $Z_\\Omega(r)$ plus two poly queries (that can be batch into one) Prover time: dominated by the time to compute $q(X)$ and then commit to $q(X)$. Product Check on $\\Omega$We omit the details of sum check and jump to the product check since they are nearly the same. Product check is a useful gadget to construct the permutation check introduced later. In product check, prover wants to convince verifier that the products of all evaluations over $\\Omega$ equals to $1$, i.e. $$\\prod_{a\\in \\Omega}f(a)=1$$ We construct a degree-$k$ polynomial to prove it. Set $t\\in \\mathbb{F}_p^{(\\le k)}[X]$ to be the degree-$k$ polynomial such that $$\\begin{aligned}t(1)&amp;=f(1), \\ t(\\omega^s)&amp;=\\prod_{i=0}^s f(\\omega^i) \\text{ for }s=1,\\dots, k \\end{aligned}$$ Note that a degree-$k$ polynomial can be uniquely specified by $k+1$ points. Then $t(\\omega^i)$ evaluates the prefix-products as follows. $t(\\omega)=f(1)\\cdot f(\\omega)$, $t(\\omega^2)=f(1)\\cdot f(\\omega)\\cdot f(\\omega^2)$ â€¦ â€¦ $t(\\omega^{k-1})=\\prod_{a\\in \\Omega}f(a)=1$ We can represent prefix-product in a iterative way: $$t(\\omega\\cdot x)=t(x)\\cdot f(\\omega \\cdot x) \\text{ for all }x\\in \\Omega$$ As a result, we can do the product check by the following lemma, which can be proved with the evaluation proof and a zero test. Lemma: If ( i ) $t(\\omega^{k-1})=1$ and ( ii ) $t(\\omega\\cdot x)-t(x)\\cdot f(\\omega\\cdot x)=0$ for all $x\\in \\Omega$ then $\\prod_{a\\in \\Omega}f(a)=1$. The IOP for product check is depicted as follows. We can split it two parts: Evaluation proof to prove $t(\\omega^{k-1})=1$ Prover construct $t(X)\\in \\mathbb{F}_p^{(\\le k)}$ and commits to it. Verifier queries $t(X)$ at $\\omega^{k-1}$. check1: Verifier checks that if $t(\\omega^{k-1})=1$. proof size: one commit, one evaluation. Let $t_1(X)=t(\\omega\\cdot X)-t(X)\\cdot f(\\omega\\cdot X)$. Zero test to prove $t_1$ is zero on $\\Omega$.Recall the lemma that $t_1$ is zero on $\\Omega$ iff $Z_{\\Omega}(X)$ divides $t_1(X)$. Prover computes the quotient polynomial $q(X)=t_1(X)/(X^{k}-1)\\in \\mathbb{F}_p^{(\\le d)}$ and commits to it. Verifier samples a random $r\\in \\mathbb{F}_p$ and need to learn $t_1(r)$ and $q(r)$. Verifier queries $q(X)$ at $r$. Verifier queries $t(X)$ at $\\omega r$, and $r$. Verifier u $f(X)$ at $\\omega r$. Verifier computes $r^{k}-1$ in time $O(\\log k)$. check2: Verifier checks if $t(\\omega\\cdot r)-t(\\omega)\\cdot f(\\omega\\cdot r)=q(r)\\cdot (r^k-1)$. proof size: one commit, four evaluations. Note that it is a public-coin interactive protocol that can be rendered non-interactive via Fiat-Shamir Transform. To sum up, the proof size is made up of two commits and five evaluations (can be batched into a single group element). Theorem: This protocol is complete and sound assuming $2d/p$ is negligible. ( $t(X)\\cdot f(\\omega\\cdot X)$ has degree at most $2d$ ). It takes verifier $O(\\log k)$ time to compute $r^{k-1}-1$. It takes prover $O(k\\log k)$ time to compute $t(X)$ and $q(X)$ using the naive way that constructs the coefficients from the point-value representation. Likewise, it works to prove the products on rational functions: $$\\prod_{a\\in \\Omega}(f/g)(a)=1$$ We construct a similar degree-$k$ polynomial to prove it. Set $t\\in \\mathbb{F}_p^{(\\le k)}[X]$ to be the degree-$k$ polynomial such that $$\\begin{aligned}t(1)&amp;=f(1)/g(1), \\ t(\\omega^s)&amp;=\\prod_{i=0}^s f(\\omega^i)/g(\\omega^i) \\text{ for }s=1,\\dots, k \\end{aligned}$$ We write the prefix-product in an iterative way: $$t(\\omega\\cdot x)=t(x)\\cdot \\frac{f(\\omega \\cdot x)}{g(\\omega\\cdot x)} \\text{ for all }x\\in \\Omega$$ Then we can prove the following two parts to fulfill the product check. Lemma: If ( i ) $t(\\omega^{k-1})=1$ and ( ii ) $t(\\omega\\cdot x)\\cdot g(\\omega \\cdot x)-t(x)\\cdot f(\\omega\\cdot x)=0$ for all $x\\in \\Omega$ then $\\prod_{a\\in \\Omega}f(a)/g(a)=1$. Note that the proof size is two commits and six evaluations. Theorem: This protocol is complete and sound assuming $2d/p$ is negligible. Compared to the original prod-check, the one extra evaluation comes from the query to $g(X)$ at $\\omega\\cdot r$. Permutation CheckLet $f,g$ be polynomials in $\\mathbb{F}_p^{(\\le d)}[X]$. Verifier has commitments to $f$ and $g$. In permutation check, that goal is that prover wants to prove that $(f(1),f(\\omega),f(\\omega^2),\\dots, f(\\omega^{k-1})\\in \\mathbb{F}_p^k$ is a permutation of $(g(1),g(\\omega),g(\\omega^2),\\dots, g(\\omega^{k-1}))\\in \\mathbb{F}_p^k$. It means to prove that $g(\\Omega)$ is the same as $f(\\Omega)$, just permuted. The main idea is to construct auxiliary polynomials that have the evaluations as its root. Let $\\hat{f}(X)=\\prod_{a\\in \\Omega}(X-f(a))$ and $\\hat{g}(X)=\\prod_{a\\in \\Omega}(X-g(a))$. Then $\\hat{f}(X)=\\hat{g}(X)$ if and only if $g$ is a permutation of $f$ on $\\Omega$. The thing to notice is that prover cannot just commits to $\\hat{f}$ and $\\hat{g}$, then verifier checks if $\\hat{f}(r)=\\hat{g}(r)$. Because there is a missed proof that $\\hat{f}$ is honestly constructed by the committed $f$. Instead, prover is needed to prove $\\hat{f}(r)=\\hat{g}(r)$ by performing a product check on a rational function $\\frac{r-f(X)}{r-g(X)}$. The IOP for permutation check is depicted as follows. Letâ€™s elaborate on the details. Start: Prover has functions $f,g$ in clear and verifier has commitments to $f,g$. Verifier samples a random $r$. Prover constructs $\\hat{f}$ using the evaluations of $f$, so is $\\hat{g}$. Then prover wants to prove $\\hat{f}(r)=\\hat{g}(r)$. It can be transformed to prove $\\frac{\\hat{f}(r)}{\\hat{g}(r)}=1$ where $r$ is fixed. They can perform prod-check to prove $$ \\frac{\\hat{f}(r)}{\\hat{g}(r)}=\\prod_{a\\in \\Omega}\\left(\\frac{r-f(a)}{r-g(a)}\\right)=1 $$ where the rational function is defined as $\\frac{r-f(X)}{r-g(X)}$ on $\\Omega$. Proof size: two commits and six evaluations, same as prod-check on rational functions. Itâ€™s a public-coin protocol that can be rendered non-interactive. Theorem: This protocol is complete and sound assuming $2d/p$ is negligible. Prescribed Permutation CheckLetâ€™s look into an embellished permutation check where the permutation is prescribed by a specific permutation $W$. We say $W:\\Omega\\rightarrow \\Omega$ is a permutation of $\\Omega$ if $\\forall i\\in [k]$: $W(\\omega^i)=\\omega^j$ is bijection. A bijection means one-to-one correspondence. A bijection function is injective and surjective. Let $f,g$ be polynomials in $\\mathbb{F}_p^{(\\le d)}[X]$. Verifier has three individual commitments to $f, g,$ and $W$. In prescribed permutation check, the goal of prover is to prove that $f(y)=g(W(y))$ for all $y\\in \\Omega$. In other works, it proves that $g(\\Omega)$ is the same as $f(\\Omega)$, permuted by the prescribed $W$. At first sight, we try to use a zero-test to prove $f(y)-g(W(y))=0$ on $\\Omega$. But the problem is the polynomial $f(y)-g(W(y))$ has degree $k^2$ since $g(W(y))$ is a composition of $f$ and $W$. Then prover would need to manipulate polynomials of degree $k^2$, resulting a quadratic time prover !! Yet we want a linear time prover. Letâ€™s reduce this to a prod-check on a polynomial of degree $2k$. Observation: If $(W(a),f(a))_{a\\in \\Omega}$ is a permutation of $(a, g(a))_{a\\in \\Omega}$, then $f(y)=g(W(y))$ for all $y\\in \\Omega$. By the definition of permutation, for $a\\in \\Omega$, there exists a $aâ€™\\in \\Omega$ $W(aâ€™)=a$ and $f(aâ€™)=g(a)$ hold. Then we have $f(aâ€™)=g(W(aâ€™))$. The following example illustrates the proof. Likewise, we construct auxiliary polynomials that have the evaluations as its root yet the evaluations are listed in form of the tuple. The intuition is to encode the tuple to a variable, then use a similar way to construct a bivariate polynomial that has the variables as its root. Hence, the tuple is encoded as variables $Y\\cdot W(a)+f(a)$ and $Y\\cdot a+g(a)$, respectively. And the bivariate polynomials of total degree $k$ is constructed as follows. $$ \\begin{cases} \\hat{f}(X,Y)&= \\prod _{a\\in \\Omega}(X-Y\\cdot W(a)-f(a)) \\\\ \\hat{g}(X,Y) &= \\prod_{a\\in \\Omega}(X-Y\\cdot a -g(a))\\end{cases} $$ The following lemma shows the correctness. Lemma: $\\hat{f}(X,Y)=\\hat{g}(X,Y)$ if and only if $(W(a),f(a))_{a\\in \\Omega}$ is a permutation of $(a,g(a))_{a\\in \\Omega}$. To prove, use the fact that $\\mathbb{F}_p[X,Y]$ is a unique factorization domain. Yet Iâ€™m not familiar with this fact. The complete protocol is depicted as follows, which composes a prod-check on the rational function $\\frac{r-s\\cdot W(X)-f(X)}{r-s\\cdot X-g(X)}$ where $r,s$ are fixed and randomly chosen by the verifier. Theorem: This protocol is complete and sound assuming $2d/p$ is negligible. The PLONK IOP for General CircuitsPLONK Finally, letâ€™s introduce PLONK IOP, a widely used proof system in practice. It is a poly-IOP for a general circuit $C(x,w)$. We can think the PLONK IOP as an abstract IOP that can be combined with different polynomial commitment schemes to construct actual SNARK system. Step1: compile circuit to a computation traceThe first step is to compile a general circuit to a computation trace that can be encoded by a polynomial. Considering a general circuit $(x_1+x_2)(x_2+w_1)$ with two public inputs and one witness input, we can write the computation trace into a table as follows. This compilation is also called arithmetization. The example is illustrated as above. Let $|C|$ denote the total # of gates in $C$. Let $|I|=|I_x|+|I_w|$ denote the # inputs to $C$. Let $d=3|C|+|I|$ and $\\Omega=\\{1,\\omega, \\omega^2, \\dots, \\omega^{d-1}\\}$ where $\\omega\\in \\mathbb{F}_p$ is the primitive $k$-th root of unity so that $\\omega ^d=1$. In the above example, $|C|=3$, $|I|=3$, and $d=12$. The plan is prover can interpolates a polynomial $T\\in \\mathbb{F}_p^{(\\le d)}[X]$ that encodes the entire trace. $T$ encodes all inputs: $T(\\omega^{-j})=\\text{input }\\# j \\text{ for } j=1,\\dots, |I|$. $T$ encodes all wires: $\\forall l=0,\\dots, |C|-1$:For each gate labeled by $l$, $T(\\omega^{3l})$: left input to gate $\\#l$ $T(\\omega^{3l+1})$: right input to gate $\\#l$ $T(\\omega^{3l+2})$: output to gate $\\#l$ In our example, prover interpolates $T(X)$ of degree 11 such that: Note that prover can use FFT / NTT to compute the coefficients of $T$ in time $O(d\\log d)$. Step2: proving validity of $T$Then prover commits to the polynomial $T$ encoded by the computation trace, and needs to prove that $T$ is a correct computation trace. Proving Validity of : $T$ encodes the correct inputs. Every gate is evaluated correctly. The wiring is implemented correctly. The output of last gates is 0.(In our example, the output is 77) Proving (4) is easy that only proves $T(\\omega^{3|C|-1})=0$. The wiring constraints contains that the second input $6$ is connected with the left wire of the gate 0 and the right wire of the gate 1 as depicted as follows. (1) $T$ encodes the correct inputNote that the statement $x$ is public. Both prover and verifier interpolate a polynomial $v(X)\\in \\mathbb{F}_p^{(\\le |I_x|)}[X]$ that encodes the $x$-inputs to the circuit: $$ \\text{for }j=1,\\dots, |I_x|: v(\\omega^{-j})=\\text{input }\\# j $$ In our example, $v(\\omega^{-1})=5,v(\\omega^{-2})=6$, hence $v$ is linear. Note that constructing $v(X)$ takes time proportional to the size of input $x$ so that verifier has time to do this. Let $\\Omega_{\\text{inp}}=\\{\\omega^{-1},\\omega^{-2},\\dots, \\omega^{-|I_x|}\\}\\subseteq \\Omega$ that contains the points encoding the inputs. Prover proves (1) by using ZeroTest on $\\Omega_{\\text{inp}}$ to prove that $$T(y)-v(y)=0 ; \\forall y\\in \\Omega_{\\text{inp}}$$ Note that verifier can construct $v(X)$ explicitly so verifier only query $T(X)$ at randomly chosen $r$. (2) every gate is evaluated correctlySuppose that the circuit only composes the additional gates and multiplication gates. The idea of differentiating is to encode gate types using a selector polynomial $S(X)$. Define $S(X)\\in\\mathbb{F}_p^{(\\le d)}[X]$ such that $\\forall l=0, \\dots, |C|-1$: $$ \\begin{cases}S(\\omega^{3l})&= 1 \\text{ if gate }\\# l \\text{ is an addition gate} \\\\ S(w^{3l})&=0\\text{ if gate }\\# l \\text{ is an multiplication gate}\\end{cases} $$ In our example, the selector polynomial is interpolated as follows. The selector polynomial will be committed in the preprocessing phase because it is a function of the circuit, which just encodes what the gates represent in the circuit. With the selector polynomial, we can encode the addition gates and the multiplication gates into a single polynomial. $$ \\forall y\\in \\Omega_{\\text{gates}}=\\{1,\\omega^3,\\omega^6,\\omega^9,\\dots, \\omega^{3(|C|-1)}\\}: \\\\ S(y)\\cdot [T(y) + T(\\omega y)] + (1-S(y))\\cdot T(y)\\cdot T(\\omega y)=T(\\omega^2 y) $$ If the above equality holds, it means that all the addition gates and multiplication gates are evaluated correctly. $\\#l$ is an **addition gate** â†’ $S(y=\\omega^{3l})=1$ Prove that the sum of the left input and right input equals to the output, i.e. $T(y)+T(\\omega y)=T(\\omega^2 y)$. $\\#l$ is a **multiplication gate** â†’ $S(y=\\omega^{3l})=0$ Prove that the product of the left input and right input equals to the output, i.e. $T(y)\\cdot T(\\omega y)=T(\\omega^2 y)$. Then prover uses ZeroTest to prove that for $\\forall y\\in \\Omega_{\\text{gates}}$: $$S(y)\\cdot [T(y) + T(\\omega y)] + (1-S(y))\\cdot T(y)\\cdot T(\\omega y)-T(\\omega^2 y)=0$$ (3) the wiring is correctThe last thing is to prove the wiring is correct. First we construct the wiring constraints to encode the wires of $C$. In our example, the(incomplete) wiring constraints are listed as follows. The first constraint means that the second input is connected to the right input of the gate 0 and the left input of the gate 1. Then define a polynomial $W:\\Omega\\rightarrow \\Omega$ that implements a rotation: $W(\\omega^{-2},\\omega^{1},\\omega^3)=(\\omega^{1},\\omega^3,\\omega^{-2})$ $W(\\omega^{-1},\\omega^0)=(\\omega^{0},\\omega^{-1})$ â€¦ â€¦ The rotation means $W$ maps $\\omega^{-2}$ â†’ $\\omega^{1}$, $w^1$ â†’ $\\omega^3$, and $\\omega^3$â†’ $\\omega^{-2}$. It means the polynomial $T$ is invariant under this rotation. Note that $W$ actually defines a prescribed permutation. Finally, the following lemma tells us we can prove the wiring constraints using a prescribed permutation check. Lemma: $\\forall y\\in \\Omega$: if $T(y)=T(W(y))$, then wire constraints are satisfied. Itâ€™s a clever way of encoding all the wiring constraints. Note that the polynomial $W$ doesnâ€™t depend on the inputs so it represents an intrinsic property of the circuit itself, which can be committed in the setup. Complete Plonk Poly-IOPThe complete Plonk poly-IOP (and SNARK) is depicted as follows. Letâ€™s elaborate on the details. The setup preprocesses the circuit $C$ and outputs the commitmens to the selector polynomial $S$ and the wiring polynomial $W$. It is untrusted that anyone can check these commitments were done correctly. Prover compiles the circuit to a computation trace, and encodes the entire trace into a polynomial $T(X)$. Verifier can construct $v(X)$ explicitly from the public inputs $x$. Then prover proves validity of $T$: gates: evaluated correctly by ZeroTest inputs: correct inputs by ZeroTest wires: correct wirings by Prescribed Permutation Check output: correct output by evaluation proof Theorem: The Plonk Poly-IOP is complete and knowledge sound, assuming $7|C|/p$ is negligible. $7|C|$ bounds the degree of the polynomial of $S\\cdot T \\cdot T$. constant proof size: a short proof with $O(1)$ commitments. fast verifier: runs in logarithmic time $O(\\log |C|)$ quasi-linear prover: $O(|C|\\log |C|)$ SNARK: rendered via Fiat-Shamir Transform Note that the SNARK is not necessarily zk since the commitments are not zk and the openings are not as well. But there are generic transformations that can efficiently convert any Poly-IOP into a zk Poly-IOP, rendering a zk-SNARK. ExtensionsHyperplonk: linear proverThe main challenge in PLONK is the prover runs in quasi-linear time. Hyperplonk replaces $\\Omega$ with ${0,1}^t$ where $t=\\log _2 |\\Omega|$ to achieve a linear prover. The polynomial $T$ is now a multilinear polynomial in $t$ variables, and the computation trace is encoded on the vertices of the $t$-dim hypercube. ZeroTest is replaced by a multilinear SumCheck. Recall that the prover time in SumCheck ([Lecture 4]) has a factor $2^t$, which is linear to $|C|$. It turns out that all tools to build for proving facts about committed univariate polynomials can be generalized to work and prove properties of multilinear polynomials. Plonkish ArithmetizationAnother extension is about the arithmetization, including the custom gates and Plookup. Having these extension allows to shrink the size of the computation traces, which speed up the prover runtime. It supports custom gates other than addition gates and multiplication gates. The plonkish computation trace can be illustrated as follows: It is defined by a custom gate that computes $v_4+w_3\\cdot t_3$ and outputs $t_4$. Likewise, we can encode it into the following polynomial $$\\forall y\\in \\Omega_{\\text{gates}}: v(y\\omega)+w(y)\\cdot t(y)-t(y\\omega)=0$$ Prover uses a ZeroTest check to prove that the custom gate is evaluated correctly. All such gate checks are included in the gate check by multiplying a selector polynomial. Furthermore, Plookup can ensure some values in the computation trace are in pre-defined list.","link":"/2023/07/20/zkp-lec5/"},{"title":"ã€ŒToolsã€:Git and GitHub","text":"è¿™ç¯‡æ–‡ç« å…·ä½“è®²è¿°äº†Gitå·¥å…·çš„åŸºæœ¬æœ¬åœ°åº“æ“ä½œå’Œä¸Žè¿œç¨‹åº“äº¤äº’çš„åŸºæœ¬æ“ä½œï¼ŒåŒ…æ‹¬ä½¿ç”¨GitHubè¿›è¡Œå›¢é˜Ÿå¤–çš„åä½œå¼€å‘ã€‚ GitGitç®€ä»‹GitåŽ†å²ï¼šLinuxçš„ä»£ç ç®¡ç†ï¼š 1991 Linusæœ¬äººæ‰‹åŠ¨åˆå¹¶ä»£ç  2002 ä½¿ç”¨BitKeeperå•†ä¸šè½¯ä»¶ï¼ŒæŽˆäºˆLinuxç¤¾åŒºå…è´¹ä½¿ç”¨ç‰ˆæœ¬æŽ§åˆ¶ 2005 Linusè‡ªå·±ç”¨Cè¯­è¨€å¼€å‘äº†ä¸€ä¸ªåˆ†å¸ƒå¼ç‰ˆæœ¬æŽ§åˆ¶ç³»ç»Ÿï¼šGit Linus: Talk is cheap, show me the code. 2008 Githubä¸Šçº¿ Gitçš„ä¼˜åŠ¿ï¼š å¤§éƒ¨åˆ†æ“ä½œåœ¨æœ¬åœ°å®Œæˆï¼Œä¸éœ€è¦è”ç½‘ã€‚ å®Œæ•´æ€§ä¿è¯ï¼šæ¯æ¬¡æäº¤è¿›è¡Œå“ˆå¸Œã€‚ å°½å¯èƒ½æ·»åŠ æ•°æ®è€Œä¸æ˜¯åˆ é™¤/ä¿®æ”¹æ•°æ®ï¼Œç‰ˆæœ¬éƒ½åœ¨ã€‚ åˆ†æ”¯æ“ä½œå¿«æ·æµç•…ï¼Œä»¥å¿«ç…§çš„å½¢å¼ã€‚ ä¸ŽLinuxå‘½ä»¤å…¨é¢å…¼å®¹ã€‚ Gitçš„ç»“æž„ Gitå’Œä»£ç æ‰˜ç®¡ä¸­å¿ƒä»£ç æ‰˜ç®¡ä¸­å¿ƒçš„ä»»åŠ¡ï¼šç»´æŠ¤è¿œç¨‹åº“ å±€åŸŸç½‘çŽ¯å¢ƒï¼šæ­å»ºGitLabä½œä¸ºä»£ç æ‰˜ç®¡ä¸­å¿ƒ å¤–ç½‘çŽ¯å¢ƒï¼šå¯ä»¥ç”¨GitHubå’Œç äº‘ä½œä¸ºä»£ç æ‰˜ç®¡ä¸­å¿ƒ æœ¬åœ°åº“å’Œè¿œç¨‹åº“çš„äº¤äº’å›¢é˜Ÿå†…ï¼š å›¢é˜Ÿå¤–ï¼š forkï¼šå¤åˆ¶ä¸€ä»½å±žäºŽè‡ªå·±çš„è¿œç¨‹åº“ å¼€å‘æ–°çš„å†…å®¹åŽå‘åº“çš„æ‹¥æœ‰è€… pull requestæ‹‰å–è¯·æ±‚ï¼ŒåŽŸæ‹¥æœ‰è€…å¯ä»¥å®¡æ ¸ï¼Œå®¡æ ¸é€šè¿‡åŽæ‰§è¡Œmergeæ“ä½œåˆå¹¶åˆ°è‡ªå·±çš„è¿œç¨‹åº“çš„åˆ†æ”¯ä¸Šã€‚ Gitå‘½ä»¤è¡ŒåŸºæœ¬æ“ä½œæœ¬åœ°åº“åˆå§‹åŒ– åˆå§‹åŒ–æœ¬åœ°åº“ git init .gitæ–‡ä»¶å­˜æ”¾çš„æ˜¯æœ¬åœ°åº“ç›¸å…³çš„å­ç›®å½•å’Œæ–‡ä»¶ï¼Œä¸è¦åˆ é™¤å’Œéšæ„ä¿®æ”¹ã€‚ åˆ é™¤æœ¬åœ°gitåº“ä¹Ÿå°±æ˜¯åˆ é™¤æœ¬åœ°çš„.git æ–‡ä»¶ åˆ é™¤æœ¬åœ°åº“ï¼šrm -rf .git æœ¬åœ°åº“è®¾ç½®ç­¾å å½¢å¼ï¼š ç”¨æˆ·åï¼š Emailï¼š ä½œç”¨ï¼šåŒºåˆ†ä¸åŒå¼€å‘äººå‘˜çš„èº«ä»½ æ³¨ï¼šè¿™é‡Œè®¾ç½®çš„ç­¾åä¸Žè¿œç¨‹ä»£ç æ‰˜ç®¡ä¸­å¿ƒæ²¡æœ‰å…³ç³»ã€‚ å‘½ä»¤ï¼š é¡¹ç›®çº§åˆ«ï¼šè®¾ç½®ç­¾åä»…åœ¨æœ¬åœ°åº“èµ·æ•ˆï¼ˆå¦‚æžœæ—¢æœ‰é¡¹ç›®çº§åˆ«å’Œç”¨æˆ·çº§åˆ«çš„ç­¾åï¼ŒæŒ‰ç…§é¡¹ç›®çº§åˆ«ä¸ºå‡†ï¼‰ è®¾ç½®ç”¨æˆ·åå‘½ä»¤ï¼šgit config user.name *** è®¾ç½®ç”¨æˆ·é‚®ç®±ï¼š git config user.email *****@outlook.com è¯¥ä¿¡æ¯ä¿å­˜åœ¨.git/configæ–‡ä»¶ä¸­ã€‚ ç”¨æˆ·çº§åˆ«ï¼šè®¾ç½®ç­¾ååœ¨å½“å‰æ“ä½œç³»ç»Ÿçš„ç”¨æˆ·èŒƒå›´ è®¾ç½®ç”¨æˆ·åå‘½ä»¤ï¼šgit config --global user.name *** è®¾ç½®ç”¨æˆ·é‚®ç®±å‘½ä»¤ï¼šgit config --global user.email **** è¯¥æ¶ˆæ¯ä¿å­˜åœ¨ç³»ç»Ÿæ–‡ä»¶~/.gitconfigæ–‡ä»¶ æŸ¥çœ‹çŠ¶æ€ æŸ¥çœ‹å·¥ä½œåŒºã€æš‚å­˜åŒºçŠ¶æ€ã€‚ git status æœ¬åœ°åº“ç‰ˆæœ¬ä¿¡æ¯æŸ¥çœ‹HEAD: æŒ‡é’ˆï¼Œè¡¨ç¤ºå½“å‰ç‰ˆæœ¬çš„ä½ç½®ã€‚ æ˜¾ç¤ºç‰ˆæœ¬ï¼š å®Œæ•´çš„ç‰ˆæœ¬ä¿¡æ¯è®°å½•ï¼ˆåŒ…æ‹¬å®Œæ•´ç‰ˆæœ¬å“ˆå¸Œå€¼ã€ä½œè€…ã€æäº¤æ—¶é—´ï¼‰ git log ï¼ˆç©ºæ ¼å‘ä¸‹ç¿»é¡µï¼›b å‘ä¸Šç¿»é¡µï¼› qé€€å‡ºæ˜¾ç¤ºï¼‰ ä¸€è¡Œåªæ˜¾ç¤ºä¸€ä¸ªç‰ˆæœ¬ï¼Œç®€æ´ç‰ˆã€‚ git log --pretty=oneline ä¸€è¡Œä¹Ÿåªæ˜¾ç¤ºä¸€ä¸ªç‰ˆæœ¬ï¼Œç»ˆæžç®€æ´ç‰ˆï¼Œå“ˆå¸Œå€¼ä¹Ÿåªæ˜¾ç¤ºå‰é¢çš„ä¸€éƒ¨åˆ†ï¼ˆå½“ä½œè¯¥ç‰ˆæœ¬çš„å±€éƒ¨ç´¢å¼•ï¼‰ã€‚ git log --oneline git reflog ï¼šè·Ÿè¸ªæœ¬åœ°åº“çš„åŽ†å²æ›´æ–°ï¼Œå¯ä»¥çœ‹åˆ°HEADä¹‹åŽï¼ˆâ€œæœªæ¥â€œï¼‰çš„ç‰ˆæœ¬ã€‚ HEAD@{i}ï¼šiè¡¨ç¤ºHEADæŒ‡é’ˆç§»åŠ¨åˆ°è¯¥ç‰ˆæœ¬éœ€è¦åŽé€€çš„æ­¥æ•°ã€‚ å·¥ä½œåŒºæ“ä½œï¼šæ·»åŠ /ä¿®æ”¹/æäº¤/åˆ é™¤å¯¹äºŽè¿™äº›å‘½ä»¤ï¼Œå…¶å®žåœ¨ä½¿ç”¨git status æ—¶ï¼Œéƒ½ä¼šæœ‰ç›¸å…³æç¤ºã€‚ æäº¤åˆ°æš‚å­˜åŒºï¼ˆå¢žåŠ ã€ä¿®æ”¹ã€åˆ é™¤ï¼‰ï¼šgit add &lt;filename&gt; æ’¤é”€å·¥ä½œåŒºçš„ä¿®æ”¹ï¼ˆdiscard changes in working directoryï¼‰ git checkout -- &lt;file&gt; æš‚å­˜åŒºæ“ä½œï¼šæ·»åŠ /ä¿®æ”¹/æäº¤/åˆ é™¤å¯¹äºŽè¿™äº›å‘½ä»¤ï¼Œå…¶å®žåœ¨ä½¿ç”¨git status æ—¶ï¼Œéƒ½ä¼šæœ‰ç›¸å…³æç¤ºã€‚ æ·»åŠ /ä¿®æ”¹ï¼šå°†å·¥ä½œåŒºçš„æ–‡ä»¶æ·»åŠ åˆ°æš‚å­˜åŒºï¼ˆ/æˆ–æ›´æ–°æš‚å­˜åŒºçš„æ–‡ä»¶ï¼‰ã€‚ git add &lt;filename&gt; åˆ é™¤ï¼šå°†æ–‡ä»¶ä»Žæš‚å­˜åŒºåˆ é™¤ git rm --cached &lt;filename&gt; æäº¤ï¼šå°†æš‚å­˜åŒºçš„æ–‡ä»¶æäº¤åˆ°æœ¬åœ°åº“ã€‚ï¼ˆè¾“å…¥æäº¤ä¿¡æ¯ï¼‰ git commit -m &lt;message&gt; [filename] ä¿®æ”¹åŽçš„æäº¤ï¼šæäº¤ä¿®æ”¹åŽçš„æ–‡ä»¶è‡³æœ¬åœ°åº“ï¼ˆå·²åœ¨æš‚å­˜åŒºæœ‰æ—§ç‰ˆæœ¬ï¼‰ï¼ŒåŒæ—¶æ›´æ–°æš‚å­˜åŒºå’Œæœ¬åœ°åº“ã€‚ git commit -m &lt;message&gt; [filename] ä¿®æ”¹commit æ³¨é‡Šgit commit --amend ç‰ˆæœ¬å‰è¿›/åŽé€€æœ¬è´¨æ˜¯HEADæŒ‡é’ˆçš„ç§»åŠ¨ã€‚ åŸºäºŽç´¢å¼•æ“ä½œï¼šç‰ˆæœ¬å¯ä»¥åŽé€€å’Œå‰è¿›ã€‚(ç´¢å¼•å°±æ˜¯reflogå½¢å¼ä¸‹çš„å±€éƒ¨å“ˆå¸Œå€¼) git reset --hard [å±€éƒ¨ç´¢å¼•å€¼] ä½¿ç”¨^ ï¼š ç‰ˆæœ¬åªèƒ½å¾€åŽé€€ ã€‚ï¼ˆåŸºäºŽreflogå½¢å¼ä¸‹çš„æ­¥æ•°ï¼‰ git reset --hard HEAD^^ (åŽé€€ä¸¤æ­¥) ä½¿ç”¨~n ï¼šç‰ˆæœ¬å¾€åŽé€€næ­¥ã€‚ git reset --hard HEAD~3 ç‰ˆæœ¬å‰è¿›/åŽé€€resetå‘½ä»¤çš„å‚æ•°å¯¹æ¯”ï¼š --soft ä»…ä»…åœ¨æœ¬åœ°åº“ç§»åŠ¨HEADæŒ‡é’ˆã€‚ å¦‚ä¸‹å›¾ï¼Œæ˜¾å¾—æš‚å­˜åŒºå’Œå·¥ä½œåŒºç‰ˆæœ¬æ¯”æœ¬åœ°åº“å‰è¿›äº†ä¸€æ­¥ã€‚ --mixed åœ¨æœ¬åœ°åº“ç§»åŠ¨HEADæŒ‡é’ˆ å¹¶é‡ç½®æš‚å­˜åŒºï¼Œæš‚å­˜åŒºå’Œæœ¬åœ°åº“ä¸€è‡´ã€‚ å¦‚ä¸‹å›¾ï¼Œæ˜¾å¾—å·¥ä½œåŒºç‰ˆæœ¬æ¯”æœ¬åœ°åº“å’Œæš‚å­˜åŒºç‰ˆæœ¬å‰è¿›äº†ä¸€æ­¥ã€‚ --hard åœ¨æœ¬åœ°åº“ç§»åŠ¨HEADæŒ‡é’ˆ é‡ç½®æš‚å­˜åŒº é‡ç½®å·¥ä½œåŒº åˆ é™¤æ–‡ä»¶åŽæ‰¾å›žå‰æï¼šåˆ é™¤å‰ï¼Œæ–‡ä»¶å­˜åœ¨çš„çŠ¶æ€æäº¤åˆ°äº†æœ¬åœ°åº“ã€‚ æ“ä½œï¼š åˆ é™¤çš„æ“ä½œå·²ç»æäº¤åˆ°æœ¬åœ°åº“ åˆ é™¤æ“ä½œï¼š 123rm a.txtgit add a.txtgit commit -m &quot;delete a.txt&quot; a.txt git reset --hard [åŽ†å²ç‰ˆæœ¬æŒ‡é’ˆä½ç½®] åˆ é™¤æ“ä½œæœªæäº¤åˆ°æœ¬åœ°åº“ åˆ é™¤æ“ä½œï¼š 12345//å·¥ä½œåŒºåˆ é™¤rm a.txt//ç¼“å­˜åŒºä¹Ÿåˆ é™¤rm a.txtgit add a.txt git reset --hard HEAD æ¯”è¾ƒå·®å¼‚ å·¥ä½œåŒºæ–‡ä»¶å’Œæš‚å­˜åŒºæ–‡ä»¶æ¯”è¾ƒ git diff [filename] æš‚å­˜åŒºå’Œæœ¬åœ°åº“å†…å®¹æ¯”è¾ƒ git diff --cached å·¥ä½œåŒºæ–‡ä»¶å’Œæœ¬åœ°åº“æ–‡ä»¶æ¯”è¾ƒï¼ŒæŒ‡é’ˆå¯ä»¥ä½¿ç”¨HEAD^ git diff [æŒ‡é’ˆ] [filename] å¯ä»¥ä¸åŠ æ–‡ä»¶åï¼Œå³æ¯”è¾ƒå…¨éƒ¨æ–‡ä»¶ã€‚ ä»»æ„ç‰ˆæœ¬ä¹‹é—´çš„å†…å®¹æ¯”è¾ƒï¼š æ˜¾ç¤ºä¸¤æ¬¡å·®å¼‚æ–‡ä»¶ git diff ç‰ˆæœ¬1hash/æŒ‡é’ˆ ç‰ˆæœ¬2hash/æŒ‡é’ˆ --stat å…·ä½“æ˜¾ç¤ºå·®å¼‚æ–‡ä»¶ git diff hash1/æŒ‡é’ˆ hash2/æŒ‡é’ˆ filename åˆ†æ”¯ç®¡ç†åˆ†æ”¯åˆ†æ”¯ï¼šç‰ˆæœ¬æŽ§åˆ¶è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¤šæ¡çº¿åŒæ—¶æŽ¨è¿›å¤šä¸ªä»»åŠ¡ã€‚ master: ä¸»ç‰ˆæœ¬åˆ†æ”¯/éƒ¨ç½²åˆ°æœåŠ¡å™¨è¿è¡Œçš„åˆ†æ”¯ã€‚ feature_ ï¼šå¼€å‘å…¶ä»–åŠŸèƒ½çš„åˆ†æ”¯ã€‚ hot_fix: çƒ­ä¿®å¤ï¼Œbugä¿®å¤åˆ†æ”¯ã€‚ åˆ†æ”¯çš„å¥½å¤„ï¼š å¹¶è¡Œï¼šåŒæ—¶å¹¶è¡ŒæŽ¨è¿›å¤šä¸ªåŠŸèƒ½å¼€å‘ã€‚ ç‹¬ç«‹ï¼šå„ä¸ªåˆ†æ”¯åœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¦‚æžœæœ‰ä¸ªåˆ†æ”¯å¼€å‘å¤±è´¥ï¼Œä¸ä¼šå½±å“å…¶ä»–åˆ†æ”¯ã€‚ åˆ†æ”¯æ“ä½œï¼šåˆ›å»º/æŸ¥çœ‹/åˆ‡æ¢/åˆå¹¶ åˆ›å»ºåˆ†æ”¯ git branch &lt;branch name&gt; åˆ é™¤åˆ†æ”¯ git branch -d &lt;branch name&gt; åˆ é™¤è¿œç¨‹åˆ†æ”¯ï¼š æŸ¥çœ‹è¿œç¨‹åˆ†æ”¯ï¼šgit branch -r åˆ é™¤æœ¬åœ°çš„è¿œç¨‹è·Ÿè¸ªåˆ†æ”¯ï¼šgit branch -r -d origin/branch-name åˆ é™¤çœŸæ­£çš„è¿œç¨‹åˆ†æ”¯ï¼šgit push origin : branch-name æŸ¥çœ‹åˆ†æ”¯ git branch -v åˆ‡æ¢åˆ†æ”¯ git checkout [branch name] åˆå¹¶åˆ†æ”¯ åˆ‡æ¢åˆ°æŽ¥å—ä¿®æ”¹çš„åˆ†æ”¯ï¼ˆå¦‚masterï¼‰ git checkout [åˆå¹¶åˆ°çš„ä¸»åˆ†æ”¯] æ‰§è¡Œmergeåˆå¹¶æ“ä½œ git merge [æœ‰ä¿®æ”¹çš„åˆ†æ”¯] è§£å†³åˆå¹¶åˆ†æ”¯åŽäº§ç”Ÿçš„å†²çªå†²çªçš„è¡¨çŽ°ï¼Œæ˜¾ç¤ºåˆ°æœ‰å†²çªçš„æ–‡ä»¶ï¼š å†²çªè§£å†³ï¼š åˆ é™¤æ–‡ä»¶ä¸­çš„ç‰¹æ®Šç¬¦å· åå•†å†ç¼–è¾‘æ–‡ä»¶ æ·»åŠ æ–°æ–‡ä»¶ git add [filename] æäº¤ï¼ˆæ³¨æ„ï¼šæ­¤æ—¶çš„æäº¤ä¸èƒ½å¸¦æ–‡ä»¶åï¼‰ git commit -m &quot;message&quot; rebaseï¼šæ•´ç†åˆ†æ”¯ rebase æ“ä½œå¯ä»¥æŠŠæœ¬åœ°æœªpushçš„åˆ†å‰æäº¤åŽ†å² æ•´ç†æˆç›´çº¿ã€‚ rebaseçš„ç›®çš„ï¼šåœ¨æŸ¥çœ‹åŽ†å²æäº¤çš„å˜åŒ–æ—¶æ›´å®¹æ˜“ã€‚ Git åŸºæœ¬åŽŸç†å“ˆå¸Œç®—æ³•ç‰¹ç‚¹ï¼š å¾—åˆ°çš„åŠ å¯†å¯†æ–‡é•¿åº¦ç›¸åŒã€‚ ç®—æ³•ç¡®å®šï¼Œè¾“å…¥ç¡®å®šåŽï¼Œè¾“å‡ºä¸€å®šç¡®å®šã€‚ è¾“å…¥æ•°æ®å‘ç”Ÿä¸€ç‚¹ç‚¹å˜åŒ–ï¼Œè¾“å‡ºçš„å˜åŒ–ä¼šå¾ˆå¤§ã€‚ Gitåº•å±‚é‡‡ç”¨SHA-1ç®—æ³•ã€‚ å“ˆå¸Œç®—æ³•ä¿è¯äº†Gitçš„æ•°æ®å®Œæ•´æ€§ã€‚ Gitä¿å­˜ç‰ˆæœ¬çš„æœºåˆ¶é›†ä¸­å¼ç‰ˆæœ¬æŽ§åˆ¶å·¥å…·ï¼ˆå¦‚SVNï¼‰ï¼šä¿å­˜çš„ä¿¡æ¯æ˜¯æ¯ä¸ªåŸºæœ¬æ–‡ä»¶å’Œæ¯ä¸ªæ–‡ä»¶éšæ—¶é—´é€æ­¥ç´¯ç§¯çš„å·®å¼‚ã€‚ Gitæ˜¯åˆ†å¸ƒå¼çš„ç‰ˆæœ¬æŽ§åˆ¶å·¥å…·ã€‚ GitæŠŠæ•°æ®çœ‹ä½œæ˜¯æ–‡ä»¶ç³»ç»Ÿçš„å¿«ç…§ï¼ˆå¯ä»¥ç†è§£ä¸ºå½“å‰å†…å­˜ç‰ˆæœ¬çš„æ–‡ä»¶çš„ç´¢å¼•ï¼‰ï¼Œæ¯æ¬¡æäº¤æ›´æ–°æ—¶Gitå¯¹å½“å‰å†…å­˜çš„å…¨éƒ¨æ–‡ä»¶åˆ¶ä½œä¸€ä¸ªå¿«ç…§å¹¶ä¿å­˜è¿™ä¸ªå¿«ç…§çš„ç´¢å¼•ã€‚å¦‚æžœæ–‡ä»¶æ²¡æœ‰ä¿®æ”¹ï¼ŒGitä¸ä¼šé‡æ–°å­˜å‚¨è¯¥æ–‡ä»¶ï¼Œåªæ˜¯ä¿ç•™ä¸€ä¸ªè¿žæŽ¥æŒ‡å‘ä¹‹å‰å­˜å‚¨çš„æ–‡ä»¶ã€‚ Gitçš„æäº¤å¯¹è±¡ï¼š ä¸Šå›¾ä¸­ï¼Œæ¯ä¸ªæ–‡ä»¶éƒ½æœ‰ä¸€ä¸ªå“ˆå¸Œå€¼/ç´¢å¼•ï¼Œæäº¤æ—¶æ–°å»ºä¸€ä¸ªæ ‘ç»“ç‚¹ï¼Œå…¶ä¸­åŒ…å«æŒ‡å‘æ¯ä¸ªæ–‡ä»¶çš„æŒ‡é’ˆ/ç´¢å¼•ï¼Œæäº¤çš„å¯¹è±¡åŒ…æ‹¬è¯¥æ ‘ç»“ç‚¹çš„æŒ‡é’ˆ/å“ˆå¸Œå€¼ã€‚ Gitç‰ˆæœ¬å¯¹è±¡é“¾æ¡ï¼š æ‰€ä»¥ï¼š Git åˆ†æ”¯çš„åˆ›å»ºï¼šç­‰äºŽæ–°å»ºä¸€ä¸ªæŒ‡å‘ç‰ˆæœ¬çš„æŒ‡é’ˆã€‚ Gitåˆ†æ”¯çš„åˆ‡æ¢ï¼šæ”¹å˜HEADæŒ‡é’ˆæ‰€æŒ‡çš„æŒ‡é’ˆã€‚ Gitåˆ†æ”¯ç‰ˆæœ¬çš„ç§»åŠ¨ï¼šåˆ†æ”¯æŒ‡é’ˆçš„ç§»åŠ¨ã€‚ GitHubåŸºæœ¬äº¤äº’åˆ›å»º/æŸ¥çœ‹è¿œç¨‹åº“åœ°å€åˆ«ååœ¨GitHubåˆ›å»ºè¿œç¨‹åº“åŽ åœ¨æœ¬åœ°æ·»åŠ è¿œç¨‹åº“åœ°å€åˆ«å git remote add [åˆ«å] [https/ssh åœ°å€] git remote add orgin https://... æŸ¥çœ‹å½“å‰æ‰€æœ‰è¿œç¨‹åº“åœ°å€åˆ«å git remote -v ç§»é™¤è¿œç¨‹åº“ git remote remove [åˆ«å] æœ¬åœ°åº“å†…å®¹æŽ¨é€åˆ°è¿œç¨‹åº“å‰æï¼šæœ¬åœ°åº“å·²æ·»åŠ è¿œç¨‹åº“åœ°å€åˆ«åã€‚ åœ¨æœ¬åœ°å°†æœ¬åœ°åº“æŽ¨é€åˆ°è¿œç¨‹åˆ†æ”¯ git push [åˆ«å] [åˆ†æ”¯å] git push origin master å°†è¿œç¨‹åº“å…‹éš†åˆ°æœ¬åœ°åº“ git clone https/ssh_address æ•ˆæžœï¼šå®Œæ•´æŠŠè¿œç¨‹åº“ä¸‹è½½åˆ°æœ¬åœ°ï¼›æ·»åŠ originä½œä¸ºè¿œç¨‹åº“åœ°å€åˆ«åï¼›åˆå§‹åŒ–æœ¬åœ°åº“ï¼ˆå«æœ‰.gitæ–‡ä»¶ï¼‰ å›¢é˜Ÿå†…åä½œå›¢é˜Ÿæˆå‘˜é‚€è¯·é¡¹ç›®åˆ›å»ºè€…åœ¨é¡¹ç›®â€Settingâ€-â€œCallaboratorsâ€é‡Œé‚€è¯·æˆå‘˜ã€‚ æ‹‰å–ï¼šåŒæ­¥æœ¬åœ°åº“ åœ¨æœ¬åœ°pullæ“ä½œåŒæ­¥æœ¬åœ°åº“ä¸Žè¿œç¨‹åº“ç›¸åŒã€‚ fetchï¼šæŸ¥çœ‹è¿œç¨‹åº“åˆ†æ”¯ï¼Œå¯ä»¥åˆ‡æ¢è‡³è¿œç¨‹åº“åˆ†æ”¯ï¼ŒæŸ¥çœ‹è¿œç¨‹åº“åˆ†æ”¯çš„æ–‡ä»¶å…·ä½“å†…å®¹ï¼Œå†³å®šæ˜¯å¦åˆå¹¶ã€‚ git fetch [è¿œç¨‹åº“åœ°å€åˆ«å] [è¿œç¨‹åˆ†æ”¯å] åˆ‡æ¢è‡³è¿œç¨‹åº“åˆ†æ”¯ git checkout orgin/master mergeï¼šï¼ˆåˆ‡æ¢è‡³æœ¬åœ°åº“masteråˆ†æ”¯ï¼‰ï¼Œåˆå¹¶è¿œç¨‹åº“åˆ†æ”¯ã€‚ git merge [è¿œç¨‹åº“åœ°å€åˆ«å]/[è¿œç¨‹åˆ†æ”¯å] pull = fetch + merge git pull [è¿œç¨‹åº“åœ°å€åˆ«å] [è¿œç¨‹åˆ†æ”¯å] æ³¨ï¼šå¦‚æžœæ˜¯ç®€å•çš„ä¿®æ”¹ï¼Œå¯ä»¥ç›´æŽ¥pullæ‹‰å–ï¼Œå¦‚æžœä¸ç¡®å®šè¿œç¨‹åº“ä¿®æ”¹å†…å®¹ï¼Œå¯ä»¥å…ˆfetchåŽå†åˆå¹¶åˆ†æ”¯ã€‚ æœ¬åœ°æ‹‰å–ä¸Žè¿œç¨‹åº“å†²çª å†²çªå‘ç”ŸåŽŸå› ï¼šä¸æ˜¯åŸºäºŽGitHubè¿œç¨‹åº“çš„æœ€æ–°ç‰ˆè¿›è¡Œä¿®æ”¹ï¼Œå°±ä¸èƒ½pushï¼Œåœ¨ä¿®æ”¹ä¹‹å‰å¿…é¡»pullã€‚ pullæ‹‰å–ä¸‹æ¥åŽå¦‚æžœè¿›å…¥å†²çªçŠ¶æ€ï¼Œå°±æŒ‰ç…§â€œåˆ†æ”¯å†²çªè§£å†³åŠžæ³•â€ è·¨å›¢é˜Ÿåä½œ forkæ“ä½œï¼šå¤åˆ¶ä¸€ä»½è¿œç¨‹åº“ã€‚ å›¢é˜Ÿå¤–çš„äººï¼Œåœ¨é¡¹ç›®èŠ‚ç›®ç‚¹forkï¼Œå³å¯forkä¸€ä»½è¿œç¨‹åº“ï¼Œè¯¥è¿œç¨‹åº“çš„æ¥æºæ˜¯åˆ›å»ºè¯¥åº“çš„å¼€å‘è€…ï¼Œè€Œforkå‡ºçš„è¿œç¨‹åº“çš„æ‰€æœ‰è€…æ˜¯æ‰§è¡Œforkæ“ä½œçš„äººã€‚ cloneæ“ä½œï¼šä¸‹è½½åˆ°æœ¬åœ°åº“ã€‚ pushæ“ä½œï¼šæœ¬åœ°ä¿®æ”¹ï¼ŒæŽ¨é€è‡³è¿œç¨‹åº“ã€‚ pull request è¯·æ±‚ï¼šåœ¨è¿œç¨‹åº“ï¼ˆä»£ç æ‰˜ç®¡ä¸­å¿ƒGitHubï¼‰æ‰§è¡Œpull requestè¯·æ±‚ï¼Œè¯·æ±‚åˆå¹¶è¯¥ä¿®æ”¹åˆ°åŽŸè¿œç¨‹åº“ã€‚ ï¼ˆåŽŸè¿œç¨‹åº“æ‰€æœ‰è€…ï¼‰å®¡æ ¸æ“ä½œï¼šç¡®è®¤æ˜¯å¦åˆå¹¶ã€‚ SSHç™»é™† åœ¨å½“å‰ç”¨æˆ·çš„æ ¹ç›®å½•ï¼Œç”Ÿäº§.sshå¯†é’¥ç›®å½• ssh-keygen -t rsa -C email@address å°†.ssh/id_rsa.pub æ–‡ä»¶çš„å†…å®¹å¤åˆ¶åˆ°GitHubæ–°å»ºsshå¯†é’¥çš„çª—å£ä¸‹ã€‚ åˆ›å»ºsshè¿œç¨‹åœ°å€åˆ«å git remote add origin ssh_address Gitä»“åº“å’ŒSSH-keyå…³è” ssh-add &quot;id_rsa address Gitå·¥ä½œæµå¾…è¡¥å……[1] GitlabæœåŠ¡å™¨æ­å»ºå¾…è¡¥å……[2] Reference Gitå·¥ä½œæµå¾…è¡¥å…… GitlabæœåŠ¡å™¨æ­å»º","link":"/2020/07/17/Git-and-GitHub/"},{"title":"ã€ŒCryptography-ZKPã€: Lec6 Poly-commit based on Pairing and Discret-log","text":"In this series, I will learn Zero Knowledge Proofs (ZKP) on this MOOC, lectured by Dan Boneh, Shafi Goldwasser, Dawn Song, Justin Thaler and Yupeng Zhang. Any corrections and advice are welcome. ^ - ^ Topics: KZG poly-commit based on bilinear pairing KZG scheme Powers-of-tau Ceremony Security Analysis Knowledge of exponent assumption Variants: multivariate; ZK; batch openings Bulletproofs poly-commit based on discrete logarithm Before proceeding to todayâ€™s topic, letâ€™s recall the common recipe for building an efficient SNARK. The common way of building SNARK is to combine a poly-IOP with a functional commitment scheme. Lecture 4 uses Plonk (poly-IOP) combined with KZG (a univariate polynomial commitment) to build SNARK for general circuits. Lecture 3 uses Sumcheck protocol combined with a multivariate polynomial commitment to build SNARK. In this lecture, we are going to introduce polynomial commitments based on bilinear pairing and discrete log. When building polynomial commitment schemes, we first choose a family of polynomial $\\mathcal{F}$, then prover commits to a function $f(x)\\in \\mathcal{F}$. Verifier receives $\\text{com}_f$ as the commitment, then verifier is able to query $f$ at point $u$. Finally, prover sends the evaluation $v$ and the proof $\\pi$ that $f(u)=v$ and $f\\in \\mathcal{F}$, and verifier accepts if proof is valid. The above procedure is depicted as follows. For ease of use, we give a formal definition for polynomial commitment schemes (PCS). It consists of four algorithms as follows. $\\text{keygen}(\\lambda, \\mathcal{F})\\rightarrow gp$In setup, this algorithm takes the family as inputs and outputs the global parameters used in the commitment and proof. $\\text{commit}(gp,f)\\rightarrow \\text{com}_f$Prover calls this algorithm to commit to a function. $\\text{eval}(gp,f,u)\\rightarrow v,\\pi$Prover calls this algorithm to compute the evaluation at the point $u$ and the corresponding proof. $\\text{verify}(gp,\\text{com}_f,u,v,\\pi)\\rightarrow \\text{accept or reject}$Verifier calls this algorithm to check the validity of the proof and accept the answer if valid. It is complete if an honest prover can convince the verifier to accept the answer. It is sound if a verifier can catch a lying prover with high probability. We compare the soundness and knowledge soundness in Lecture 3. To put it simply, soundness establishes the existence of the witness while knowledge soundness establishes that the prover necessarily knows the witness. As a result, knowledge soundness is stronger. We give a formal definition of knowledge soundness. Knowledge Sound: For every poly. time adversary $A(A_0, A_1)$ such that $$ \\text{keygen}(\\lambda,\\mathcal{F})\\rightarrow gp, A_0(gp)\\rightarrow \\text{com}_f, A_1(gp, u)\\rightarrow v, \\pi: \\\\ \\operatorname{Pr}[V(vp, x,\\pi)=\\text{accept}]=1 $$ there is an efficient extractor $E$ (that uses $A$) such that $$ \\text{keygen}(\\lambda,\\mathcal{F})\\rightarrow gp, A_0(gp)\\rightarrow \\text{com}_f, E(gp, \\text{com}_f)\\rightarrow f:\\\\ \\operatorname{Pr}[f(u)=v \\text{ and } f(x)\\in \\mathcal{F}]> 1-\\epsilon $$ where $\\epsilon$ is negligible. BackgroundLetâ€™s quickly go through the basic conceptions in number theory, which is widely used in the following sections. I refer readers to This Blog for a detailed description. Group: A set $\\mathbb{G}$ and an operation $*$ Closure: For all $a,b\\in \\mathbb{G}$, $a* b \\in \\mathbb{G}$ Associativity: For all $a,b,c\\in \\mathbb{G}$, $(a*b)*c =a*(b*c)$ Identity: There exists a unique element $e\\in \\mathbb{G}$ s.t. for every $a\\in \\mathbb{G}$, $e*a=a*e=a$. Inverse: For each $a\\in \\mathbb{G}$, there exists $b\\in \\mathbb{G}$ s.t. $a*b=b*a=e$. A simple example is the group that contains integers $\\{\\dots, -2,-1,0,1,2,\\dots\\}$ under addition operation $+$. The common Groups considered in Cryptography are the group that contains positive integers mod prime $p:\\{1,2,\\dots, p-1\\}$ under multiplication operation $\\times$ and the groups defined by elliptic curves. Generator of a group: An element $g$ that generates all elements in the group by taking all powers of $g$. For example, $3$ is a generator of the group $\\mathbb{Z}_7^*={1,2,3,4,5,6}$. We can write every group element in the power of $3$. $3^1=3;3^2=2;3^3=6;3^4=4;3^5=5;3^6=1 \\mod 7$ Discrete-logA group $\\mathbb{G}$ has an alternative representation as the powers of the generator $g:\\{g,g^2, g^3,\\dots,g^{p-1}\\}$. Discrete logarithm problem: Given $y\\in \\mathbb{G}$, find $x$ such that $g^x=y$. The quantum computer can actually solve the discrete logarithm problem in polynomial time. Discrete logarithm assumption: Discrete-log problem is computationally hard. Note that the DL assumption does not hold in all groups but it is believed to hold in certain groups. Computational Diffie-Hellman assumption: Given $\\mathbb{G},g,g^x,g^y$, cannot compute $g^{xy}$. It is worth noting that a stronger assumption means the underlying problem is easier. Hence, the CDH assumption is a stronger assumption than the DL assumption since the CDH problem is reducible to the DL problem. Bilinear PairingThe bilinear pairing is defined over $(p,\\mathbb{G},g,\\mathbb{G}_T,e)$. $\\mathbb{G}$: the base group of order $p$, a multiplicative cyclic group $\\mathbb{G}_T$: the target group of order $p$, a multiplicative cyclic group $g$: the generator of $\\mathbb{G}$ $e:\\mathbb{G}\\times \\mathbb{G} \\rightarrow \\mathbb{G}_T$, the pairing operation The pairing possesses the following bilinear property: $$ \\forall P,Q\\in \\mathbb{G}: e(P^x,Q^y)=e(P,Q)^{xy} $$ The pairing takes two elements in the base group $\\mathbb{G}$ as inputs, and outputs an element of the target group. For example, $e(g^x,g^y)=e(g,g)^{xy}=e(g^{xy},g)$. By the CDH assumption, we know computing $g^{xy}$ is hard given $g^x$ and $g^y$. It means that computing the product in the exponent is hard. But with pairing, we can check that some element $h=g^{xy}$ without knowing $x$ and $y$. Note that with pairing, we cannot break the CDH assumption with pairing. It actually gives us a tool to verify the product relationship in the exponent rather than computing the product in the exponent. A pairing example is the BLS signature proposed by Boneh, Lynn, and Shacham in 2001. [Bonth-Lynn-Shachamâ€™2001] $\\text{Keygen}(p,\\mathbb{G},g, \\mathbb{G}_T,e):$ private key $x$ and public key $g^x$. $\\text{Sign}(sk,m)\\rightarrow \\sigma:H(m)^x$ where $H$ is a cryptographic hash that maps the message space to $\\mathbb{G}$. $\\text{Verify}(\\sigma, m):e(H(m),g^x)=e(\\sigma,g)$ The verification is to check the pairing equation. The LHS is the pairing of the hash of the message and the public key. The verifier cannot compute $H(m)^x$ without knowing $x$. The RHS is the pairing of the signature and the generator. Security Analysis: The correctness holds since the verifier will pass if the signer honestly computes $H(m)^x$. The idea of proving soundness is by contradiction. Assuming there is an adversary that can forge a signature to pass the verification, then we can break CDH assumption using this bilinear group. For your information, not all groups in which DL is hard are believed to support efficient computable pairing, but some groups especially those defined by elliptic curves. KZG based on Bilinear PairingLecture 4 has introduced KZG polynomial commitment scheme [Kate-Zaverucha-Goldbergâ€™2010] with the multiplication notation but omits the details of pairing. In this section, we use the exponent notation and consider a bilinear group defined by $p, \\mathbb{G},g,\\mathbb{G}_T,e$ and the univariate polynomials $\\mathcal{F}=\\mathbb{F}_p^{(\\le d)}[X]$ with degree $\\le d$. Note that the degree $d$ is public to the verifier. KZG schemeLetâ€™s elaborate on the four algorithms one by one. keygen $(\\lambda, \\mathcal{F})\\rightarrow gp$ : compute the global parameters $\\text{keygen}(\\lambda, \\mathcal{F})\\rightarrow gp$ Sample random $\\tau\\in \\mathbb{F}_p$ $gp=(g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})$ delete $\\tau$ !! (trusted setup) Intuitively, it uses a group where DL assumption is hard so that no one can compute $\\tau$. It suffices to use $gp$ to commit and generate the proof for the prover, and to check the pairing equation for the verifier, without knowing the secret $\\tau$. Once the prover learns the secret $\\tau$, the prover can generate fake proof to fool the verifier and break the security of the polynomial commitment scheme. It is worth noting that the secret $\\tau$ should be deleted so that it requires a trusted setup, which is the main drawback of KZG. For some practical applications, it is actually hard to find a trusted party to run a trusted setup. commit $(gp,f)\\rightarrow \\text{com}_f$ $f(x)=f_0+f_1x+f_2x^2+\\dots+f_d x^d$ $gp=(g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})$ Compute the commitment using $gp$ $$ \\begin{aligned}\\text{com}_f&=g^{f(\\tau)} \\\\ &=g^{f_0+f_1\\tau +f_2\\tau^2+\\dots+f_d \\tau^d} \\\\ &=(g)^{f_0} \\cdot (g^{\\tau})^{f_1} \\cdot (g^{\\tau^2})^{f_2}\\dots (g^{\\tau^d})^{f_d} \\end{aligned} $$ â€‹ eval $(gp,f,u)\\rightarrow v,\\pi$ $f(x)-f(u)=(x-u)q(x)$ as $u$ is a root of $f(x)-f(u)$ Compute $q(x)$ and $\\pi=g^{q(\\tau)}$ using $gp$ Note that the proof can be computed without accessing $\\tau$ and the proof size is only one group element. Finally, we are going to introduce the verification part, which is the highlight of KZG. The equation that the verifier wants to check is $f(x)-f(u)=(x-u)q(x)$. A naive idea is to verify the equation at the point $\\tau$ in the exponent on the base $g$. $$ g^{f(\\tau)-f(u)}=g^{(\\tau-u)q(\\tau)} $$ Verify has received $\\text{com}_f=g^{f(\\tau)}$ as commitment to $f$, $\\pi=g^{q(\\tau)}$ as eval proof, and $v=f(u)$ as evaluation from an honest prover. Verifier can compute $g^{(\\tau-u)}$ and $g^{q(\\tau)}$ using $gp$. Unfortunately, under CDH assumption, the verifier cannot compute $g^{(\\tau-u)q(\\tau)}$, which is the product in the exponent. The solution is pairing, which gives us a way to check the relation in the exponent of the equation instead of computing it. verify $(gp,\\text{com}_f,u,v,\\pi)$ Idea: check the equation at point $\\tau$ Challenge: only know $g^{\\tau-u}$ and $g^{q(\\tau)}$ Solution: pairing! Pairing! $$ \\begin{aligned}e(\\text{com}_f/g^v,g)&=e(g^{\\tau-u},\\pi) \\\\ e(g,g)^{f(\\tau)-f(u)}&=e(g,g)^{(\\tau-u)q(\\tau)}\\end{aligned} $$ â€‹ With pairing, the verifier can check the equation at the point $u$ in the exponent. The complete protocol is depicted as follows. Properties of KZGLetâ€™s sum up the properties of KZG poly-commit. Properties of KZG: Keygen: trusted setup! Commit: $O(d)$ group exponentiations, $O(1)$ commitment size. Eval: $O(d)$ group exponentiations where $q(x)$ can be computed efficiently in linear time!Note: The polynomial division algorithm with nearly linear time is referred to this Lecture. Proof size: $O(1)$, 1 group element. Verifier time: $O(1)$, 1 pairing. Powers-of-tau CeremonyThe main drawback of KZG is the requirement of a trusted setup. A way to relax the trusted setup is Ceremony which uses a distributed generation of $gp$ so that no one can reconstruct the trapdoor if at least one of the participants is honest and discards their secrets. The main idea of distributed generation is using the product of secrets from all parties. The first party generates global parameters $gp=(g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})=(g_1,g_2,\\dots, g_d)$. Then the next party samples random $s$, and update $$ \\begin{aligned}gp' &=(g_1',g_2',\\dots, g_d') \\\\ &=(g_1^s,g_2^{s^2},\\dots, g_d^{s^d}) \\\\ &= (g^{\\tau s},g^{(\\tau s)^2},\\dots, g^{(\\tau s)^d})\\end{aligned} $$ with secret $\\tau\\cdot s$. It introduces a secret $s$ from updating /ma Finally, if all parties are honest, then the above procedure can generate the global parameters with the product of secrets from all parties. Meanwhile, it is required to check the correctness of $gpâ€™$. Correctness of : (See [Nikolaenko-Ragsdale-Bonneau-Bonehâ€™22]) The contributor knows $s$ s.t. $g_1â€™=(g_1)^s$.It can be verified by the $\\Sigma$ protocol. $gpâ€™$ consists of consecutive powers $e(g_iâ€™,g_1â€™)=e(gâ€™_{i+1},g)$, and $g_1â€™\\ne 1$. Note that the check of $g_1â€™\\ne 1$ guarantees that the next party cannot remove the product of the preceding secrets and change it to 0. Security AnalysisThe completeness of KZG is evident. The soundness of KZG is based on the following assumption. $q$-Strong Bilinear Diffie-Hellman ($q$-SBDH) assumption: Given $(p,\\mathbb{G},g,\\mathbb{G}_T,e)$ and $(g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})$, cannot compute $e(g,g)^{\\frac{1}{\\tau-u}}$ for any $u$. There is an exposition [Tanaka-Saito] of reductions among the $q$-strong Diffie-Hellman problem and related problems. The $q$-SBDH problem is reducible to the CDH problem, so $q$-SBDH is a strictly stronger assumption. Proof of Soundness: (By Contradiction) Assume for contradiction that an adversary returns a wrong answer $v^*\\ne f(u)$ but the fake proof $\\pi^*$ pass the verification. Then we can break $q$-SBDH assumption, which arrives to a contradiction. $e(\\text{com}_f/g^{v^*})=e(g^{\\tau-u},\\pi^*)$ The pairing equation of verification holds by assumption. $e(g^{f(\\tau)-v^{*}},g)=e(g^{\\tau-u},\\pi^*)$ Knowledge of Exponent (KoE) assumption Later we are going to introduce the KoE assumption, which proves that the prover necessarily knows $f$ s.t. $\\text{com}_f=g^{f(\\tau)}$ rather than a random element. Because a random element as the commitment cannot be written in $g^{f(\\tau)}$ for some $f$. By KoE assumption, it means the prover necessarily knows an explicit $f$ so prover can compute $f(u)$. Define $\\delta=f(u)-v^*$, which is $\\ne 0$ by assumption. This is the key idea of the proof that decomposes $v^*$ to the correct answer $f(u)$ and $\\delta$. $\\Leftrightarrow e(g^{\\color{red}{f(\\tau)-f(u)+f(u)-v^*}},g)=e(g^{\\tau-u},\\pi ^*)$ $\\Leftrightarrow e(g^{\\color{red}{(\\tau-u)q(\\tau)+\\delta}},g)=e(g^{\\tau-u},\\pi ^*)$ $\\Leftrightarrow e(g,g)^{(\\tau-u)q(\\tau)+\\delta}=e(g,\\pi ^*)^{\\tau-u}$ Then we can extract the common factor $\\tau-u$ and put them outside the pairing to achieve our goal of computing$e(g,g)^{\\frac{1}{\\tau-u}}$. $\\Leftrightarrow e(g,g)^{\\delta}=(e(g,\\pi ^*)/e(g,g)^{q(\\tau)})^{\\tau-u}$ $\\Leftrightarrow e(g,g)^{\\frac{\\delta}{\\tau -u}}=e(g,\\pi^*)/e(g^{q(\\tau)},g)$ which breaks $q$-SBDH assumption. Then we are going to prove knowledge soundness by the knowledge of exponent assumption. Knowledge of Exponent assumptionIn the above security proof, we assume that the prover necessarily knows $f$ such that $\\text{com}_f=g^{f(\\tau)}$ rather than a random element. We make use of the Knowledge of Exponent (KoE) assumption to refine KZG protocol, achieving knowledge soundness. I excerpt the following descriptions from [Bellare-Palacioâ€™04] to intuitively introduce the knowledge of exponent assumption. Let $q$ be a prime such that $2q+1$ is also prime (safe prime), and let $g$ be a generator of the order $q$ subgroup of $Z_{2q+1}^*$. Suppose we are given input $q,g,g^a$ and want to output a pair $(C,Y)$ such that $Y=C^a$. One way to do this is to pick some $c\\in \\mathbb{Z}_q$, let $C=g^c$, and let $Y=(g^a)^c$. Intuitively, it can be viewed as saying that this is the â€œonlyâ€ way to produce such a pair. The assumption captures this by saying that any adversary outputting such a pair must â€œknowâ€ an exponent $c$ such that $g^c=C$. The formalization asks that there be an â€œextractorâ€ that can return $c$. Knowledge of Exponent assumption : For any adversary $A$ that takes input $q,g,g^a$ and returns $(C,Y)$ with $Y=C^a$, there exists an â€œextractorâ€ $\\bar{A}$, which given the same inputs as $A$ returns $c$ such that $g^c=C$. KZG with Knowledge SoundnessHaving this assumption, we can refine the KZG protocol. The goal is to prove that prover necessarily â€œknowsâ€ an exponent $f(\\tau)$ such that $\\text{com}_f=g^{f(\\tau)}$. Weâ€™d like to ask the prover to generate such a pair $g^{f(\\tau)}$ and $g^{\\alpha f(\\tau)}$ given â€œ $gp$ and $(gp)^{\\alpha}$â€. The sketch of design is as follows. Sketch: $gp=(g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d})$ Sample random $\\alpha$, compute $g^\\alpha,g^{\\alpha \\tau},g^{\\alpha \\tau^2},\\dots, g^{\\alpha \\tau^d}$ $\\text{com}_f=g^{f(\\tau)}$ and $\\text{com}â€™_f=g^{\\alpha f(\\tau)}$. If $e(\\text{com}_f,g^\\alpha)=e(\\text{com}_fâ€™,g)$, there exists an extractor $E$ that extracts $f$ s.t. $\\text{com}_f=g^{f(\\tau)}$. Letâ€™s elaborate on the details. In addition to the original global parameters $gp$, we need to sample random $\\alpha$ and compute $(gp)^\\alpha:\\{g^\\alpha, g^{\\alpha \\tau}, g^{\\alpha \\tau^2},\\dots, g^{\\alpha\\tau^d}\\}$, which is raising each element of the original $gp$ to random $\\alpha$. Note that the random $\\alpha$ is secret as $\\tau$. The prover commits to $f$ by computing such a pair, $\\text{com}_f=g^{f(\\tau)}$ and $\\text{com}_fâ€™=g^{\\alpha f(\\tau)}$. Finally, the verifier can check the relation of these two commitments in the exponent by pairing. If the pairing equation $e(\\text{com}_f,g^\\alpha)=e(\\text{com}_fâ€™,g)$ holds, by KoE assumption, there exists an extractor $E$ that extracts $f$ such that $\\text{com}_f=g^{f(\\tau)}$. Here is the KZG scheme with knowledge soundness. KZG with Knowledge Soundness: $\\text{Keygen}$: $gp$ includes both $g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d}$ and $g^\\alpha,g^{\\alpha\\tau},g^{\\alpha\\tau^2},\\dots, g^{\\alpha\\tau^d}$ where $\\tau$ and $r$ are both secret and required to be deleted. (trusted setup) $\\text{Commit}$: $\\text{com}_f=g^{f(\\tau)}$ and $\\text{com}_fâ€™=g^{\\alpha f(\\tau)}$. $\\text{Verify}$: additionally checks $e(\\text{com}_f,g^\\alpha)=e(\\text{com}_fâ€™,g)$. The idea of proving knowledge soundness is to extract $f$ in the first step by the KoE assumption. But it brings an overhead that the prove size is two group elements and the verifier time involves two pairing equations. Generic Group Model (GGM) [Shoupâ€™97, Maurerâ€™05] can replace the KoE assumption and reduce the commitment size in KZG. Informally speaking, the adversary is only given an oracle to compute the group operation. E.g., given $g,g^\\tau,g^{\\tau^2},\\dots, g^{\\tau^d}$, the adversary can only compute their linear combinations. As a result, the adversary cannot sample a random element that happens to be the power of another group element. See book â€œA Graduate Course in Applied Cryptographyâ€ by Dan Boneh and Victor Shoup, section 16.3 for more details. Variants of KZGMultivariate poly-commit Reference: [Papamanthou-Shi-Tamassiaâ€™13] KZG can be generalized to the multivariate polynomial commitment. The key idea is the following equation: $$ f(x_1,\\dots, x_k)-f(u_1,\\dots,u_k)=\\sum_{i=1}^k (x_i-u_i) q_i(\\vec{x}) $$ where $q(\\vec{x})$ is a multivariate polynomial. Keygen: sample $\\tau_1,\\tau_2,\\dots, \\tau_k$, each representing one variable, and compute $gp$ as $g$ raised to all possible monomials of $\\tau_1,\\tau_2,\\dots, \\tau_k$.e.g. $2^k$ monomials for multilinear polynomials. Commit: $\\text{com}=g^{f(\\tau_1,\\tau_2,\\dots, \\tau_k)}$ Eval: compute $\\pi_i=g^{q_i(\\vec{\\tau})}$.Note that the proof consists of multiple elements. Verify: $e(\\text{com}_f/g^v,g)=\\prod_{i=1}^ke(g^{\\tau_i-u_i},\\pi_i)$ Let $N\\le 2^k$ denote the total size of the polynomial. The proof size and verifier time are $O(\\log N)$. Achieving zero-knowledge Reference: [ZGKPPâ€™2018] We say a poly-commit scheme is ZK if there is a simulator without knowing the polynomial can simulate the view of the verifier. The plain KZG is not ZK. E.g. the commit algorithm $\\text{com}_f=g^{f(\\tau)}$ is deterministic. The solution to achieve zero-knowledge is masking the commitment and proof with randomizers. Commit: $\\text{com}_f=g^{f(\\tau)+r\\eta}$ Eval: the main idea is to check whether a polynomial with randomizers is of a certain form. bivariate polynomial with randomizer: $\\begin{aligned} f(x)+ry-f(u) =(x-u)(q(x)+râ€™y)+y(r-râ€™(x-u))\\end{aligned}$ proof: $\\pi=g^{q(\\tau)+râ€™\\eta},g^{r-râ€™(\\tau-u)}$ Letâ€™s elaborate on the details. First look at the commitment $\\text{com}_f=g^{f(\\tau)+r\\eta}$. The commitment to $f$ is randomized by $r$ randomly chosen by the prover. Note that $\\eta$ is another secret sampled in the trusted setup so $g^\\eta$ is included in the global parameters, which enables the prover to compute it. With random $r$ in the commitment, the idea of evaluation is to check the randomized bivariate polynomial is of a certain form: $$\\begin{aligned} f(x)+ry-f(u) =(x-u)(q(x)+râ€™y)+y(r-râ€™(x-u))\\end{aligned}$$ Likewise the check $f(x)-f(u)=(x-u)q(x)$ in the univariate polynomial, we can check the above relation in the exponent with pairing yet it is split into two terms of products. Consequently, the proof consists of two elements, the first evaluating $q(x)+râ€™y$ and the second evaluating $r-râ€™(x-u)$ at point $(x=\\tau,y=\\eta)$. Note that the verifier can compute $g^{\\tau-u}$ and $g^\\eta$ using $gp$. Finally, the verifier can check the relation in the pairing equation. Batch opening: single polynomialsAnother variant of KZG is batch opening or batch proofs. Letâ€™s consider the batch proofs for a single polynomial in which the prover wants to prove $f$ at $u_1,\\dots, u_m$ for $m&lt;d$. Note that $m&lt;d$ is necessary since $m (&gt;d)$ points can interpolate the polynomial in clear. The key idea is to extrapolate $f(u_1),\\dots, f(u_m)$ to get $h(x)$ such that $h(u_i)=f(u_i)$. Recall that in Lecture 5 we introduce a vanishing polynomial in ZeroTest on the set $\\Omega$. Itâ€™s actually the same that we can prove $f(x)-h(x)=0$ on the set ${u_1,\\dots, u_m}$ using ZeroTest: $$f(x)-h(x)=\\prod_{i=1}^m (x-u_i) q(x)$$ $f(x)-h(x)$ is zero over the set ${u_1,\\dots, u_m}$ if and only if it is divisible by the vanishing polynomial $\\prod_{i=1}^m (x-u_i)$. The prover needs to compute the quotient polynomial $q(x)$ and generates the proof $\\pi =g^{q(\\tau)}$, a single group element as the batch proofs. The verifier checks the pairing equation $$ e(\\text{com}_f/g^{h(\\tau)},g)=e(g^{\\prod_{i=1}^m(\\tau-u_i)},\\pi) $$ where $g^{h(\\tau)}$ and $g^{\\prod_{i=1}^m(\\tau-u_i)}$ can be computed using $gp$. Note that the proof size is only one group element but the verifier time grows linearly in the number of evaluations. Batch opening: multiple polynomialsThen we extend batch opening for multiple polynomials (and multiple evaluations) where the prover wants to prove $f_i(u_{i,j})=v_{i,j}$ for $i\\in [n]$, $j\\in [m]$. The key idea kind of similar to the single polynomial case that extrapolates multiple polynomials. Specifically, the extrapolates $f_i(u_1),\\dots, f_i(u_m)$ to get $h_i(x)$ for each $i\\in [n]$. For each polynomial, we have $f_i(x)-h_i(x)=\\prod_{i=1}^m (x-u_m)q_i(x)$. The prover needs to compute every quotient polynomial $q_i(x)$ combine them via a random linear combination. Then prover can compute the proof as $g$ to the equation of the random linear combination, which is a single element. The verifier can check the relation in the exponent using bilinear pairing. KZG and its variants play an important role in building SNARKs. Plonk poly-IOP is combined with the univariate version of KZG to build SNARK for general circuits. vSQL and Libra both combine the Sumcheck protocol (Lecture 4) and the multivariate KZG. Before ending up discussing the poly-commit scheme based on the bilinear pairing, letâ€™s sum up the pros and cons of KZG poly-commit. The pros contains that the commitment and proof size is $O(1)$, 1 group element and the verifier time involves $O(1)$ pairing. The main cons is that KZG requires a trusted setup to generate $gp$. The trusted setup is a fundamental problem to solve although the ceremony process relaxes trust a little bit and it is good for many applications in practice. Bulletproofs based on discrete-logBulletproofs is proposed by [BCCGPâ€™16] and refined by [BBBPWMâ€™18] to build SNARKs using a transparent setup. Moreover, they proposed an inner product protocol and a special protocol for range proof that can be generalized to build SNARKs for a general arithmetic circuit. Poly-commit based on BulletproofsIn this section, we rephrase the Bulletproofs as a poly-commit because it still shows the key idea of the reduction but significantly simplifies the protocol. The transparent setup samples random $gp=(g_0,g_1,g_2,\\dots, g_d)\\in \\mathbb{G}$ without the trapdoor $\\tau$ whose size is still linear to the degree $d$. The prover commits to $f(x)=f_0+f_1x+f_2x^2+\\dots +f_dx^d$ as usual, which raises each element of $gp$ to the corresponding coefficients of the polynomials and multiply them together to get a single element. $$\\text{com}_f=g_0^{f_0}g_1^{f_1}g_2^{f_2}\\dots g_d^{f_d}$$ Note that the random term is omitted. Letâ€™s describe the high-level idea of Bulletproofs reduction using an example of a degree-3 polynomial. After receiving the commitment to $f$ from the prover, verifier queries at $u$. Prover replies with the evaluation $v$. The key idea is to reduce the claim of evaluating $v$ at point $u$ for the polynomial $f$ inside the commitment $\\text{com}_f$ to a new claim about a new polynomial of only half of the size. In our example, we reduce the original polynomial of degree 3 to a new polynomial of degree only 1 with only two coefficients $f_0â€™$ and $f_1â€™$. Furthermore, the verifier will receive a new instance of the commitment $\\text{com}_{fâ€™}$ to this new polynomial of only half of the size. Then we keep doing recursively to reduce the polynomial of degree $d/2$ to a new polynomial of degree $d/4,d/8,\\dots,$ to a constant degree. Finally, in the last round, the prover can just send a polynomial of constant size to the verifier directly, and the verifier opens the polynomial and checks the evaluation of the last round is indeed true. It completes the entire reduction and guarantees that the claim of the evaluation of $v$ for the original polynomial is correct. The main challenge of the reduction is how to go from the original polynomial to a new polynomial of half of the size. We canâ€™t have the prover commit to a random polynomial of half of the size without any relationship. Otherwise, the prover can cheat and lie about the evaluation since this new polynomial has no relation to the original polynomial. It has to check the relationship between the two polynomials. Letâ€™s elaborate on the details of the reduction. Prover first sends the evaluation $v=f_0+f_1u+f_2u^2+f_3u^3$ at point $u$. A common way of reduction for the prover is reducing the polynomial to two polynomials of half of size. Then prover evaluates these two reduced polynomials at point $u$ to get $v_L=f_0+f_1u$ and $v_R=f_2+f_3u$ such that $v=v_L+v_Ru^2$. It is safe for the verifier to believe that $v$ is the correct evaluation if and only if $v_L$ and $v_R$ are correctly evaluated for the reduced polynomials. So prover also commits to the two reduced polynomials with two cross terms $L=g_2^{f_0}g_3^{f_1}$ and $R=g_0^{f_2}g_1^{f_3}$, which uses $g_2,g_3$ as bases to commit to the left reduced polynomial and $g_0,g_1$ to commit to the right reduced polynomial. As depicted follows, prover sends two commitments $L$ and $R$ and two evaluations $v_L$ and $v_R$ on the two reduced polynomials. But these two polynomials are actually temporary reduced polynomials. The actual reduced polynomial is a single polynomial with two coefficients $rf_0+f_2$ and $rf_1+f_3$, which is a randomized linear combination of the original coefficients where the randomness $r$ is sampled by the verifier. This new claim about this new reduced polynomial actually combines two claims about the old temporary polynomials through randomized linear combinations. And the claim about the evaluation in the next round is to altered to $vâ€™=rv_L+v_R$. The only remaining challenge is to generate the commitment of this randomized reduced polynomial. We canâ€™t let the prover commit $g_0^{rf_0+f_2}g_1^{rf_1+f_3}$ with the original global parameters because the transparent setup doesnâ€™t know the relationship ( defined by $r$ ) between the two polynomials. In order to address the issue, Bulletproofs proposed a clever design to allow the verifier to compute the new commitment from the old commitments with the help of the commitments to the temporary polynomials. Recall that $\\text{com}_f=g_0^{f_0}g_1^{f_1}g_2^{f_2}\\dots g_d^{f_d}$, $L=g_2^{f_0}g_3^{f_1}$ and $R=g_0^{f_2}g_1^{f_3}$. Then verifier can compute the commitment $\\text{com}â€™$ from $L$ and $R$ such that $$ \\begin{aligned} \\text{com}' &=L^r\\cdot \\text{com}_f \\cdot R^{r^{-1}} \\\\ &=g_0^{f_0+r^{-1}f_2}g_2^{rf_0+f_2}\\cdot g_1^{f_1+r^{-1}f_3}g_3^{rf_1+f_3} \\\\ &= (g_0^{r^{-1}}g_2)^{rf_0+f_2} \\cdot (g_1^{r^{-1}}g_3)^{rf_1+f_3}\\end{aligned} $$ where the global parameter is updated to $gpâ€™=(g_0^{r^{-1}}g_2,g_1^{r^{-1}}g_3)$. The last equation holds by extracting the common factor to commit to the new polynomial with coefficients $rf_0+f_2$ and $rf_1+f_3$. And the verifier can compute the new global parameters related to the new commitment, which allows the verifier to check some pairing equations. Letâ€™s sum up the reduction procedure. Poly-commitment based on Bulletproofs: Recurse $\\log d$ rounds: Eval: (Prover) Compute $L,R,v_L,v_R$ Receive $r$ from the verifier, reduce $f$ to $fâ€™$ of degree $d/2$ Update the bases $gpâ€™$ Verify: (Verifier) Check $v=v_L+v_Ru^{d/2}$ Generate $r$ randomly Update $\\text{com}â€™=L^r\\cdot \\text{com}_f\\cdot R^{r^{-1}}$, $gpâ€™$, and $vâ€™=rv_L+v_R$ In the last round: The prover sends the constant-size polynomial to the verifier. The verifier checks the commitment and the evaluation is correct. Note that the above protocol can be rendered non-interactive via Fiat Shamir. Properties of BulletproofsLetâ€™s sum up the properties of poly-commitment based on Bulletproofs. Properties of Bulletproofs: Keygen: $O(d)$, transparent setup! Commit: $O(d)$ group exponentiations, $O(1)$ commitment size. Eval: $O(d)$ group exponentiations Proof size: $O(\\log d)$In each round, the prover sends 4 elements as proof. Verifier time: $O(d)$The verifier has to recursively update the global parameters the number of which falls geometrically so the verifier time depends on the first round that is nearly linear in $d$. Other worksHyrax Reference: [Wahby-Tzialla-shelat-Thaler-Walfishâ€™18] The main drawback of Bulletproofs is the linear verifier time. Hyrax improves the verifier time to $O(\\sqrt{d})$ by representing the coefficients as a 2-D matrix with proof size $O(\\sqrt{d})$. In fact, the product of verifier time and the proof size is linear in $d$ so the proof size can be reduced to $\\sqrt[n]{d}$ while the verifier time is $d^{1-1/n}$. Dory Reference: [Leeâ€™2021] Dory improves the verifier time to $O(\\log d)$ without any asymptotic overhead on other parts. Itâ€™s a nice improvement over the poly-commitment based on Bulletproofs. The key idea is delegating the structured verifier computation to the prover using inner pairing product arguments. [BMMTVâ€™2021] It also improves the prover time to $O(\\sqrt{d})$ exponentiations plus $O(d)$ field operations. Dark Reference: [BÃ¼nz-Fisch-Szepieniecâ€™20] Dark achieves $O(\\log d)$ proof size and verifier time based on the cryptographic primitive of group of unknown order. Letâ€™s end up with a summary of all works mentioned in this lecture.","link":"/2023/07/25/zkp-lec6/"}],"tags":[{"name":"MPC","slug":"MPC","link":"/tags/MPC/"},{"name":"OT","slug":"OT","link":"/tags/OT/"},{"name":"GMW","slug":"GMW","link":"/tags/GMW/"},{"name":"ABY2.0","slug":"ABY2-0","link":"/tags/ABY2-0/"},{"name":"BeaverTriples","slug":"BeaverTriples","link":"/tags/BeaverTriples/"},{"name":"Machine-Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Gradient-Descent","slug":"Gradient-Descent","link":"/tags/Gradient-Descent/"},{"name":"æœºå™¨å­¦ä¹ ","slug":"æœºå™¨å­¦ä¹ ","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Backpropagation","slug":"Backpropagation","link":"/tags/Backpropagation/"},{"name":"open-classes","slug":"open-classes","link":"/tags/open-classes/"},{"name":"å…¬å¼€è¯¾","slug":"å…¬å¼€è¯¾","link":"/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"Tools","slug":"Tools","link":"/tags/Tools/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Gradient","slug":"Gradient","link":"/tags/Gradient/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"Algorithms","slug":"Algorithms","link":"/tags/Algorithms/"},{"name":"String","slug":"String","link":"/tags/String/"},{"name":"Data-Structure","slug":"Data-Structure","link":"/tags/Data-Structure/"},{"name":"Logistic Regression","slug":"Logistic-Regression","link":"/tags/Logistic-Regression/"},{"name":"Softmax","slug":"Softmax","link":"/tags/Softmax/"},{"name":"Math","slug":"Math","link":"/tags/Math/"},{"name":"Lectures","slug":"Lectures","link":"/tags/Lectures/"},{"name":"Secure Computation","slug":"Secure-Computation","link":"/tags/Secure-Computation/"},{"name":"Garbled Circuits","slug":"Garbled-Circuits","link":"/tags/Garbled-Circuits/"},{"name":"Oblivious Transfer","slug":"Oblivious-Transfer","link":"/tags/Oblivious-Transfer/"},{"name":"IKNP","slug":"IKNP","link":"/tags/IKNP/"},{"name":"Mersenne Prime","slug":"Mersenne-Prime","link":"/tags/Mersenne-Prime/"},{"name":"Prime","slug":"Prime","link":"/tags/Prime/"},{"name":"Array","slug":"Array","link":"/tags/Array/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Information-Theory","slug":"Information-Theory","link":"/tags/Information-Theory/"},{"name":"Paper","slug":"Paper","link":"/tags/Paper/"},{"name":"fuzz","slug":"fuzz","link":"/tags/fuzz/"},{"name":"AFL","slug":"AFL","link":"/tags/AFL/"},{"name":"AFL++","slug":"AFL","link":"/tags/AFL/"},{"name":"MOpt","slug":"MOpt","link":"/tags/MOpt/"},{"name":"RedQueen","slug":"RedQueen","link":"/tags/RedQueen/"},{"name":"AFLFast","slug":"AFLFast","link":"/tags/AFLFast/"},{"name":"AFLSmart","slug":"AFLSmart","link":"/tags/AFLSmart/"},{"name":"LAF-Intel","slug":"LAF-Intel","link":"/tags/LAF-Intel/"},{"name":"error","slug":"error","link":"/tags/error/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"HTML","slug":"HTML","link":"/tags/HTML/"},{"name":"CSS","slug":"CSS","link":"/tags/CSS/"},{"name":"Cryptography","slug":"Cryptography","link":"/tags/Cryptography/"},{"name":"MIT6875","slug":"MIT6875","link":"/tags/MIT6875/"},{"name":"Perfect Secrecy","slug":"Perfect-Secrecy","link":"/tags/Perfect-Secrecy/"},{"name":"Perfect Indistinguishability","slug":"Perfect-Indistinguishability","link":"/tags/Perfect-Indistinguishability/"},{"name":"One-time Pad","slug":"One-time-Pad","link":"/tags/One-time-Pad/"},{"name":"Shannon&#39;s lower bound","slug":"Shannon-s-lower-bound","link":"/tags/Shannon-s-lower-bound/"},{"name":"Digital Signatures","slug":"Digital-Signatures","link":"/tags/Digital-Signatures/"},{"name":"EUF-CMA Security","slug":"EUF-CMA-Security","link":"/tags/EUF-CMA-Security/"},{"name":"Lamport Signature","slug":"Lamport-Signature","link":"/tags/Lamport-Signature/"},{"name":"One-time Signature","slug":"One-time-Signature","link":"/tags/One-time-Signature/"},{"name":"Many-time Signature","slug":"Many-time-Signature","link":"/tags/Many-time-Signature/"},{"name":"Random Oracles","slug":"Random-Oracles","link":"/tags/Random-Oracles/"},{"name":"Hashed RSA","slug":"Hashed-RSA","link":"/tags/Hashed-RSA/"},{"name":"ZK Proof","slug":"ZK-Proof","link":"/tags/ZK-Proof/"},{"name":"Commitment","slug":"Commitment","link":"/tags/Commitment/"},{"name":"ZK","slug":"ZK","link":"/tags/ZK/"},{"name":"PoK","slug":"PoK","link":"/tags/PoK/"},{"name":"NIZK","slug":"NIZK","link":"/tags/NIZK/"},{"name":"3COL","slug":"3COL","link":"/tags/3COL/"},{"name":"Random Oracle Model","slug":"Random-Oracle-Model","link":"/tags/Random-Oracle-Model/"},{"name":"CRS Model","slug":"CRS-Model","link":"/tags/CRS-Model/"},{"name":"QNR","slug":"QNR","link":"/tags/QNR/"},{"name":"3SAT","slug":"3SAT","link":"/tags/3SAT/"},{"name":"Application of NIZK","slug":"Application-of-NIZK","link":"/tags/Application-of-NIZK/"},{"name":"IND-CCA Security","slug":"IND-CCA-Security","link":"/tags/IND-CCA-Security/"},{"name":"CCA-Secure Encryption","slug":"CCA-Secure-Encryption","link":"/tags/CCA-Secure-Encryption/"},{"name":"Computational Indistinguishability","slug":"Computational-Indistinguishability","link":"/tags/Computational-Indistinguishability/"},{"name":"PRG","slug":"PRG","link":"/tags/PRG/"},{"name":"PRF","slug":"PRF","link":"/tags/PRF/"},{"name":"Hybrid Argument","slug":"Hybrid-Argument","link":"/tags/Hybrid-Argument/"},{"name":"GGM PRF","slug":"GGM-PRF","link":"/tags/GGM-PRF/"},{"name":"Public-key Encryption","slug":"Public-key-Encryption","link":"/tags/Public-key-Encryption/"},{"name":"IND-Secure","slug":"IND-Secure","link":"/tags/IND-Secure/"},{"name":"IND-CPA","slug":"IND-CPA","link":"/tags/IND-CPA/"},{"name":"Trapdoor Permutations","slug":"Trapdoor-Permutations","link":"/tags/Trapdoor-Permutations/"},{"name":"Number Theory","slug":"Number-Theory","link":"/tags/Number-Theory/"},{"name":"Multiplicative Group","slug":"Multiplicative-Group","link":"/tags/Multiplicative-Group/"},{"name":"Generators","slug":"Generators","link":"/tags/Generators/"},{"name":"Diffie-Hellman Assumptions","slug":"Diffie-Hellman-Assumptions","link":"/tags/Diffie-Hellman-Assumptions/"},{"name":"OWF","slug":"OWF","link":"/tags/OWF/"},{"name":"OWP","slug":"OWP","link":"/tags/OWP/"},{"name":"HCB","slug":"HCB","link":"/tags/HCB/"},{"name":"GL Theorem","slug":"GL-Theorem","link":"/tags/GL-Theorem/"},{"name":"Quadratic Residue","slug":"Quadratic-Residue","link":"/tags/Quadratic-Residue/"},{"name":"QRA","slug":"QRA","link":"/tags/QRA/"},{"name":"GM Encryption","slug":"GM-Encryption","link":"/tags/GM-Encryption/"},{"name":"DH","slug":"DH","link":"/tags/DH/"},{"name":"El Gamla","slug":"El-Gamla","link":"/tags/El-Gamla/"},{"name":"DEEPLIZARD","slug":"DEEPLIZARD","link":"/tags/DEEPLIZARD/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"LSTM","slug":"LSTM","link":"/tags/LSTM/"},{"name":"Semi-supervised","slug":"Semi-supervised","link":"/tags/Semi-supervised/"},{"name":"blockchain","slug":"blockchain","link":"/tags/blockchain/"},{"name":"solidity","slug":"solidity","link":"/tags/solidity/"},{"name":"Intro-to-Algorithms","slug":"Intro-to-Algorithms","link":"/tags/Intro-to-Algorithms/"},{"name":"Sort","slug":"Sort","link":"/tags/Sort/"},{"name":"BlockCipher","slug":"BlockCipher","link":"/tags/BlockCipher/"},{"name":"StreamCipher","slug":"StreamCipher","link":"/tags/StreamCipher/"},{"name":"ECC","slug":"ECC","link":"/tags/ECC/"},{"name":"Hamming Bound","slug":"Hamming-Bound","link":"/tags/Hamming-Bound/"},{"name":"Linear Code","slug":"Linear-Code","link":"/tags/Linear-Code/"},{"name":"GV Bound","slug":"GV-Bound","link":"/tags/GV-Bound/"},{"name":"q-ary Entropy","slug":"q-ary-Entropy","link":"/tags/q-ary-Entropy/"},{"name":"Integrity","slug":"Integrity","link":"/tags/Integrity/"},{"name":"MAC","slug":"MAC","link":"/tags/MAC/"},{"name":"collision-resistance","slug":"collision-resistance","link":"/tags/collision-resistance/"},{"name":"HMAC","slug":"HMAC","link":"/tags/HMAC/"},{"name":"birthday-paradox","slug":"birthday-paradox","link":"/tags/birthday-paradox/"},{"name":"MD-paradigm","slug":"MD-paradigm","link":"/tags/MD-paradigm/"},{"name":"DM-compression-function","slug":"DM-compression-function","link":"/tags/DM-compression-function/"},{"name":"Singleton Bound","slug":"Singleton-Bound","link":"/tags/Singleton-Bound/"},{"name":"Plotkin Bound","slug":"Plotkin-Bound","link":"/tags/Plotkin-Bound/"},{"name":"RS Code","slug":"RS-Code","link":"/tags/RS-Code/"},{"name":"GRS Code","slug":"GRS-Code","link":"/tags/GRS-Code/"},{"name":"DNN","slug":"DNN","link":"/tags/DNN/"},{"name":"Unsupervised","slug":"Unsupervised","link":"/tags/Unsupervised/"},{"name":"PCA","slug":"PCA","link":"/tags/PCA/"},{"name":"Unsupervised-learning","slug":"Unsupervised-learning","link":"/tags/Unsupervised-learning/"},{"name":"Word Embedding","slug":"Word-Embedding","link":"/tags/Word-Embedding/"},{"name":"VSCode","slug":"VSCode","link":"/tags/VSCode/"},{"name":"ZKP","slug":"ZKP","link":"/tags/ZKP/"},{"name":"IP","slug":"IP","link":"/tags/IP/"},{"name":"SNARKs","slug":"SNARKs","link":"/tags/SNARKs/"},{"name":"Sum-check","slug":"Sum-check","link":"/tags/Sum-check/"},{"name":"Poly-commit","slug":"Poly-commit","link":"/tags/Poly-commit/"},{"name":"Bulletproofs","slug":"Bulletproofs","link":"/tags/Bulletproofs/"},{"name":"Plonk","slug":"Plonk","link":"/tags/Plonk/"},{"name":"KZG","slug":"KZG","link":"/tags/KZG/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"}],"categories":[{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"æœºå™¨å­¦ä¹ -æŽå®æ¯…","slug":"æœºå™¨å­¦ä¹ -æŽå®æ¯…","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"Tools","slug":"Tools","link":"/categories/Tools/"},{"name":"LeetCode","slug":"LeetCode","link":"/categories/LeetCode/"},{"name":"MPC","slug":"MPC","link":"/categories/MPC/"},{"name":"Math","slug":"Math","link":"/categories/Math/"},{"name":"Information-Theory","slug":"Information-Theory","link":"/categories/Information-Theory/"},{"name":"Cryptography-MIT6875","slug":"Cryptography-MIT6875","link":"/categories/Cryptography-MIT6875/"},{"name":"PyTorch","slug":"PyTorch","link":"/categories/PyTorch/"},{"name":"åŒºå—é“¾","slug":"åŒºå—é“¾","link":"/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"ç®—æ³•å¯¼è®º","slug":"ç®—æ³•å¯¼è®º","link":"/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/"},{"name":"Cryptography-Boneh","slug":"Cryptography-Boneh","link":"/categories/Cryptography-Boneh/"},{"name":"Cryptography-ECCs","slug":"Cryptography-ECCs","link":"/categories/Cryptography-ECCs/"},{"name":"Cryptography-ZKP","slug":"Cryptography-ZKP","link":"/categories/Cryptography-ZKP/"}]}